MAUVE is a metric for automatically evaluating the quality of open-ended text generation. Developed by researchers at the University of Washington, Allen Institute for AI, and Stanford University, it was first introduced at NeurIPS 2021, where it received and Outstanding Paper Award.
Unlike earlier metrics such as BLEU or ROUGE, which rely on n-gram overlap between a candidate and a reference, MAUVE measures how close the distribution of generated text is to the distribution of human-written text in a high-dimensional embedding space.

Background
Evaluation of open-ended generation (such as story generation or long-form dialogue) is notoriously difficult. Traditional metrics penalize "creative" but valid deviations from a single reference text. Furthermore, neural language models often suffer from issues like repetitive loops or lack of long-range coherence that n-gram metrics fail to capture.
MAUVE was designed to align more closely with human judgments of "quality" and "diversity" by treating text evalution as a comparison of two probability distributions: the distribution of human-written text (
  
    
      
        P
      
    
    {\displaystyle P}
  
) versus the distribution of machine-generate text (
  
    
      
        Q
      
    
    {\displaystyle Q}
  
).

Methodology
The calculation of MAUVE involves three primary steps:

Embedding: large batches of human and machine-generated text are mapped into a vector space using a pre-trained transformer model.
Quantization: the continuous embeddings are clustered into a finite set of 
  
    
      
        k
      
    
    {\displaystyle k}
  
 codewords using k-means clustering to form discrete distributions.
Divergence frontier: the metric calculates the trade-off between Type I and type II errors (precision and recall) between the two distributions using the Kullback-Leibler divergence.

Mathematical definition
MAUVE is based on the area under the divergence frontier. For a mixing parameter 
  
    
      
        λ
        ∈
        (
        0
        ,
        1
        )
      
    
    {\displaystyle \lambda \in (0,1)}
  
, the mixture distribution is defined as:

  
    
      
        
          R
          
            λ
          
        
        =
        λ
        P
        +
        
          (
          
            1
            −
            λ
          
          )
        
        Q
      
    
    {\displaystyle R_{\lambda }=\lambda P+\left(1-\lambda \right)Q}
  

The frontier is composed of the points 
  
    
      
        (
        
          R
          
            1
          
        
        ,
        
          R
          
            2
          
        
        )
      
    
    {\displaystyle (R_{1},R_{2})}
  
 defined by:

  
    
      
        
          
            
              
                
                  R
                  
                    1
                  
                
                (
                λ
                )
              
              
                
                =
                exp
                ⁡
                
                  (
                  
                    −
                    
                      KL
                    
                    (
                    P
                    ‖
                    
                      R
                      
                        λ
                      
                    
                    )
                  
                  )
                
              
            
            
              
                
                  R
                  
                    2
                  
                
                (
                λ
                )
              
              
                
                =
                exp
                ⁡
                
                  (
                  
                    −
                    
                      KL
                    
                    (
                    Q
                    ‖
                    
                      R
                      
                        λ
                      
                    
                    )
                  
                  )
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}R_{1}(\lambda )&=\exp \left(-{\text{KL}}(P\Vert R_{\lambda })\right)\\R_{2}(\lambda )&=\exp \left(-{\text{KL}}(Q\Vert R_{\lambda })\right)\end{aligned}}}
  

where 
  
    
      
        
          KL
        
        (
        )
      
    
    {\displaystyle {\text{KL}}()}
  
 refers to the Kullback-Leibler divergence. MAUVE is the integral of this curve, providing a single scalar value between 0 and 1. A higher MAUVE score indicates the model distribution 
  
    
      
        Q
      
    
    {\displaystyle Q}
  
 is more similar to the human distribution 
  
    
      
        P
      
    
    {\displaystyle P}
  
.

Comparison with other metrics
Advantages
MAUVE has shown a much higher correlation with human judgement in tasks like web text generation compare to earlier metrics. It effectively capture the "self-repetition" problem where models become stuck in loops.

Limitations
The metric requires a large sample size (often more than 1000 generations) to provide a stable distributional estimate. It is also computationally expensive as it requires running a large model to generate embeddings and perform clustering.

References
External links
Official Implementation on GitHub