In computational neuroscience, SUPS (for Synaptic Updates Per Second) or formerly CUPS (Connections Updates Per Second) is a measure of a neuronal network performance, useful in fields of neuroscience, cognitive science, artificial intelligence, and computer science.

Computing
For a processor or computer designed to simulate a neural network SUPS is measured as the product of simulated neurons 
  
    
      
        N
      
    
    {\displaystyle N}
  
 and average connectivity 
  
    
      
        c
      
    
    {\displaystyle c}
  
(synapses) per neuron per second:

  
    
      
        S
        U
        P
        S
        =
        c
        ×
        N
      
    
    {\displaystyle SUPS=c\times N}
  

Depending on the type of simulation it is usually equal to the total number of synapses simulated.
In an "asynchronous" dynamic simulation if a neuron spikes at 
  
    
      
        υ
      
    
    {\displaystyle \upsilon }
  
 Hz, the average rate of synaptic updates provoked by the activity of that neuron is 
  
    
      
        υ
        c
        N
      
    
    {\displaystyle \upsilon cN}
  
. In a synchronous simulation with step 
  
    
      
        Δ
        t
      
    
    {\displaystyle \Delta t}
  
 the number of synaptic updates per second would be 
  
    
      
        
          
            
              c
              N
            
            
              Δ
              t
            
          
        
      
    
    {\displaystyle {\frac {cN}{\Delta t}}}
  
. As 
  
    
      
        Δ
        t
      
    
    {\displaystyle \Delta t}
  
 has to be chosen much smaller than the average interval between two successive afferent spikes, which implies 
  
    
      
        Δ
        t
        <
        
          
            1
            
              υ
              N
            
          
        
      
    
    {\displaystyle \Delta t<{\frac {1}{\upsilon N}}}
  
, giving an average of synaptic updates equal to 
  
    
      
        υ
        c
        
          N
          
            2
          
        
      
    
    {\displaystyle \upsilon cN^{2}}
  
. Therefore, spike-driven synaptic dynamics leads to a linear scaling of computational complexity O(N) per neuron, compared with the O(N2) in the "synchronous" case.

Records
Developed in the 1980s  Adaptive Solutions' CNAPS-1064 Digital Parallel Processor chip is a full neural network (NNW). It was designed as a coprocessor to a host and has 64 sub-processors arranged in a 1D array and operating in a SIMD mode. Each sub-processor can emulate one or more neurons and multiple chips can be grouped together. At 25 MHz it is capable of 1.28 GMAC.
After the presentation of the RN-100 (12 MHz) single neuron chip at Seattle 1991 Ricoh developed the multi-neuron chip RN-200. It had 16 neurons and 16 synapses per neuron. The chip has on-chip learning ability using a proprietary backdrop algorithm. It came in a 257-pin PGA encapsulation and drew 3.0 W at a maximum. It was capable of 3 GCPS (1 GCPS at 32 MHz).

In 1991–97, Siemens developed the MA-16 chip, SYNAPSE-1 and SYNAPSE-3 Neurocomputer. The MA-16 was a fast matrix-matrix multiplier that can be combined to form systolic arrays. It could process 4 patterns of 16 elements each (16-bit), with 16 neuron values (16-bit) at a rate of 800 MMAC or 400 MCPS at 50 MHz. The SYNAPSE3-PC PCI card contained 2 MA-16 with a peak performance of 2560 MOPS (1.28 GMAC); 7160 MOPS (3.58 GMAC) when using three boards.
In 2013, the K computer was used to simulate a neural network of 1.73 billion neurons with a total of 10.4 trillion synapses (1% of the human brain). The simulation ran for 40 minutes to simulate 1 s of brain activity at a normal activity level (4.4 on average). The simulation required 1 Petabyte of storage.

See also
FLOP
SPECint
SPECfp
Multiply–accumulate operation
Orders of magnitude (computing)
SyNAPSE


== References ==