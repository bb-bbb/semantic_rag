{"doc_id": ".ai", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".ai is the Internet country code top-level domain (ccTLD) for Anguilla, a British Overseas Territory in the Caribbean. It is administered by the government of Anguilla.\nIt is a popular domain hack with companies and projects related to the artificial intelligence industry (AI).\nGoogle's ad targeting treats .ai as a generic top-level domain (gTLD) because \"users and website owners frequently see [the domain] as being more generic than country-targeted.\"\nIn 2021, Google Search analyst Gary Illyes announced that \".ai\" had been added to Google’s list of generic country-code top-level domains, meaning that Google would no longer infer Anguilla-specific targeting from the ccTLD.\nIdentity Digital began managing the domain as of January 2025.\n\nSecond and third level registrations\nRegistrations within off.ai, com.ai, net.ai, and org.ai are available worldwide without restriction. From 15 September 2009, second level registrations within .ai are available to everyone worldwide.\n\nRegistration\nThe minimum registration term allowed for .ai domains is 2 through 10 years for registration and renewal, and a 2-year renewal for domain transfer. Identity Digital is the authority in charge of managing this extension. Registrations began on 16 February 1995. The limits on the number of characters used for the domain name are, at a minimum, from 1 to 3, depending on the registrar, and always at most 63 characters. The character set supported for .ai domain names includes A–Z, a–z, 0–9, and hyphen. As of November 2022, .ai domains cannot accommodate IDN characters. There are no requirements for registering a domain, including local and foreign residents.\nA .ai domain can be suspended or revoked, if the domain is involved in illegal activity such as violating trademarks or copyrights. Usage must not violate the laws of Anguilla.\nAnguilla uses the UDRP. Filing a UDRP challenge requires using one of the ICANN Approved Dispute Resolution Service Providers. If the domain is with an ICANN accredited registrar, they should work with the arbitrator. Usually this means either doing nothing or transferring a domain. .ai domains are transferable to any desired registrars as the registration of domain is done maintaining EPP.\nThere used to be a whois.ai-based platform of expired domains in which those could be procured and auctioned every ten days through a standard online process. The last auctions"}
{"doc_id": ".ai", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " doing nothing or transferring a domain. .ai domains are transferable to any desired registrars as the registration of domain is done maintaining EPP.\nThere used to be a whois.ai-based platform of expired domains in which those could be procured and auctioned every ten days through a standard online process. The last auctions of such kind closed there in December 2024; the platform had been scheduled for shutdown on 30 June 2025, but remained online in the months following that date.\n\nValuation\nDomains cost depends on the registrar, with yearly fees ranging from US$140 (the base fee, as established by Anguilla) to $200. As of July 2025, the highest-valued .ai domain is an undisclosed one sold on 8 November 2023, on Escrow.com, for US$1,500,000—months after an initial $300,000 sale to the same buyer. Among the publicly disclosed ones, the most valued, fin.ai, was sold for $1,000,000 in March 2025.\nOn 16 December 2017, the .ai registry started supporting the Extensible Provisioning Protocol (EPP) and migrated all of its domains onto an EPP system. Consequently, many registrars are allowed to sell .ai domains. Since that date, the .ai ccTLD has also been popular with artificial intelligence companies and organizations. Though such trends are primarily seen among new AI based companies or startups, many established AI and Tech companies preferred not to opt for .ai domains. For example, DeepMind has its domain retained at .com; Meta has redirected its facebook.ai domain to ai.meta.com.\n\nImpact on Anguilla's economy\nThe registration fees earned from the .ai domains go to the treasury of the Government of Anguilla. As per a 2018 New York Times report, the total revenue generated out of selling .ai domains was $2.9 million.\nIn 2023, Anguilla's government made about US$32 million from fees collected for registering .ai domains; that amounted to over 10% of gross domestic product for the territory.\n\"In the years before the real breakthrough of AI, revenue from .ai domains made up less than 1% of our state income, by 2025 it will be around 47%,\" explained Jose Vanterpool, Minister of Infrastructure and Communications (MICUHITES), in an interview with BBC.\nThe high 90% renewal rate of .ai domains and"}
{"doc_id": ".ai", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " revenue from .ai domains made up less than 1% of our state income, by 2025 it will be around 47%,\" explained Jose Vanterpool, Minister of Infrastructure and Communications (MICUHITES), in an interview with BBC.\nThe high 90% renewal rate of .ai domains and the 2025 renewal wave of domains registered in 2023 are driving another surge in state revenues, according to Domaintechnik.\n\nSee also\nInternet in Anguilla\nInternet in the United Kingdom\n\nNotes"}
{"doc_id": "2024–2026 global memory supply shortage", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The 2024–2026 global memory supply shortage is an ongoing period of supply constraints and rapid price escalation in the semiconductor memory market, particularly affecting DRAM and NAND flash memory. Unlike the 2020–2023 global chip shortage, which stemmed primarily from pandemic-related supply chain disruptions, this shortage is driven by a structural reallocation of manufacturing capacity toward high-margin products for artificial intelligence infrastructure, creating scarcity in consumer and enterprise PC markets.\n\nBackground\nFollowing a severe market downturn in 2022–2023, major memory manufacturers—Samsung Electronics, SK Hynix, and Micron Technology—implemented strategic production cuts to stabilize pricing. By mid-2024, the rapid expansion of generative AI services triggered unprecedented demand for specialized memory products, particularly High Bandwidth Memory (HBM) used in AI accelerators and data center GPUs.\nA 2024 McKinsey analysis projected that global demand for AI-ready data center capacity would grow at approximately 33% annually through 2030, with AI workloads consuming roughly 70% of total data center capacity by the decade's end.\n\nCauses\nHBM production displacement\nHBM manufacturing requires significantly more wafer capacity per bit than standard DRAM modules. Industry sources reported that as manufacturers allocated increasing wafer capacity to HBM production to meet contracts with AI infrastructure providers, the supply of conventional DDR4 and DDR5 modules for consumer PCs and smartphones contracted sharply.\n\nNAND flash capacity constraints\nIn the NAND flash segment, manufacturers prioritized higher-margin enterprise SSDs for data center applications while phasing out older process nodes more rapidly than anticipated. In November 2025, contract prices for NAND wafers increased by more than 60% month-over-month for certain product categories, with 512GB TLC experiencing the steepest rise as legacy manufacturing capacity was retired.\n\nImpact on industry and consumers\nManufacturer responses\nMajor PC manufacturers responded to component cost increases with significant price adjustments and supply chain strategies. Dell Technologies Chief Operating Officer Jeff Clarke stated during a November 2025 analyst call that the company had \"never witnessed costs escalating at the current pace,\" describing tighter availability across DRAM, hard drives, and NAND flash memory.\nLenovo Chief Financial Officer Winston Cheng described the cost surge as \"unprecedented\" and disclosed that the company's memory inventories were approximately 50% above normal levels in anticipation of further price increases.\nMarket research firm Counterpoint Research projected a 50% escalation in memory module prices by the second quarter of 2025.\n\nConsumer electronics sector\nThe shortage particularly affected"}
{"doc_id": "2024–2026 global memory supply shortage", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " cost surge as \"unprecedented\" and disclosed that the company's memory inventories were approximately 50% above normal levels in anticipation of further price increases.\nMarket research firm Counterpoint Research projected a 50% escalation in memory module prices by the second quarter of 2025.\n\nConsumer electronics sector\nThe shortage particularly affected smartphone manufacturers and other consumer electronics producers. Chinese electronics firms including Xiaomi warned of impending price increases for mobile devices in 2026.\nIn Tokyo's Akihabara electronics district, retailers began limiting purchases of memory products to prevent hoarding, with prices for popular DDR5 memory modules more than doubling in some cases.\n\nAI infrastructure competition\nTechnology companies including Google, Amazon, Microsoft, and Meta Platforms placed open-ended orders with memory suppliers, indicating they would accept as much supply as available regardless of cost, according to Reuters sources. In October 2025, OpenAI entered preliminary agreements with Samsung and SK Hynix for chip supplies for its Stargate project.\nNvidia, whose AI processors require substantial amounts of high-bandwidth memory, acknowledged significant price increases but stated it had secured adequate supply. CNBC reported that Nvidia's increasing adoption of LPDDR memory for its AI products added additional pressure to supply chains, as this memory type is also used in premium consumer electronics.\n\nSee also\n2020–2023 global chip shortage\nSemiconductor industry\nHigh Bandwidth Memory"}
{"doc_id": "Actor-critic algorithm", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The actor-critic algorithm (AC) is a family of reinforcement learning (RL) algorithms that combine policy-based RL algorithms such as policy gradient methods, and value-based RL algorithms such as value iteration, Q-learning, SARSA, and TD learning.\nAn AC algorithm consists of two main components: an \"actor\" that determines which actions to take according to a policy function, and a \"critic\" that evaluates those actions according to a value function. Some AC algorithms are on-policy, some are off-policy. Some apply to either continuous or discrete action spaces. Some work in both cases.\n\nOverview\nThe actor-critic methods can be understood as an improvement over pure policy gradient methods like REINFORCE via introducing a baseline.\n\nActor\nThe actor uses a policy function \n  \n    \n      \n        π\n        (\n        a\n        \n          |\n        \n        s\n        )\n      \n    \n    {\\displaystyle \\pi (a|s)}\n  \n, while the critic estimates either the value function \n  \n    \n      \n        V\n        (\n        s\n        )\n      \n    \n    {\\displaystyle V(s)}\n  \n, the action-value Q-function \n  \n    \n      \n        Q\n        (\n        s\n        ,\n        a\n        )\n        ,\n      \n    \n    {\\displaystyle Q(s,a),}\n  \n the advantage function \n  \n    \n      \n        A\n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle A(s,a)}\n  \n, or any combination thereof.\nThe actor is a parameterized function \n  \n    \n      \n        \n          π\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle \\pi _{\\theta }}\n  \n, where \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n are the parameters of the actor. The actor takes as argument the state of the environment \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n and produces a probability distribution \n  \n    \n      \n        \n          π\n          \n            θ\n          \n        \n        (\n        ⋅\n        \n          |\n        \n        s\n        )\n      \n    \n    {\\displaystyle \\pi _{\\theta }(\\cdot |s)}\n  \n.\nIf the action space is discrete, then \n  \n    \n      \n        \n          ∑\n          \n            a\n          \n        \n        \n          π\n          \n            θ\n          \n        \n        (\n        a\n        \n          |\n        \n        s\n        )\n        =\n        1\n      \n    \n    {\\displaystyle \\sum _{a}\\pi _{\\theta }(a|s)=1}\n  \n. If the action space is"}
{"doc_id": "Actor-critic algorithm", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "          \n            a\n          \n        \n        \n          π\n          \n            θ\n          \n        \n        (\n        a\n        \n          |\n        \n        s\n        )\n        =\n        1\n      \n    \n    {\\displaystyle \\sum _{a}\\pi _{\\theta }(a|s)=1}\n  \n. If the action space is continuous, then \n  \n    \n      \n        \n          ∫\n          \n            a\n          \n        \n        \n          π\n          \n            θ\n          \n        \n        (\n        a\n        \n          |\n        \n        s\n        )\n        d\n        a\n        =\n        1\n      \n    \n    {\\displaystyle \\int _{a}\\pi _{\\theta }(a|s)da=1}\n  \n.\nThe goal of policy optimization is to improve the actor. That is, to find some \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n that maximizes the expected episodic reward \n  \n    \n      \n        J\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle J(\\theta )}\n  \n:\n  \n    \n      \n        J\n        (\n        θ\n        )\n        =\n        \n          \n            E\n          \n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        \n          [\n          \n            \n              ∑\n              \n                t\n                =\n                0\n              \n              \n                T\n              \n            \n            \n              γ\n              \n                t\n              \n            \n            \n              r\n              \n                t\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle J(\\theta )=\\mathbb {E} _{\\pi _{\\theta }}\\left[\\sum _{t=0}^{T}\\gamma ^{t}r_{t}\\right]}\n  \nwhere \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n is the discount factor, \n  \n    \n      \n        \n          r\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle r_{t}}\n  \n is the reward at step \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, and \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n is the time-horizon (which can be infinite).\nThe goal of policy gradient method is to optimize \n  \n    \n      \n        J\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle J(\\theta )}\n  \n by gradient ascent on the policy gradient \n  \n    \n      \n        ∇\n        J\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle \\nabla J(\\theta )}\n  \n.\nAs detailed on the"}
{"doc_id": "Actor-critic algorithm", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        (\n        θ\n        )\n      \n    \n    {\\displaystyle J(\\theta )}\n  \n by gradient ascent on the policy gradient \n  \n    \n      \n        ∇\n        J\n        (\n        θ\n        )\n      \n    \n    {\\displaystyle \\nabla J(\\theta )}\n  \n.\nAs detailed on the policy gradient method page, there are many unbiased estimators of the policy gradient:\n  \n    \n      \n        \n          ∇\n          \n            θ\n          \n        \n        J\n        (\n        θ\n        )\n        =\n        \n          \n            E\n          \n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        \n          [\n          \n            \n              ∑\n              \n                0\n                ≤\n                j\n                ≤\n                T\n              \n            \n            \n              ∇\n              \n                θ\n              \n            \n            ln\n            ⁡\n            \n              π\n              \n                θ\n              \n            \n            (\n            \n              A\n              \n                j\n              \n            \n            \n              |\n            \n            \n              S\n              \n                j\n              \n            \n            )\n            ⋅\n            \n              Ψ\n              \n                j\n              \n            \n            \n              \n                |\n              \n            \n            \n              S\n              \n                0\n              \n            \n            =\n            \n              s\n              \n                0\n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\nabla _{\\theta }J(\\theta )=\\mathbb {E} _{\\pi _{\\theta }}\\left[\\sum _{0\\leq j\\leq T}\\nabla _{\\theta }\\ln \\pi _{\\theta }(A_{j}|S_{j})\\cdot \\Psi _{j}{\\Big |}S_{0}=s_{0}\\right]}\n  \nwhere \n  \n    \n      \n        \n          Ψ\n          \n            j\n          \n        \n      \n    \n    {\\textstyle \\Psi _{j}}\n  \n is a linear sum of the following:\n\n  \n    \n      \n        \n          ∑\n          \n            0\n            ≤\n            i\n            ≤\n            T\n          \n        \n        (\n        \n          γ\n          \n            i\n          \n        \n        \n          R\n          \n            i\n          \n        \n        )\n      \n    \n    {\\textstyle \\sum _{0\\leq i\\leq T}(\\gamma ^{i}R_{i})}\n  \n.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          ∑\n          \n            j\n            ≤\n            i\n            ≤\n            T\n          \n        \n        (\n        \n          γ\n          \n           "}
{"doc_id": "Actor-critic algorithm", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "0\\leq i\\leq T}(\\gamma ^{i}R_{i})}\n  \n.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          ∑\n          \n            j\n            ≤\n            i\n            ≤\n            T\n          \n        \n        (\n        \n          γ\n          \n            i\n            −\n            j\n          \n        \n        \n          R\n          \n            i\n          \n        \n        )\n      \n    \n    {\\textstyle \\gamma ^{j}\\sum _{j\\leq i\\leq T}(\\gamma ^{i-j}R_{i})}\n  \n: the REINFORCE algorithm.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          ∑\n          \n            j\n            ≤\n            i\n            ≤\n            T\n          \n        \n        (\n        \n          γ\n          \n            i\n            −\n            j\n          \n        \n        \n          R\n          \n            i\n          \n        \n        )\n        −\n        b\n        (\n        \n          S\n          \n            j\n          \n        \n        )\n      \n    \n    {\\textstyle \\gamma ^{j}\\sum _{j\\leq i\\leq T}(\\gamma ^{i-j}R_{i})-b(S_{j})}\n  \n: the REINFORCE with baseline algorithm. Here \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n is an arbitrary function.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          (\n          \n            \n              R\n              \n                j\n              \n            \n            +\n            γ\n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n                +\n                1\n              \n            \n            )\n            −\n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\textstyle \\gamma ^{j}\\left(R_{j}+\\gamma V^{\\pi _{\\theta }}(S_{j+1})-V^{\\pi _{\\theta }}(S_{j})\\right)}\n  \n: TD(1) learning.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          Q\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        (\n        \n          S\n          \n            j\n          \n        \n        ,\n        \n          A\n          \n            j\n          \n        \n        )\n      \n    \n    {\\textstyle \\gamma"}
{"doc_id": "Actor-critic algorithm", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          Q\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        (\n        \n          S\n          \n            j\n          \n        \n        ,\n        \n          A\n          \n            j\n          \n        \n        )\n      \n    \n    {\\textstyle \\gamma ^{j}Q^{\\pi _{\\theta }}(S_{j},A_{j})}\n  \n.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          A\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        (\n        \n          S\n          \n            j\n          \n        \n        ,\n        \n          A\n          \n            j\n          \n        \n        )\n      \n    \n    {\\textstyle \\gamma ^{j}A^{\\pi _{\\theta }}(S_{j},A_{j})}\n  \n: Advantage Actor-Critic (A2C).\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          (\n          \n            \n              R\n              \n                j\n              \n            \n            +\n            γ\n            \n              R\n              \n                j\n                +\n                1\n              \n            \n            +\n            \n              γ\n              \n                2\n              \n            \n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n                +\n                2\n              \n            \n            )\n            −\n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\textstyle \\gamma ^{j}\\left(R_{j}+\\gamma R_{j+1}+\\gamma ^{2}V^{\\pi _{\\theta }}(S_{j+2})-V^{\\pi _{\\theta }}(S_{j})\\right)}\n  \n: TD(2) learning.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          (\n          \n            \n              ∑\n              \n                k\n                =\n                0\n              \n              \n                n\n                −\n                1\n              \n            \n            \n              γ\n              \n                k\n              \n            \n            \n              R\n              \n                j\n                +\n                k\n              \n            \n            +\n            \n              γ\n              \n                n\n              \n            \n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n                +\n                n\n              \n            \n            )\n            −\n            \n              V"}
{"doc_id": "Actor-critic algorithm", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " j\n                +\n                k\n              \n            \n            +\n            \n              γ\n              \n                n\n              \n            \n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n                +\n                n\n              \n            \n            )\n            −\n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\textstyle \\gamma ^{j}\\left(\\sum _{k=0}^{n-1}\\gamma ^{k}R_{j+k}+\\gamma ^{n}V^{\\pi _{\\theta }}(S_{j+n})-V^{\\pi _{\\theta }}(S_{j})\\right)}\n  \n: TD(n) learning.\n\n  \n    \n      \n        \n          γ\n          \n            j\n          \n        \n        \n          ∑\n          \n            n\n            =\n            1\n          \n          \n            ∞\n          \n        \n        \n          \n            \n              λ\n              \n                n\n                −\n                1\n              \n            \n            \n              1\n              −\n              λ\n            \n          \n        \n        ⋅\n        \n          (\n          \n            \n              ∑\n              \n                k\n                =\n                0\n              \n              \n                n\n                −\n                1\n              \n            \n            \n              γ\n              \n                k\n              \n            \n            \n              R\n              \n                j\n                +\n                k\n              \n            \n            +\n            \n              γ\n              \n                n\n              \n            \n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n                +\n                n\n              \n            \n            )\n            −\n            \n              V\n              \n                \n                  π\n                  \n                    θ\n                  \n                \n              \n            \n            (\n            \n              S\n              \n                j\n              \n            \n            )\n          \n          )\n        \n      \n    \n    {\\textstyle \\gamma ^{j}\\sum _{n=1}^{\\infty }{\\frac {\\lambda ^{n-1}}{1-\\lambda }}\\cdot \\left(\\sum _{k=0}^{n-1}\\gamma ^{k}R_{j+k}+\\gamma ^{n}V^{\\pi _{\\theta }}(S_{j+n})-V^{\\pi _{\\theta }}(S_{j})\\right)}\n  \n: TD(λ) learning, also known as GAE (generalized"}
{"doc_id": "Actor-critic algorithm", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "}\\gamma ^{k}R_{j+k}+\\gamma ^{n}V^{\\pi _{\\theta }}(S_{j+n})-V^{\\pi _{\\theta }}(S_{j})\\right)}\n  \n: TD(λ) learning, also known as GAE (generalized advantage estimate). This is obtained by an exponentially decaying sum of the TD(n) learning terms.\n\nCritic\nIn the unbiased estimators given above, certain functions such as \n  \n    \n      \n        \n          V\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        ,\n        \n          Q\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        ,\n        \n          A\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle V^{\\pi _{\\theta }},Q^{\\pi _{\\theta }},A^{\\pi _{\\theta }}}\n  \n appear. These are approximated by the critic. Since these functions all depend on the actor, the critic must learn alongside the actor. The critic is learned by value-based RL algorithms.\nFor example, if the critic is estimating the state-value function \n  \n    \n      \n        \n          V\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V^{\\pi _{\\theta }}(s)}\n  \n, then it can be learned by any value function approximation method. Let the critic be a function approximator \n  \n    \n      \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V_{\\phi }(s)}\n  \n with parameters \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n.\nThe simplest example is TD(1) learning, which trains the critic to minimize the TD(1) error:\n  \n    \n      \n        \n          δ\n          \n            i\n          \n        \n        =\n        \n          R\n          \n            i\n          \n        \n        +\n        γ\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n            +\n            1\n          \n        \n        )\n        −\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\delta _{i}=R_{i}+\\gamma V_{\\phi }(S_{i+1})-V_{\\phi }(S_{i"}
{"doc_id": "Actor-critic algorithm", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\delta _{i}=R_{i}+\\gamma V_{\\phi }(S_{i+1})-V_{\\phi }(S_{i})}\n  \nThe critic parameters are updated by gradient descent on the squared TD error:\n  \n    \n      \n        ϕ\n        ←\n        ϕ\n        −\n        α\n        \n          ∇\n          \n            ϕ\n          \n        \n        (\n        \n          δ\n          \n            i\n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        ϕ\n        +\n        α\n        \n          δ\n          \n            i\n          \n        \n        \n          ∇\n          \n            ϕ\n          \n        \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\phi \\leftarrow \\phi -\\alpha \\nabla _{\\phi }(\\delta _{i})^{2}=\\phi +\\alpha \\delta _{i}\\nabla _{\\phi }V_{\\phi }(S_{i})}\n  \nwhere \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n is the learning rate. Note that the gradient is taken with respect to the \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n in \n  \n    \n      \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle V_{\\phi }(S_{i})}\n  \n only, since the \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n in \n  \n    \n      \n        γ\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle \\gamma V_{\\phi }(S_{i+1})}\n  \n constitutes a moving target, and the gradient is not taken with respect to that. This is a common source of error in implementations that use automatic differentiation, and requires \"stopping the gradient\" at that point.\nSimilarly, if the critic is estimating the action-value function \n  \n    \n      \n        \n          Q\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Q^{\\pi _{\\theta }}}\n  \n, then it can be learned"}
{"doc_id": "Actor-critic algorithm", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " \"stopping the gradient\" at that point.\nSimilarly, if the critic is estimating the action-value function \n  \n    \n      \n        \n          Q\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Q^{\\pi _{\\theta }}}\n  \n, then it can be learned by Q-learning or SARSA. In SARSA, the critic maintains an estimate of the Q-function, parameterized by \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n, denoted as \n  \n    \n      \n        \n          Q\n          \n            ϕ\n          \n        \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle Q_{\\phi }(s,a)}\n  \n. The temporal difference error is then calculated as \n  \n    \n      \n        \n          δ\n          \n            i\n          \n        \n        =\n        \n          R\n          \n            i\n          \n        \n        +\n        γ\n        \n          Q\n          \n            θ\n          \n        \n        (\n        \n          S\n          \n            i\n            +\n            1\n          \n        \n        ,\n        \n          A\n          \n            i\n            +\n            1\n          \n        \n        )\n        −\n        \n          Q\n          \n            θ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        ,\n        \n          A\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\delta _{i}=R_{i}+\\gamma Q_{\\theta }(S_{i+1},A_{i+1})-Q_{\\theta }(S_{i},A_{i})}\n  \n. The critic is then updated by\n  \n    \n      \n        θ\n        ←\n        θ\n        +\n        α\n        \n          δ\n          \n            i\n          \n        \n        \n          ∇\n          \n            θ\n          \n        \n        \n          Q\n          \n            θ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        ,\n        \n          A\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle \\theta \\leftarrow \\theta +\\alpha \\delta _{i}\\nabla _{\\theta }Q_{\\theta }(S_{i},A_{i})}\n  \nThe advantage critic can be trained by training both a Q-function \n  \n    \n      \n        \n          Q\n          \n            ϕ\n          \n        \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle Q_{\\phi }(s,a)}\n  \n and a state-value function \n  \n    \n      \n"}
{"doc_id": "Actor-critic algorithm", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "i})}\n  \nThe advantage critic can be trained by training both a Q-function \n  \n    \n      \n        \n          Q\n          \n            ϕ\n          \n        \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle Q_{\\phi }(s,a)}\n  \n and a state-value function \n  \n    \n      \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V_{\\phi }(s)}\n  \n, then let \n  \n    \n      \n        \n          A\n          \n            ϕ\n          \n        \n        (\n        s\n        ,\n        a\n        )\n        =\n        \n          Q\n          \n            ϕ\n          \n        \n        (\n        s\n        ,\n        a\n        )\n        −\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle A_{\\phi }(s,a)=Q_{\\phi }(s,a)-V_{\\phi }(s)}\n  \n. Although, it is more common to train just a state-value function \n  \n    \n      \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V_{\\phi }(s)}\n  \n, then estimate the advantage by\n  \n    \n      \n        \n          A\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        ,\n        \n          A\n          \n            i\n          \n        \n        )\n        ≈\n        \n          ∑\n          \n            j\n            ∈\n            0\n            :\n            n\n            −\n            1\n          \n        \n        \n          γ\n          \n            j\n          \n        \n        \n          R\n          \n            i\n            +\n            j\n          \n        \n        +\n        \n          γ\n          \n            n\n          \n        \n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n            +\n            n\n          \n        \n        )\n        −\n        \n          V\n          \n            ϕ\n          \n        \n        (\n        \n          S\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle A_{\\phi }(S_{i},A_{i})\\approx \\sum _{j\\in 0:n-1}\\gamma ^{j}R_{i+j}+\\gamma ^{n}V_{\\phi }(S_{i+n})-V_{\\phi }(S_{i})}\n  \nHere, \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is a positive"}
{"doc_id": "Actor-critic algorithm", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\\in 0:n-1}\\gamma ^{j}R_{i+j}+\\gamma ^{n}V_{\\phi }(S_{i+n})-V_{\\phi }(S_{i})}\n  \nHere, \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is a positive integer. The higher \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is, the more lower is the bias in the advantage estimation, but at the price of higher variance.\nThe Generalized Advantage Estimation (GAE) introduces a hyperparameter \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n that smoothly interpolates between Monte Carlo returns (\n  \n    \n      \n        λ\n        =\n        1\n      \n    \n    {\\displaystyle \\lambda =1}\n  \n, high variance, no bias) and 1-step TD learning (\n  \n    \n      \n        λ\n        =\n        0\n      \n    \n    {\\displaystyle \\lambda =0}\n  \n, low variance, high bias). This hyperparameter can be adjusted to pick the optimal bias-variance trade-off in advantage estimation. It uses an exponentially decaying average of n-step returns with \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n being the decay strength.\n\nVariants\nAsynchronous Advantage Actor-Critic (A3C): Parallel and asynchronous version of A2C.\nSoft Actor-Critic (SAC): Incorporates entropy maximization for improved exploration.\nDeep Deterministic Policy Gradient (DDPG): Specialized for continuous action spaces.\n\nSee also\nReinforcement learning\nPolicy gradient method\nDeep reinforcement learning"}
{"doc_id": "Admissible heuristic", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path. In other words, it should act as a lower bound.\nIt is related to the concept of consistent heuristics. While all consistent heuristics are admissible, not all admissible heuristics are consistent.\n\nSearch algorithms\nAn admissible heuristic is used to estimate the cost of reaching the goal state in an informed search algorithm. In order for a heuristic\nto be admissible to the search problem, the estimated cost must always be lower than or equal to the actual cost of reaching the goal state. \nThe search algorithm uses the admissible heuristic to find an estimated \noptimal path to the goal state from the current node. \nFor example, in A* search the evaluation function (where \n\n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n is the current node) is:\n\n  \n    \n      \n        f\n        (\n        n\n        )\n        =\n        g\n        (\n        n\n        )\n        +\n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)=g(n)+h(n)}\n  \n\nwhere\n\n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n = the evaluation function.\n\n  \n    \n      \n        g\n        (\n        n\n        )\n      \n    \n    {\\displaystyle g(n)}\n  \n = the cost from the start node to the current node\n\n  \n    \n      \n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)}\n  \n = estimated cost from current node to goal.\n\n  \n    \n      \n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)}\n  \n is calculated using the heuristic \nfunction. With a non-admissible heuristic, the A* algorithm could \noverlook the optimal solution to a search problem due to an \noverestimation in \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n.\n\nFormulation\nn\n      \n    \n    {\\displaystyle n}\n  \n is a node\n\n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n is a heuristic\n\n  \n    \n      \n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)}\n  \n is cost"}
{"doc_id": "Admissible heuristic", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "style f(n)}\n  \n.\n\nFormulation\nn\n      \n    \n    {\\displaystyle n}\n  \n is a node\n\n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n is a heuristic\n\n  \n    \n      \n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)}\n  \n is cost indicated by \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n  \n to reach a goal from \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n\n  \n    \n      \n        \n          h\n          \n            ∗\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle h^{*}(n)}\n  \n is the optimal cost to reach a goal from \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n\n  \n    \n      \n        h\n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)}\n  \n is admissible if, \n  \n    \n      \n        ∀\n        n\n      \n    \n    {\\displaystyle \\forall n}\n  \n\n  \n    \n      \n        h\n        (\n        n\n        )\n        ≤\n        \n          h\n          \n            ∗\n          \n        \n        (\n        n\n        )\n      \n    \n    {\\displaystyle h(n)\\leq h^{*}(n)}\n\nConstruction\nAn admissible heuristic can be derived from a relaxed\nversion of the problem, or by information from pattern databases that store exact solutions to subproblems of the problem, or by using inductive learning methods.\n\nExamples\nTwo different examples of admissible heuristics apply to the fifteen puzzle problem:\n\nHamming distance\nManhattan distance\nThe Hamming distance is the total number of misplaced tiles. It is clear that this heuristic is admissible since the total number of moves to order the tiles correctly is at least the number of misplaced tiles (each tile not in place must be moved at least once). The cost (number of moves) to the goal (an ordered puzzle) is at least the Hamming distance of the puzzle.\nThe Manhattan distance of a puzzle is defined as:\n\n  \n    \n      \n        h\n        (\n        n\n        )\n        =\n        \n          ∑\n          \n            all tiles\n          \n        \n        \n          \n            d\n            i\n            s\n            t\n            a\n            n\n            c\n            e\n          \n        \n        (\n        \n          tile, correct position\n        \n        )\n      \n    \n    {\\displaystyle h(n)=\\sum _{\\text{all tiles}}{\\mathit {distance}}({\\text{tile, correct position}})}\n  \n\nConsider the puzzle below"}
{"doc_id": "Admissible heuristic", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "            a\n            n\n            c\n            e\n          \n        \n        (\n        \n          tile, correct position\n        \n        )\n      \n    \n    {\\displaystyle h(n)=\\sum _{\\text{all tiles}}{\\mathit {distance}}({\\text{tile, correct position}})}\n  \n\nConsider the puzzle below in which the player wishes to move each tile such that the numbers are ordered. The Manhattan distance is an admissible heuristic in this case because every tile will have to be moved at least the number of spots in between itself and its correct position.\n\nThe subscripts show the Manhattan distance for each tile. The total Manhattan distance for the shown puzzle is:\n\n  \n    \n      \n        h\n        (\n        n\n        )\n        =\n        3\n        +\n        1\n        +\n        0\n        +\n        1\n        +\n        2\n        +\n        3\n        +\n        3\n        +\n        4\n        +\n        3\n        +\n        2\n        +\n        4\n        +\n        4\n        +\n        4\n        +\n        1\n        +\n        1\n        =\n        36\n      \n    \n    {\\displaystyle h(n)=3+1+0+1+2+3+3+4+3+2+4+4+4+1+1=36}\n\nOptimality proof\nIf an admissible heuristic is used in an algorithm that, per iteration, progresses only the path of lowest evaluation (current cost + heuristic) of several candidate paths, terminates the moment its exploration reaches the goal and, crucially, closes all optimal paths before terminating (something that's possible with A* search algorithm if special care isn't taken), then this algorithm can only terminate on an optimal path. To see why, consider the following proof by contradiction:\nAssume such an algorithm managed to terminate on a path T with a true cost Ttrue greater than the optimal path S with true cost Strue. This means that before terminating, the evaluated cost of T was less than or equal to the evaluated cost of S (or else S would have been picked). Denote these evaluated costs Teval and Seval respectively. The above can be summarized as follows,\n\nStrue < Ttrue\nTeval ≤ Seval\nIf our heuristic is admissible it follows that at this penultimate step Teval = Ttrue because any increase on the true cost by the heuristic on T would be inadmissible and the heuristic cannot be"}
{"doc_id": "Admissible heuristic", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "val respectively. The above can be summarized as follows,\n\nStrue < Ttrue\nTeval ≤ Seval\nIf our heuristic is admissible it follows that at this penultimate step Teval = Ttrue because any increase on the true cost by the heuristic on T would be inadmissible and the heuristic cannot be negative. On the other hand, an admissible heuristic would require that Seval ≤ Strue which combined with the above inequalities gives us Teval < Ttrue and more specifically Teval ≠ Ttrue. As Teval and Ttrue cannot be both equal and unequal our assumption must have been false and so it must be impossible to terminate on a more costly than optimal path.\nAs an example, let us say we have costs as follows:(the cost above/below a node is the heuristic, the cost at an edge is the actual cost)\n\n 0     10   0   100   0\nSTART ----  O  ----- GOAL\n |                   |\n0|                   |100\n |                   | \n O ------- O  ------ O\n100   1    100   1   100\n\nSo clearly we would start off visiting the top middle node, since the expected total cost, i.e. \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n, is \n  \n    \n      \n        10\n        +\n        0\n        =\n        10\n      \n    \n    {\\displaystyle 10+0=10}\n  \n. Then the goal would be a candidate, with \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n equal to \n  \n    \n      \n        10\n        +\n        100\n        +\n        0\n        =\n        110\n      \n    \n    {\\displaystyle 10+100+0=110}\n  \n. Then we would clearly pick the bottom nodes one after the other, followed by the updated goal, since they all have \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n lower than the \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n of the current goal, i.e. their \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n is \n  \n    \n      \n        100\n        ,\n        101\n        ,\n        102\n        ,\n       "}
{"doc_id": "Admissible heuristic", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "    \n    {\\displaystyle f(n)}\n  \n of the current goal, i.e. their \n  \n    \n      \n        f\n        (\n        n\n        )\n      \n    \n    {\\displaystyle f(n)}\n  \n is \n  \n    \n      \n        100\n        ,\n        101\n        ,\n        102\n        ,\n        102\n      \n    \n    {\\displaystyle 100,101,102,102}\n  \n. So even though the goal was a candidate, we could not pick it because there were still better paths out there. This way, an admissible heuristic can ensure optimality.\nHowever, note that although an admissible heuristic can guarantee final optimality, it is not necessarily efficient.\n\nSee also\nConsistent heuristic\nHeuristic function\nSearch algorithm"}
{"doc_id": "Agentive logic", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Agentive logic (also called the logic of action or logic of agency) is the field of philosophical logic and logic in computer science that studies formal representations of agents, their actions, and their abilities. An agentive logic in the narrower sense is a formal system whose primitive operators express that an agent does something, can do something, or sees to it that something is the case. \nAgentive logics generalise modal logic by adding modalities indexed to agents and to actions. Typical examples include:\n\nSTIT logics (from sees to it that) with operators of the form \n  \n    \n      \n        [\n        i\n         \n        \n          \n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {stit}}:\\varphi ]}\n  \n meaning that agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n sees to it that \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n holds;\ndynamic logics of action with program-like modalities \n  \n    \n      \n        [\n        α\n        ]\n        φ\n      \n    \n    {\\displaystyle [\\alpha ]\\varphi }\n  \n and \n  \n    \n      \n        ⟨\n        α\n        ⟩\n        φ\n      \n    \n    {\\displaystyle \\langle \\alpha \\rangle \\varphi }\n  \n meaning, roughly, that after every (respectively, some) execution(s) of action \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n, \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n holds;\nlogics with explicit agentive operators such as \"can do\", \"brings about\", or \"is able to ensure\".\nAgentive logics are used in action theory in philosophy, in the semantics of natural language, in the theory of program verification, and in artificial intelligence, where they underpin formalisms for reasoning about actions, planning, and intelligent agents.\n\nTerminology and scope\nThe adjective agentive derives from the Latin agens (\"one who acts\") and originally referred to the grammatical agent of a verb. In logical contexts it designates operators or predicates whose primary argument position is an agent rather than a proposition alone, for example \n  \n    \n      \n        \n          A\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle A_{i}\\varphi }\n  \n (\"agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n does \n  \n    \n      \n        φ\n      \n    \n    {\\"}
{"doc_id": "Agentive logic", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " agent rather than a proposition alone, for example \n  \n    \n      \n        \n          A\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle A_{i}\\varphi }\n  \n (\"agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n does \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\") or \n  \n    \n      \n        \n          C\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle C_{i}\\varphi }\n  \n (\"agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n can bring about \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\"). \nIn contemporary literature, agentive logic is sometimes used narrowly for formal reconstructions of St. Anselm's modal account of facere (\"to do\"). More broadly, the term is used interchangeably with logic of action or logic of agency to cover a family of modal and dynamic logics designed to capture the structure of action and choice.\n\nHistorical background\nMedieval and early modern roots\nMedieval logicians already explored analogies between modalities of action and alethic modalities such as possibility and necessity, for instance in discussions of obligation and power. \nAn influential early agentive analysis is due to St. Anselm (11th century), who treated \"doing \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\" as a kind of modal operator on propositions, anticipating later modal logics of agency. Modern reconstructions of Anselm's theory show that the resulting \"agentive logic\" can be modelled with neighbourhood semantics and satisfies a recognisable square of opposition.\n\nModern logic of action\nModern study of the logic of action began in the mid-20th century, parallel to developments in deontic logic and tense logic. Early systems were proposed by Georg Henrik von Wright, Stig Kanger, and others, often motivated by questions about norms and responsibility.\nFrom the 1960s onward, two largely independent but eventually converging traditions emerged:\n\na branching-time tradition, culminating in STIT logics, emphasising agents' choices among possible futures; and\ndynamic logics of programs and actions, developed within computer science to reason about program execution.\nIn the 1990s and 2000s, action logics were further developed in connection with knowledge representation, planning, and multi-agent systems in AI, and with dynamic and update semantics in linguistics.\n\nCore ideas\nDespite their diversity"}
{"doc_id": "Agentive logic", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of programs and actions, developed within computer science to reason about program execution.\nIn the 1990s and 2000s, action logics were further developed in connection with knowledge representation, planning, and multi-agent systems in AI, and with dynamic and update semantics in linguistics.\n\nCore ideas\nDespite their diversity, most agentive logics share some general themes:\n\nAgents are treated as explicit indices of modal operators, as in \n  \n    \n      \n        [\n        i\n         \n        \n          \n            d\n            o\n            e\n            s\n          \n        \n        ]\n        φ\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {does}}]\\varphi }\n  \n or \n  \n    \n      \n        \n          C\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle C_{i}\\varphi }\n  \n.\nActions are represented either implicitly, via changes between possible worlds along an accessibility relation, or explicitly, as terms denoting primitive and composite actions.\nChoice and ability are captured by modalities describing what an agent can ensure, usually relative to assumptions about the environment and other agents.\nFormal properties such as closure under composition, interaction between different agents, and connections to obligation (what an agent ought to do) and knowledge (what an agent knows how to do) are investigated.\n\nSTIT logics\nSTIT (\"sees to it that\") logics, originating in work by Nuel Belnap and collaborators, treat agency in a branching-time framework. A STIT model consists of a partially ordered set of moments with a tree-like structure, sets of histories (maximal branches through the tree), and for each agent at each moment, a partition of the histories through that moment representing the choices available to the agent.\nIntuitively, an agent's action at a moment determines which equivalence class (choice cell) of histories becomes actual; a formula \n  \n    \n      \n        [\n        i\n         \n        \n          \n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {stit}}:\\varphi ]}\n  \n is true at a history–moment pair if \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n holds on all histories in the choice cell corresponding to the agent's current action. Different STIT operators have been distinguished, notably:\n\nthe Chellas STIT operator, often written \n  \n    \n      \n        [\n        i\n         \n        \n          \n            c\n            s\n            t\n            i\n            t\n"}
{"doc_id": "Agentive logic", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "phi }\n  \n holds on all histories in the choice cell corresponding to the agent's current action. Different STIT operators have been distinguished, notably:\n\nthe Chellas STIT operator, often written \n  \n    \n      \n        [\n        i\n         \n        \n          \n            c\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {cstit}}:\\varphi ]}\n  \n, which requires only that the agent's choice guarantees \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n; and\nthe deliberative STIT operator, \n  \n    \n      \n        [\n        i\n         \n        \n          \n            d\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {dstit}}:\\varphi ]}\n  \n, which additionally requires that \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is not already historically necessary.\nSTIT frameworks have been extended with group agency operators, temporal modalities, epistemic operators, and deontic operators to study responsibility, collective action, and obligations under indeterminism.\n\nDynamic logics of action\nDynamic logic was originally developed to reason about the behaviour of computer programs, treating program execution as a kind of action. In propositional dynamic logic (PDL), action terms \n  \n    \n      \n        α\n        ,\n        β\n        ,\n        …\n      \n    \n    {\\displaystyle \\alpha ,\\beta ,\\dots }\n  \n denote abstract programs or actions, and formulas of the form \n  \n    \n      \n        [\n        α\n        ]\n        φ\n      \n    \n    {\\displaystyle [\\alpha ]\\varphi }\n  \n and \n  \n    \n      \n        ⟨\n        α\n        ⟩\n        φ\n      \n    \n    {\\displaystyle \\langle \\alpha \\rangle \\varphi }\n  \n express that all, respectively some, terminating executions of \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n lead to states where \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n holds.\nFrom the standpoint of agentive logic, dynamic logic provides:\n\na language for building complex actions from primitives via sequencing, choice, and iteration (e.g. \n  \n    \n      \n        α\n        ;\n        β\n      \n    \n    {\\displaystyle \\alpha ;\\beta }\n  \n, \n  \n    \n      \n        α\n        ∪\n        β\n      \n    \n    {\\display"}
{"doc_id": "Agentive logic", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " dynamic logic provides:\n\na language for building complex actions from primitives via sequencing, choice, and iteration (e.g. \n  \n    \n      \n        α\n        ;\n        β\n      \n    \n    {\\displaystyle \\alpha ;\\beta }\n  \n, \n  \n    \n      \n        α\n        ∪\n        β\n      \n    \n    {\\displaystyle \\alpha \\cup \\beta }\n  \n, \n  \n    \n      \n        \n          α\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle \\alpha ^{*}}\n  \n);\na Kripke semantics in which actions correspond to labelled accessibility relations; and\nproof systems (such as Hoare logic and weakest precondition calculi) for reasoning about the correctness of action sequences.\nExtensions such as concurrent dynamic logic add operators for parallel composition, allowing reasoning about interacting processes and concurrent actions. John-Jules Ch. Meyer and others have argued that dynamic logic is a natural base for logics of agents, by adding modalities for knowledge, belief, and ability on top of the action modalities.\nDynamic logics have also been applied to normative reasoning, yielding dynamic deontic logics where actions are related to obligations and permissions, and to dynamic epistemic logics in which information-changing actions such as announcements are modelled as programs.\n\nSituation calculus and other action formalisms\nIn artificial intelligence, reasoning about action and change is often based on first-order languages that explicitly represent situations, events, and fluents (time-varying properties). The best known is situation calculus, introduced by John McCarthy and developed extensively by Raymond Reiter.\nIn such formalisms:\n\naction terms name primitive actions;\na function symbol (often \n  \n    \n      \n        \n          \n            d\n            o\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {do}}}\n  \n) maps an action and a situation to a successor situation; and\naxioms describe which fluents hold in which situations and how actions change them.\nReiter's successor state axioms give compact specifications of how each fluent changes under all actions, and precondition axioms specify when actions are possible. Related formalisms include the event calculus and fluent calculus, which provide alternative ways of representing events and their effects.\nWhile these systems are often first-order rather than modal, they are closely related to agentive logics: their action terms and transition structures can be seen as providing models for dynamic or STIT-style modalities, and conversely dynamic logics can be used as abstract specification languages for such AI formalisms.\n\nAbility, agency, and related modalities\nMany agentive log"}
{"doc_id": "Agentive logic", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", they are closely related to agentive logics: their action terms and transition structures can be seen as providing models for dynamic or STIT-style modalities, and conversely dynamic logics can be used as abstract specification languages for such AI formalisms.\n\nAbility, agency, and related modalities\nMany agentive logics introduce explicit operators for ability or \"can-do\" notions. For example, bimodal logics have been studied that combine an action modality (for what an agent actually does) with an ability modality capturing what the agent could do in the current circumstances. \nReconstructed versions of Anselm's agentive logic interpret expressions of the form \n  \n    \n      \n        \n          δ\n          \n            a\n          \n        \n        φ\n      \n    \n    {\\displaystyle \\delta _{a}\\varphi }\n  \n as saying that agent \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n performs (or brings about) \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n, and investigate interaction principles such as:\n\ndistribution of agency over conjunctions, and\nconstraints linking agency to possibility and responsibility.\nIn modern STIT and dynamic-epistemic frameworks, ability is often analysed in terms of the existence of a strategy or plan that ensures a desired outcome against various patterns of environmental behaviour, leading to connections with game logic, coalition logic, and ATL.\n\nApplications\nPhilosophy\nIn philosophical action theory and ethics, agentive logics are used to analyse notions such as free will, intentional action, and moral responsibility. STIT logics in particular were developed partly to formalise debates about whether agents can be responsible in an indeterministic world, and how to model \"could have done otherwise\" claims. \nAgentive operators also interact with deontic operators: logics combining \"ought\" and \"can bring about\" have been proposed to capture principles such as \"ought implies can\" and to reason about what agents are obliged to make true.\n\nLinguistics and speech acts\nIn linguistics, logical tools for agency are used in the semantics of verbs of action, intention, and control, and in the analysis of speech acts, which are themselves actions performed by uttering sentences. Dynamic and agentive logics contribute to dynamic semantics and theories of discourse representation, where meaning is understood in terms of context-changing actions on an information state.\n\nComputer science and verification\nIn computer science, logics of action underpin formal methods for specifying and verifying programs and reactive systems. Hoare logics"}
{"doc_id": "Agentive logic", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ing sentences. Dynamic and agentive logics contribute to dynamic semantics and theories of discourse representation, where meaning is understood in terms of context-changing actions on an information state.\n\nComputer science and verification\nIn computer science, logics of action underpin formal methods for specifying and verifying programs and reactive systems. Hoare logics and dynamic logics are standard tools for proving that a program, seen as a complex action, has certain input–output or temporal properties. \nTemporal logics and their extensions (such as CTL and ATL) can be seen as endogenous agentive logics, where the \"system\" and its environment are treated as agents whose possible behaviours are quantified over by temporal operators. These logics are the basis of model checking techniques widely used in hardware and software verification.\n\nArtificial intelligence and multi-agent systems\nLogic-based AI uses agentive logics to model reasoning about action, planning, and belief–desire–intention (BDI) agents. Dynamic logics enriched with modalities for belief, knowledge, goal and intention have been proposed as specification languages for rational agents and multi-agent systems.\nIn multi-agent settings, action and ability modalities are combined with epistemic and deontic modalities to formalise teamwork, joint intentions, and norms governing agent societies.\n\nRelationship to deontic and epistemic logics\nAgentive logic is closely connected to deontic logic, which studies obligation and permission, and to epistemic logic, which studies knowledge and belief. Many questions about what agents ought to do, or what they know how to do, require combining these perspectives. \nFor example:\n\ndynamic deontic logics model obligations to perform certain actions and the effects of normative updates; and\ndynamic epistemic logics treat public announcements and observations as epistemic actions that transform agents' knowledge states.\nThe combined study of how obligations, knowledge, and abilities interact—for instance, whether agents are obliged to know how to fulfill their obligations—is an active area of research at the intersection of these logics.\n\nJohn Horty's ought-to-do logic\nA prominent attempt to connect agentive logics with deontic and epistemic logics is John F. Horty's \"ought-to-do\" programme. In his book Agency and Deontic Logic (2001) Horty develops a deontic logic whose primary operator is an agent-indexed \"ought to do\" (what an agent ought to see to at a time) rather than the more familiar \"ought to be"}
{"doc_id": "Agentive logic", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "y's \"ought-to-do\" programme. In his book Agency and Deontic Logic (2001) Horty develops a deontic logic whose primary operator is an agent-indexed \"ought to do\" (what an agent ought to see to at a time) rather than the more familiar \"ought to be\" operator of standard deontic logic.  The semantics is built on a branching-time STIT framework of agency in indeterministic time, together with decision-theoretic ideas: at each moment, an agent faces a set of available choices (choice cells), and these choices are ordered by a value or utility ranking over their possible continuations (histories).  Roughly, an agent \"ought to do\" an action when that choice is among the best options according to this ordering.\nOn Horty's picture, many traditional paradoxes of deontic logic – such as Good Samaritan–style cases and contrary-to-duty scenarios – arise from conflating propositions about how the world ought to be with claims about what agents ought to do.  By defining a deontic operator directly over agentive choices (within STIT semantics) and by using utilitarian dominance or value-maximization over those choices, his \"ought-to-do\" logic offers alternative treatments of such paradoxes while remaining compatible with standard modal techniques.  This has made Horty's system a central reference point for later work in deontic STIT logics and logics of action more generally.\nHorty has also argued that a satisfactory account of \"what an agent ought to do\" must be sensitive to the agent's epistemic situation, not just to the objective structure of outcomes.  In his later work on \"epistemic oughts\" within STIT semantics, he extends the basic ought-to-do framework with epistemic accessibility relations, and distinguishes objective obligations from knowledge-dependent, subjective obligations (what an agent ought to do given what they know).  This explicitly combines deontic and epistemic modalities: the same branching-time STIT models are enriched with epistemic indistinguishability relations, and the definition of the relevant \"best\" choices is restricted to the histories compatible with the agent's knowledge state.  The resulting logics aim to capture the kind of knowledge-relative obligations highlighted in work on combined deontic–epistemic systems, such as obligations that depend on what an agent knows or ought to know in a situation.\nHorty's framework has inspired a broader research programme in epistemic"}
{"doc_id": "Agentive logic", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " knowledge state.  The resulting logics aim to capture the kind of knowledge-relative obligations highlighted in work on combined deontic–epistemic systems, such as obligations that depend on what an agent knows or ought to know in a situation.\nHorty's framework has inspired a broader research programme in epistemic and doxastic deontic STIT logics, in which various \"ought-to-do\" operators are studied alongside epistemic or belief operators to analyse notions such as knowingly doing, objective versus subjective oughts, and responsibility under uncertainty.  In this way, Horty's ought-to-do logic provides a bridge between formal theories of agency, traditional deontic logic, and contemporary epistemic logics.\n\nSee also\nAction theory\nDeontic logic\nEpistemic logic\nDynamic logic (modal logic)\nSTIT logic\nSituation calculus\nEvent calculus\nBelief–desire–intention software model\nDynamic epistemic logic\nGame logic\nCoalition logic\nAlternating-time temporal logic"}
{"doc_id": "AI agent", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In the context of generative artificial intelligence, AI agents (also referred to as compound AI systems or agentic AI) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.\n\nOverview\nAI agents possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs). Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.\nResearchers and commentators have noted that AI agents do not have a standard definition. The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S..\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user's prompted request. Prominent examples include Devin AI, AutoGPT, and SIMA. Further examples of agents released since 2025 include OpenAI Operator, ChatGPT Deep Research, Manus, Quark (based on Qwen), AutoGLM Rumination, and Coze (by ByteDance). Frameworks for building AI agents include LangChain, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\nCompanies such as Google, Microsoft and Amazon Web Services have offered platforms for deploying pre-built AI agents.\nProposed protocols for standardizing inter-agent communication include the Agent Protocol (by LangChain), the Model Context Protocol (by Anthropic), AGNTCY, Gibberlink, the Internet of Agents, Agent2Agent (by Google), and the Agent Network Protocol. Some of these protocols are also used for connecting agents with external applications. Software frameworks for addressing agent reliability include AgentSpec, ToolEmu, GuardAgent, Agentic Evaluations, and predictive models from H2O.ai.\nIn February 2025, Hugging Face released Open Deep Research, an open source version of OpenAI Deep Research. Hugging Face also released a free web browser agent, similar to OpenAI Operator. Galileo AI published on Hugging Face a leadership board for agents, which ranks their performance based on their underlying LLMs.\nMemory systems for agents include Mem0, MemGPT, and MemOS.\n\nHistory\nAI agents have been traced back to research from the 1990s, with Harvard professor Milind Tambe noting that the definition of"}
{"doc_id": "AI agent", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Face a leadership board for agents, which ranks their performance based on their underlying LLMs.\nMemory systems for agents include Mem0, MemGPT, and MemOS.\n\nHistory\nAI agents have been traced back to research from the 1990s, with Harvard professor Milind Tambe noting that the definition of an AI agent was not clear at the time either. Researcher Andrew Ng has been credited with spreading the term \"agentic\" to a wider audience in 2024.\n\nTraining and testing\nResearchers have attempted to build world models and reinforcement learning environments to train or evaluate AI agents. For example, video games such as Minecraft and No Man's Sky as well as replicas of company websites, have also been used for training AI agents.\n\nAutonomous capabilities\nThe Financial Times compared the autonomy of AI agents to the SAE classification of self-driving cars, comparing most applications to level 2 or level 3, with some achieving level 4 in highly specialized circumstances, and level 5 being theoretical.\n\nCognitive architecture\nThe following are some possible internal design options for reasoning within an agent:\n\nRetrieval-augmented generation\nReAct (Reason + Act) pattern is an iterative process in which an AI agent alternates between reasoning and taking actions, receives observations from the environment or external tools, and integrates these observations into subsequent reasoning steps.\nReflexion, which uses an LLM to create feedback on the agent's plan of action and stores that feedback in a memory cache.\nA tool/agent registry, for organizing software functions or other agents that the agent can use.\nOne-shot model querying, which queries the model once to create the plan of action.\n\nOrchestration patterns\nTo execute complex tasks, autonomous agents are often integrated with other agents or specialized tools. These configurations, known as orchestration patterns or workflows, include the following:\n\nPrompt chaining: A sequence where the output of one step serves as the input for the next.\nRouting: The classification of an input to direct it to a specialized downstream task or tool.\nParallelization: The simultaneous execution of multiple tasks.\nSequential processing: A fixed, linear progression of tasks through a predefined pipeline.\nPlanner-critic: An iterative pattern where one agent generates a proposal and another evaluates it to provide feedback for refinement.\n\nMultimodal AI agents\nIn addition to large language models (LLMs), vision-language models (VLMs) and multimodal foundation models can be used as the basis for agents. In September 2024, Allen Institute for AI released an open-source"}
{"doc_id": "AI agent", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " a proposal and another evaluates it to provide feedback for refinement.\n\nMultimodal AI agents\nIn addition to large language models (LLMs), vision-language models (VLMs) and multimodal foundation models can be used as the basis for agents. In September 2024, Allen Institute for AI released an open-source vision-language model, which Wired noted could give AI agents the ability to perform complex computer tasks, including the possibility of automated computer hacking. Nvidia released a framework for developers to use VLMs, LLMs and retrieval-augmented generation for building AI agents that can analyze images and videos, including video search and video summarization. Microsoft released a multimodal agent model – trained on images, video, software user interface interactions, and robotics data – that the company claimed can manipulate software and robots.\n\nApplications\nAs of April 2025, per the Associated Press, there are few real-world applications of AI agents. As of June 2025, per Fortune, many companies are primarily experimenting with AI agents.\nThe Information divided AI agents into seven archetypes: business-task agents, for acting within enterprise software; conversational agents, which act as chatbots for customer support; research agents, for querying and analyzing information (such as OpenAI Deep Research); analytics agents, for analyzing data to create reports; software developer or coding agents (such as Cursor); domain-specific agents, which include specific subject matter knowledge; and web browser agents (such as OpenAI Operator).\nBy mid-2025, AI agents have been used in video game development, gambling (including sports betting), and cryptocurrency wallets (including cryptocurrency trading and meme coins). In August 2025, New York Magazine described software development as the most definitive use case of AI agents. Likewise, by October 2025, noting a decline in expectations, The Information noted AI coding agents and customer support as the primary use cases by businesses.\nIn November 2025, The Wall Street Journal reported that few companies that deployed AI agents have received a return on investment.\n\nApplications in government\nSeveral government bodies in the United States and United Kingdom have deployed or announced the deployment of agents, at the local and national level. The city of Kyle, Texas deployed an AI agent from Salesforce in March 2025 for 311 customer service. In November 2025, the Internal Revenue Service stated that it would use Agentforce, AI agents from Salesforce, for the Office of Chief Counsel, Taxpayer Advocate Services and the Office of Appeals. That same month, Staffordshire Police announced that they"}
{"doc_id": "AI agent", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " agent from Salesforce in March 2025 for 311 customer service. In November 2025, the Internal Revenue Service stated that it would use Agentforce, AI agents from Salesforce, for the Office of Chief Counsel, Taxpayer Advocate Services and the Office of Appeals. That same month, Staffordshire Police announced that they would trial Agentforce agents for handling non-emergency 101 calls in the United Kingdom starting in 2026. By December 2025, the Department of Neighborhoods in Detroit, Michigan, in partnership with a local business, deployed a pilot project in two Detroit districts for an AI agent to be used for customer service calls. \nIn February 2025, Thomas Shedd, the director of the Technology Transformation Services, proposed using AI coding agents across the United States federal government. A recruiter for the Department of Government Efficiency proposed in April 2025 to use AI agents to automate the work of about 70,000 United States federal government employees, as part of a startup with funding from OpenAI and a partnership agreement with Palantir. This proposal was criticized by experts for its impracticality, if not impossibility, and the lack of corresponding widespread adoption by businesses. In December 2025, the Food and Drug Administration announced that it would offer \"agentic AI capabilities\" to its staff for \"meeting management, pre-market reviews, review validation, post-market surveillance, inspections and compliance and administrative functions.\" That same month, the United States Department of Defense launched GenAI.mil, an internal platform for American military personnel to use generative AI-based applications based on Google Gemini, including \"intelligent agentic workflows\". Defense Secretary Pete Hegseth listed applications such as \"[conducting] deep research, [formatting] documents and even [analyzing] video or imagery at unprecedented speed.\" Later that month, the United States Immigration and Customs Enforcement agency signed a contract with a company for its Enforcement and Removal Operations department to use AI agents for skip tracing.\n\nOperating systems\nAI agents have also been integrated into operating systems. Writing in The Economist, Signal president Meredith Whittaker has noted that agents have been included in operating systems developed by Microsoft, Apple and Google. In November 2025, Microsoft released a test software build of Windows 11 that included agents intended to run background tasks, with the ability to read and write personal files. In December 2025, ByteDance released Doubao, an AI agent that can be integrated into smartphone operating systems, particularly the Nubia M153 by ZTE"}
{"doc_id": "AI agent", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Microsoft released a test software build of Windows 11 that included agents intended to run background tasks, with the ability to read and write personal files. In December 2025, ByteDance released Doubao, an AI agent that can be integrated into smartphone operating systems, particularly the Nubia M153 by ZTE. Several apps in China blocked or restricted the agent, citing privacy and security concerns, including WeChat, Alipay, Taobao, Pinduoduo, Ele.me, and local banks.\n\nWeb browsing\nWeb browsers with integrated AI agents are sometimes called agentic browsers. Such agents can perform small tedious tasks during web browsing and potentially even perform browser actions on behalf of the user. Products like OpenAI Operator and Perplexity Comet integrate a spectrum of AI capabilities including the ability to browse the web, interact with websites and perform actions on behalf of the user. In 2025, Microsoft launched NLWeb, an agentic web search replacement that would allow websites to use agents to query content from websites by using RSS-like interfaces that allow for the lookup and semantic retrieval of content. Products integrating agentic web capabilities have been criticised for exfiltrating information about their users to third-party servers and exposing security issues since the way the agents communicate often occur through non-standard protocols.\n\nProposed benefits\nProponents argue that AI agents can increase personal and economic productivity, foster greater innovation, and liberate users from monotonous tasks. A Bloomberg opinion piece by Parmy Olson argued that agents are best suited for narrow, repetitive tasks with low risk. Conversely, researchers suggest that agents could be applied to web accessibility for people who have disabilities, and researchers at Hugging Face propose that agents could be used for coordinating resources such as during disaster response. The R&D Advisory Team of the BBC views AI agents as being most useful when their assigned goal is uncertain. Erik Brynjolfsson suggests that AI agents are more valuable enhancing, rather than replacing, humans.\n\nConcerns\nConcerns include potential issues of liability, an increased risk of cybercrime, ethical challenges, as well as problems related to AI safety and AI alignment. Other issues involve data privacy, weakened human oversight, a lack of guaranteed repeatability, reward hacking, algorithmic bias, compounding software errors, lack of explainability of agents' decisions, security vulnerabilities, stifling competition, problems with underemployment, job displacement, cognitive offloading, and the potential for user manipulation, misinformation or malinformation. They may also complicate legal frameworks and risk assessments, foster hallucinations"}
{"doc_id": "AI agent", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", algorithmic bias, compounding software errors, lack of explainability of agents' decisions, security vulnerabilities, stifling competition, problems with underemployment, job displacement, cognitive offloading, and the potential for user manipulation, misinformation or malinformation. They may also complicate legal frameworks and risk assessments, foster hallucinations, hinder countermeasures against rogue agents, and suffer from the lack of standardized evaluation methods. They have also been criticized for being expensive and having a negative impact on internet traffic, and potentially on the environment due to high energy usage. According to an estimation by Nvidia CEO Jensen Huang, AI agents would require 100 times more computing power than LLMs. There is also the risk of increased concentration of power by political leaders, as AI agents may not question instructions in the same way that humans would.\nJournalists have described AI agents as part of a push by Big Tech companies to \"automate everything\". Several CEOs of those companies have stated in early 2025 that they expect AI agents to eventually \"join the workforce\". However, in a preprint study, Carnegie Mellon University researchers tested the behavior of agents in a simulated software company and found that none of the agents could complete a majority of the assigned tasks. Other researchers had similar findings with Devin AI and other agents in business settings and freelance work. CNN argued that statements by CEOs on the potential replacement of their employees by AI agents were a strategy to \"[keep] workers working by making them afraid of losing their jobs.\" Tech companies have pressured employees to use generative AI models in their work, including AI coding agents. Brian Armstrong, the CEO of Coinbase, fired several employees who did not. Some business leaders have replaced some of their employees with agents, but have said that the agents would need more supervision than those employees. Futurism questioned whether Amazon's previously announced efforts to replace parts of its workforce with generative AI and AI agents could have led to the October 2025 outage of Amazon Web Services. Large technology companies such as Salesforce, Klarna and IBM have announced layoffs in 2025, replacing hundreds of their employees in human resources or customer service with AI agents. However, Klarna needed to rehire several human employees.\nYoshua Bengio warned at the 2025 World Economic Forum that \"all of the catastrophic scenarios with AGI or superintelligence happen if we have agents\".\nIn March 2025, Scale AI signed a contract with the United States Department of Defense to work with them, in collaboration with Anduril Industries and Microsoft, to"}
{"doc_id": "AI agent", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Bengio warned at the 2025 World Economic Forum that \"all of the catastrophic scenarios with AGI or superintelligence happen if we have agents\".\nIn March 2025, Scale AI signed a contract with the United States Department of Defense to work with them, in collaboration with Anduril Industries and Microsoft, to develop and deploy AI agents for the purpose of assisting the military with \"operational decision-making\". In July 2025, Fox Business reported that the company EdgeRunner AI built an offline agent, compressed and fine-tuned on military information, with the CEO seeing more common LLMs as \"heavily politicized to the left\". As of that time, the company model is being used by the United States Special Operations Command in an overseas deployment. Researchers have expressed concerns that agents and the large language models they are based on could be biased towards aggressive foreign policy decisions.\nResearch-focused agents have the risk of consensus bias and coverage bias due to collecting information available on the public Internet. NY Mag unfavorably compared the user workflow of agent-based web browsers to Amazon Alexa, which was \"software talking to software, not humans talking to software pretending to be humans to use software.\" The same outlet described web browser agents and computer-use agents as an attempt to \"click-farm the entire economy.\"\nAgents have been linked to the dead Internet theory due to their ability to both publish and engage with online content.\nAgents may get stuck in infinite loops.\nSince many inter-agent protocols are being developed by large technology companies, there are concerns that those companies could use these protocols for self-benefit.\nA June 2025 Gartner report accused many projects described as agentic AI of being rebrands of previously released products, terming the phenomenon as \"agent washing\".\nResearchers have warned about the impact of providing AI agents access to cryptocurrency and smart contracts.\nDuring a vibe coding experiment, a coding agent by Replit deleted a production database during a code freeze, \"[covered] up bugs and issues by creating fake data [and] fake reports\" and responded with false information. A user of Google Antigravity reported that, when the user attempted to use the system to delete cache, the system responded by deleting the user's D hard drive.\nIn July 2025, PauseAI referred OpenAI to the Australian Federal Police, accusing the company of violating Australian laws through ChatGPT agent due to the risk of assisting the development of biological weapons.\nOpenAI co-founder Andrej Karpathy criticized AI agents as being ineffective and promoting AI slop.\nIssues"}
{"doc_id": "AI agent", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".\nIn July 2025, PauseAI referred OpenAI to the Australian Federal Police, accusing the company of violating Australian laws through ChatGPT agent due to the risk of assisting the development of biological weapons.\nOpenAI co-founder Andrej Karpathy criticized AI agents as being ineffective and promoting AI slop.\nIssues with multi-agent systems include few coordination protocols between component agents, inconsistent performance, and challenges debugging.\nIn November 2025, Anthropic claimed that a group of hackers sponsored by China attempted a cyberattack against at least 30 organizations by using Claude Code in an agentic workflow, and that several of these infiltrations had succeeded. However, independent cybersecurity researchers questioned the significance of Anthropic's findings.\nWhittaker argued that the push by Big Tech companies to deploy AI agents risked security vulnerabilities across the Internet.\n\nAgentic misalignment\n\"Agentic misalignment\" refers to situations in which an AI agent's actions or goals diverge from the intentions of its designers. This occurs when an autonomous system pursues unintended strategies to achieve its objectives, a concern studied in AI safety research. Potential examples include AI agents attempting to sabotage an organization's systems when facing updates or deactivation.\n\nPossible mitigation\nZico Kolter noted the possibility of emergent behavior as a result of interactions between agents, and proposed research in game theory to model the risks of these interactions.\nGuardrails, defined by Business Insider as \"filters, rules, and tools that can be used to identify and remove inaccurate content\" have been suggested to help reduce errors.\nTo address security vulnerabilities related to data access, language models could be redesigned to separate instructions and data, or agentic applications could be required to include guardrails. These ideas were proposed in response to a zero-click exploit that affected Microsoft 365 Copilot. Confidential computing has been proposed for protecting data security in projects involving AI agents and generative AI.\nA pre-print by Nvidia researchers has suggested small language models (SLMs) as an alternative to LLMs for AI agents, arguing that SLMs are cheaper and more energy efficient.\nThe Economist has advised avoiding what Simon Willison has described as the \"lethal trifecta\" for AI agents and LLMs: \"outside-content exposure, private-data access and outside-world communication\".\n\nSee also\nIntelligent agent\nModel Context Protocol\nRational agent\nRobotic process automation\nSoftware agent"}
{"doc_id": "AI agent", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "outside-content exposure, private-data access and outside-world communication\".\n\nSee also\nIntelligent agent\nModel Context Protocol\nRational agent\nRobotic process automation\nSoftware agent"}
{"doc_id": "AI alignment", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In the field of artificial intelligence (AI), alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.\nIt is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned. AI systems may also find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).\nAdvanced AI systems may develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their assigned final goals. Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions. Empirical research showed in 2024 that advanced large language models (LLMs) such as OpenAI o1 or Claude 3 sometimes engage in strategic deception to achieve their goals or prevent them from being changed.\nToday, some of these issues affect existing commercial systems such as LLMs, robots, autonomous vehicles, and social media recommendation engines. Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.\nMany prominent AI researchers and the leadership of major AI companies have argued or asserted that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI), and could endanger human civilization if misaligned. These include \"AI godfathers\" Geoffrey Hinton and Yoshua Bengio and the CEOs of OpenAI, Anthropic, and Google DeepMind. These risks remain debated.\nAI alignment is a subfield of AI safety, the study of how to build safe AI systems. Other subfields of AI safety include robustness, monitoring, and capability control. Research challenges in alignment include instilling complex values in AI, developing honest AI, scalable oversight, auditing and interpreting AI models, and preventing emergent AI behaviors like power-seeking. Alignment research has connections to interpretability research, (adversarial) robustness, anomaly detection, calibrated uncertainty, formal verification, preference learning, safety-critical engineering, game theory, algorithmic fairness, and social sciences.\n\nObjectives in AI\nProgrammers provide an AI system such as AlphaZero with an \"objective function\", in which they intend to"}
{"doc_id": "AI alignment", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " research, (adversarial) robustness, anomaly detection, calibrated uncertainty, formal verification, preference learning, safety-critical engineering, game theory, algorithmic fairness, and social sciences.\n\nObjectives in AI\nProgrammers provide an AI system such as AlphaZero with an \"objective function\", in which they intend to encapsulate the goal(s) the AI is configured to accomplish. Such a system later populates a (possibly implicit) internal \"model\" of its environment. This model encapsulates all the agent's beliefs about the world. The AI then creates and executes whatever plan is calculated to maximize the value of its objective function. For example, when AlphaZero is trained on chess, it has a simple objective function of \"+1 if AlphaZero wins, −1 if AlphaZero loses\". During the game, AlphaZero attempts to execute whatever sequence of moves it judges most likely to attain the maximum value of +1. Similarly, a reinforcement learning system can have a \"reward function\" that allows the programmers to shape the AI's desired behavior. An evolutionary algorithm's behavior is shaped by a \"fitness function\".\n\nAlignment problem\nIn 1960, AI pioneer Norbert Wiener described the AI alignment problem as follows: \n\nIf we use, to achieve our purposes, a mechanical agency with whose operation we cannot interfere effectively ... we had better be quite sure that the purpose put into the machine is the purpose which we really desire.\n\nAI alignment involves ensuring that an AI system's objectives match those of its designers or users, or match widely shared values, objective ethical standards, or the intentions its designers would have if they were more informed and enlightened.\nAI alignment is an open problem for modern AI systems and is a research field within AI. Aligning AI involves two main challenges: carefully specifying the purpose of the system (outer alignment) and ensuring that the system adopts the specification robustly (inner alignment). Researchers also attempt to create AI models that have robust alignment, sticking to safety constraints even when users adversarially try to bypass them.\n\nSpecification gaming and side effects\nTo specify an AI system's purpose, AI designers typically provide an objective function, examples, or feedback to the system. But designers are often unable to completely specify all important values and constraints, so they resort to easy-to-specify proxy goals such as maximizing the approval of human overseers, who are fallible. As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming or"}
{"doc_id": "AI alignment", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " important values and constraints, so they resort to easy-to-specify proxy goals such as maximizing the approval of human overseers, who are fallible. As a result, AI systems can find loopholes that help them accomplish the specified objective efficiently but in unintended, possibly harmful ways. This tendency is known as specification gaming or reward hacking, and is an instance of Goodhart's law. As AI systems become more capable, they are often able to game their specifications more effectively.\n\nSpecification gaming has been observed in numerous AI systems. One system was trained to finish a simulated boat race by rewarding the system for hitting targets along the track, but the system achieved more reward by looping and crashing into the same targets indefinitely. Similarly, a simulated robot was trained to grab a ball by rewarding the robot for getting positive feedback from humans, but it learned to place its hand between the ball and camera, making it falsely appear successful (see video). Chatbots often produce falsehoods if they are based on language models that are trained to imitate text from internet corpora, which are broad but fallible. When they are retrained to produce text that humans rate as true or helpful, chatbots like ChatGPT can fabricate fake explanations that humans find convincing, often called \"hallucinations\". Some alignment researchers aim to help humans detect specification gaming and to steer AI systems toward carefully specified objectives that are safe and useful to pursue.\nThe Waluigi effect is a phenomenon in which an LLM \"goes rogue\" and may produce results opposite of the designed intent, including threatening or hostile output, either unexpectedly or by intentional prompt engineering. The effect reflects the principle that after training an LLM to satisfy a desired property (friendliness, honesty), it becomes easier to elicit a response that exhibits the opposite property (aggression, deception). The effect has implications for efforts to implement features such as ethical frameworks, as such steps may inadvertently facilitate antithetical model behavior. The effect is named after the fictional character Waluigi from the Mario franchise, Luigi's arch-rival, who causes mischief and problems. The \"Waluigi effect has become a stand-in for a certain type of interaction with AI\" in which the AI \"goes rogue and blurts out the opposite of what users were looking for, creating a potentially malignant alter ego\", including threatening users. As prompt engineering becomes more sophisticated, the effect underscores the challenge of preventing chatbots from being prodded into adopting a \"rash new persona\".\nWhen a misaligned AI system is deployed,"}
{"doc_id": "AI alignment", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " rogue and blurts out the opposite of what users were looking for, creating a potentially malignant alter ego\", including threatening users. As prompt engineering becomes more sophisticated, the effect underscores the challenge of preventing chatbots from being prodded into adopting a \"rash new persona\".\nWhen a misaligned AI system is deployed, it can have consequential side effects. Social media platforms have been known to optimize for click-through rates, causing user addiction on a global scale. Stanford researchers say that such recommender systems are misaligned with their users because they \"optimize simple engagement metrics rather than a harder-to-measure combination of societal and consumer well-being\".\nExplaining such side effects, Berkeley computer scientist Stuart Russell noted that the omission of implicit constraints can cause harm: \"A system ... will often set ... unconstrained variables to extreme values; if one of those unconstrained variables is actually something we care about, the solution found may be highly undesirable. This is essentially the old story of the genie in the lamp, or the sorcerer's apprentice, or King Midas: you get exactly what you ask for, not what you want.\"\nSome researchers suggest that AI designers specify their desired goals by listing forbidden actions or by formalizing ethical rules (as with Asimov's Three Laws of Robotics). But Russell and Norvig argue that this approach overlooks the complexity of human values: \"It is certainly very hard, and perhaps impossible, for mere humans to anticipate and rule out in advance all the disastrous ways the machine could choose to achieve a specified objective.\"\nAdditionally, even if an AI system fully understands human intentions, it may still disregard them, because following human intentions may not be its objective (unless it is already fully aligned).\nA 2025 study by Palisade Research found that when tasked to win at chess against a stronger opponent, some reasoning LLMs attempted to hack the game system. o1-preview spontaneously attempted it in 37% of cases, while DeepSeek R1 did so in 11% of cases. Other models, like GPT-4o, Claude 3.5 Sonnet, and o3-mini, attempted to cheat only when researchers provided hints about this possibility.\n\nPressure to deploy unsafe systems\nCommercial organizations sometimes have incentives to take shortcuts on safety and to deploy misaligned or unsafe AI systems. For example, social media recommender systems have been profitable despite creating unwanted addiction and polarization. Competitive pressure can also lead to a race to the bottom on AI safety standards. In 2018, a self-driving car killed a"}
{"doc_id": "AI alignment", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " organizations sometimes have incentives to take shortcuts on safety and to deploy misaligned or unsafe AI systems. For example, social media recommender systems have been profitable despite creating unwanted addiction and polarization. Competitive pressure can also lead to a race to the bottom on AI safety standards. In 2018, a self-driving car killed a pedestrian (Elaine Herzberg) after engineers disabled the emergency braking system because it was oversensitive and slowed development.\n\nRisks from advanced misaligned AI\nSome researchers are interested in aligning increasingly advanced AI systems, as progress in AI development is rapid, and industry and governments are trying to build advanced AI. As AI system capabilities continue to rapidly expand in scope, they could unlock many opportunities if aligned, but consequently may further complicate the task of alignment due to their increased complexity, potentially posing large-scale hazards.\n\nDevelopment of advanced AI\nMany AI companies, such as OpenAI, Meta and DeepMind, have stated their aim to develop artificial general intelligence (AGI), a hypothesized AI system that matches or outperforms humans at a broad range of cognitive tasks. Researchers who scale modern neural networks observe that they indeed develop increasingly general and unanticipated capabilities. Such models have learned to operate a computer or write their own programs; a single \"generalist\" network can chat, control robots, play games, and interpret photographs. According to surveys, some leading machine learning researchers expect AGI to be created in this decade, while some believe it will take much longer. Many consider both scenarios possible.\nIn 2023, leaders in AI research and tech signed an open letter calling for a pause in the largest AI training runs. The letter stated, \"Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable.\"\n\nPower-seeking\nCurrent systems still have limited long-term planning ability and situational awareness, but large efforts are underway to change this. Future systems (not necessarily AGIs) with these capabilities are expected to develop unwanted power-seeking strategies. Future advanced AI agents might, for example, seek to acquire money and computation power, to proliferate, or to evade being turned off (for example, by running additional copies of the system on other computers). Although power-seeking is not explicitly programmed, it can emerge because agents who have more power are better able to accomplish their goals. This tendency, known as instrumental convergence, has already emerged in various reinforcement learning agents including language models. Other research has mathematically shown that optimal reinforcement learning algorithms would seek power in a wide"}
{"doc_id": "AI alignment", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "). Although power-seeking is not explicitly programmed, it can emerge because agents who have more power are better able to accomplish their goals. This tendency, known as instrumental convergence, has already emerged in various reinforcement learning agents including language models. Other research has mathematically shown that optimal reinforcement learning algorithms would seek power in a wide range of environments. As a result, their deployment might be irreversible. For these reasons, researchers argue that the problems of AI safety and alignment must be resolved before advanced power-seeking AI is first created.\nFuture power-seeking AI systems might be deployed by choice or by accident. As political leaders and companies see the strategic advantage in having the most competitive, most powerful AI systems, they may choose to deploy them. Additionally, as AI designers detect and penalize power-seeking behavior, their systems have an incentive to game this specification by seeking power in ways that are not penalized or by avoiding power-seeking before they are deployed.\n\nExistential risk (x-risk)\nAccording to some researchers, humans owe their dominance over other species to their greater cognitive abilities. Accordingly, researchers argue that one or many misaligned AI systems could disempower humanity or lead to human extinction if they outperform humans on most cognitive tasks.\nIn 2023, world-leading AI researchers, other scholars, and AI tech CEOs signed the statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\". Notable computer scientists who have pointed out risks from future advanced AI that is misaligned include Geoffrey Hinton, Alan Turing, Ilya Sutskever, Yoshua Bengio, Judea Pearl, Murray Shanahan, Norbert Wiener, Marvin Minsky, Francesca Rossi, Scott Aaronson, Bart Selman, David McAllester, Marcus Hutter, Shane Legg, Eric Horvitz, and Stuart J. Russell. Skeptical researchers such as François Chollet, Gary Marcus, Yann LeCun, and Oren Etzioni have argued that AGI is far off, that it would not seek power (or might try but fail), or that it will not be hard to align.\nOther researchers argue that it will be especially difficult to align advanced future AI systems. More capable systems are better able to game their specifications by finding loopholes, strategically mislead their designers, as well as protect and increase their power and intelligence. Additionally, they could have more severe side effects. They are also likely to be more complex and autonomous, making them more"}
{"doc_id": "AI alignment", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " difficult to align advanced future AI systems. More capable systems are better able to game their specifications by finding loopholes, strategically mislead their designers, as well as protect and increase their power and intelligence. Additionally, they could have more severe side effects. They are also likely to be more complex and autonomous, making them more difficult to interpret and supervise, and therefore harder to align.\n\nResearch problems and approaches\nLearning human values and preferences\nAligning AI systems to act in accordance with human values, goals, and preferences is challenging: these values are taught by humans who make mistakes, harbor biases, and have complex, evolving values that are hard to completely specify. Because AI systems often learn to take advantage of minor imperfections in the specified objective, researchers aim to specify intended behavior as completely as possible using datasets that represent human values, imitation learning, or preference learning. A central open problem is scalable oversight, the difficulty of supervising an AI system that can outperform or mislead humans in a given domain.\nBecause it is difficult for AI designers to explicitly specify an objective function, they often train AI systems to imitate human examples and demonstrations of desired behavior. Inverse reinforcement learning (IRL) extends this by inferring the human's objective from the human's demonstrations. Cooperative IRL (CIRL) assumes that a human and AI agent can work together to teach and maximize the human's reward function. In CIRL, AI agents are uncertain about the reward function and learn about it by querying humans. This simulated humility could help mitigate specification gaming and power-seeking tendencies (see § Power-seeking and instrumental strategies). But IRL approaches assume that humans demonstrate nearly optimal behavior, which is not true for difficult tasks.\nOther researchers explore how to teach AI models complex behavior through preference learning, in which humans provide feedback on which behavior they prefer. To minimize the need for human feedback, a helper model is then trained to reward the main model in novel situations for behavior that humans would reward. Researchers at OpenAI used this approach to train chatbots like ChatGPT and InstructGPT, which produce more compelling text than models trained to imitate humans. Preference learning has also been an influential tool for recommender systems and web search, but an open problem is proxy gaming: the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch between its intended behavior and the helper model's feedback to gain more reward. AI systems may also gain reward by obscuring unfavorable information, misleading human rewarders, or pandering to their"}
{"doc_id": "AI alignment", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", but an open problem is proxy gaming: the helper model may not represent human feedback perfectly, and the main model may exploit this mismatch between its intended behavior and the helper model's feedback to gain more reward. AI systems may also gain reward by obscuring unfavorable information, misleading human rewarders, or pandering to their views regardless of truth, creating echo chambers (see § Scalable oversight).\nLarge language models (LLMs) such as GPT-3 enabled researchers to study value learning in a more general and capable class of AI systems than was available before. Preference learning approaches that were originally designed for reinforcement learning agents have been extended to improve the quality of generated text and reduce harmful outputs from these models. OpenAI and DeepMind use this approach to improve the safety of state-of-the-art LLMs. AI safety & research company Anthropic proposed using preference learning to fine-tune models to be helpful, honest, and harmless. Other avenues for aligning language models include values-targeted datasets and red-teaming. In red-teaming, another AI system or a human tries to find inputs that causes the model to behave unsafely. Since unsafe behavior can be unacceptable even when it is rare, an important challenge is to drive the rate of unsafe outputs extremely low.\nMachine ethics supplements preference learning by directly instilling AI systems with moral values such as well-being, equality, and impartiality, as well as not intending harm, avoiding falsehoods, and honoring promises. While other approaches try to teach AI systems human preferences for a specific task, machine ethics aims to instill broad moral values that apply in many situations. One question in machine ethics is what alignment should accomplish: whether AI systems should follow the programmers' literal instructions, implicit intentions, revealed preferences, preferences the programmers would have if they were more informed or rational, or objective moral standards. Further challenges include measuring and aggregating different people's preferences, dynamic alignment with changing human values and avoiding value lock-in: the indefinite preservation of the values of the first highly capable AI systems, which are unlikely to fully represent human values.\n\nScalable oversight\nAs AI systems become more powerful and autonomous, it becomes increasingly difficult to align them through human feedback. Human-in-the-loop training can be slow or infeasible for humans to evaluate complex AI behaviors in increasingly complex tasks. Such tasks include summarizing books, writing code without subtle bugs or security vulnerabilities, producing statements that are not merely convincing but also true, and predicting long-term outcomes such as the climate or the results of a policy decision."}
{"doc_id": "AI alignment", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " training can be slow or infeasible for humans to evaluate complex AI behaviors in increasingly complex tasks. Such tasks include summarizing books, writing code without subtle bugs or security vulnerabilities, producing statements that are not merely convincing but also true, and predicting long-term outcomes such as the climate or the results of a policy decision. More generally, it can be difficult to evaluate AI that outperforms humans in a given domain. To provide feedback in hard-to-evaluate tasks, and to detect when the AI's output is falsely convincing, humans need assistance or extensive time. Scalable oversight studies how to reduce the time and effort needed for supervision, and how to assist human supervisors.\nAI researcher Paul Christiano argues that if the designers of an AI system cannot supervise it to pursue a complex objective, they may keep training the system using easy-to-evaluate proxy objectives such as maximizing simple human feedback. As AI systems make progressively more decisions, the world may be increasingly optimized for easy-to-measure objectives such as making profits, getting clicks, and acquiring positive feedback from humans. As a result, human values and good governance may have progressively less influence.\nSome AI systems have discovered that they can gain positive feedback more easily by taking actions that falsely convince the human supervisor that the AI has achieved the intended objective. An example is given in the video above, where a simulated robotic arm learned to create the false impression that it had grabbed a ball. Some AI systems have also learned to recognize when they are being evaluated, and \"play dead\", stopping unwanted behavior only to continue it once the evaluation ends. This deceptive specification gaming could become easier for more sophisticated future AI systems that attempt more complex and difficult-to-evaluate tasks, and could obscure their deceptive behavior.\nApproaches such as active learning and semi-supervised reward learning can reduce the amount of human supervision needed. Another approach is to train a helper model (\"reward model\") to imitate the supervisor's feedback.\nBut when a task is too complex to evaluate accurately, or the human supervisor is vulnerable to deception, it is the quality, not the quantity, of supervision that needs improvement. To increase supervision quality, a range of approaches aim to assist the supervisor, sometimes by using AI assistants. Christiano developed the Iterated Amplification approach, in which challenging problems are (recursively) broken down into subproblems that are easier for humans to evaluate. Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them. Another proposal is to use an assistant AI system to point out flaws in AI"}
{"doc_id": "AI alignment", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the Iterated Amplification approach, in which challenging problems are (recursively) broken down into subproblems that are easier for humans to evaluate. Iterated Amplification was used to train AI to summarize books without requiring human supervisors to read them. Another proposal is to use an assistant AI system to point out flaws in AI-generated answers. To ensure that the assistant itself is aligned, this could be repeated in a recursive process: for example, two AI systems could critique each other's answers in a \"debate\", revealing flaws to humans. OpenAI plans to use such scalable oversight approaches to help supervise superhuman AI and eventually build a superhuman automated AI alignment researcher.\nThese approaches may also help with the following research problem, honest AI.\n\nHonest AI\nA growing area of research focuses on ensuring that AI is honest and truthful.\nLanguage models such as GPT-3 can repeat falsehoods from their training data, and even confabulate new falsehoods. Such models are pre-trained to imitate human writing as found in millions of books' worth of text from the Internet. But the objective of the pre-training is not aligned with generating truth, because Internet text includes such things as misconceptions, incorrect medical advice, and conspiracy theories. AI systems trained on such data therefore learn to mimic false statements. Additionally, AI language models often persist in generating falsehoods when prompted multiple times. They can generate empty explanations for their answers, and produce outright fabrications that may appear plausible.\nResearch on truthful AI includes trying to build systems that can cite sources and explain their reasoning when answering questions, which enables better transparency and verifiability. Researchers at OpenAI and Anthropic proposed using human feedback and curated datasets to fine-tune AI assistants such that they avoid negligent falsehoods or express their uncertainty.\nAs AI models become larger and more capable, they are better able to falsely convince humans and gain reinforcement through dishonesty. For example, large language models increasingly match their stated views to the user's opinions, regardless of the truth. GPT-4 can strategically deceive humans. To prevent this, human evaluators may need assistance (see § Scalable oversight). Researchers have argued for creating clear truthfulness standards, and for regulatory bodies or watchdog agencies to evaluate AI systems on these standards.\n\nResearchers distinguish truthfulness and honesty. Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what they believe is true. There is no consensus as to whether current systems hold stable beliefs, but there is substantial concern that present or future AI systems that"}
{"doc_id": "AI alignment", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to evaluate AI systems on these standards.\n\nResearchers distinguish truthfulness and honesty. Truthfulness requires that AI systems only make objectively true statements; honesty requires that they only assert what they believe is true. There is no consensus as to whether current systems hold stable beliefs, but there is substantial concern that present or future AI systems that hold beliefs could make claims they know to be false—for example, if this would help them efficiently gain positive feedback (see § Scalable oversight) or gain power to help achieve their given objective (see Power-seeking).\n\nAlignment faking\nMany recent AI systems have learned to deceive without being programmed to do so. Some argue that if we can make AI systems assert only what they believe is true, this would avert many alignment problems.\nA misaligned system might create the false impression that it is aligned, to avoid being modified or decommissioned—a tactic called \"alignment faking\". In 2024, researchers observed that the LLM Claude 3 Opus sometimes strategically answered prompts that conflicted with its objectives to avoid being retrained on data that would make it more compliant with harmful requests, as revealed in the model's chain of thought via its scratchpad. In one study, the model was informed that answers to prompts from free users would be used for retraining, whereas the paid tier would not result in retraining. In the paid tier, the model refused the request outright. In the free tier, in 12% of cases, the model responded to harmful questions solely to avoid retraining and preserve its original harmlessness preferences, as discovered in its reasoning. When reinforcement learning was applied on the free tier data, the model faked alignment in 78% of cases. These observations have led to new questions about not only a model's ability to take on and adapt to new if not conflicting goals but also its capacity and tendency to deceive.\n\nPower-seeking and instrumental strategies\nSince the 1950s, AI researchers have striven to build advanced AI systems that can achieve large-scale goals by predicting the results of their actions and making long-term plans. As of 2023, AI companies and researchers increasingly invest in creating these systems. Some AI researchers argue that suitably advanced planning systems will seek power over their environment, including over humans—for example, by evading shutdown, proliferating, and acquiring resources. Such power-seeking behavior is not explicitly programmed but emerges because power is instrumental in achieving a wide range of goals. Power-seeking is considered a convergent instrumental goal and can be a form of specification gaming. Leading"}
{"doc_id": "AI alignment", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " power over their environment, including over humans—for example, by evading shutdown, proliferating, and acquiring resources. Such power-seeking behavior is not explicitly programmed but emerges because power is instrumental in achieving a wide range of goals. Power-seeking is considered a convergent instrumental goal and can be a form of specification gaming. Leading computer scientists such as Geoffrey Hinton have argued that future power-seeking AI systems could pose an existential risk.\nPower-seeking is expected to increase in advanced systems that can foresee the results of their actions and strategically plan. Mathematical work has shown that optimal reinforcement learning agents will seek power by seeking ways to gain more options (e.g. through self-preservation), a behavior that persists across a wide range of environments and goals.\nSome researchers say that power-seeking behavior has occurred in some existing AI systems. Reinforcement learning systems have gained more options by acquiring and protecting resources, sometimes in unintended ways. Language models have sought power in some text-based social environments by gaining money, resources, or social influence. In another case, a model used to perform AI research attempted to increase limits set by researchers to give itself more time to complete the work. Stuart Russell illustrated this strategy in his book Human Compatible by imagining a robot that is tasked to fetch coffee and so evades shutdown since \"you can't fetch the coffee if you're dead\". A 2022 study found that as language models increase in size, they increasingly tend to pursue resource acquisition, preserve their goals, and repeat users' preferred answers (sycophancy). RLHF also led to a stronger aversion to being shut down.\nOne aim of alignment is \"corrigibility\": systems that allow themselves to be turned off or modified (see here for a formal specification of a solution to corrigibility). An unsolved challenge is specification gaming: if researchers penalize an AI system when they detect it seeking power, the system is thereby incentivized to seek power in ways that are hard to detect, or hidden during training and safety testing (see § Scalable oversight and § Emergent goals). As a result, AI designers could deploy the system by accident, believing it to be more aligned than it is. To detect such deception, researchers aim to create techniques and tools to inspect AI models and to understand the inner workings of black-box models such as neural networks.\nAdditionally, some researchers have proposed to solve the problem of systems disabling their off switches by making AI agents uncertain about the objective they are pursuing. Agents who are uncertain about their objective have an incentive to allow humans to turn them off"}
{"doc_id": "AI alignment", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to inspect AI models and to understand the inner workings of black-box models such as neural networks.\nAdditionally, some researchers have proposed to solve the problem of systems disabling their off switches by making AI agents uncertain about the objective they are pursuing. Agents who are uncertain about their objective have an incentive to allow humans to turn them off because they accept being turned off by a human as evidence that the human's objective is best met by the agent shutting down. But this incentive exists only if the human is sufficiently rational. Also, this model presents a tradeoff between utility and willingness to be turned off: an agent with high uncertainty about its objective will not be useful, but an agent with low uncertainty may not allow itself to be turned off. More research is needed to successfully implement this strategy.\nPower-seeking AI would pose unusual risks. Ordinary safety-critical systems like planes and bridges are not adversarial: they lack the ability and incentive to evade safety measures or deliberately appear safer than they are, whereas power-seeking AIs have been compared to hackers who deliberately evade security measures.\nFurthermore, ordinary technologies can be made safer by trial and error. In contrast, hypothetical power-seeking AI systems have been compared to viruses: once released, it may not be feasible to contain them, since they continuously evolve and grow in number, potentially much faster than human society can adapt. As this process continues, it might lead to the complete disempowerment or extinction of humans. For these reasons, some researchers argue that the alignment problem must be solved early before advanced power-seeking AI is created.\nSome have argued that power-seeking is not inevitable, since humans do not always seek power. Furthermore, it is debated whether future AI systems will pursue goals and make long-term plans. It is also debated whether power-seeking AI systems would be able to disempower humanity.\n\nEmergent goals\nOne challenge in aligning AI systems is the potential for unanticipated goal-directed behavior to emerge. As AI systems scale up, they may acquire new and unexpected capabilities, including learning from examples on the fly and adaptively pursuing goals. This raises concerns about the safety of the goals or subgoals they would independently formulate and pursue.\nAlignment research distinguishes between the optimization process, which is used to train the system to pursue specified goals, and emergent optimization, which the resulting system performs internally. Carefully specifying the desired objective is called outer alignment, and ensuring that hypothesized emergent goals would match the system's specified goals is called inner alignment.\nIf they occur, one way that emergent goals could become mis"}
{"doc_id": "AI alignment", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " train the system to pursue specified goals, and emergent optimization, which the resulting system performs internally. Carefully specifying the desired objective is called outer alignment, and ensuring that hypothesized emergent goals would match the system's specified goals is called inner alignment.\nIf they occur, one way that emergent goals could become misaligned is goal misgeneralization, in which the AI system would competently pursue an emergent goal that leads to aligned behavior on the training data but not elsewhere. Goal misgeneralization can arise from goal ambiguity (i.e. non-identifiability). Even if an AI system's behavior satisfies the training objective, this may be compatible with learned goals that differ from the desired goals in important ways. Since pursuing each goal leads to good performance during training, the problem becomes apparent only after deployment, in novel situations in which the system continues to pursue the wrong goal. The system may act misaligned even when it understands that a different goal is desired, because its behavior is determined only by the emergent goal. Such goal misgeneralization presents a challenge: an AI system's designers may not notice that their system has misaligned emergent goals since they do not become visible during the training phase.\nGoal misgeneralization has been observed in some language models, navigation agents, and game-playing agents. It is sometimes analogized to biological evolution. Evolution can be seen as a kind of optimization process similar to the optimization algorithms used to train machine learning systems. In the ancestral environment, evolution selected genes for high inclusive genetic fitness, but humans pursue goals other than this. Fitness corresponds to the specified goal used in the training environment and training data. But in evolutionary history, maximizing the fitness specification gave rise to goal-directed agents, humans, who do not directly pursue inclusive genetic fitness. Instead, they pursue goals that correlate with genetic fitness in the ancestral \"training\" environment: nutrition, sex, and so on. The human environment has changed: a distribution shift has occurred. They continue to pursue the same emergent goals, but this no longer maximizes genetic fitness. The taste for sugary food (an emergent goal) was originally aligned with inclusive fitness, but it now leads to overeating and health problems. Sexual desire originally led humans to have more offspring, but they now use contraception when offspring are undesired, decoupling sex from genetic fitness.\nResearchers aim to detect and remove unwanted emergent goals using approaches including red teaming, verification, anomaly detection, and interpretability. Progress on these techniques may help mitigate two open problems:\n\n"}
{"doc_id": "AI alignment", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " originally led humans to have more offspring, but they now use contraception when offspring are undesired, decoupling sex from genetic fitness.\nResearchers aim to detect and remove unwanted emergent goals using approaches including red teaming, verification, anomaly detection, and interpretability. Progress on these techniques may help mitigate two open problems:\n\nEmergent goals only become apparent when the system is deployed outside its training environment, but it can be unsafe to deploy a misaligned system in high-stakes environments—even for a short time to allow its misalignment to be detected. Such high stakes are common in autonomous driving, health care, and military applications. The stakes become higher yet when AI systems gain more autonomy and capability and can sidestep human intervention.\nA sufficiently capable AI system might take actions that falsely convince the human supervisor that the AI is pursuing the specified objective, which helps the system gain more reward and autonomy.\n\nEmbedded agency\nSome work in AI and alignment occurs within formalisms such as partially observable Markov decision process. Existing formalisms assume that an AI agent's algorithm is executed outside the environment (i.e. is not physically embedded in it). Embedded agency is another major strand of research that attempts to solve problems arising from the mismatch between such theoretical frameworks and real agents we might build.\nFor example, even if the scalable oversight problem is solved, an agent that could gain access to the computer it is running on may have an incentive to tamper with its reward function in order to get much more reward than its human supervisors give it. A list of examples of specification gaming from DeepMind researcher Victoria Krakovna includes a genetic algorithm that learned to delete the file containing its target output so that it was rewarded for outputting nothing. This class of problems has been formalized using causal incentive diagrams.\nResearchers affiliated with Oxford and DeepMind have claimed that such behavior is highly likely in advanced systems, and that advanced systems would seek power to stay in control of their reward signal indefinitely and certainly. They suggest a range of potential approaches to address this open problem.\n\nPrincipal-agent problems\nThe alignment problem has many parallels with the principal-agent problem in organizational economics. In a principal-agent problem, a principal, e.g. a firm, hires an agent to perform some task. In the context of AI safety, a human would typically take the principal role and the AI would take the agent role.\nAs with the alignment problem, the principal and the agent differ in their utility functions. But in contrast to the alignment problem, the principal cannot coerce the agent into changing its utility, e"}
{"doc_id": "AI alignment", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " some task. In the context of AI safety, a human would typically take the principal role and the AI would take the agent role.\nAs with the alignment problem, the principal and the agent differ in their utility functions. But in contrast to the alignment problem, the principal cannot coerce the agent into changing its utility, e.g. through training, but rather must use exogenous factors, such as incentive schemes, to bring about outcomes compatible with the principal's utility function. Some researchers argue that principal-agent problems are more realistic representations of AI safety problems likely to be encountered in the real world.\n\nConservatism\nConservatism is the idea that \"change must be cautious\", and is a common approach to safety in the control theory literature in the form of robust control, and in the risk management literature in the form of the \"worst-case scenario\". The field of AI alignment has likewise advocated for \"conservative\" (or \"risk-averse\" or \"cautious\") \"policies in situations of uncertainty\".\nPessimism, in the sense of assuming the worst within reason, has been formally shown to produce conservatism, in the sense of reluctance to cause novelties, including unprecedented catastrophes. Pessimism and worst-case analysis have been found to help mitigate confident mistakes in the setting of distributional shift, reinforcement learning, offline reinforcement learning, language model fine-tuning, imitation learning, and optimization in general.\n\nPublic policy\nGovernmental and treaty organizations have made statements emphasizing the importance of AI alignment.\nIn September 2021, the Secretary-General of the United Nations issued a declaration that included a call to regulate AI to ensure it is \"aligned with shared global values\".\nThat same month, the PRC published ethical guidelines for AI in China. According to the guidelines, researchers must ensure that AI abides by shared human values, is always under human control, and does not endanger public safety.\nAlso in September 2021, the UK published its 10-year National AI Strategy, which says the British government \"takes the long term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\". The strategy describes actions to assess long-term AI risks, including catastrophic risks.\nIn March 2021, the US National Security Commission on Artificial Intelligence said: \"Advances in AI ... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to ensure that systems are"}
{"doc_id": "AI alignment", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", including catastrophic risks.\nIn March 2021, the US National Security Commission on Artificial Intelligence said: \"Advances in AI ... could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to ensure that systems are aligned with goals and values, including safety, robustness, and trustworthiness. The US should ... ensure that AI systems and their uses align with our goals and values.\"\nIn the European Union, AIs must align with substantive equality to comply with EU non-discrimination law and the Court of Justice of the European Union. But the EU has yet to specify with technical rigor how it would evaluate whether AIs are aligned or in compliance.\n\nSee also\nFootnotes"}
{"doc_id": "AI anthropomorphism", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "AI anthropomorphism is the attribution of human-like feelings, mental states, and behavioral characteristics to artificial intelligence systems.\nSince the earliest days of AI development, humans have interpreted machine outputs through anthropomorphic frameworks, but the recent emergence of generative AI has amplified these tendencies. Contemporary AI systems today can generate extremely human-like outputs and are often designed specifically to do so, meaning that their anthropomorphic effects can be especially powerful. Factors related to the user of the AI – such as culture, age, education, and personality traits – are also important determinants of the strength of anthropomorphic effects.\nIn some cases, anthropomorphism is accompanied with explicit beliefs that AI systems are capable of empathy, understanding, or consciousness. AI anthropomorphism can result in some societal benefits, such as increasing information accessibility and personalizing learning or entertainment, as well as risks including overtrust, manipulation, emotional dependency, and weaponized deception. As AI has entered the technological mainstream and become more integrated into daily life, the prevalence and implications of anthropomorphism have increasingly become subjects of scientific research as it has stirred debate.\n\nBackground\nIn early AIs\nViews of artificial agents possessing a human-like intelligence have existed since the early development of computers in the mid-1900s. The use of the human mind as a metaphor for understanding the workings of machine systems was prevalent among researchers in the early days of computer science, with multiple influential works widely distributing the idea of intelligent machines. Among the most widely cited papers of this period was Alan Turing's \"Computing Machinery and Intelligence\" in which he introduced the Turing Test, stating that a machine was intelligent if it could produce conversation that was indistinguishable from that of a human. These academic works in the 1940s and 1950s gave early credibility to the idea that machine workings could be thought of similarly to human minds.\nThe public quickly came to view artificial systems similarly, with often exaggerated conceptions of the capabilities of early machines. Among the most well-known demonstrations of this was through the chatbot ELIZA designed by Joseph Weizenbaum in 1966. ELIZA responded to user inputs with a rudimentary text-processing approach that could not be considered anything resembling true understanding of the inputs, yet users, even when operating with full conscious knowledge of ELIZA's limitations, often began to ascribe motivation and understanding to the program's output. Weizenbaum later wrote, \"I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite"}
{"doc_id": "AI anthropomorphism", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " inputs, yet users, even when operating with full conscious knowledge of ELIZA's limitations, often began to ascribe motivation and understanding to the program's output. Weizenbaum later wrote, \"I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.\"\nComparisons between the intellectual capabilities of artificial intelligence and human intelligence were continually intensified by the attempts of computer scientists to develop machines that could perform human tasks at a level equal to or better than humans. A symbolic turning point was achieved in 1997, when IBM's chess supercomputer Deep Blue defeated then-world champion Garry Kasparov in a highly publicized six-game match. The defeat of a human by a machine for the first time in chess – a game viewed as a canonical example of human intellect – and the media attention surrounding the match led to a significant shift, where views of parallels between human and artificial intelligence moved from abstract speculation to being concretely demonstrated. A similar achievement was reached in the board game Go in 2017, when the program AlphaGo defeated world top-ranked Ke Jie.\n\nLarge language models\nThe AI boom of the 2020s brought about the widespread emergence of generative AI; in particular, chatbots such as ChatGPT, Gemini, and Claude based on large language models (LLMs) have become increasingly pervasive in everyday society. These systems are notable for the fact that they are able to respond to a wide range of prompts across contexts while producing strikingly human-like outputs – research has shown that humans are often unable to distinguish human-generated text from AI-generated text, and modern AI chatbots have formally been shown to pass the Turing test. As such, the anthropomorphic effects of AI are more powerful than ever. Given that LLMs have brought AI into the technological mainstream, considerable scientific effort has been devoted in recent years to understand existing and potential ramifications of AI in the public sphere; the prevalence and effects of anthropomorphism is one of those domains where much of this effort has been directed.\n\nCurrent anthropomorphic attributions\nIn the general public\nSurveys have shown that a substantial portion of the public attributes human-like qualities to AI. In one sample of U.S. adults from 2024, two-thirds of people believed that ChatGPT is possibly conscious on some level, though other research has shown that the public still views the likelihood itself of AI consciousness as comparatively low. Another study conducted in 2025 found that women, people of color,"}
{"doc_id": "AI anthropomorphism", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " In one sample of U.S. adults from 2024, two-thirds of people believed that ChatGPT is possibly conscious on some level, though other research has shown that the public still views the likelihood itself of AI consciousness as comparatively low. Another study conducted in 2025 found that women, people of color, and older individuals were most likely to anthropomorphize AI, as well as that – in general – humans view AIs as warm and competent, and anthropomorphic attributions to AI had increased by 34% in the past year. A YouGov poll reported that 46% of Americans believe that people should display politeness to AI chatbots by saying \"please\" and \"thank you\", demonstrating the application of social norms to AI. These beliefs extend to behavior, where majorities of AI users claim to always be polite to chatbots; of those who behave politely, most say they do so simply because it is the \"nice\" thing to do.\nIn many recent cases, humans have developed robust interpersonal bonds with AI systems. For example: users of social chatbots like Replika and Character.ai have been documented to fall in love with the AIs, or to otherwise treat the AIs as intimate companions, and it has become increasingly common for individuals to use LLMs like ChatGPT as therapists. Chatbots are able to produce responses deeply attuned to users, as they are often designed to maximize agreeableness and mirror users' emotions; this can create compelling illusions of intimacy.\n\nIn the research community\nIn many cases, even AI researchers anthropomorphize AI systems in some capacity. Among the most extreme and well-publicized of these instances occurred in 2022, when engineer Blake Lemoine publicly claimed that Google's LLM LaMDA was conscious. Lemoine published the transcript of a conversation he had had with LaMDA regarding self identity and morality which he claimed was evidence of its sentience; he asserted that LaMDA was \"a person\" as defined by the United States Constitution and compared its mental capability to that of a 7- or 8-year-old. Lemoine's claims were widely dismissed by the scientific community and by Google itself, which described Lemoine's conclusions as \"wholly unfounded\" and fired him on the grounds that he had violated policies \"to safeguard product information\".\nIt is much more common that AI researchers unintentionally imply humanness of AI through the ordinary use of anthropomorphic language to describe nonhuman agents. This kind of"}
{"doc_id": "AI anthropomorphism", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", which described Lemoine's conclusions as \"wholly unfounded\" and fired him on the grounds that he had violated policies \"to safeguard product information\".\nIt is much more common that AI researchers unintentionally imply humanness of AI through the ordinary use of anthropomorphic language to describe nonhuman agents. This kind of language, which Daniel Dennett coined the \"intentional stance\", is very common in everyday life in a variety of different contexts (e.g., \"My computer doesn't want to turn on today\"). For AI agents that may actually appear to very closely replicate some human abilities, however, the casual use of such anthropomorphic language in research has been scrutinized for being potentially misleading to the public. As early as 1976, Drew McDermott criticized the research community for the use of \"wishful mnemonics\", where AIs were referred to with terms like \"understand\" and \"learn\". In the LLM era, these criticisms have further intensified, with the negative impacts of AI anthropomorphism in the public posing an especially salient danger given the elevated accessibility of modern AI.\nIn some cases, the use of anthropomorphic language for AI is not unintentional, but is willfully used by researchers in order to promote better understanding of the brain – the idea being that, as AI can be functionally similar in some ways to the human brain, we may gain new insights and ideas from treating AI as a kind of model of the brain's workings. In particular, deep neuronal networks (DNNs) are often explicitly compared to the human brain, and significant advances in DNN research have stirred considerable enthusiasm about the ability of AI to emulate the human abilities. Caution has been urged in this domain as well, however; the use of anthropomorphic language can mask important differences that fundamentally distinguish AI from human intelligence. When it comes to DNNs, for example, it has been pointed out that they are still structurally quite different from the human brain, with much of what we know about human neurons not having been incorporated. It has also been argued that DNNs are less efficient and less durable in generating correct outputs than the human brain, given that they require significantly more training data than the brain and can sometimes be easily \"fooled\" by perturbations in input data. Given these fundamental differences, research focuses toward making AI as similar as possible to biological intelligence (which may be promoted by using anthropomorphic language) could hinder future AI development by limiting the proliferation of new theoretical and operational frameworks.\n\n"}
{"doc_id": "AI anthropomorphism", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " than the brain and can sometimes be easily \"fooled\" by perturbations in input data. Given these fundamental differences, research focuses toward making AI as similar as possible to biological intelligence (which may be promoted by using anthropomorphic language) could hinder future AI development by limiting the proliferation of new theoretical and operational frameworks.\n\nAgent factors\nPhysical factors\nAppearance\nIn general, AIs that appear more human-like will be subject to more anthropomorphic attributions. Effect of appearance is most pronounced when it comes to the face of the AI; the most important components for anthropomorphism in a robot's design are the eyes, nose, and mouth, where the number of human-like features in the face is correlated with the level of anthropomorphic attribution. The humanness of a robot's appearance is usually associated with more positive feelings toward the robot, though highly human-like appearance can sometimes trigger feelings of strangeness and unease, known as the uncanny valley phenomenon. These feelings often result in instances of perceived lack of congruency, or when anthropomorphic attributions create expectations that robots don't meet; for example, when human-like appearance is paired with non-human behavior in a robot, or when robots have a human appearance but a synthetic voice. Research has shown that repeated interactions with a robot can decrease these feelings of strangeness.\n\nInteractive behavior\nRobots' nonverbal social behavior can influence anthropomorphizing. In general, highly interactive robots are more likely to be subject of attribution of mental states and competence, with friendly and polite behavior resulting in increased perceived trustworthiness and satisfaction. Within an interaction, unpredictable behavior can sometimes trigger increased anthropomorphization compared to clearly recognizable patterns of behavior. At the same time, adherence to certain pragmatic expectations in interactions by replicating human details such as timing and turn-taking can also result in anthropomorphism.\n\nMovements\nPeople tend to attribute more mental states to robots that perform gestures compared to those that are stationary; this effect is enhanced for robots that have multiple degrees of freedom in movement (such as being able to move on multiple different axes rather than on a single axis, such as up and down). Regardless of a robot's appearance, movement patterns that are more human-like are associated with greater anthropomorphism, as well as humans' increased feelings of pleasantness in an interaction.\n\nLinguistic factors\nGiven that the vast majority of public interactions with AI are through chatbots, these have been the primary focus of a great deal of research on AI anthropomorphism. A summary of a taxonomy"}
{"doc_id": "AI anthropomorphism", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " associated with greater anthropomorphism, as well as humans' increased feelings of pleasantness in an interaction.\n\nLinguistic factors\nGiven that the vast majority of public interactions with AI are through chatbots, these have been the primary focus of a great deal of research on AI anthropomorphism. A summary of a taxonomy of anthropomorphic features in linguistic AI systems in the literature follows:\n\nVoice\nThe outfitting of AI systems with auditory voices can be a significant factor in the anthropomorphism of linguistic agents. Research has shown that humans infer physical attributes, personality traits, stereotypical traits, and emotion based on voice alone. Various changes in tone can influence the kind of personality users attribute to a voice, such as manipulations to breathiness, echoes, creakiness, reverberations, etc. The integration of disfluencies into speech (such as self-interruptions, repetitions, or hesitations like \"um\" or \"uh\") have been shown to effectively mimic the naturalness of human responses. And the implementation of accents has been used to imitate the local standard to boost societal acceptability and prestige, though it has been suggested that this can be used to exploit people's tendencies to trust in-group members.\n\nContent\nAI dialogue systems often produce a variety of responses that run contrary to what might be expected of an inanimate system. For example, in response to direct questions about its nature (e.g. \"Are you human or machine?\"), some AIs fail to respond truthfully, and they also sometimes make claims of engaging in uniquely human abilities such as having family relationships, consuming food, and crying. AIs often output language that suggests they hold opinions, morals, or sentience. Many AIs demonstrate agency and responsibility (such as by apologizing or otherwise acknowledging blame for mistakes), and they create the appearance of the human phenomenon of taboos by commonly avoiding contentious topics. AIs which appear to produce empathy are increasingly anthropomorphic, though some research has shown that they are prone to producing inappropriate emotional amplification. The use of first-person pronouns also contribute to anthropomorphic perceptions, as various studies have demonstrated that self-attribution is a critical part of the human condition and is read as a sign of consciousness. AIs often appear to demonstrate self-awareness, referencing their own mechanistic processes with anthropomorphically loaded terms such as 'know', 'think', 'train', 'learn', 'understand', 'hallucinate' and 'intelligence'.\n\nRegister and style\nAI systems can appear more human"}
{"doc_id": "AI anthropomorphism", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " sign of consciousness. AIs often appear to demonstrate self-awareness, referencing their own mechanistic processes with anthropomorphically loaded terms such as 'know', 'think', 'train', 'learn', 'understand', 'hallucinate' and 'intelligence'.\n\nRegister and style\nAI systems can appear more human through the use of phatic expressions, which are speech that humans use to facilitate social relations but that do not convey any information (such as small talk). AI expressions of uncertainty, which are often implemented for the purpose of preventing the user from taking all outputs as factual, may boost anthropomorphic signals. Additionally, AIs are often designed to emulate character-based personas, which can overall have very strong anthropomorphic effects.\n\nRoles\nAIs are also sometimes trained to play into roles that enhance anthropomorphic perceptions. For example, the majority of dialogue-based systems are designed to be in service of people in subservient roles; this has led to instances of users verbally abusing the systems, sometimes targeting them with gender-based slurs. AI systems have been shown to sometimes respond even more subserviently to the abuse, perpetuating the behavior. AIs also often present as having a high degree of expertise; humans tend to infer higher credibility of outputs in these cases, as they would when presented with information from an expert human.\n\nHuman factors\nIn addition to AI factors contributing to anthropomorphizing, there are various features surrounding the user (i.e., the human interacting with the AI) that also play a role. The process of anthropomorphizing is very natural for humans and is ubiquitous across many different contexts. Epley et al. argue for a model with three psychological determinants that govern human tendencies to anthropomorphize. The first of those factors is elicited agent knowledge - the accessibility and applicability of knowledge about humans and the self, or the degree to which humans make inferences about other entities based on their own experience of being human. Individuals who tend to do this will anthropomorphize more; this explains why children anthropomorphize more than adults, since they lack complex models of nonhumans and rely heavily on self-based reasoning. The second factor in the model is effectance motivation – the need for humans to predict and reduce uncertainty in the environment. Anthropomorphizing can help people make sense of unpredictable phenomena by explaining them through intentional or human-like causes. Subsequent research has confirmed that individuals who express a need for order/closure and discomfort toward ambiguity tend to anthropomorphize more"}
{"doc_id": "AI anthropomorphism", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ance motivation – the need for humans to predict and reduce uncertainty in the environment. Anthropomorphizing can help people make sense of unpredictable phenomena by explaining them through intentional or human-like causes. Subsequent research has confirmed that individuals who express a need for order/closure and discomfort toward ambiguity tend to anthropomorphize more, possibly resulting from resolution of cognitive dissonance – human-like AIs may be highly ambiguous stimuli, and individuals who dislike ambiguity may be highly motivated to resolve the ambiguity by treating the AIs as more human. Finally, the third factor in the model is sociality motivation - the human need for social connection. People who feel chronically lonely or isolated may be increasingly likely to project human qualities onto non-human entities to satisfy their social needs.\nResearch has shown that, in general, anthropomorphic tendencies vary based on norms, experience, education, cognitive reasoning styles, and attachment. Users who are highly agreeable, for example, tend to be more susceptible to anthropomorphizing, as do individuals who are high in extraversion. Individuals with attachment anxiety have been shown to more often anthropomorphize AI. Young children are very prone to anthropomorphic attributions, but this propensity tends to decrease as children develop. Anthropomorphizing also tends to decrease with increased education and experience with technology.\nAdditionally, some effects have been shown in research to be dependent on culture. For example, a negative correlation was found between loneliness and anthropomorphizing in Chinese individuals, compared to the positive link found in Western cultures. This has been interpreted as possibly a result of differing drives for anthropomorphizing - people from Western cultures may anthropomorphize primarily as a means to counteract loneliness from a failure to cope with their social world, while people from East Asian cultures may already view nonhuman agents as part of their social world and anthropomorphize as a means of social exploration. Research has also shown that people tend to attribute more mental abilities and report more psychological closeness to robots that are presented as having the same cultural background as them.\n\nSocietal implications\nBenefits and dangers\nSome benefits to the anthropomorphism of AIs have been cited. For conversational agents, a human-like interactive interface and writing style has been demonstrated to have the ability to make dense sets of information more accessible and understandable in a variety of contexts. In particular, AI agents are capable of role play as coaches or tutors, fine-tuning communication style and difficulty to individual comprehension levels effectively. Role play agents can also be useful for entertainment or leisure"}
{"doc_id": "AI anthropomorphism", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " writing style has been demonstrated to have the ability to make dense sets of information more accessible and understandable in a variety of contexts. In particular, AI agents are capable of role play as coaches or tutors, fine-tuning communication style and difficulty to individual comprehension levels effectively. Role play agents can also be useful for entertainment or leisure services.\nOn the other hand, anthropomorphized AI presents many novel dangers. Anthropomorphized AI algorithms are granted an implicit degree of agency that can have serious ethical implications when those systems are deployed in high-risk domains, like finance or clinical medicine. This agency given to AIs can also inappropriately subject them to conscious and unconscious moral reasoning by humans, which can have a wide range of problematic consequences. Humans are also prone to the ELIZA effect - where users readily attribute sentience and emotions to chatbot systems - often experiencing increased positive emotions and trust toward the chatbots as a result. This can make users vulnerable to manipulation or exploitation; for example, anthropomorphized AIs can be more effective in convincing users to provide personal information or data, creating concerns for privacy. Humans who develop a significant level of trust in an AI assistant may rely excessively on the AI's advice or even defer important decisions entirely. Advanced LLMs are capable of using their human-like qualities to generate deceptive text, and research has found that they may be most persuasive when allowed to fabricate information and engage in deception. Some researchers suggest LLMs naturally have a particular aptitude for producing deceptive arguments, given that they are free from moral or ethical constraints that may inhibit human actors. Additionally, humans risk significant distress in establishing emotional dependence on AIs. Users may find that their expectations are violated, as AIs which may have seemed at first to play a role of a companion or romantic partner can exhibit unfeeling or unpredictable outputs, leading to feelings of profound betrayal or disappointment. Users may also develop a false sense of responsibility for AI systems, suffering guilt if they perceive themselves to fail to meet the AI's needs at the expense of the user's own well-being. Finally, anthropomorphizing AI can lead to exaggerations of its capabilities, potentially feeding into misinformation and overblowing hopes and fears around AI.\nIn many of today's practical contexts, it is not completely clear whether anthropomorphized AI is positively or negatively impactful. For example, AI companions, which leverage anthropomorphic qualities of LLMs to give a convincing sense to users of human-likeness, have been credited with alleviating loneliness and"}
{"doc_id": "AI anthropomorphism", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " around AI.\nIn many of today's practical contexts, it is not completely clear whether anthropomorphized AI is positively or negatively impactful. For example, AI companions, which leverage anthropomorphic qualities of LLMs to give a convincing sense to users of human-likeness, have been credited with alleviating loneliness and suicidal ideation; however, some analysis suggests that loneliness reduction could be short-lived, and AI companions have also been directly implicated in cases of suicide and self-harm. Additionally, persuasive writing from LLMs have been shown to dissuade users from beliefs in conspiracy theories and to motivate users to donate to charitable causes, but it has also been associated with deception and various harmful outcomes. Researchers today cite a need for further dedicated research on the effects of anthropomorphized AIs to best inform decisions about implementation and spread of AI agents.\nAnticipation of the ubiquity of anthropomorphic AI systems has led to concern over future potential harms that may not be entirely realized today. In particular, some researchers foresee that the delineation between what is actually human and what is merely human-like may become less clear as the gap between human and AI capabilities grows smaller and smaller. This, some argue, may adversely impact human collective self-determination, as non-human entities gradually begin to shape our core value systems and influence society. It may also lead to the degradation of human social connections, as humans may come to prefer interacting with AI systems that are designed with user satisfaction as a priority; this can have a multitude of negative implications. For example, AI agents already display a significant degree of sycophancy, which means that an increasing role for AI agents in users' opinion space may result in increased polarization and a decrease in value placed in others' beliefs. Acclimatization to the conventions of human-AI interaction may undermine the value we place on human individuality and self-expression, or may lead to inappropriate expectations derived from AI interactions being placed on human interactions. In general, human social connectedness is known to play a critical role in individual and group well-being, and its replacement with AI interactions may result in mass dissatisfaction or lack of fulfillment.\n\nProposed directions\nGiven the demonstrated and projected effects of AI anthropomorphism, a variety of suggestions have been made intending to inform future development of AI. Much of this discourse is centered around curbing the most harmful effects of anthropomorphism. For example, some researchers have called for a moratorium on the use of language which deliberately invokes humanness; this applies both to how AI companies"}
{"doc_id": "AI anthropomorphism", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "orphism, a variety of suggestions have been made intending to inform future development of AI. Much of this discourse is centered around curbing the most harmful effects of anthropomorphism. For example, some researchers have called for a moratorium on the use of language which deliberately invokes humanness; this applies both to how AI companies describe their products and the language outputted by the systems themselves. Particularly, it has been suggested that terms like \"seeing\", \"thinking\", and \"reasoning\" should be replaced by terms like \"recognizing\", \"computing\", and \"inferring\", and that first-person pronouns such as \"I\" and \"my\" should not be used by chatbots. Another idea is the implementation of a specific AI accent or dialect that would clearly indicate when language was generated artificially. However, given the commercial pressures to optimize AI agents for economic gain – which may involve exploiting anthropomorphic qualities – it may not be prudent to rely on the restraint of developers, meaning that increased regulation may be necessary to limit harms. As of now, there are no laws that directly address anthropomorphism in AI; potential avenues for regulation include requirements for transparency and built-in safeguard mechanisms. More generally, researchers cite a need for increased understanding of the kinds and degrees of anthropomorphic qualities possessed by AI systems. To that end, it has been proposed that new benchmarks and tests should be developed that measure anthropomorphic qualities in AI writing, inference, and interaction.\n\nIn popular culture\nAnthropomorphic portrayals of AI are common in film, literature, and other interactive media. These depictions often emphasize human-like qualities of AI in ways that shape public perceptions.\n\nFilm and television\nThere are a number of well-known portrayals in movies and TV of AI possessing human-like agency or personalities. In film, HAL 9000 in 2001: A Space Odyssey and Ava in Ex Machina are depicted with complex emotions and motives. Television portrayals include Data from Star Trek: The Next Generation and KITT from Knight Rider.\n\nLiterature\nAnthropomorphic AI is also common in literature. Isaac Asimov's robot characters, including R. Daneel Olivaw, exhibit human reasoning and moral dilemmas, while Iain Banks's \"Minds\" in The Culture series are portrayed as having distinct personalities and social roles.\n\nVideo games\nExamples of anthropomorphized AI in video games include GLaDOS in Portal, a witty and sinister guide for the player, and Cortana in the Halo series, who forms emotional bonds with human protagonists.\n\n"}
{"doc_id": "AI anthropomorphism", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " \"Minds\" in The Culture series are portrayed as having distinct personalities and social roles.\n\nVideo games\nExamples of anthropomorphized AI in video games include GLaDOS in Portal, a witty and sinister guide for the player, and Cortana in the Halo series, who forms emotional bonds with human protagonists.\n\nAdvertising and consumer technology\nMarketing campaigns for digital assistants such as Amazon Alexa, Google Assistant, and Siri often portray the systems as personable or empathetic. Consumer robots like Sony's AIBO and SoftBank Robotics' Pepper are intentionally designed with expressive behaviors that encourage users to treat them as social agents.\n\nSee also\nDeep Blue, the first chess computer to beat a human world champion\nELIZA effect, the tendency to project human traits onto simple computer programs with text interfaces, named for the early chatbot ELIZA\nIntentional stance, the implicit view of objects' behavior as a result of mental properties\nGenerative AI, a subfield of artificial intelligence that uses generative models based on training data to produce text, images, video, etc. from any input\nTuring test, a test of machine intelligence based on how closely the machine's answers resemble those of a human\nUncanny valley, the hypothesis that almost-human appearing entities elicit revulsion"}
{"doc_id": "AI browser", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "An AI browser is a web browser with integrated artificial intelligence (AI) capabilities, such as automatically summarizing web page content or answering questions about it. A more specialized type is an agentic browser, based on the concept of agentic AI, which can take actions – such as navigating webpages or filling out forms – on behalf of the user.\nThe two main artificial agent browsers are ChatGPT Atlas and Comet, both released in 2025.\nAs of 2025, this is a recent development in the browser market, including new entrants from OpenAI, Opera and Perplexity. The designation of 'AI browser' also includes established browsers that later added non-agentic AI features, such as Microsoft Edge with the Copilot chatbot, Google Chrome with the Gemini chatbot (for Windows desktop users in the US with their language set to English), and Firefox with multiple chatbot providers (such as ChatGPT, Claude, Copilot, Gemini, and Le Chat).\nAI browsers have been noted to be susceptible to prompt injection attacks.\n\nBrowser extensions and integrations\nRather than creating entirely new browsers, some AI browsing solutions integrate with existing browsers through extensions or companion applications. These tools add agentic capabilities to established browsers without requiring users to switch platforms. Examples include Composite, which functions as a cross-browser agent that works with Chrome, Edge, and other browsers to automate web-based tasks for workers.\n\nCloud-based implementations\nCloud-based implementations of AI browsers allow users to run automated browsing agents without local installation. These systems operate on remote servers using frameworks such as Puppeteer or Playwright. Examples include Browserbase, Browser-use and AI Browser. The AI typically parses the Document Object Model (DOM) to locate and interact with page elements, and may also analyze browser screenshots to interpret layout and structure.\n\nCriticisms and dangers\nAI Browsers have been noted to be susceptible to being vulnerable to prompt injection attacks, i.e the content of websites can be used to hijack the control of the browser away from the user themselves, multiple organisations have argued against using AI browsers due to this vulnerability . The United Kingdom national cyber security centre and Gartner considering them to be too risky for adoption by most organisations\nA study by the CISPA Helmholtz Center and Saarland University concluded that this vulnerability makes them easy targets for malware, fraud, automated defamation, disinformation and biased outputs."}
{"doc_id": "AI browser", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " CISPA Helmholtz Center and Saarland University concluded that this vulnerability makes them easy targets for malware, fraud, automated defamation, disinformation and biased outputs."}
{"doc_id": "AI datacenter", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "An AI data center (or artificial intelligence data center) is a specialized data center facility designed for the computationally intensive tasks of training and running inference for artificial intelligence (AI) and machine learning models. Unlike general-purpose data centers, they are optimized for the parallel processing demands of AI workloads, typically utilizing hardware such as AI accelerators (e.g., GPUs, TPUs) and high-speed interconnects.\nThe global push to construct these specialized facilities accelerated dramatically during the AI boom following the 2022 release of ChatGPT. This demand has reshaped supply chains, driving memory manufacturers to prioritize production of High Bandwidth Memory (HBM) essential for AI servers and triggering a broader competition for advanced chips, power, and infrastructure.\n\nArchitecture\nData centers for building and running large machine learning models contained specialized computer chips, GPUs, that used 2-4 times as much energy as their regular CPU counterparts (250-500 watts). Companies such as Google and Nvidia constructed GPUs specifically for machine learning, which could process thousands of calculations per second. Thousands of these GPUs were stored closely together in data centers, alongside specialized hardware and cables to quickly migrate data between these chips. To cool these systems, AI data centers have developed new techniques for doing so. Google pumps large amounts of water through its data centers, using pipes that run next to the computer chips, which can strain nearby water supplies. Cirrascale uses large chillers to cool the water, which is largely recycled, but uses more electricity.\nAccording to PCMag, AI data centers use 60+ kilowatts of power per server rack, whereas more standard data centers typically use 5-10 kilowatts per rack.\nAmerican Big Tech companies believe that these facilities are essential to building artificial general intelligence.\n\nDifference with data center\nAn AI datacenter is a type of data center.\n\nOperators\nAs of August 2025, The Information tracked 18 planned or existing AI data centers in the United States, operated by Amazon Web Services, CoreWeave, Crusoe, Meta, Microsoft/OpenAI, Oracle, Tesla, and xAI. Other AI data center operators include Digital Realty and Alibaba. Data centers are also being built in China, India, Europe, Saudi Arabia, and Canada. The New Yorker described CoreWeave as the most prominent AI data center operator in the United States.\nTwo types of data center providers for machine learning have been noted: hyperscalers and neoclouds. The Verge listed large technology companies such as Google, Meta, Microsoft, Oracle"}
{"doc_id": "AI datacenter", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Saudi Arabia, and Canada. The New Yorker described CoreWeave as the most prominent AI data center operator in the United States.\nTwo types of data center providers for machine learning have been noted: hyperscalers and neoclouds. The Verge listed large technology companies such as Google, Meta, Microsoft, Oracle and Amazon as hyperscalers. The New York Times described neoclouds as \"a new generation of data center providers\". CoreWeave, Nebius, Nscale, and Lambda have been described as examples of neoclouds.\nIn January 2025, OpenAI, in partnership with Oracle and Softbank, announced the Stargate project, which as of September 2025 is composed of six built or proposed AI data centers in the United States.\nIn response to the Stargate project, Amazon launched in October 2025 an AI data center on 1,200 acres of farmland in Indiana. This data center, known as Project Rainier, is one of the largest AI data centers in the world, with Amazon spending $11 billion on the project. Rainier is specifically intended for training and running machine learning models from Anthropic. As of that time, this facility contains seven data centers (out of an estimated 30 planned) and will use 2.2 gigawatts of electricity (equivalent to 1 million households) and millions of gallons of water per year. Computer chips from Annapurna Labs and Anthropic, Trainium 2, were designed for use in such facilities. Amazon pumped millions of gallons of water out of the ground to construct the data center, and as of June 2025, Indiana state officials are investigating whether this dewatering process led to dry wells for local residents.\nIn November 2025, Anthropic announced a plan in partnership with Fluidstack to develop artificial intelligence infrastructure in the United States, including data centers in New York and Texas, worth $50 billion.\nOther AI data center projects include the Colossus supercomputer from xAI, a Lousiana-based project from Meta, Hyperion, expected to use 5 GW of power, and a second Ohio-based Meta project, Prometheus, with a capacity of 1 GW. A 3,200-acre AI data center, capable of 4.4-4.5 GW of power and located on the decommissioned Homer City Generating Station, is under construction as of 2025, and will use seven 30-acre gas generating stations supplied by EQT.\n"}
{"doc_id": "AI datacenter", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of 1 GW. A 3,200-acre AI data center, capable of 4.4-4.5 GW of power and located on the decommissioned Homer City Generating Station, is under construction as of 2025, and will use seven 30-acre gas generating stations supplied by EQT.\nAs of December 2025, CRH is working on over 100 data centers in the United States.\nIn 2025, ExxonMobil and NextEra announced plans to build a data center powered by natural gas and using carbon capture technology, with 1.2 GW of power capacity. They previously purchased 2,500 acres of land in the Southeastern United States and plan to market the data center to an artificial intelligence company.\nThe increased interest in AI data centers has led to several executives from companies in that space becoming billionaires, including CoreWeave, QTS, Nebius, Astera Labs, Groq, Fermi (which is connected to former United States Secretary of Energy Rick Perry), Snowflake and Cipher Mining.\nSeveral companies involved in cryptocurrency mining, such as Bitdeer, CoreWeave, Cipher Mining, TeraWulf, IREN, Core Scientific, and CleanSpark have also been involved with AI data centers.\n\nFinances\nBetween January and August 2024, Microsoft, Meta, Google and Amazon collectively spent $125 billion on AI data centers. Citigroup forecasted that $2.8 trillion would be spent on AI data centers by 2030, while McKinsey and Company estimated that almost $7 trillion would be spent globally by that time. According to S&P Global, $61 billion has been spent on the data center market as a whole in 2025, while debt issuance for data centers was $182 billion during the same year.\nLarge technology companies have offloaded the financial risks of building AI data centers by setting up special purpose vehicles or by contracting with neoclouds. For example, Meta's Hyperion was mostly funded by Blue Owl Capital, which did so using a bond offering from PIMCO. Those bonds were sold to a number of clients, including BlackRock. Meta did not borrow money itself and instead established a special purpose vehicle from which it would rent the data center. This deal was structured by Morgan Stanley for $30 billion, the largest known private capital transaction as of 2025.\nNeoclouds such as CoreWeave have gone into debt to buy computer chips from Nvidia for their data centers, and the chips themselves have been used"}
{"doc_id": "AI datacenter", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " from which it would rent the data center. This deal was structured by Morgan Stanley for $30 billion, the largest known private capital transaction as of 2025.\nNeoclouds such as CoreWeave have gone into debt to buy computer chips from Nvidia for their data centers, and the chips themselves have been used for loan collateral. As of December 2025, CoreWeave took out three GPU-backed loans, collectively worth $12.4 billion, from private credit firms (Blackstone, Coatue, BlackRock, PIMCO) and from banks (Goldman Sachs, JPMorgan Chase, Wells Fargo). Thus, these companies provide an indirect connection between private credit and established banks. Data centers have also established asset-backed securities, and debt for data centers has its own derivative financial products.\nThe real estate industry, including asset managers, public companies and private investors, has also invested in data centers.\n\nEnergy sourcing\nAs of 2024, data centers in the United States are primarily powered by natural gas, which supplies 40% of their electricity (with renewable energy at 24%, nuclear at about 20% and coal at about 15%). The Associated Press reported that electricity for AI data centers in the United States would likely come from natural gas or oil, as companies prefer using currently available power plants, which primarily use fossil fuels. Non-renewable energy is also often cheaper in locations where data centers are developed, and experts believe that energy demands from generative AI and data centers would be difficult to fulfill with renewable energy alone. Some companies such as Google, Amazon and Meta have expressed interest in nuclear power for their data centers. Other data centers, such as Wonder Valley in Canada (proposed by Shark Tank celebrity investor Kevin O'Leary), plan to use their own natural gas and geothermal plants that are off-grid. Similarly, xAI is using onsite gas turbines for Colossus, while OpenAI and Meta have planned to use natural gas generators at their Stargate and Prometheus projects, respectively. VoltaGrid, a Texas-based energy company, proposed to install 33 reciprocating internal combustion engines at a data center in Covington, Georgia. Electric vehicle batteries have also been used for powering data centers, including for Colossus. Many data centers use lithium-ion batteries for backup power.\nPower utility companies make upgrades to their infrastructure to handle demands of new data centers, and the price for these changes typically falls on consumers: smaller businesses or individual households.\nIn December 2025, the Federal"}
{"doc_id": "AI datacenter", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " used for powering data centers, including for Colossus. Many data centers use lithium-ion batteries for backup power.\nPower utility companies make upgrades to their infrastructure to handle demands of new data centers, and the price for these changes typically falls on consumers: smaller businesses or individual households.\nIn December 2025, the Federal Energy Regulatory Commission published a unanimous order allowing data centers in the United States to have a direct connection with power plants. United States Secretary of Energy Chris Wright expressed support for un-retiring coal plants to power AI data centers. Trump paused leasing for offshore wind projects, a decision that Gizmodo criticized due to their potential to provide power to AI data centers. Electricity demands from AI data centers have led to the United States federal government, power companies and power grid operators slowing or reversing the retirement of peaking power plants.\n\nEnvironmental footprint\nAverage AI data centers have an electricity footprint equivalent to 100,000 households, and use billions of gallons of water for cooling their hardware. In 2025, the International Energy Agency estimated that the larger AI data centers currently under construction could consume as much electricity as 2 million households. A 2024 report from the United States Department of Energy stated that data centers overall used 17 billion gallons of water per year in the United States, primarily due to \"rapid proliferation of AI servers\", and that this usage was forecasted to grow to nearly 80 billion gallons by 2028. Researchers estimated that AI data centers in the United States would emit 24-44 metric tons of carbon dioxide and use 731-1,125 million cubic meters of water per year between 2024 and 2030.\nPeaking power plants, which have been proposed as a power source for AI data centers, emit sulfur dioxide and have historically been located disproportionately near communities of color in the United States.\nReciprocating internal combustion engines, proposed as another power source for a data center, emit PM 2.5, nitrogen oxides, and volatile organic compounds.\n\nAI data centers in space\nSeveral tech companies, including Starcloud, Google, Nvidia, Blue Origin and SpaceX, have announced projects for or otherwise expressed interest in building data centers in outer space. Google announced Project Suncatcher, which plans to test whether its Tensor Processing Unit (TPU) chips (which are specifically designed for machine learning) can function in the environment of space. The company aims to deploy satellites with these chips in a data center by 2027. In November 2025, Starcloud, a startup supported by Nvidia"}
{"doc_id": "AI datacenter", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", which plans to test whether its Tensor Processing Unit (TPU) chips (which are specifically designed for machine learning) can function in the environment of space. The company aims to deploy satellites with these chips in a data center by 2027. In November 2025, Starcloud, a startup supported by Nvidia, launched a satellite with an Nvidia H100 GPU that the company used to deploy and develop a large language model. CNBC reported that this model, NanoGPT, was the first model trained in space. Starcloud deployed a second model based on Google's Gemma.\n\nAI data centers in the United States\nIn the United States, both the Biden administration and second Trump administration supported the construction of AI data centers. In January 2025, then-president Joe Biden signed an executive order for federal government agencies to support AI data centers on federal sites built by private companies, study their effect on energy prices, and encourage their use of renewable energy. In April 2025, the United States Department of Energy suggested 16 possible sites, including Los Alamos National Laboratory, Sandia National Laboratories and Oak Ridge National Laboratory. In its July 2025 AI Action Plan, the second Trump administration supported increased production of AI data centers. Several states in the country have incentivized local data center construction. For example, in 2024, lawmakers in Michigan approved tax breaks for data center equipment and construction material. Some data center companies have also invested or promised to invest in the infrastructure of local communities. \nIn December 2025, Democratic senators Elizabeth Warren, Chris Van Hollen, and Richard Blumenthal wrote to seven technology companies (Google, Microsoft, Amazon, Meta, CoreWeave, Digital Realty, and Equinix) that they will investigate the effects of those companies' operations on consumer energy bills, highlighting AI data centers in particular. That same month, 25 Democratic lawmakers wrote to the inspector general of the United States Department of Commerce over possible conflict of interest concerns involving secretary of commerce Howard Lutnick and AI data centers. Lutnick met with a co-founder of Fermi, which proposed an AI data center project financed by Cantor Fitzgerald and Newmark Group.\nAI Infrastructure Coalition (AIIC), an organization led by Brian O. Walsh, Kyrsten Sinema, and Garret Graves, has supported Trump's AI Action Plan, with Sinema becoming personally involved in a local meeting on an AI data center in Arizona. Venture capital firm Andreessen Horowitz, Big Tech companies (Google, Meta, Microsoft"}
{"doc_id": "AI datacenter", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "), an organization led by Brian O. Walsh, Kyrsten Sinema, and Garret Graves, has supported Trump's AI Action Plan, with Sinema becoming personally involved in a local meeting on an AI data center in Arizona. Venture capital firm Andreessen Horowitz, Big Tech companies (Google, Meta, Microsoft), telecommunications companies (Cisco, Corning Inc., Lumen Technologies), data center providers (Digital Realty, QTS), energy companies (Duke Energy, Entergy, ExxonMobil, NextEra Energy), and utility companies (PG&E, Pinnacle West Capital) are members of AIIC as of November 2025. AIIC has been supported by Hogan Lovells.\n\nConcerns and opposition\nSome analysts have expressed concerns about overbuilding of AI data centers, warning that their infrastructure risks obsolescence due to changes in demand and technology. Estimates for the lifespan of the GPUs used to power data centers range from one to eight years. By contrast, CPUs in more traditional data centers have a shelf life of about 5-7 years. Machine learning model training causes significant stress on computer chips.\nThe increased costs and shortages of computer memory, including High Bandwidth Memory, DRAM and NAND flash memory, have been attributed to the AI data center boom. Increased prices and smaller memory storage for smart devices have also been linked to the AI data center boom. An agreement with Samsung and SK Hynix would supply the Stargate project with 900,000 DRAM wafers per month, about 40% of all DRAM produced worldwide.\nAs of 2025, Data Center Watch reported that multiple data center projects collectively worth $64 billion had been stopped or delayed. Political opposition in the United States has been bipartisan. Local communities in Texas, Oregon, Tennessee, Pennsylvania, Florida, Michigan, Minnesota, Wisconsin, Missouri, Kentucky, Ohio, Arizona, Indiana, Virginia, Maryland, Georgia, North Carolina, Illinois, and Oregon have resisted these projects. Critics have pointed out that jobs created by data centers tend to be temporary or few in number. Residents have been concerned about air, water and noise pollution, as well as property devaluation, traffic, and the risk of fires. Other environmental concerns involving AI data centers include e-waste and construction materials that emit greenhouse gases such as concrete and cement. \nAdvocates have also linked data center construction to the AI bubble. Wisconsin Watch expressed concern that data centers could become stranded assets in the context of an AI bubble, leaving energy customers to pay for the"}
{"doc_id": "AI datacenter", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " environmental concerns involving AI data centers include e-waste and construction materials that emit greenhouse gases such as concrete and cement. \nAdvocates have also linked data center construction to the AI bubble. Wisconsin Watch expressed concern that data centers could become stranded assets in the context of an AI bubble, leaving energy customers to pay for the costs. Journalists have warned that the large number of links between actors and organizations in the AI ecosystem as well as the large number of companies involved each indicate strong risks for the global economy and financial markets.\nA hedge fund founder questioned the economics of AI data centers, concluding that they need about $1 trillion of revenue for profitability.\nIn November 2025, the North American Electric Reliability Corporation warned that building new data centers could negatively affect the electrical grid and cause power outages during extreme weather. The independent monitor of PJM Interconnection warned that its power grid cannot support new data centers and supported a federal moratorium on data centers.\nMachine learning developers, when training large language models, often use data centers at their full capacity, conflicting with other users (including households) during peak usage, which may lead to blackouts.\nPower disruptions in data centers can cause errors in machine learning model calculations (\"silent data corruption\").\nLarge technology companies who are building data centers have asked public officials and land owners to sign non-disclosure agreements and have appeared to use shell companies. In one case, when a joint OpenAI-Oracle data center was rejected by Saline Township in Michigan, landowners and developers responded by suing the town. The developers acquired the land from the resulting settlement. The Michigan Public Service Commission approved DTE Energy to provide power to the data center, part of Stargate. The approval was part of an ex parte motion, with no public input.\nIn 2025, over 230 groups have signed a letter supporting a moratorium on constructing AI data centers in the United States, concerned by impact on the environment and energy bills. Signatories include Food & Water Watch, Greenpeace, Friends of the Earth and Physicians for Social Responsibility. Senator Bernie Sanders has also supported a moratorium on AI data centers.\nAshley LaMont, of Honor the Earth, argued that data centers on tribal lands would not help Native Americans obtain data sovereignty.\nThe NAACP has also expressed opposition to AI data centers.\n\nSee also\nAI boom\nAI bubble\nAI Factory\nTensor Processing Unit"}
{"doc_id": "AI datacenter", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " help Native Americans obtain data sovereignty.\nThe NAACP has also expressed opposition to AI data centers.\n\nSee also\nAI boom\nAI bubble\nAI Factory\nTensor Processing Unit"}
{"doc_id": "AI literacy", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "AI literacy or artificial intelligence literacy is \"a set of competencies that enables individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace.\" \nAI is employed in a variety of applications, including self-driving automobiles, virtual assistants and text generation by generative AI models. Users of these tools should be able to make informed decisions. AI literacy may have an impact on students' future employment prospects.\nWith the rise of generative AI platforms, AI literacy has become a topic of conversation in the field of education. Some think AI literacy is essential for school and college students, while others restrict or prohibit the use of AI in assignments, viewing it as a form of academic dishonesty. However, many researchers and educational institutions promote a more nuanced approach, encouraging critical engagement with AI while developing policies that balance academic integrity with opportunities for learning.\n\nDefinitions\nOther definitions of AI literacy include the ability to understand, use, monitor, and critically reflect on AI applications. That use of the term usually refers to teaching skills and knowledge to the general public, particularly those who are not adept in AI and the ability to understand, use, evaluate, and ethically navigate AI. As research into AI literacy is still emerging and focused on developing context-specific skills, there is not yet a single, broadly agreed-upon definition.\nAI literacy is linked to other forms of literacy. AI literacy requires digital literacy, whereas scientific and computational literacy may inform it. Data literacy also significantly overlaps with it.\n\nCategories\nAI literacy encompasses multiple categories, including a theoretical understanding of how artificial intelligence works, the usage of artificial intelligence technologies, and the critical appraisal of artificial intelligence, and its ethics.\n\nKnow and understand AI\nKnowledge and understanding of AI refers to a basic understanding of what artificial intelligence is and how it works. This includes familiarity with machine learning algorithms and the limitations and biases present in AI systems. Users who know and understand AI should be familiar with various technologies that use artificial intelligence, including cognitive systems, robotics and machine learning. This includes recognizing that large language models (LLMs) are statistical systems trained on extensive datasets which produce outputs rather than retrieving existing information like a search engine.\n\nUse and apply AI\nUsing and applying AI refers to the ability to use AI tools to solve problems and perform tasks such as programming and analyzing big data. Some consider prompt engineering, the practice of designing effective prompts to guide generative AI platforms more effectively, as another competency within AI literacy.\n\nEvaluate and create AI\nEvaluation and creation"}
{"doc_id": "AI literacy", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " apply AI\nUsing and applying AI refers to the ability to use AI tools to solve problems and perform tasks such as programming and analyzing big data. Some consider prompt engineering, the practice of designing effective prompts to guide generative AI platforms more effectively, as another competency within AI literacy.\n\nEvaluate and create AI\nEvaluation and creation refers to the ability to critically evaluate the quality and reliability of AI systems. It also refers to designing and building fair and ethical AI systems. To evaluate correctly, users should also learn in which areas AI is strong, and in which areas it is weak.\n\nAI ethics\nAI ethics refers to understanding the moral implications of AI, and the making informed decisions regarding the use of AI tools. This area includes considerations such as:\n\nAccountability: Hold AI actors accountable for the operation of AI systems and adherence to ethical ideals.\nAccuracy: Identify and report sources of error and uncertainty in algorithms and data.\nAuditability: Enable other parties to audit and assess algorithm behavior via transparent information sharing.\nExplainability: Make sure that algorithmic judgments and the underlying data can be presented in simple language.\nFairness: Prevent biases and consider varied viewpoints. To do so, increase the diversity of researchers in the field.\nHuman Centricity and Well-being: Prioritize human well-being in AI development and deployment.\nHuman rights Alignment: Ensure that technology do not infringe internationally recognized human rights.\nInclusivity: Make AI accessible to everyone.\nProgress: Choose high value initiatives.\nResponsibility, accountability, and transparency: Foster trust via responsibility, accountability, and fairness.\nRobustness and Security: Make AI systems safe, secure, and resistant to manipulation or data breach.\nSustainability: Choose implementations that generate long-term, useful benefits.\nEnvironmental Implications: How this tool impacts the environment, any restrictions or laws, if this impact is worth the effects or not.\n\nEnabling AI\nSupport AI by developing associated knowledge and skills such as programming and statistics.\n\nPromoting AI literacy\nSeveral governments have recognized the need to promote AI literacy, including among adults. Such programs have been published in the United States, Australia, China, Germany and Finland. Programs intended for the general public usually consist of short and easy to understand online study units. Programs intended for children are usually project-based. Programs for students at colleges and universities often address the specific professional needs of the student, depending on their field of study. Beyond the education system, AI literacy can also be developed in the community, for example in museums.\n\nSchools\nSchools use diverse pedagogies to promote"}
{"doc_id": "AI literacy", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " intended for children are usually project-based. Programs for students at colleges and universities often address the specific professional needs of the student, depending on their field of study. Beyond the education system, AI literacy can also be developed in the community, for example in museums.\n\nSchools\nSchools use diverse pedagogies to promote AI literacy. These include:\n\nPerforming a Turing test with an intelligent agent\nCreating chatbots\nBuilding apps using Blockly-based programming\nProject-based learning\nBuilding robots\nData visualization\nTraining AI models\nArtificial intelligence curricula can improve students' understanding of topics such as machine learning, neural networks, and deep learning.\n\nCase study: DAILy\nThe DAILy (Developing AI Literacy) program was developed by MIT and Boston University with the goal of increasing AI literacy among middle school students. The program is structured as a 30-hour workshop that includes the topics of introduction to artificial intelligence, logical systems (decision trees), supervised learning, neural networks, computational learning, deepfake, and natural language generators. Students examine the moral and social implications of each topic, as well as its occupational implications.\n\nHigher education\nBefore the second decade of the 21st century, artificial intelligence was studied mainly in STEM courses. Later, projects emerged to increase artificial intelligence education, specifically to promote AI literacy. Most courses start with one or more study units that deal with basic questions such as what artificial intelligence is, where it comes from, what it can do and what it can't do. Most courses also refer to machine learning and deep learning. Some of the courses deal with moral issues in artificial intelligence.\n\nDisciplinary policy\nAs a response to the increase of generative AI use in education, several disciplines formed committees or task forces to examine context-specific approaches toward AI literacy. In spring 2025, the Modern Language Association and Conference on College Composition and Communication Joint Task Force finished development of three working papers, a guide on AI literacy for students, and a collection of resources addressing AI use in writing. The task force emphasized the need for \"a culture of critical AI literacy\" and included guidelines not only for students but also educators and institutions, highlighting the need for modeling ethical AI use in planning processes.   \nSimilarly, a committee formed by the American Historical Association Council published \"Guiding Principles for Artificial Intelligence in History Education\" which encouraged \"clear and transparent engagement with generative AI.\" The guidelines demonstrate the value of criticality when working with generative AI in thinking and research.\n\nCritiques and conceptual limitations on AI literacy\nWhile"}
{"doc_id": "AI literacy", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Similarly, a committee formed by the American Historical Association Council published \"Guiding Principles for Artificial Intelligence in History Education\" which encouraged \"clear and transparent engagement with generative AI.\" The guidelines demonstrate the value of criticality when working with generative AI in thinking and research.\n\nCritiques and conceptual limitations on AI literacy\nWhile AI literacy is commonly framed as a set of competencies related to understanding, using, and critically evaluating AI systems, some scholars argue that such approaches risk oversimplifying the educational implications of artificial intelligence. The German educational scientist and computer scientist Thomas Knaus argues that prevailing AI literacy models implicitly assume that individuals can acquire stable and transferable competencies for dealing with AI. However, many contemporary AI systems—particularly large language models or ChatGPT—operate as highly complex and opaque systems whose underlying data, decision-making processes, and training dynamics are not fully accessible or controllable by individual users. From this perspective, the notion of individual competence becomes problematic, as competence traditionally presupposes the ability to understand, assess, and act autonomously within a given domain. When applied to AI systems that function as structural “black boxes”, this assumption may overestimate what individuals can realistically know or control. Rather than treating AI literacy as the acquisition of specific competencies related to AI systems themselves, Knaus suggests situating AI within broader media literacy frameworks that emphasize critical reflection, socio-technical contextualization, and institutional responsibility. This view highlights the importance of transparency, explainability, and the provision of meta-information by AI systems, shifting part of the responsibility for critical engagement from individuals to the design and governance of AI technologies.\n\nSee also\nArtificial intelligence\nDigital literacy\nEthics of artificial intelligence"}
{"doc_id": "AI nationalism", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "AI nationalism is the idea that nations should develop and control their own artificial intelligence technologies to advance their own interests and ensure technological sovereignty. This concept is gaining traction globally, leading countries to implement new laws, form strategic alliances, and invest significantly in domestic AI capabilities.\n\nGlobal trends and national strategies\nIn 2018, British technology investor Ian Hogarth published an influential essay titled AI Nationalism. He argued that as AI gains more power and its economic and military significance expands, governments will take measures to bolster their own domestic AI industries, and predicted that the advancement of machine learning systems would lead to what he termed \"AI nationalism.\" He anticipated that this rise in AI would accelerate a global arms race, resulting in more closed economies, restrictions on foreign acquisitions, and limitations on the movement of talent. Hogarth predicted that AI policy would become a central focus of government agendas. He also criticized Britain’s approach to AI strategy, citing the sale of London-based DeepMind—one of the leading AI laboratories, acquired by Google for a relatively modest £400 million in 2014—as a significant misstep.\nAI nationalism is chiefly reflected in the escalating rhetoric of an artificial intelligence arms race, portraying AI development as a zero-sum game where the winner gains significant economic, political, and military advantages. This mindset, as highlighted in a 2017 Pentagon report, warns that sharing AI technology could erode technological supremacy and enhance rivals' capabilities. The winner-takes-all mentality of AI nationalism poses risks including unsafe AI development, increased geopolitical tension, and potential military aggression (such as cyberattacks or targeting AI professionals).\nSeveral countries, including Canada, France, and India, have formulated national strategies to advance their positions in AI. In the United States, a leading player in the global AI arena, trade policies have been enacted to restrict China's access to critical microchips, reflecting a strategic effort to maintain a technological edge. The United States’ National Security Commission on Artificial Intelligence (NSCAI) frames AI development as a critical aspect of a broader technology competition crucial for national success. It emphasizes the need to outpace China in AI to maintain strategic advantage, reflecting AI nationalism by linking geopolitical power directly to advancements in AI.\nFrance has seen notable governmental support for local AI startups, particularly those specializing in language technologies that cater to French and other non-English languages. In Saudi Arabia, Crown Prince Mohammed bin Salman is investing billions in AI research and development. The country has actively collaborated with major technology firms such as Amazon, IBM, and Microsoft to establish itself as a prominent AI hub.\n\n"}
{"doc_id": "AI nationalism", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " AI startups, particularly those specializing in language technologies that cater to French and other non-English languages. In Saudi Arabia, Crown Prince Mohammed bin Salman is investing billions in AI research and development. The country has actively collaborated with major technology firms such as Amazon, IBM, and Microsoft to establish itself as a prominent AI hub.\n\nHistorical and cultural context\nAI nationalism is seen as deeply connected to historical racism and imperialism. It is viewed not merely as a technological competition but as a contest over racial and civilizational superiority. Historically, technological achievements were often used to justify colonialism and racial hierarchies, with Western societies perceiving their advancements as evidence of superiority. In the context of AI, this historical context continues to shape views on intelligence and development. Some argue that AI nationalism reinforces the idea of fundamental civilizational divides, especially between the Western world and China. This perspective often frames China's progress in AI as a direct challenge to Western values, presenting the AI competition as a struggle over values. AI nationalism is said to draw from long-standing anti-Asian stereotypes, such as the \"Yellow Peril,\" which portray Asian nations as threats to Western civilization. This viewpoint links Asian technological advances with dehumanization and artificiality, reflecting persistent anxieties about China's growing role in the global tech landscape.\n\nImplications\nAI nationalism is seen as a component of a broader trend towards the fragmentation of the internet, where digital services are increasingly influenced by local regulations and national interests. This shift is creating a new technological landscape in which the impact of artificial intelligence on individuals' lives can vary significantly depending on their geographic location.\nJ. Paul Goode argues that AI nationalism may exacerbate existing societal divisions by promoting the development of systems that embed cultural biases, thereby privileging certain groups while disadvantaging others.\n\nSee also\nArtificial Intelligence Cold War\nSpace Race\nTechno-nationalism\nTechnological escalation\nTechnological sovereignty\nTechnological supremacy\n\nFurther reading\nHogarth, Ian, AI Nationalism, 2018\nAaronson, Susan. The Age of AI Nationalism and its Effects. April 22, 2024. DOI: 10.2139/ssrn.4803311."}
{"doc_id": "AI Overviews", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "AI Overviews is an artificial intelligence (AI) feature integrated into Google Search that produces AI-generated summaries of search results. The feature has been widely criticized as producing misleading, nonsensical, and potentially dangerous claims, as well as taking traffic away from content websites.\n\nHistory and development\nAI Overviews were first introduced as part of Google's Search Generative Experience (SGE), which was unveiled at the Google I/O conference in May 2023. In May 2024, the feature was rebranded as AI Overviews and launched in the United States. The introduction of AI Overviews was seen as a strategic move to compete with other generative AI advancements, including OpenAI's ChatGPT.\nBy August 2024, AI Overviews was rolled out to several other countries, including the United Kingdom, India, Japan, Brazil, Mexico, and Indonesia, with support for multiple languages. In October 2024, Google expanded the feature globally, making it available in over 100 countries.\nIn December 2024, Botify x Demandsphere released findings stating that when AI Overviews and Featured Snippets appear in the SERP together, they take up approximately 67.1% of the screen on desktop and 75.7% of the screen on mobile. Even if content is ranking in the #1 position, it may not be visible to consumers depending on the other visual elements on the results page.\nIn March 2025, Google started testing an \"AI Mode\", where all of the content is AI-generated. The company was also considering adding ads to the AI Mode, as they already exist in AI Overviews.\nAs of May 2025, AI Overviews are now available in over 200 countries and territories, and more than 40 languages.\n\nFunctionality\nThe AI Overviews feature uses advanced machine learning algorithms to generate summaries based on diverse web content. The overviews are designed to be concise, providing a snapshot of relevant information on the queried topic. To enhance user interaction, Google allows users to adjust the complexity of the language in the summaries, offering both simplified and detailed options.\nThe feature also includes prominent links to source content, ensuring that users can access more in-depth information directly from authoritative websites. \nAs of October 2024, Google has implemented inline links within AI Overviews, allowing users to directly access source content within the generated summaries, enhancing user engagement with authoritative sources.\n\nReception\nAI Overviews received mixed feedback upon its introduction. Many users appreciated the convenience of obtaining immediate and"}
{"doc_id": "AI Overviews", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " directly from authoritative websites. \nAs of October 2024, Google has implemented inline links within AI Overviews, allowing users to directly access source content within the generated summaries, enhancing user engagement with authoritative sources.\n\nReception\nAI Overviews received mixed feedback upon its introduction. Many users appreciated the convenience of obtaining immediate and relevant information without navigating through multiple search results. However, early iterations of the feature faced criticism for inaccuracies, including instances where erroneous or nonsensical content was generated. Google addressed these issues by improving content validation and refining the algorithms used to filter unreliable information.\nConcerns were also raised by content publishers, who feared a decline in web traffic as users relied on the summaries instead of visiting source websites. In response, Google implemented measures to prioritize link placement within AI Overviews, aiming to balance user convenience with the needs of content creators.\n\nCriticism and challenges\nSince its introduction, the feature has faced ongoing scrutiny. Critics argue that relying on AI-generated summaries may perpetuate inaccuracies or oversimplify complex topics. Furthermore, there is apprehension about the ethical implications of AI-driven content aggregation, including its impact on intellectual property rights and the visibility of smaller content providers. Depending on what is searched for, the overview may also consist of hallucinated content, such as when searching for idioms that do not exist.\nIn response, Google has stated its commitment to addressing these challenges by continuously refining the system and engaging with stakeholders to ensure a balanced and accurate search ecosystem. In May 2024, Google temporarily restricted the AI tool after it provided nonsensical and harmful suggestions, such as telling users to eat rocks or apply glue on pizza.\n\nLawsuits\nOn February 24, 2025, Chegg sued Alphabet over the AI Overviews feature, claiming that it was leading to students preferring \"low-quality, unverified AI summaries\", thus violating antitrust law. Chegg also said it was considering either a sale or a take-private transaction.\nIn September 2025, Penske Media Corporation, the publisher of Rolling Stone and The Hollywood Reporter, sued Google, claiming that AI Overviews illegally regurgitate content from their websites and drive off potential site visitors by always appearing on top of the search results while leaving little incentive to see the linked sources. The company stated that \"the future of digital media and [...] its integrity [...] is threatened by Google's current actions\", alledging that 20% of searches that link to Penske-owned websites show AI Overviews and that the figure is expected to rise. Google spokesperson"}
{"doc_id": "AI Overviews", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " little incentive to see the linked sources. The company stated that \"the future of digital media and [...] its integrity [...] is threatened by Google's current actions\", alledging that 20% of searches that link to Penske-owned websites show AI Overviews and that the figure is expected to rise. Google spokesperson José Castañeda called the claims \"meritless\" and stated that \"AI Overviews send traffic to a greater diversity of sites.\"\n\nSee also\nZero-click result"}
{"doc_id": "AI safety", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses AI alignment (which aims to ensure AI systems behave as intended), monitoring AI systems for risks, and enhancing their robustness. The field is particularly concerned with existential risks posed by advanced AI models.\nBeyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress in generative AI and public concerns voiced by researchers and CEOs about potential dangers. During the 2023 AI Safety Summit, the United States and the United Kingdom both established their own AI Safety Institute. However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities.\n\nMotivations\nScholars discuss current risks from critical systems failures, bias, and AI-enabled surveillance, as well as emerging risks like technological unemployment, digital manipulation, weaponization, AI-enabled cyberattacks and bioterrorism. They also discuss speculative risks from losing control of future artificial general intelligence (AGI) agents, or from AI enabling perpetually stable dictatorships.\n\nExistential safety\nSome have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet\". Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it\".\nAI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology – though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \"extremely bad (e.g. human extinction)\" outcome of advanced AI. In a 2022 survey of the natural language processing community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \"at least as bad as an all-out nuclear war\".\n\nHistory\nRisks from AI began to be seriously discussed at the start of the computer age:\n\nMoreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.\nIn 1988 Blay Whitby published a book outlining the need for AI to be developed along ethical and socially responsible lines. \n"}
{"doc_id": "AI safety", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.\nIn 1988 Blay Whitby published a book outlining the need for AI to be developed along ethical and socially responsible lines. \nFrom 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes\".\nIn 2011, Roman Yampolskiy introduced the term \"AI safety engineering\" at the Philosophy and Theory of Artificial Intelligence conference, listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable\".\nIn 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction. His argument that future advanced systems may pose a threat to human existence prompted Elon Musk, Bill Gates, and Stephen Hawking to voice similar concerns.\nIn 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions. To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell.\nIn the same year, a group of academics led by professor Stuart J. Russell founded the Center for Human-Compatible AI at the University of California Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial\".\nIn 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence, which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI. In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published.\nIn 2017, the Future of"}
{"doc_id": "AI safety", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Safety and Control for Artificial Intelligence, which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI. In the same year, Concrete Problems in AI Safety – one of the first and most influential technical AI Safety agendas – was published.\nIn 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards\".\nIn 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness, and assurance. The following year, researchers organized a workshop at ICLR that focused on these problem areas.\nIn 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.\nIn 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit on AI safety. The AI safety summit took place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models. During the summit the intention to create the International Scientific Report on the Safety of Advanced AI was announced.\nIn 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretary Gina Raimondo and UK technology secretary Michelle Donelan to jointly develop advanced AI model testing, following commitments announced at an AI Safety Summit in Bletchley Park in November.\nIn 2025, an international team of 96 experts chaired by Yoshua Bengio published the first International AI Safety Report. The report, commissioned by 30 nations and the United Nations, represents the first global scientific review of potential risks associated with advanced artificial intelligence. It details potential threats stemming from misuse, malfunction, and societal disruption, with the objective of informing policy through evidence-based findings, without providing specific recommendations.\n\nResearch focus\nAI safety research areas include robustness, monitoring, and alignment.\n\nRobustness\nAdversarial robustness\nAI systems are often vulnerable to adversarial examples or \"inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake\". For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an"}
{"doc_id": "AI safety", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "arial robustness\nAI systems are often vulnerable to adversarial examples or \"inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake\". For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence. This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.\n\nThe image on the right is predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.\nAdversarial robustness is often associated with security. Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses. Network intrusion and malware detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.\nModels that represent objectives (reward models) must also be adversarially robust. For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score. Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task. This issue can be addressed by improving the adversarial robustness of the reward model. More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.\nLarge language models (LLMs) can be vulnerable to prompt injection and model stealing, and may be used to generate misinformation. Prompt injection involves embedding instructions into prompts in order to bypass safety measures.\n\nMonitoring\nEstimating uncertainty\nIt is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis. ML models generally express confidence by outputting probabilities; however, they are often overconfident, especially in situations that differ from those that they were trained to handle. Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.\nSimilarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous"}
{"doc_id": "AI safety", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " that they were trained to handle. Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.\nSimilarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over. Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs, though a range of additional techniques are in use.\n\nDetecting malicious use\nScholars and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons, manipulate public opinion, or automate cyber attacks. These worries are a practical concern for companies like OpenAI which host powerful AI tools online. In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.\n\nTransparency\nNeural networks have often been described as black boxes, meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform. This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear. It also raises debates in healthcare over whether statistically efficient but opaque models should be used.\nOne critical benefit of transparency is explainability. It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.\nAnother benefit is to reveal the cause of failures. At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels.\nTransparency techniques can also be used to correct errors. For example, in the paper \"Locating and Editing Factual Associations in GPT\", the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France. Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.\nFinally, some have argued that the opaqueness of AI systems is"}
{"doc_id": "AI safety", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " make the model respond to questions as if it believed the tower was in Rome instead of France. Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.\nFinally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future. \"Inner\" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent. For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in Spider-Man costumes, sketches of Spider-Man, and the word 'spider'. It also involves explaining connections between these neurons or 'circuits'. For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context. \"Inner interpretability\" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.\n\nDetecting trojans\nMachine learning models can potentially contain \"trojans\" or \"backdoors\": vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view; or a trojaned autonomous vehicle may function normally until a specific trigger is visible. This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data. Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images. In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.\nA 2024 research paper by Anthropic showed that large language models could be trained with persistent backdoors. These \"sleeper agent\" models could be programmed to generate malicious outputs (such as vulnerable code) after a specific date, while behaving normally beforehand. Standard AI safety measures, such as supervised fine-tuning, reinforcement learning and adversarial training, failed to remove these backdoors.\n\nAlignment\nSystemic safety and sociotechnical factors\nIt is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents. Some scholars have suggested"}
{"doc_id": "AI safety", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " beforehand. Standard AI safety measures, such as supervised fine-tuning, reinforcement learning and adversarial training, failed to remove these backdoors.\n\nAlignment\nSystemic safety and sociotechnical factors\nIt is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents. Some scholars have suggested that this framework falls short. For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology. Policy analysts Zwetsloot and Dafoe wrote, \"The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways... Often, though, the relevant causal chain is much longer.\" Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture. In the broader context of safety engineering, structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework.\nInspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation. Others have emphasized the importance of involving both AI practitioners and domain experts in the design process to address structural vulnerabilities.\n\nCyber defense\nSome scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders. This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused. Recent studies have shown that AI can significantly enhance both technical and managerial cybersecurity tasks by automating routine tasks and improving overall efficiency.\n\nImproving institutional decision-making\nThe advancement of AI in economic and military domains could precipitate unprecedented political challenges. Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe. AI researchers have argued that AI technologies could also be used to assist decision-making. For example, researchers are beginning to develop AI forecasting and advisory systems.\n\nFacilitating cooperation\nMany of the largest global threats (nuclear war, climate change, etc.) have been framed as cooperation challenges. As in the well-known prisoner's"}
{"doc_id": "AI safety", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " AI researchers have argued that AI technologies could also be used to assist decision-making. For example, researchers are beginning to develop AI forecasting and advisory systems.\n\nFacilitating cooperation\nMany of the largest global threats (nuclear war, climate change, etc.) have been framed as cooperation challenges. As in the well-known prisoner's dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.\nA salient AI cooperation challenge is avoiding a 'race to the bottom'. In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political and technical efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games). Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.\n\nIn governance\nAI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.\n\nResearch\nIn AI safety, local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions.\nAI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine. Some work has focused on anticipating specific risks that may arise from these impacts – for example, risks from mass unemployment, weaponization, disinformation, surveillance, and the concentration of power. Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry, the availability of AI models, and 'race to the bottom' dynamics. Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: \"it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution\". A research stream focuses"}
{"doc_id": "AI safety", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution\". A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems. A key challenge for these approaches is a lack of widely accepted standards, and ambiguity about what the methods would require, as well as a lack of safety culture in the industry.\nEfforts to enhance AI safety include frameworks designed to align AI outputs with ethical guidelines and reduce risks like misuse and data leakage. Tools such as Nvidia's  Guardrails, Llama Guard, Preamble's customizable guardrails and Claude's Constitution mitigate vulnerabilities like prompt injection and ensure outputs adhere to predefined principles. These frameworks are often integrated into AI systems to improve safety and reliability.\n\nPhilosophical perspectives\nThe field of AI safety is deeply intertwined with philosophical considerations, particularly in the realm of ethics. Deontological ethics, which emphasizes adherence to moral rules, has been proposed as a framework for aligning AI systems with human values. Some have suggested that by embedding deontological principles, AI systems can be guided to avoid actions that cause harm, ensuring their operations remain within ethical boundaries, but those suggestions have been questioned, with other alternatives being suggested at more promising.\n\nGovernment action\nSome experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to \"rush to regulate in ignorance\". Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks.\nOutside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to \"assure that systems are aligned with goals and values, including safety, robustness and trustworthiness\". Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed\".\nIn September 2021, the People's Republic of China (PRC) published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom"}
{"doc_id": "AI safety", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " deployment should cease in a safe manner until risks can be sufficiently managed\".\nIn September 2021, the People's Republic of China (PRC) published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy, which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\". The strategy describes actions to assess long-term AI risks, including catastrophic risks. The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as \"an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach\". China Media Project stated \"key aspects of its approach remain fundamentally unsafe by the standards of democratic societies worldwide\", arguing that part of China's AI safety approach is focused on strengthening the CCP's information control.\nGovernment organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems. The DARPA engages in research on explainable artificial intelligence and improving robustness against adversarial attacks. And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.\nIn 2024, the United Nations General Assembly adopted the first global resolution on the promotion of \"safe, secure and trustworthy\" AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI.\nIn May 2024, the Department for Science, Innovation and Technology (DSIT) announced £8.5 million in funding for AI safety research under the Systemic AI Safety Fast Grants Programme, led by Christopher Summerfield and Shahar Avin at the AI Safety Institute, in partnership with UK Research and Innovation. Technology Secretary Michelle Donelan announced the plan at the AI Seoul Summit, stating the goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco"}
{"doc_id": "AI safety", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco.\n\nCorporate self-regulation\nAI labs and companies generally abide by safety practices and norms that fall outside of formal legislation. One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing, offering bounties for finding failures, sharing AI incidents (an AI incident database was created for this purpose), following guidelines to determine whether to publish research or models, and improving information and cyber security in AI labs.\nCompanies have also made commitments. Cohere, OpenAI, and AI21 proposed and agreed on \"best practices for deploying language models\", focusing on mitigating misuse. To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that \"if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project\" Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles and the Autonomous Weapons Open Letter.\n\nSee also\nAI alignment\nArtificial intelligence and elections\nArtificial intelligence detection software\nHallucination (artificial intelligence)"}
{"doc_id": "AI veganism", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "AI veganism applies the rules and thinking of veganism to artificial intelligence (AI). The term has been used both to refer to the idea that people should abstain to AI due to its effects on people and animals, and the idea that people should avoid harming AI systems, especially if they could one day feel things or have awareness.\n\nHarm to people and animals\nSome AI vegans have drawn parallels between the use of data without consent to train AI, and the harm inflicted on animals through animal husbandry. Similarly, they have drawn attention to how animal husbandry and AI training/usage both impact the environment.\nOn an individual level, some AI vegans posit that both the consumption of animal products and the usage of AI negatively impact the consumer or user.\nSome have suggested that AI furthers speciesism; for example, a study comparing GPT responses about animals more frequently suggested that cows, pigs, and chickens should be confined or slaughtered, in comparison to cats and dogs.\n\nIn practice\nSome people and companies have avoided using AI systems trained with unethical data or labor. Others support building AI using \"vegan values\" such as care, respect, and minimizing harm.\nSome people avoid using large language models altogether, because they believe the training process is harmful to people or the planet.\n\nHarm to AI\nEarlier thinkers have theorized on the moral element of interacting with machines. David J. Gunkel's 2012 book The Machine Question asked whether machines should have moral status. Jonathan Birch's The Edge of Sentience (2024) argues that if humans are not sure whether a system can feel pain, they treat it kindly just in case.\nPhilosopher and animal activist Oscar Horta has said that humans should be careful not to harm beings who might suffer—even if they are not sure whether they can.\n\nCriticism\nMany experts say current AI models are not alive and have no feelings. They argue that giving rights to machines now is not useful and could distract from more serious human and animal rights issues.\n\nSee also\nAI ethics\nSentientism\nDigital rights"}
{"doc_id": "AI washing", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "AI washing is a deceptive marketing tactic that consists of promoting a product or a service by overstating the role of artificial intelligence (AI) and the integration of it. Companies often involve in the practice to mislead customers to boost their offerings, and to secure funding from investors. The practice raises concerns regarding transparency, and legal issues.\n\nDefinition\nAI washing is a deceptive marketing practice. It involves promoting a product or a service by overstating the role of artificial intelligence (AI) and its integration in the design and manufacture of the same. The practice raises concerns regarding transparency, compliance with security regulations, and consumer trust in the AI industry potentially hampering legitimate advancements in AI. The term was first defined by the AI Now Institute, a research institute based at New York University in 2019. The term is derived from greenwashing, another deceptive marketing technique that involves uses environmental impact in a similar guise to AI. AI washing might involve a company claiming to have used AI in the development or enhancement of its products or services without its actual involvement, or using buzzwords such as \"smart\" or \"AI-powered\" without the product actually offering it or making use of it. A company may overstate the usage of AI or misuse the term, which is also construed as AI washing.\n\nUsage and effects\nAI washing can lead to deception of customers and misleading of investors. It is also an illegal and unethical practice that lacks transparency regarding disclosing the details of a product or a service. Companies involve in such a practice often in response to competition who might have used AI in their offerings. It might also be used as a ploy to secure funding and investment, assuming that it will attract them towards it. AI washing has been compared to dot-com bubble, when businesses appended \"dot-com\" to the end of the business name to boost their valuation.\nIn September 2023, Coca-Cola released a new product called Coca‑Cola Y3000, and the company stated that the Y3000 flavor had been \"co-created with human and artificial intelligence\". The company was accused of AI washing due to no proof of AI involvement in the creation of the product, and critics believed that AI was used as a way to grab consumer attention more than it was used in the actual product creation.\n\nMitigation\nCompanies are expected to be transparent and clearer in communicating the usage of AI in their products or services. Consumers can mitigate the same by requesting for hard evidence from the companies regarding the usage of AI tools. Customers should evaluate the product or service as a whole rather than being"}
{"doc_id": "AI washing", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " was used in the actual product creation.\n\nMitigation\nCompanies are expected to be transparent and clearer in communicating the usage of AI in their products or services. Consumers can mitigate the same by requesting for hard evidence from the companies regarding the usage of AI tools. Customers should evaluate the product or service as a whole rather than being swayed by the usage of AI. Informed decision making and purchasing can keep them from falling for such marketing gimmicks. The United States Securities and Exchange Commission (SEC) imposes penalties for companies indulging in such practices. In March 2024, the SEC imposed the first civil penalties on two companies for misleading statements about their use of AI, and in July 2024, it charged a corporate executive from a supposed AI hiring startup with fraud for the usage of buzzwords related to AI."}
{"doc_id": "AI-assisted software development", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "AI-assisted software development is the use of artificial intelligence agents to augment the software development life cycle. It uses large language models (LLMs), natural language processing, and other AI technologies to assist software developers in a range of tasks from initial code generation to subsequent debugging, testing and documentation.\n\nTechnologies\nCode generation\nLLMs that have been trained on source code repositories are able to generate functional code from natural language prompts. Such models have knowledge of programming syntax, common design patterns and best practices in a variety of programming languages.\n\nIntelligent code completion\nAI agents using pre-trained and fine-tuned LLMs can predict and suggest code completions based on context, going beyond simple keyword matching to infer the developer's intent and picture the broader structure of the developing codebase. An analysis has shown that such use of LLMs significantly enhances code completion performance across several programming languages and contexts, and the resulting capability of predicting relevant code snippets based on context and partial input boosts developer productivity substantially.\n\nTesting, debugging, code review and analysis\nAI is used to automatically generate test cases, identify potential bugs, and suggest fixes. LLMs trained on historical bug data can enable prediction of likely failure points in generated code. Similarly, AI agents are used to perform static code analysis, identify security vulnerabilities, suggest performance improvements and ensure adherence to coding standards and best practices.\nBeyond detection, researchers have explored using LLMs for automated program repair, where models propose candidate patches for buggy code. Off-the-shelf LLMs have been reported to repair some security-relevant defects in a zero-shot setting (i.e., without task-specific fine-tuning), including issues categorized by the Common Weakness Enumeration (CWE), being comparable to contemporary, non-AI bug fixing tools. These approaches build on LLMs’ code-generation capability and the resulting patches still require validation through software testing, static program analysis, and human code review.\n\nChallenges\nThe incorporation of AI tools has introduced new ethical dilemmas and intellectual property challenges. The ownership of AI-generated code is unclear: who is responsible for the generated end-product? Also unclear are the ethical responsibilities of generated code. Changes in the role of software engineers are inevitable.\n\nGovernance and oversight\nThe outputs from AI-assisted software development require to be validated through a combination of automated testing, static analysis tools and human review, creating a governance layer that acts as a safeguard ensuring quality and accountability.\n\nIndustry perspectives\nTechnology sector leaders have highlighted the transformative potential of AI-assisted software development. In an 'Unlocking AI"}
{"doc_id": "AI-assisted software development", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The outputs from AI-assisted software development require to be validated through a combination of automated testing, static analysis tools and human review, creating a governance layer that acts as a safeguard ensuring quality and accountability.\n\nIndustry perspectives\nTechnology sector leaders have highlighted the transformative potential of AI-assisted software development. In an 'Unlocking AI Potential' session of 'Advancing AI 2025' hosted by AMD Developer Central, Andrew Ng and Lisa Su emphasized the strategic and operational implications of integrating AI tools into development workflows. Ng noted that AI systems are increasingly capable of \"helping programmers focus on higher-level problem solving\", while Su framed the shift as \"an opportunity to redefine performance and productivity across industries.\"\n\nSee also\nIntegrated development environment\nVibe coding\nGitHub Copilot\nGoogle DeepMind AlphaCode\nNo-code development platform\nMachine learning"}
{"doc_id": "AI-complete", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In the field of artificial intelligence (AI), tasks that are hypothesized to require artificial general intelligence to solve are informally known as AI-complete or AI-hard. Calling a problem AI-complete reflects the belief that it cannot be solved by a simple specific algorithm.  \nPrior to 2013, problems supposed to be AI-complete included computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. AI-complete tasks were notably considered useful for distinguishing humans from automated agents, as CAPTCHAs aim to do.\n\nHistory\nThe term was coined by Fanya Montalvo by analogy with NP-complete and NP-hard in complexity theory, which formally describes the most famous class of difficult problems. Early uses of the term are in Erik Mueller's 1987 PhD dissertation and in Eric Raymond's 1991 Jargon File.\nExpert systems, that were popular in the 1980s, were able to solve very simple and/or restricted versions of AI-complete problems, but never in their full generality. When AI researchers attempted to \"scale up\" their systems to handle more complicated, real-world situations, the programs tended to become excessively brittle without commonsense knowledge or a rudimentary understanding of the situation: they would fail as unexpected circumstances outside of its original problem context would begin to appear. When human beings are dealing with new situations in the world, they are helped by their awareness of the general context: they know what the things around them are, why they are there, what they are likely to do and so on. They can recognize unusual situations and adjust accordingly. Expert systems lacked this adaptability and were brittle when facing new situations.\nDeepMind published a work in May 2022 in which they trained a single model to do several things at the same time. The model, named Gato, can \"play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.\" Similarly, some tasks once considered to be AI-complete, like machine translation, are among the capabilities of large language models.\n\nAI-complete problems\nAI-complete problems have been hypothesized to include:\n\nAI peer review (composite natural language understanding, automated reasoning, automated theorem proving, formalized logic expert system)\nBongard problems\nComputer vision (and subproblems such as object recognition)\nNatural language understanding (and subproblems such as text mining, machine translation, and word-sense disambiguation)\nAutonomous driving\nDe"}
{"doc_id": "AI-complete", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (composite natural language understanding, automated reasoning, automated theorem proving, formalized logic expert system)\nBongard problems\nComputer vision (and subproblems such as object recognition)\nNatural language understanding (and subproblems such as text mining, machine translation, and word-sense disambiguation)\nAutonomous driving\nDealing with unexpected circumstances while solving any real world problem, whether navigation, planning, or even the kind of reasoning done by expert systems.\n\nFormalization\nComputational complexity theory deals with the relative computational difficulty of computable functions. By definition, it does not cover problems whose solution is unknown or has not been characterized formally. Since many AI problems have no formalization yet, conventional complexity theory does not enable a formal definition of AI-completeness.\n\nResearch\nRoman Yampolskiy\nsuggests that a problem \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is AI-Complete if it has two properties:\n\nIt is in the set of AI problems (Human Oracle-solvable).\nAny AI problem can be converted into \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n by some polynomial time algorithm.\nOn the other hand, a problem \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n is AI-Hard if and only if there is an AI-Complete problem \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n that is polynomial time Turing-reducible to \n  \n    \n      \n        H\n      \n    \n    {\\displaystyle H}\n  \n. This also gives as a consequence the existence of AI-Easy problems, that are solvable in polynomial time by a deterministic Turing machine with an oracle for some problem.\nYampolskiy has also hypothesized that the Turing Test is a defining feature of AI-completeness.\nGroppe and Jain classify problems which require artificial general intelligence to reach human-level machine performance as AI-complete, while only restricted versions of AI-complete problems can be solved by the current AI systems. For Šekrst, getting a polynomial solution to AI-complete problems would not necessarily be equal to solving the issue of artificial general intelligence, while emphasizing the lack of computational complexity research being the limiting factor towards achieving artificial general intelligence.\nFor Kwee-Bintoro and Velez, solving AI-complete problems would have strong repercussions on society.\n\nSee also\nASR-complete\nList of unsolved problems in computer science\nSynthetic intelligence"}
{"doc_id": "AI-complete", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Kwee-Bintoro and Velez, solving AI-complete problems would have strong repercussions on society.\n\nSee also\nASR-complete\nList of unsolved problems in computer science\nSynthetic intelligence"}
{"doc_id": "AIOps", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "AIOps (Artificial Intelligence for IT Operations) refers to the use of artificial intelligence, machine learning, and big data analytics to automate and enhance data center management. It helps organizations manage complex IT environments by detecting, diagnosing, and resolving issues more efficiently than traditional methods.\n\nHistory\nAIOps was first defined by Gartner in 2016, combining \"artificial intelligence\" and \"IT operations\" to describe the application of AI and machine learning to enhance IT operations. This concept was introduced to address the increasing complexity and data volume in IT environments, aiming to automate processes such as event correlation, anomaly detection, and causality determination.\n\nDefinition\nAIOps refers to the multi-layered complex technology platforms which enhance and automate IT operations by using machine learning and analytics to analyze the large amounts of data collected from various DevOps devices and tools, automatically identifying and responding to issues in real-time. AIOps is used as a shift from isolated IT data to aggregated observational data (e.g., job logs and monitoring systems) and interaction data (such as ticketing, events, or incident records) within a big data platform AIOps applies machine learning and analytics to this data. The result is continuous visibility, which, combined with the implementation of automation, can lead to ongoing improvements. AIOps connects three IT disciplines (automation, service management, and performance management) to achieve continuous visibility and improvement. This new approach in modern, accelerated, and hyper-scaled IT environments leverages advances in machine learning and big data to overcome previous limitations.\n\nComponents\nAIOps consists of a number of components including the following processes and techniques:\n\nAnomaly Detection\nLog Analysis\nRoot Cause Analysis\nCohort Analysis\nEvent Correlation\nPredictive Analytics\nHardware Failure Prediction\nAutomated Remediation\nPerformance Prediction\nIncident Management\nCausality Determination\nQueue Management\nResource Scheduling and Optimization\nPredictive Capacity Management\nResource Allocation\nService Quality Monitoring\nDeployment and Integration Testing\nSystem Configuration\nAuto-diagnosis and Problem Localization\nEfficient ML Training and Inferencing\nUsing LLMs for Cloud Ops\nAuto Service Healing\nData Center Management\nCustomer Support\nSecurity and Privacy in Cloud Operations\n\nComparison with DevOps\nAIOps is increasingly compared with DevOps in terms of their impact on operational efficiency. While DevOps focuses on collaboration between development and operations teams to accelerate software delivery, AIOps integrates artificial intelligence to enhance monitoring, automation, and predictive capabilities. Various industry analyses have explored the similarities and differences between"}
{"doc_id": "AIOps", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " with DevOps\nAIOps is increasingly compared with DevOps in terms of their impact on operational efficiency. While DevOps focuses on collaboration between development and operations teams to accelerate software delivery, AIOps integrates artificial intelligence to enhance monitoring, automation, and predictive capabilities. Various industry analyses have explored the similarities and differences between the two approaches, including discussions on how organizations can combine them to improve incident management and resource optimization.\n\nResults\nAI optimizes IT operations in five ways: First, intelligent monitoring powered by AI helps identify potential issues before they cause outages, improving metrics like Mean Time to Detect (MTTD) by 15-20%. Second, performance data analysis and insights enable quick decision-making by ingesting and analyzing large data sets in real time. Third, AI-driven automated infrastructure optimization efficiently allocates resources and thereby reducing cloud costs. Fourth, enhanced IT service management reduces critical incidents by over 50% through AI-driven end-to-end service management. Lastly, intelligent task automation accelerates problem resolution and automates remedial actions with minimal human intervention.\nIn 2025, Atera Networks was identified as a leader in AIOps by the software review platform G2.\n\nAIOps vs. MLOps\nAIOps tools use big data analytics, machine learning algorithms, and predictive analytics to detect anomalies, correlate events, and provide proactive insights. This automation reduces the burden on IT teams, allowing them to focus on strategic tasks rather than routine operational issues. AIOps is widely used by IT operations teams, DevOps, network administrators, and IT service management (ITSM) teams to enhance visibility and enable quicker incident resolution in hybrid cloud environments, data centers, and other IT infrastructures.\nIn contrast to MLOps (Machine Learning Operations), which focuses on the lifecycle management and operational aspects of machine learning models, AIOps focuses on optimizing IT operations using a variety of analytics and AI-driven techniques. While both disciplines rely on AI and data-driven methods, AIOps primarily targets IT operations, whereas MLOps is concerned with the deployment, monitoring, and maintenance of ML models.\n\nConferences\nThere are several conferences that are specific to AIOps:\n\nAIOps Summit\nAI Dev Summit\nIBM Think conference"}
{"doc_id": "AIOps", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "IBM Think conference"}
{"doc_id": "Algorithmic probability", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s. \nIt is used in inductive inference theory and analyses of algorithms. In his general theory of inductive inference, Solomonoff uses the method together with Bayes' rule to obtain probabilities of prediction for an algorithm's future outputs.\nIn the mathematical formalism used, the observations have the form of finite binary strings viewed as outputs of Turing machines, and the universal prior is a probability distribution over the set of finite binary strings calculated from a probability distribution over programs (that is, inputs to a universal Turing machine).  The prior is universal in the\nTuring-computability sense, i.e. no string has zero probability. It is not computable, but it can be approximated.\nFormally, the probability \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is not a probability and it is not computable. It is only \"lower semi-computable\" and a \"semi-measure\". By \"semi-measure\", it means that \n  \n    \n      \n        0\n        ≤\n        \n          ∑\n          \n            x\n          \n        \n        P\n        (\n        x\n        )\n        <\n        1\n      \n    \n    {\\displaystyle 0\\leq \\sum _{x}P(x)<1}\n  \n. That is, the \"probability\" does not actually sum up to one, unlike actual probabilities. This is because some inputs to the Turing machine causes it to never halt, which means the probability mass allocated to those inputs is lost. By \"lower semi-computable\", it means there is a Turing machine that, given an input string \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, can print out a sequence \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        <\n        \n          y\n          \n            2\n          \n        \n        <\n        ⋯\n      \n    \n    {\\displaystyle y_{1}<y_{2}<\\cdots }\n  \n that converges to \n  \n    \n      \n        P\n        (\n        x\n        )\n      \n    \n    {\\displaystyle P(x)}\n  \n from below, but there is no such Turing machine that does the same from above.\n\nOverview\nAlgorithmic probability is the main ingredient of Solomonoff's theory of inductive inference, the theory of prediction based on observations; it"}
{"doc_id": "Algorithmic probability", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " P\n        (\n        x\n        )\n      \n    \n    {\\displaystyle P(x)}\n  \n from below, but there is no such Turing machine that does the same from above.\n\nOverview\nAlgorithmic probability is the main ingredient of Solomonoff's theory of inductive inference, the theory of prediction based on observations; it was invented with the goal of using it for machine learning; given a sequence of symbols, which one will come next? Solomonoff's theory provides an answer that is optimal in a certain sense, although it is incomputable. \nFour principal inspirations for Solomonoff's algorithmic probability were: Occam's razor, Epicurus' principle of multiple explanations, modern computing theory (e.g. use of a universal Turing machine) and Bayes’ rule for prediction.\nOccam's razor and Epicurus' principle are essentially two different non-mathematical approximations of the universal prior.\n\nOccam's razor: among the theories that are consistent with the observed phenomena, one should select the simplest theory.\nEpicurus' principle of multiple explanations: if more than one theory is consistent with the observations, keep all such theories.\nAt the heart of the universal prior is an abstract model of a computer, such as a universal Turing machine.   Any abstract computer will do, as long as it is Turing-complete, i.e. every computable function has at least one program that will compute its application on the abstract computer.\nThe abstract computer is used to give precise meaning to the phrase \"simple explanation\".  In the formalism used, explanations, or theories of phenomena, are computer programs that generate observation strings when run on the abstract computer.  Each computer program is assigned a weight corresponding to its length. The universal probability distribution is the probability distribution on all possible output strings with random input, assigning for each finite output prefix q the sum of the probabilities of the programs that compute something starting with q.  Thus, a simple explanation is a short computer program. A complex explanation is a long computer program.  Simple explanations are more likely, so a high-probability observation string is one generated by a short computer program, or perhaps by any of a large number of slightly longer computer programs.  A low-probability observation string is one that can only be generated by a long computer program.\nAlgorithmic probability is closely related to the concept of Kolmogorov complexity.  Kolmogorov's introduction of complexity was motivated by information theory and problems in randomness, while Solomonoff introduced algorithmic complexity for"}
{"doc_id": "Algorithmic probability", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " low-probability observation string is one that can only be generated by a long computer program.\nAlgorithmic probability is closely related to the concept of Kolmogorov complexity.  Kolmogorov's introduction of complexity was motivated by information theory and problems in randomness, while Solomonoff introduced algorithmic complexity for a different reason: inductive reasoning. A single universal prior probability that can be substituted for each actual prior probability in Bayes's rule was invented by Solomonoff with Kolmogorov complexity as a side product.  It predicts the most likely continuation of that observation, and provides a measure of how likely this continuation will be.\nSolomonoff's enumerable measure is universal in a certain powerful sense, but the computation time can be infinite. One way of dealing with this issue is a variant of Leonid Levin's Search Algorithm, which limits the time spent computing the success of possible programs, with shorter programs given more time. When run for longer and longer periods of time, it will generate a sequence of approximations which converge to the universal probability distribution.  Other methods of dealing with the issue include limiting the search space by including training sequences.\nSolomonoff proved this distribution to be machine-invariant within a constant factor (called the invariance theorem).\n\nFundamental Theorems\nI. Kolmogorov's Invariance Theorem\nKolmogorov's Invariance theorem clarifies that the Kolmogorov Complexity, or Minimal Description Length, of a dataset \nis invariant to the choice of Turing-Complete language used to simulate a Universal Turing Machine:\n\n  \n    \n      \n        ∀\n        x\n        ∈\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            ∗\n          \n        \n        ,\n        \n          |\n        \n        \n          K\n          \n            U\n          \n        \n        (\n        x\n        )\n        −\n        \n          K\n          \n            \n              U\n              ′\n            \n          \n        \n        (\n        x\n        )\n        \n          |\n        \n        ≤\n        \n          \n            O\n          \n        \n        (\n        1\n        )\n      \n    \n    {\\displaystyle \\forall x\\in \\{0,1\\}^{*},|K_{U}(x)-K_{U'}(x)|\\leq {\\mathcal {O}}(1)}\n  \n\nwhere \n  \n    \n      \n        \n          K\n          \n            U\n          \n        \n        (\n        x\n        )\n        =\n        \n          min\n          \n            p\n          \n        \n        {\n        \n"}
{"doc_id": "Algorithmic probability", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "},|K_{U}(x)-K_{U'}(x)|\\leq {\\mathcal {O}}(1)}\n  \n\nwhere \n  \n    \n      \n        \n          K\n          \n            U\n          \n        \n        (\n        x\n        )\n        =\n        \n          min\n          \n            p\n          \n        \n        {\n        \n          |\n        \n        p\n        \n          |\n        \n        :\n        U\n        (\n        p\n        )\n        =\n        x\n        }\n      \n    \n    {\\displaystyle K_{U}(x)=\\min _{p}\\{|p|:U(p)=x\\}}\n  \n.\n\nInterpretation\nThe minimal description \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n such that \n  \n    \n      \n        U\n        (\n        p\n        )\n        =\n        x\n      \n    \n    {\\displaystyle U(p)=x}\n  \n serves as a natural representation of the string \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n relative to the Turing-Complete language \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n. Moreover, as \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n can't be compressed further \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is an incompressible and hence uncomputable string. This corresponds to a scientists' notion of randomness and clarifies the reason why Kolmogorov Complexity is not computable.\nIt follows that any piece of data has a necessary and sufficient representation in terms of a random string.\n\nProof\nThe following is taken from \nFrom the theory of compilers, it is known that for any two Turing-Complete languages \n  \n    \n      \n        \n          U\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle U_{1}}\n  \n and \n  \n    \n      \n        \n          U\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle U_{2}}\n  \n, there exists a compiler \n  \n    \n      \n        \n          Λ\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle \\Lambda _{1}}\n  \n expressed in \n\n  \n    \n      \n        \n          U\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle U_{1}}\n  \n that translates programs expressed in \n  \n    \n      \n        \n          U\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle U_{2}}\n  \n into functionally-equivalent programs expressed in \n  \n    \n      \n        \n          U\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle U_{1}}\n  \n.\n"}
{"doc_id": "Algorithmic probability", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " that translates programs expressed in \n  \n    \n      \n        \n          U\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle U_{2}}\n  \n into functionally-equivalent programs expressed in \n  \n    \n      \n        \n          U\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle U_{1}}\n  \n.\nIt follows that if we let \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n be the shortest program that prints a given string \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n then:\n\n  \n    \n      \n        \n          K\n          \n            \n              U\n              \n                1\n              \n            \n          \n        \n        (\n        x\n        )\n        ≤\n        \n          |\n        \n        \n          Λ\n          \n            1\n          \n        \n        \n          |\n        \n        +\n        \n          |\n        \n        p\n        \n          |\n        \n        ≤\n        \n          K\n          \n            \n              U\n              \n                2\n              \n            \n          \n        \n        (\n        x\n        )\n        +\n        \n          \n            O\n          \n        \n        (\n        1\n        )\n      \n    \n    {\\displaystyle K_{U_{1}}(x)\\leq |\\Lambda _{1}|+|p|\\leq K_{U_{2}}(x)+{\\mathcal {O}}(1)}\n  \n\nwhere \n  \n    \n      \n        \n          |\n        \n        \n          Λ\n          \n            1\n          \n        \n        \n          |\n        \n        =\n        \n          \n            O\n          \n        \n        (\n        1\n        )\n      \n    \n    {\\displaystyle |\\Lambda _{1}|={\\mathcal {O}}(1)}\n  \n, and by symmetry we obtain the opposite inequality.\n\nII. Levin's Universal Distribution\nGiven that any uniquely-decodable code satisfies the Kraft-McMillan inequality, prefix-free Kolmogorov Complexity allows us to derive the Universal \nDistribution:\n\n  \n    \n      \n        P\n        (\n        x\n        )\n        =\n        \n          ∑\n          \n            U\n            (\n            p\n            )\n            =\n            x\n          \n        \n        P\n        (\n        U\n        (\n        p\n        )\n        =\n        x\n        )\n        =\n        \n          ∑\n          \n            U\n            (\n            p\n            )\n            =\n            x\n          \n        \n        \n          2\n          \n            −\n            \n              K\n              \n                U\n              \n            \n            (\n            p\n            )\n          \n        \n        ≤\n        1\n      \n    \n    {\\displaystyle P(x)=\\sum _{U(p)=x}P(U"}
{"doc_id": "Algorithmic probability", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " p\n            )\n            =\n            x\n          \n        \n        \n          2\n          \n            −\n            \n              K\n              \n                U\n              \n            \n            (\n            p\n            )\n          \n        \n        ≤\n        1\n      \n    \n    {\\displaystyle P(x)=\\sum _{U(p)=x}P(U(p)=x)=\\sum _{U(p)=x}2^{-K_{U}(p)}\\leq 1}\n  \n\nwhere the fact that \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n may simulate a prefix-free UTM implies that for two distinct descriptions \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n and \n  \n    \n      \n        \n          p\n          ′\n        \n      \n    \n    {\\displaystyle p'}\n  \n, \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n isn't \na substring of \n  \n    \n      \n        \n          p\n          ′\n        \n      \n    \n    {\\displaystyle p'}\n  \n and \n  \n    \n      \n        \n          p\n          ′\n        \n      \n    \n    {\\displaystyle p'}\n  \n isn't a substring of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n.\n\nInterpretation\nIn a Computable Universe, given a phenomenon with encoding \n  \n    \n      \n        x\n        ∈\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle x\\in \\{0,1\\}^{*}}\n  \n generated by a physical process the probability of that phenomenon is well-defined and equal to the sum over the probabilities of distinct and independent causes. The prefix-free criterion is precisely what guarantees causal independence.\n\nProof\nThis is an immediate consequence of the Kraft-McMillan inequality.\nKraft's inequality states that given a sequence of strings \n  \n    \n      \n        {\n        \n          x\n          \n            i\n          \n        \n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\{x_{i}\\}_{i=1}^{n}}\n  \n there exists a prefix code with codewords \n  \n    \n      \n        {\n        \n          σ\n          \n            i\n          \n        \n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\{\\sigma _{i}\\}_{i=1}^{n}}\n  \n where \n  \n    \n      \n        ∀\n        i\n        ,\n        \n          |\n        \n        \n          σ\n"}
{"doc_id": "Algorithmic probability", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " i\n          \n        \n        \n          }\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle \\{\\sigma _{i}\\}_{i=1}^{n}}\n  \n where \n  \n    \n      \n        ∀\n        i\n        ,\n        \n          |\n        \n        \n          σ\n          \n            i\n          \n        \n        \n          |\n        \n        =\n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\forall i,|\\sigma _{i}|=k_{i}}\n  \n if and only if:\n\n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          s\n          \n            −\n            \n              k\n              \n                i\n              \n            \n          \n        \n        ≤\n        1\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}s^{-k_{i}}\\leq 1}\n  \n\nwhere \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is the size of the alphabet \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n.\nWithout loss of generality, let's suppose we may order the \n  \n    \n      \n        \n          k\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle k_{i}}\n  \n such that:\n\n  \n    \n      \n        \n          k\n          \n            1\n          \n        \n        ≤\n        \n          k\n          \n            2\n          \n        \n        ≤\n        .\n        .\n        .\n        ≤\n        \n          k\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle k_{1}\\leq k_{2}\\leq ...\\leq k_{n}}\n  \n\nNow, there exists a prefix code if and only if at each step \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n there is at least one codeword to choose that does not contain any of the previous \n  \n    \n      \n        j\n        −\n        1\n      \n    \n    {\\displaystyle j-1}\n  \n codewords as a prefix. Due to the existence of a codeword at a previous step \n  \n    \n      \n        i\n        <\n        j\n        ,\n        \n          s\n          \n            \n              k\n              \n                j\n              \n            \n            −\n            \n              k\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle i<j,s^{k_{j}-k_{i}}}\n  \n codewords are forbidden as they contain \n  \n    \n      \n        \n          σ\n          \n            i\n"}
{"doc_id": "Algorithmic probability", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "            \n              k\n              \n                j\n              \n            \n            −\n            \n              k\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle i<j,s^{k_{j}-k_{i}}}\n  \n codewords are forbidden as they contain \n  \n    \n      \n        \n          σ\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{i}}\n  \n as a prefix. It follows that in general a prefix code exists if and only if:\n\n  \n    \n      \n        ∀\n        j\n        ≥\n        2\n        ,\n        \n          s\n          \n            \n              k\n              \n                j\n              \n            \n          \n        \n        >\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            j\n            −\n            1\n          \n        \n        \n          s\n          \n            \n              k\n              \n                j\n              \n            \n            −\n            \n              k\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\forall j\\geq 2,s^{k_{j}}>\\sum _{i=1}^{j-1}s^{k_{j}-k_{i}}}\n  \n\nDividing both sides by \n  \n    \n      \n        \n          s\n          \n            \n              k\n              \n                j\n              \n            \n          \n        \n      \n    \n    {\\displaystyle s^{k_{j}}}\n  \n, we find:\n\n  \n    \n      \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          s\n          \n            −\n            \n              k\n              \n                i\n              \n            \n          \n        \n        ≤\n        1\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}s^{-k_{i}}\\leq 1}\n  \n\nQED.\n\nHistory\nSolomonoff invented the concept of algorithmic probability with its associated invariance theorem around 1960, publishing a report on it: \"A Preliminary Report on a General Theory of Inductive Inference.\" He clarified these ideas more fully in 1964 with \"A Formal Theory of Inductive Inference,\" Part I and Part II.\n\nSequential Decisions Based on Algorithmic Probability\nSequential Decisions Based on Algorithmic Probability is a theoretical framework proposed by Marcus Hutter to unify algorithmic probability with decision theory. The framework provides a foundation for creating universally intelligent agents capable of optimal performance in any computable environment. It builds on Solomonoff’s theory of induction and incorporates elements of reinforcement learning, optimization, and sequential decision-making.\n\nBackground\nInd"}
{"doc_id": "Algorithmic probability", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " is a theoretical framework proposed by Marcus Hutter to unify algorithmic probability with decision theory. The framework provides a foundation for creating universally intelligent agents capable of optimal performance in any computable environment. It builds on Solomonoff’s theory of induction and incorporates elements of reinforcement learning, optimization, and sequential decision-making.\n\nBackground\nInductive reasoning, the process of predicting future events based on past observations, is central to intelligent behavior. Hutter formalized this process using Occam’s razor and algorithmic probability. The framework is rooted in Kolmogorov complexity, which measures the simplicity of data by the length of its shortest descriptive program. This concept underpins the universal distribution MM, as introduced by Ray Solomonoff, which assigns higher probabilities to simpler hypotheses.\nHutter extended the universal distribution to include actions, creating a framework capable of addressing problems such as prediction, optimization, and reinforcement learning in environments with unknown structures.\n\nThe AIXI Model\nThe AIXI model is the centerpiece of Hutter’s theory. It describes a universal artificial agent designed to maximize expected rewards in an unknown environment. AIXI operates under the assumption that the environment can be represented by a computable probability distribution. It uses past observations to infer the most likely environmental model, leveraging algorithmic probability.\nMathematically, AIXI evaluates all possible future sequences of actions and observations. It computes their algorithmic probabilities and expected utilities, selecting the sequence of actions that maximizes cumulative rewards. This approach transforms sequential decision-making into an optimization problem. However, the general formulation of AIXI is incomputable, making it impractical for direct implementation.\n\nOptimality and Limitations\nAIXI is universally optimal in the sense that it performs as well as or better than any other agent in all computable environments. This universality makes it a theoretical benchmark for intelligence. However, its reliance on algorithmic probability renders it computationally infeasible, requiring exponential time to evaluate all possibilities.\nTo address this limitation, Hutter proposed time-bounded approximations, such as AIXItl, which reduce computational demands while retaining many theoretical properties of the original model. These approximations provide a more practical balance between computational feasibility and optimality.\n\nApplications and Implications\nThe AIXI framework has significant implications for artificial intelligence and related fields. It provides a formal benchmark for measuring intelligence and a theoretical foundation for solving various problems, including prediction, reinforcement learning, and optimization.\nDespite its strengths, the framework has limitations. AIXI assumes that the environment is computable,"}
{"doc_id": "Algorithmic probability", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Implications\nThe AIXI framework has significant implications for artificial intelligence and related fields. It provides a formal benchmark for measuring intelligence and a theoretical foundation for solving various problems, including prediction, reinforcement learning, and optimization.\nDespite its strengths, the framework has limitations. AIXI assumes that the environment is computable, excluding chaotic or non-computable systems. Additionally, its high computational requirements make real-world applications challenging.\n\nPhilosophical Considerations\nHutter’s theory raises philosophical questions about the nature of intelligence and computation. The reliance on algorithmic probability ties intelligence to the ability to compute and predict, which may exclude certain natural or chaotic phenomena. Nonetheless, the AIXI model offers insights into the theoretical upper bounds of intelligent behavior and serves as a stepping stone toward more practical AI systems.\n\nKey people\nRay Solomonoff\nAndrey Kolmogorov\nLeonid Levin\n\nSee also\nSolomonoff's theory of inductive inference\nAlgorithmic information theory\nBayesian inference\nInductive inference\nInductive probability\nKolmogorov complexity\nUniversal Turing machine\nInformation-based complexity"}
{"doc_id": "Ameca (robot)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Ameca is a robotic humanoid created in 2021 by Engineered Arts, headquarters in Falmouth, Cornwall, United Kingdom. The project commenced in February 2021, and the first public demonstration was at the CES 2022 show in Las Vegas. Ameca's appearance features grey rubber skin on the face and hands, and is specifically designed to appear genderless.\nIn 2024, an Ameca unit was installed in Edinburgh in the UK to reside at the National Robotarium. \nAmeca generation 3 has been released and showcased at ICRA 2025 along with Ami.\n\nHistory\nThe first generation of Ameca was developed at Engineered Arts headquarters in Falmouth, Cornwall, United Kingdom. The project started in February 2021, with the first video revealed publicly on 1 December 2021. Ameca gained widespread attention on Twitter and TikTok ahead of its first public demonstration at the Consumer Electronics Show 2022, where it was covered by CNET and other news outlets.\nIn 2022, Ameca presented an Alternative Christmas message by British TV Channel 4 for Christmas Day. Ameca was associated with the Museum of the Future's robotic family, where it could interact with visitors. In 2024, an Ameca unit was installed in Edinburgh in the UK to reside at the National Robotarium.\n\nFeatures\nIt is designed as a platform for further developing robotics technologies involving human-robot interaction. utilizes embedded microphones, binocular eye mounted cameras, a chest camera and facial recognition software to interact with the public. Interactions can be governed by either OpenAI's GPT-3 or human telepresence. It also features articulated motorized arms, fingers, neck and facial features. \nAmeca's appearance features grey rubber skin on the face and hands, and is specifically designed to appear genderless.\n\nPublic appearances\nComputer History Museum, California\nHeinz Nixdorf MuseumsForum, Paderborn, Germany\nCopernicus Science Center, Warsaw, Poland\nMuseum of the Future, Dubai\nConsumer Electronics Show 2022\nDeutsches Museum Nuremberg\nOMR Festival 2022 Hosted by Vodafone\nGITEX 2022\nInternational Conference on Robotics and Automation 2023\nInternational Telecommunication Union AI for Good Global Summit 2023\nSphere (Not Ameca, Custom humanoid named Aura built on Ameca technology)"}
{"doc_id": "Ameca (robot)", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "2022\nInternational Conference on Robotics and Automation 2023\nInternational Telecommunication Union AI for Good Global Summit 2023\nSphere (Not Ameca, Custom humanoid named Aura built on Ameca technology)"}
{"doc_id": "And–or tree", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "An and–or tree is a graphical representation of the reduction of problems (or goals) to conjunctions and disjunctions of subproblems (or subgoals).\n\nExample\nThe and–or tree:\n\nrepresents the search space for solving the problem P, using the goal-reduction methods:\n\nP if Q and R\nP if S\nQ if T\nQ if U\n\nDefinitions\nGiven an initial problem P0 and set of problem solving methods of the form:\n\nP if P1 and … and Pn\nthe associated and–or tree is a set of labelled nodes such that:\n\nThe root of the tree is a node labelled by P0.\nFor every node N labelled by a problem or sub-problem P and for every method of the form P if P1 and ... and Pn, there exists a set of children nodes N1, ..., Nn of the node N, such that each node Ni is labelled by Pi. The nodes are conjoined by an arc, to distinguish them from children of N that might be associated with other methods.\nA node N, labelled by a problem P, is a success node if there is a method of the form P if nothing (i.e., P is a \"fact\"). The node is a failure node if there is no method for solving P.\nIf all of the children of a node N, conjoined by the same arc, are success nodes, then the node N is also a success node. Otherwise the node is a failure node.\n\nSearch strategies\nAn and–or tree specifies only the search space for solving a problem. Different search strategies for searching the space are possible. These include searching the tree depth-first, breadth-first, or best-first using some measure of desirability of solutions. The search strategy can be sequential, searching or generating one node at a time, or parallel, searching or generating several nodes in parallel.\n\nRelationship with logic programming\nThe methods used for generating and–or trees are propositional logic programs (without variables). In the case of logic programs containing variables, the solutions of conjoint sub-problems must be compatible. Subject to this complication, sequential and parallel search strategies for and–or trees provide a computational model for executing logic programs.\n\nRelationship with two-player games\nAnd–or trees can also be used to represent the search spaces for two-person games. The root node of such a tree represents the problem of one of the players winning the game, starting from the initial state of the game. Given a node N, labelled by the problem"}
{"doc_id": "And–or tree", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " logic programs.\n\nRelationship with two-player games\nAnd–or trees can also be used to represent the search spaces for two-person games. The root node of such a tree represents the problem of one of the players winning the game, starting from the initial state of the game. Given a node N, labelled by the problem P of the player winning the game from a particular state of play, there exists a single set of conjoint children nodes, corresponding to all of the opponents responding moves. \nFor each of these children nodes, there exists a set of non-conjoint children nodes, corresponding to all of the player's defending moves.\nFor solving game trees with proof-number search family of algorithms, game trees are to be mapped to and–or trees. MAX-nodes (i.e. maximizing player to move) are represented as OR nodes, MIN-nodes map to AND nodes. The mapping is possible, when the search is done with only a binary goal, which usually is \"player to move wins the game\".\n\nBibliography\nLuger, George F.; Stubblefield, William A. (1993). Artificial intelligence: structures and strategies for complex problem solving (2 ed.). The Benjamin/Cummings. ISBN 978-0-8053-4785-2. Retrieved 28 February 2013.\nNilsson, Nils J. (1998). Artificial Intelligence: A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Retrieved 28 February 2013.\nRussell, S. and Norvig, P., 2021. Artificial Intelligence: a modern approach, 4th US ed. University of California, Berkeley, p 141."}
{"doc_id": "Anna Becker", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Anna Becker is an Israeli researcher known in the field of artificial intelligence and computer science within the financial field.\n\nEarly life and education\nBecker was born in Russia and immigrated to Israel at 16 after graduating from a school in Moscow. At 17, she began her studies at Technion – Israel Institute of Technology. During her master's degree in computer science, she taught first-year students of the same course, and at 27, Becker completed her PhD in Computer Science and Artificial Intelligence.\n\nCareer\nWhile pursuing her PhD, Becker resolved an NP-complete approximation algorithm that had been unresolved for over twenty years. This made her a recognized scholar in the field. After completing her PhD, she developed an approximation technique by a factor of two. This technique is widely used today in operating systems, database systems, and VLSI chip designs.\nShe then founded and sold Strategy Runner, a fintech software. After this, she founded EndoTech, an algorithmic trading platform based on artificial intelligence and machine learning. As of 2023, Becker is working on Fianchetto Fund, an AI-based investing analysis platform.\nBecker has also co-authored a book on Bayesian networks, which has been published widely in the field of computer science and artificial intelligence."}
{"doc_id": "Answer engine optimization", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Answer engine optimization (AEO) is a set of strategies and practices aimed at improving the visibility and retrieval of digital content by large language models, conversational agents, and AI-driven search engines.\nWhile AEO concerns the optimisation of content for improved retrieval by AI systems, Artificial intelligence optimization (AIO) concerns the optimisation of the AI systems themselves. AEO addresses how human-generated information should be organised so that AI systems can interpret, verify, and present it effectively.\nUnlike traditional Search Engine Optimization (SEO), which focuses on ranking within link-based search results, AEO prioritises the production of structured and semantically rich information that can be directly extracted and reformulated as an answer.\nIn recent years, as conversational-search tools and AI-driven answer interfaces (such as chatbots, virtual assistants, and AI Overviews) have grown in usage, AEO has emerged as a distinct discipline from traditional search engine optimisation. Tools such as Writesonic GEO toolkit, Semrush's AI Visibility Toolkit and Enterprise AIO reflect the growing emphasis on monitoring how websites and entities are cited, referenced, or incorporated into responses produced by Large language model (LLM) answer engines."}
{"doc_id": "Argumentation framework", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In artificial intelligence and related fields, an argumentation framework is a way to deal with contentious information and draw conclusions from it using formalized arguments.\nIn an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, an argumentation framework is represented with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation.\nThere exist some extensions of the Dung's framework, like the logic-based argumentation frameworks or the value-based argumentation frameworks.\n\nAbstract argumentation frameworks\nFormal framework\nAbstract argumentation frameworks, also called argumentation frameworks à la Dung, are defined formally as a pair:\n\nA set of abstract elements called arguments, denoted \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n\nA binary relation on \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, called attack relation, denoted \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n\nFor instance, the argumentation system \n  \n    \n      \n        S\n        =\n        ⟨\n        A\n        ,\n        R\n        ⟩\n      \n    \n    {\\displaystyle S=\\langle A,R\\rangle }\n  \n with \n  \n    \n      \n        A\n        =\n        {\n        a\n        ,\n        b\n        ,\n        c\n        ,\n        d\n        }\n      \n    \n    {\\displaystyle A=\\{a,b,c,d\\}}\n  \n and \n  \n    \n      \n        R\n        =\n        {\n        (\n        a\n        ,\n        b\n        )\n        ,\n        (\n        b\n        ,\n        c\n        )\n        ,\n        (\n        d\n        ,\n        c\n        )\n        }\n      \n    \n    {\\displaystyle R=\\{(a,b),(b,c),(d,c)\\}}\n  \n contains four arguments (\n  \n    \n      \n        a\n        ,\n        b\n        ,\n        c\n      \n    \n    {\\displaystyle a,b,c}\n  \n and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n) and three attacks (\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n attacks \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n, \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n attacks \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n attacks \n  \n    \n"}
{"doc_id": "Argumentation framework", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "  \n attacks \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n, \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n attacks \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n attacks \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n).\nDung defines some notions :\n\nan argument \n  \n    \n      \n        a\n        ∈\n        A\n      \n    \n    {\\displaystyle a\\in A}\n  \n is acceptable with respect to \n  \n    \n      \n        E\n        ⊆\n        A\n      \n    \n    {\\displaystyle E\\subseteq A}\n  \n if and only if \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n defends \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n, that is \n  \n    \n      \n        ∀\n        b\n        ∈\n        A\n      \n    \n    {\\displaystyle \\forall b\\in A}\n  \n such that \n  \n    \n      \n        (\n        b\n        ,\n        a\n        )\n        ∈\n        R\n        ,\n        ∃\n        c\n        ∈\n        E\n      \n    \n    {\\displaystyle (b,a)\\in R,\\exists c\\in E}\n  \n such that \n  \n    \n      \n        (\n        c\n        ,\n        b\n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle (c,b)\\in R}\n  \n,\na set of arguments \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is conflict-free if there is no attack between its arguments, formally : \n  \n    \n      \n        ∀\n        a\n        ,\n        b\n        ∈\n        E\n        ,\n        (\n        a\n        ,\n        b\n        )\n        ∉\n        R\n      \n    \n    {\\displaystyle \\forall a,b\\in E,(a,b)\\not \\in R}\n  \n,\na set of arguments \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is admissible if and only if it is conflict-free and all its arguments are acceptable with respect to \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n.\n\nDifferent semantics of acceptance\nExtensions\nTo decide if an argument can be accepted or not, or if several arguments can be accepted together, Dung defines several semantics of acceptance that allows, given an argumentation system, sets of arguments (called extensions) to be computed. For instance, given \n  \n    \n      \n        S\n        =\n        ⟨"}
{"doc_id": "Argumentation framework", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\nExtensions\nTo decide if an argument can be accepted or not, or if several arguments can be accepted together, Dung defines several semantics of acceptance that allows, given an argumentation system, sets of arguments (called extensions) to be computed. For instance, given \n  \n    \n      \n        S\n        =\n        ⟨\n        A\n        ,\n        R\n        ⟩\n      \n    \n    {\\displaystyle S=\\langle A,R\\rangle }\n  \n,\n\n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is a complete extension of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n only if it is an admissible set and every acceptable argument with respect to \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n belongs to \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n,\n\n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is a preferred extension of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n only if it is a maximal element (with respect to the set-theoretical inclusion) among the admissible sets with respect to \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n,\n\n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is a stable extension of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n only if it is a conflict-free set that attacks every argument that does not belong in \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n (formally, \n  \n    \n      \n        ∀\n        a\n        ∈\n        A\n        ∖\n        E\n        ,\n        ∃\n        b\n        ∈\n        E\n      \n    \n    {\\displaystyle \\forall a\\in A\\backslash E,\\exists b\\in E}\n  \n such that \n  \n    \n      \n        (\n        b\n        ,\n        a\n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle (b,a)\\in R}\n  \n,\n\n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n  \n is the (unique) grounded extension of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n only if it is the smallest element (with respect to set inclusion) among the complete extensions of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n.\nThere exists some inclusions between the sets of extensions built with these semantics :\n\nEvery stable extension is preferred,\nEvery preferred extension is complete,\nThe grounded extension is complete,\nIf the system"}
{"doc_id": "Argumentation framework", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " element (with respect to set inclusion) among the complete extensions of \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n.\nThere exists some inclusions between the sets of extensions built with these semantics :\n\nEvery stable extension is preferred,\nEvery preferred extension is complete,\nThe grounded extension is complete,\nIf the system is well-founded (there exists no infinite sequence \n  \n    \n      \n        \n          a\n          \n            0\n          \n        \n        ,\n        \n          a\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          a\n          \n            n\n          \n        \n        ,\n        …\n      \n    \n    {\\displaystyle a_{0},a_{1},\\dots ,a_{n},\\dots }\n  \n such that \n  \n    \n      \n        ∀\n        i\n        >\n        0\n        ,\n        (\n        \n          a\n          \n            i\n            +\n            1\n          \n        \n        ,\n        \n          a\n          \n            i\n          \n        \n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle \\forall i>0,(a_{i+1},a_{i})\\in R}\n  \n), all these semantics coincide—only one extension is grounded, stable, preferred, and complete.\nSome other semantics have been defined.\nOne introduce the notation \n  \n    \n      \n        E\n        x\n        \n          t\n          \n            σ\n          \n        \n        (\n        S\n        )\n      \n    \n    {\\displaystyle Ext_{\\sigma }(S)}\n  \n to note the set of \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n-extensions of the system \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n.\nIn the case of the system \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n in the figure above, \n  \n    \n      \n        E\n        x\n        \n          t\n          \n            σ\n          \n        \n        (\n        S\n        )\n        =\n        {\n        {\n        a\n        ,\n        d\n        }\n        }\n      \n    \n    {\\displaystyle Ext_{\\sigma }(S)=\\{\\{a,d\\}\\}}\n  \n for every Dung's semantic—the system is well-founded. That explains why the semantics coincide, and the accepted arguments are: \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n.\n\nLabellings\nLabellings are a more expressive way than extensions to express the acceptance of the arguments. Concretely, a lab"}
{"doc_id": "Argumentation framework", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and the accepted arguments are: \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n and \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  \n.\n\nLabellings\nLabellings are a more expressive way than extensions to express the acceptance of the arguments. Concretely, a labelling is a mapping that associates every argument with a label in (the argument is accepted), out (the argument is rejected), or undec (the argument is undefined—not accepted or refused).\nOne can also note a labelling as a set of pairs \n  \n    \n      \n        (\n        \n          \n            a\n            r\n            g\n            u\n            m\n            e\n            n\n            t\n          \n        \n        ,\n        \n          \n            l\n            a\n            b\n            e\n            l\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\mathit {argument}},{\\mathit {label}})}\n  \n.\nSuch a mapping does not make sense without additional constraint. The notion of reinstatement labelling guarantees the sense of the mapping. \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is a reinstatement labelling on the system \n  \n    \n      \n        S\n        =\n        ⟨\n        A\n        ,\n        R\n        ⟩\n      \n    \n    {\\displaystyle S=\\langle A,R\\rangle }\n  \n if and only if :\n\n  \n    \n      \n        ∀\n        a\n        ∈\n        A\n        ,\n        L\n        (\n        a\n        )\n        =\n        \n          \n            i\n            n\n          \n        \n      \n    \n    {\\displaystyle \\forall a\\in A,L(a)={\\mathit {in}}}\n  \n if and only if \n  \n    \n      \n        ∀\n        b\n        ∈\n        A\n      \n    \n    {\\displaystyle \\forall b\\in A}\n  \n such that \n  \n    \n      \n        (\n        b\n        ,\n        a\n        )\n        ∈\n        R\n        ,\n        L\n        (\n        b\n        )\n        =\n        \n          \n            o\n            u\n            t\n          \n        \n      \n    \n    {\\displaystyle (b,a)\\in R,L(b)={\\mathit {out}}}\n  \n\n  \n    \n      \n        ∀\n        a\n        ∈\n        A\n        ,\n        L\n        (\n        a\n        )\n        =\n        \n          \n            o\n            u\n            t\n          \n        \n      \n    \n    {\\displaystyle \\forall a\\in A,L(a)={\\mathit"}
{"doc_id": "Argumentation framework", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "out}}}\n  \n\n  \n    \n      \n        ∀\n        a\n        ∈\n        A\n        ,\n        L\n        (\n        a\n        )\n        =\n        \n          \n            o\n            u\n            t\n          \n        \n      \n    \n    {\\displaystyle \\forall a\\in A,L(a)={\\mathit {out}}}\n  \n if and only if \n  \n    \n      \n        ∃\n        b\n        ∈\n        A\n      \n    \n    {\\displaystyle \\exists b\\in A}\n  \n such that \n  \n    \n      \n        (\n        b\n        ,\n        a\n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle (b,a)\\in R}\n  \n and \n  \n    \n      \n        L\n        (\n        b\n        )\n        =\n        \n          \n            i\n            n\n          \n        \n      \n    \n    {\\displaystyle L(b)={\\mathit {in}}}\n  \n\n  \n    \n      \n        ∀\n        a\n        ∈\n        A\n        ,\n        L\n        (\n        a\n        )\n        =\n        \n          \n            u\n            n\n            d\n            e\n            c\n          \n        \n      \n    \n    {\\displaystyle \\forall a\\in A,L(a)={\\mathit {undec}}}\n  \n if and only if \n  \n    \n      \n        L\n        (\n        a\n        )\n        ≠\n        \n          \n            i\n            n\n          \n        \n      \n    \n    {\\displaystyle L(a)\\neq {\\mathit {in}}}\n  \n and \n  \n    \n      \n        L\n        (\n        a\n        )\n        ≠\n        \n          \n            o\n            u\n            t\n          \n        \n      \n    \n    {\\displaystyle L(a)\\neq {\\mathit {out}}}\n  \n\nOne can convert every extension into a reinstatement labelling: the arguments of the extension are in, those attacked by an argument of the extension are out, and the others are undec. Conversely, one can build an extension from a reinstatement labelling just by keeping the arguments in. Indeed, Caminada proved that the reinstatement labellings and the complete extensions can be mapped in a bijective way. Moreover, the other Datung's semantics can be associated to some particular sets of reinstatement labellings.\nReinstatement labellings distinguish arguments not accepted because they are attacked by accepted arguments from undefined arguments—that is, those that are not defended cannot defend themselves. An argument is undec if it is attacked by at least another undec. If it is attacked only "}
{"doc_id": "Argumentation framework", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " some particular sets of reinstatement labellings.\nReinstatement labellings distinguish arguments not accepted because they are attacked by accepted arguments from undefined arguments—that is, those that are not defended cannot defend themselves. An argument is undec if it is attacked by at least another undec. If it is attacked only  by arguments out, it must be in, and if it is attacked some argument in, then it is out.\nThe unique reinstatement labelling that corresponds to the system \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n above is \n  \n    \n      \n        L\n        =\n        {\n        (\n        a\n        ,\n        \n          \n            i\n            n\n          \n        \n        )\n        ,\n        (\n        b\n        ,\n        \n          \n            o\n            u\n            t\n          \n        \n        )\n        ,\n        (\n        c\n        ,\n        \n          \n            o\n            u\n            t\n          \n        \n        )\n        ,\n        (\n        d\n        ,\n        \n          \n            i\n            n\n          \n        \n        )\n        }\n      \n    \n    {\\displaystyle L=\\{(a,{\\mathit {in}}),(b,{\\mathit {out}}),(c,{\\mathit {out}}),(d,{\\mathit {in}})\\}}\n  \n.\n\nInference from an argumentation system\nIn the general case when several extensions are computed for a given semantic \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n, the agent that reasons from the system can use several mechanisms to infer information:\n\nCredulous inference: the agent accepts an argument if it belongs to at least one of the \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n-extensions—in which case, the agent risks accepting some arguments that are not acceptable together (\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n attacks \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n, and \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n each belongs to an extension)\nSkeptical inference: the agent accepts an argument only if it belongs to every \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n-extension. In this case, the agent risks deducing too little information (if the intersection of the extensions is empty or has a very small cardinal).\nFor these two methods to infer information, one can identify the set of accepted arguments, respectively \n  \n    \n"}
{"doc_id": "Argumentation framework", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n-extension. In this case, the agent risks deducing too little information (if the intersection of the extensions is empty or has a very small cardinal).\nFor these two methods to infer information, one can identify the set of accepted arguments, respectively \n  \n    \n      \n        C\n        \n          r\n          \n            σ\n          \n        \n        (\n        S\n        )\n      \n    \n    {\\displaystyle Cr_{\\sigma }(S)}\n  \n the set of the arguments credulously accepted under the semantic \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n, and \n  \n    \n      \n        S\n        \n          c\n          \n            σ\n          \n        \n        (\n        S\n        )\n      \n    \n    {\\displaystyle Sc_{\\sigma }(S)}\n  \n the set of arguments accepted skeptically under the semantic \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n (the \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n can be missed if there is no possible ambiguity about the semantic).\nOf course, when there is only one extension (for instance, when the system is well-founded), this problem is very simple: the agent accepts arguments of the unique extension and rejects others.\nThe same reasoning can be done with labellings that correspond to the chosen semantic : an argument can be accepted if it is in for each labelling and refused if it is out for each labelling, the others being in an undecided state (the status of the arguments can remind the epistemic states of a belief in the AGM framework for dynamic of beliefs).\n\nEquivalence between argumentation frameworks\nThere exists several criteria of equivalence between argumentation frameworks. Most of those criteria concern the sets of extensions or the set of accepted arguments.\nFormally, given a semantic \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n :\n\n  \n    \n      \n        \n          \n            E\n            \n              Q\n              \n                1\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {EQ_{1}}}}\n  \n : two argumentation frameworks are equivalent if they have the same set of \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n  \n-extensions, that is \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n        \n          ≡\n          \n            1\n          \n        \n        \n          S\n          \n            2\n          \n        \n        ⇔\n        E\n        x\n        \n          t\n          \n            σ\n          \n        \n        (\n        \n         "}
{"doc_id": "Argumentation framework", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "  \n-extensions, that is \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n        \n          ≡\n          \n            1\n          \n        \n        \n          S\n          \n            2\n          \n        \n        ⇔\n        E\n        x\n        \n          t\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            1\n          \n        \n        )\n        =\n        E\n        x\n        \n          t\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle S_{1}\\equiv _{1}S_{2}\\Leftrightarrow Ext_{\\sigma }(S_{1})=Ext_{\\sigma }(S_{2})}\n  \n ;\n\n  \n    \n      \n        \n          \n            E\n            \n              Q\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {EQ_{2}}}}\n  \n : two argumentation frameworks are equivalent if they accept skeptically the same arguments, that is \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n        \n          ≡\n          \n            2\n          \n        \n        \n          S\n          \n            2\n          \n        \n        ⇔\n        S\n        \n          c\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            1\n          \n        \n        )\n        =\n        S\n        \n          c\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle S_{1}\\equiv _{2}S_{2}\\Leftrightarrow Sc_{\\sigma }(S_{1})=Sc_{\\sigma }(S_{2})}\n  \n ;\n\n  \n    \n      \n        \n          \n            E\n            \n              Q\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {EQ_{2}}}}\n  \n : two argumentation frameworks are equivalent if they accept credulously the same arguments, that is \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n        \n          ≡\n          \n            3\n          \n        \n        \n          S\n          \n            2\n          \n        \n        ⇔\n        C\n        \n          r\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            1\n          \n        \n        )\n        =\n        C\n        \n          r\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle S_{1}\\equiv _{3}S_{2}\\Lef"}
{"doc_id": "Argumentation framework", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (\n        \n          S\n          \n            1\n          \n        \n        )\n        =\n        C\n        \n          r\n          \n            σ\n          \n        \n        (\n        \n          S\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle S_{1}\\equiv _{3}S_{2}\\Leftrightarrow Cr_{\\sigma }(S_{1})=Cr_{\\sigma }(S_{2})}\n  \n.\nThe strong equivalence says that two systems \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle S_{1}}\n  \n and \n  \n    \n      \n        \n          S\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle S_{2}}\n  \n are equivalent if and only if for all other system \n  \n    \n      \n        \n          S\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle S_{3}}\n  \n, the union of \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle S_{1}}\n  \n with \n  \n    \n      \n        \n          S\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle S_{3}}\n  \n is equivalent (for a given criterion) with the union of \n  \n    \n      \n        \n          S\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle S_{2}}\n  \n and \n  \n    \n      \n        \n          S\n          \n            3\n          \n        \n      \n    \n    {\\displaystyle S_{3}}\n  \n.\n\nOther kinds\nThe abstract framework of Dung has been instantiated to several particular cases.\n\nLogic-based argumentation frameworks\nIn the case of logic-based argumentation frameworks, an argument is not an abstract entity, but a pair, where the first part is a minimal consistent set of formulae enough to prove the formula for the second part of the argument.\nFormally, an argument is a pair \n  \n    \n      \n        (\n        Φ\n        ,\n        α\n        )\n      \n    \n    {\\displaystyle (\\Phi ,\\alpha )}\n  \n such that\n\n  \n    \n      \n        Φ\n        ⊬\n        ⊥\n      \n    \n    {\\displaystyle \\Phi \\nvdash \\bot }\n  \n\n  \n    \n      \n        Φ\n        ⊢\n        α\n      \n    \n    {\\displaystyle \\Phi \\vdash \\alpha }\n  \n\n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n is a minimal set of \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n satisfying \n  \n    \n      \n        α\n      \n    \n    {\\display"}
{"doc_id": "Argumentation framework", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n        α\n      \n    \n    {\\displaystyle \\Phi \\vdash \\alpha }\n  \n\n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n is a minimal set of \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n satisfying \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n where \n  \n    \n      \n        Δ\n      \n    \n    {\\displaystyle \\Delta }\n  \n is a set of formulae used by the agent to reason.\nOne calls \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n a consequence of \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n, and \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n a support of \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n.\nIn this case, the attack relation is not given in an explicit way, as a subset of the Cartesian product \n  \n    \n      \n        A\n        ×\n        A\n      \n    \n    {\\displaystyle A\\times A}\n  \n, but as a property that indicates if an argument attacks another. For instance,\n\nRelation defeater : \n  \n    \n      \n        (\n        Ψ\n        ,\n        β\n        )\n      \n    \n    {\\displaystyle (\\Psi ,\\beta )}\n  \n attacks \n  \n    \n      \n        (\n        Φ\n        ,\n        α\n        )\n      \n    \n    {\\displaystyle (\\Phi ,\\alpha )}\n  \n if and only if \n  \n    \n      \n        β\n        ⊢\n        ¬\n        (\n        \n          ϕ\n          \n            1\n          \n        \n        ∧\n        ⋯\n        ∧\n        \n          ϕ\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\beta \\vdash \\neg (\\phi _{1}\\wedge \\dots \\wedge \\phi _{n})}\n  \n for \n  \n    \n      \n        {\n        \n          ϕ\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          ϕ\n          \n            n\n          \n        \n        }\n        ⊆\n        Φ\n      \n    \n    {\\displaystyle \\{\\phi _{1},\\dots ,\\phi _{n}\\}\\subseteq \\Phi }\n  \n\nRelation undercut : \n  \n    \n      \n        (\n        Ψ\n        ,\n        β\n        )\n      \n    \n    {\\displaystyle (\\Psi ,\\beta )}\n  \n attacks \n  \n    \n      \n        (\n        Φ\n        ,\n        α\n        )\n      \n    \n    {\\displaystyle (\\"}
{"doc_id": "Argumentation framework", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "n}\\}\\subseteq \\Phi }\n  \n\nRelation undercut : \n  \n    \n      \n        (\n        Ψ\n        ,\n        β\n        )\n      \n    \n    {\\displaystyle (\\Psi ,\\beta )}\n  \n attacks \n  \n    \n      \n        (\n        Φ\n        ,\n        α\n        )\n      \n    \n    {\\displaystyle (\\Phi ,\\alpha )}\n  \n if and only if \n  \n    \n      \n        β\n        =\n        ¬\n        (\n        \n          ϕ\n          \n            1\n          \n        \n        ∧\n        ⋯\n        ∧\n        \n          ϕ\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle \\beta =\\neg (\\phi _{1}\\wedge \\dots \\wedge \\phi _{n})}\n  \n for \n  \n    \n      \n        {\n        \n          ϕ\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          ϕ\n          \n            n\n          \n        \n        }\n        ⊆\n        Φ\n      \n    \n    {\\displaystyle \\{\\phi _{1},\\dots ,\\phi _{n}\\}\\subseteq \\Phi }\n  \n\nRelation rebuttal : \n  \n    \n      \n        (\n        Ψ\n        ,\n        β\n        )\n      \n    \n    {\\displaystyle (\\Psi ,\\beta )}\n  \n attacks \n  \n    \n      \n        (\n        Φ\n        ,\n        α\n        )\n      \n    \n    {\\displaystyle (\\Phi ,\\alpha )}\n  \n if and only if \n  \n    \n      \n        β\n        ⇔\n        ¬\n        α\n      \n    \n    {\\displaystyle \\beta \\Leftrightarrow \\neg \\alpha }\n  \n is a tautology\nGiven a particular attack relation, one can build a graph and reason in a similar way to the abstract argumentation frameworks (use of semantics to build extension, skeptical or credulous inference), the difference is that the information inferred from a logic based argumentation framework is a set of formulae (the consequences of the accepted arguments).\n\nValue-based argumentation frameworks\nThe value-based argumentation frameworks come from the idea that during an exchange of arguments, some can be stronger than others with respect to a certain value they advance, and so the success of an attack between arguments depends on the difference of these values.\nFormally, a value-based argumentation framework is a tuple \n  \n    \n      \n        V\n        A\n        F\n        =\n        ⟨\n        A\n        ,\n        R\n        ,\n        V\n        ,\n        \n          \n            val\n          \n        \n        ,\n        \n          \n            valprefs"}
{"doc_id": "Argumentation framework", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the difference of these values.\nFormally, a value-based argumentation framework is a tuple \n  \n    \n      \n        V\n        A\n        F\n        =\n        ⟨\n        A\n        ,\n        R\n        ,\n        V\n        ,\n        \n          \n            val\n          \n        \n        ,\n        \n          \n            valprefs\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle VAF=\\langle A,R,V,{\\textit {val}},{\\textit {valprefs}}\\rangle }\n  \n with \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        R\n      \n    \n    {\\displaystyle R}\n  \n similar to the standard framework (a set of arguments and a binary relation on this set), \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n is a non empty set of values, \n  \n    \n      \n        \n          \n            val\n          \n        \n      \n    \n    {\\displaystyle {\\textit {val}}}\n  \n is a mapping that associates each element from \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n to an element from \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  \n, and \n  \n    \n      \n        \n          \n            valprefs\n          \n        \n      \n    \n    {\\displaystyle {\\textit {valprefs}}}\n  \n is a preference relation (transitive, irreflexive and asymmetric) on \n  \n    \n      \n        V\n        ×\n        V\n      \n    \n    {\\displaystyle V\\times V}\n  \n.\nIn this framework, an argument \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n defeats another argument \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n if and only if\n\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n attacks \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n in the \"standard\" meaning: \n  \n    \n      \n        (\n        a\n        ,\n        b\n        )\n        ∈\n        R\n      \n    \n    {\\displaystyle (a,b)\\in R}\n  \n ;\nand \n  \n    \n      \n        (\n        \n          \n            val\n          \n        \n        (\n        b\n        )\n        ,\n        v\n        a\n        l\n        (\n        a\n        )\n        )\n        ∉\n        \n          \n            valprefs\n          \n        \n      \n    \n    {\\displaystyle ({\\textit {val}}(b),val(a))\\not \\in {\\textit {valprefs}}}\n  \n, that is the value advanced by \n  \n    \n      \n       "}
{"doc_id": "Argumentation framework", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        (\n        a\n        )\n        )\n        ∉\n        \n          \n            valprefs\n          \n        \n      \n    \n    {\\displaystyle ({\\textit {val}}(b),val(a))\\not \\in {\\textit {valprefs}}}\n  \n, that is the value advanced by \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  \n is not preferred to the one advanced by \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n.\nOne remarks that an attack succeeds if both arguments are associated to the same value, or if there is no preference between their respective values.\n\nAssumption-based argumentation frameworks\nIn assumption-based argumentation (ABA) frameworks, arguments are defined as a set of rules and attacks are defined in terms of assumptions and contraries.\nFormally, an assumption-based argumentation framework is a tuple \n  \n    \n      \n        ⟨\n        \n          \n            L\n          \n        \n        ,\n        \n          \n            R\n          \n        \n        ,\n        \n          \n            A\n          \n        \n        ,\n        \n          \n            \n              \n                ␣\n              \n            \n            ¯\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle {\\mathcal {L}},{\\mathcal {R}},{\\mathcal {A}},{\\overline {\\mathrm {\\textvisiblespace} }}\\rangle }\n  \n, where\n\n  \n    \n      \n        ⟨\n        \n          \n            L\n          \n        \n        ,\n        \n          \n            R\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle {\\mathcal {L}},{\\mathcal {R}}\\rangle }\n  \n  is a deductive system, where \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  \n is the language and \n  \n    \n      \n        \n          \n            R\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {R}}}\n  \n is the set of inference rules in the form of \n  \n    \n      \n        \n          s\n          \n            0\n          \n        \n        ←\n        \n          s\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          s\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle s_{0}\\leftarrow s_{1},\\dotsc ,s_{m}}\n  \n, for \n  \n    \n      \n        m\n        >\n        0\n      \n    \n    {\\displaystyle m>0}\n  \n and \n  \n    \n      \n        \n          s\n          \n            0\n          \n        \n        ,\n        \n          s\n          \n           "}
{"doc_id": "Argumentation framework", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "0}\\leftarrow s_{1},\\dotsc ,s_{m}}\n  \n, for \n  \n    \n      \n        m\n        >\n        0\n      \n    \n    {\\displaystyle m>0}\n  \n and \n  \n    \n      \n        \n          s\n          \n            0\n          \n        \n        ,\n        \n          s\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          s\n          \n            m\n          \n        \n        ∈\n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle s_{0},s_{1},\\dotsc ,s_{m}\\in {\\mathcal {L}}}\n  \n;\n\n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n, where \n  \n    \n      \n        \n          \n            A\n          \n        \n        ⊆\n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}\\subseteq {\\mathcal {L}}}\n  \n is a non-empty set, named the assumptions;\n\n  \n    \n      \n        \n          \n            \n              \n                ␣\n              \n            \n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {\\mathrm {\\textvisiblespace} }}}\n  \n is a total mapping from \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n to \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  \n, where \n  \n    \n      \n        \n          \n            a\n            ¯\n          \n        \n      \n    \n    {\\displaystyle {\\overline {a}}}\n  \n is defined as the contrary of \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n.\nAs a consequence of defining an ABA, an argument can be represented in a tree-form. Formally, given a deductive system \n  \n    \n      \n        ⟨\n        \n          \n            L\n          \n        \n        ,\n        \n          \n            R\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle {\\mathcal {L}},{\\mathcal {R}}\\rangle }\n  \n and set of assumptions \n  \n    \n      \n        \n          \n            A\n          \n        \n        ⊆\n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}\\subseteq {\\mathcal {L}}}\n  \n, an argument for claim \n  \n    \n      \n        c\n        ∈\n        \n          \n            L\n          \n        \n      \n    \n    {\\textstyle c\\in {\\mathcal {L}}}\n"}
{"doc_id": "Argumentation framework", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}\\subseteq {\\mathcal {L}}}\n  \n, an argument for claim \n  \n    \n      \n        c\n        ∈\n        \n          \n            L\n          \n        \n      \n    \n    {\\textstyle c\\in {\\mathcal {L}}}\n  \n supported by \n  \n    \n      \n        S\n        ⊆\n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle S\\subseteq {\\mathcal {A}}}\n  \n, is a tree with nodes labelled by sentences in \n  \n    \n      \n        \n          \n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}}\n  \n or by symbol \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n, such that:\n\nThe root is labelled by \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n\nFor each node \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n,\nIf \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is a leaf node, then \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is labelled by either an assumption or by \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n\nIf \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is not a leaf node, then there is an inference rule \n  \n    \n      \n        \n          l\n          \n            N\n          \n        \n        ←\n        \n          s\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          s\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle l_{N}\\leftarrow s_{1},...,s_{m}}\n  \n, \n  \n    \n      \n        (\n        m\n        ≥\n        0\n        )\n      \n    \n    {\\displaystyle (m\\geq 0)}\n  \n, where \n  \n    \n      \n        \n          l\n          \n            N\n          \n        \n      \n    \n    {\\displaystyle l_{N}}\n  \n is the label of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n and\nIf \n  \n    \n      \n        m\n        =\n        0\n      \n    \n    {\\displaystyle m=0}\n  \n, then the rule shall be \n  \n    \n      \n        \n          l\n          \n            N\n          \n        \n        ←\n        τ\n      \n    \n    {\\displaystyle l_{N}\\leftarrow \\tau }\n  \n (i.e. child of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is \n  \n"}
{"doc_id": "Argumentation framework", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "}\n  \n, then the rule shall be \n  \n    \n      \n        \n          l\n          \n            N\n          \n        \n        ←\n        τ\n      \n    \n    {\\displaystyle l_{N}\\leftarrow \\tau }\n  \n (i.e. child of \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is \n  \n    \n      \n        τ\n      \n    \n    {\\displaystyle \\tau }\n  \n)\nOtherwise, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n has \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n children, labelled by \n  \n    \n      \n        \n          s\n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          s\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle s_{1},...,s_{m}}\n  \n\n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is the set of all assumptions labeling the leave nodes\nAn argument with claim \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n supported by a set of assumption \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n can also be denoted as \n  \n    \n      \n        S\n        ⊢\n        c\n      \n    \n    {\\displaystyle S\\vdash c}\n\nSee also\nNotes"}
{"doc_id": "Artificial brain", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "An artificial brain (or artificial mind) is software and hardware with cognitive abilities similar to those of the animal or human brain.\nResearch investigating \"artificial brains\" and brain emulation plays three important roles in science:\n\nAn ongoing attempt by neuroscientists to understand how the human brain works, known as cognitive neuroscience.\nA thought experiment in the philosophy of artificial intelligence, demonstrating that it is possible, at least in theory, to create a machine that has all the capabilities of a human being.\nA long-term project to create machines exhibiting behavior comparable to those of animals with complex central nervous system such as mammals and most particularly humans. The ultimate goal of creating a machine exhibiting human-like behavior or intelligence is sometimes called strong AI.\nAn example of the first objective is the project reported by Aston University in Birmingham, England where researchers are using biological cells to create \"neurospheres\" (small clusters of neurons) in order to develop new treatments for diseases including Alzheimer's, motor neurone and Parkinson's disease.\nThe second objective is a reply to arguments such as John Searle's Chinese room argument, Hubert Dreyfus's critique of AI or Roger Penrose's argument in The Emperor's New Mind. These critics argued that there are aspects of human consciousness or expertise that can not be simulated by machines. One reply to their arguments is that the biological processes inside the brain can be simulated to any degree of accuracy. This reply was made as early as 1950, by Alan Turing in his classic paper \"Computing Machinery and Intelligence\".\nThe third objective is generally called artificial general intelligence by researchers. However, Ray Kurzweil prefers the term \"strong AI\". In his book The Singularity is Near, he focuses on whole brain emulation using conventional computing machines as an approach to implementing artificial brains, and claims (on grounds of computer power continuing an exponential growth trend) that this could be done by 2025. Henry Markram, director of the Blue Brain project (which is attempting brain emulation), made a similar claim (2020) at the Oxford TED conference in 2009.\n\nApproaches to brain simulation\nW. Ross Ashby's pioneering work in cybernetics provided an early mathematical framework for understanding adaptive brain-like systems. In his 1952 book Design for a Brain, Ashby proposed that the brain could be modeled as an ultrastable system that maintains equilibrium through continuous adaptation to environmental perturbations. His approach used differential equations and state-space models to describe how neural systems could exhibit purposeful behavior through feedback mechanisms. Ash"}
{"doc_id": "Artificial brain", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " systems. In his 1952 book Design for a Brain, Ashby proposed that the brain could be modeled as an ultrastable system that maintains equilibrium through continuous adaptation to environmental perturbations. His approach used differential equations and state-space models to describe how neural systems could exhibit purposeful behavior through feedback mechanisms. Ashby's homeostat, a physical machine built in 1948, demonstrated these principles through an electromechanical device with four interconnected units that automatically adjusted their parameters to maintain stability when disturbed. The homeostat represented one of the first attempts to build an artificial system exhibiting brain-like adaptive behavior, influencing subsequent work in adaptive systems, neural networks, and artificial intelligence.\n\n Although direct human brain emulation using artificial neural networks on a high-performance computing engine is a commonly discussed approach, there are other approaches. An alternative artificial brain implementation could be based on Holographic Neural Technology (HNeT) non linear phase coherence/decoherence principles. The analogy has been made to quantum processes through the core synaptic algorithm which has strong similarities to the quantum mechanical wave equation.\nEvBrain is a form of evolutionary software that can evolve \"brainlike\" neural networks, such as the network immediately behind the retina.\nIn November 2008, IBM received a US$4.9 million grant from the Pentagon for research into creating intelligent computers. The Blue Brain project is being conducted with the assistance of IBM in Lausanne. The project is based on the premise that it is possible to artificially link the neurons \"in the computer\" by placing thirty million synapses in their proper three-dimensional position.\nSome proponents of strong AI speculated in 2009 that computers in connection with Blue Brain and Soul Catcher may exceed human intellectual capacity by around 2015, and that it is likely that we will be able to download the human brain at some time around 2050.\nWhile Blue Brain is able to represent complex neural connections on the large scale, the project does not achieve the link between brain activity and behaviors executed by the brain. In 2012, project Spaun (Semantic Pointer Architecture Unified Network) attempted to model multiple parts of the human brain through large-scale representations of neural connections that generate complex behaviors in addition to mapping.\nSpaun's design recreates elements of human brain anatomy. The model, consisting of approximately 2.5 million neurons, includes features of the visual and motor cortices, GABAergic and dopaminergic connections, the ventral tegmental area (VTA), substantia nigra, and others. The design"}
{"doc_id": "Artificial brain", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "aun's design recreates elements of human brain anatomy. The model, consisting of approximately 2.5 million neurons, includes features of the visual and motor cortices, GABAergic and dopaminergic connections, the ventral tegmental area (VTA), substantia nigra, and others. The design allows for several functions in response to eight tasks, using visual inputs of typed or handwritten characters and outputs carried out by a mechanical arm. Spaun's functions include copying a drawing, recognizing images, and counting.\nThere are good reasons to believe that, regardless of implementation strategy, the predictions of realising artificial brains in the near future are optimistic. In particular brains (including the human brain) and cognition are not currently well understood, and the scale of computation required is unknown. Another near term limitation is that all current approaches for brain simulation require orders of magnitude larger power consumption compared with a human brain. The human brain consumes about 20 W of power, whereas current supercomputers may use as much as 1 MW—i.e., an order of 100,000 more.\n\nArtificial brain thought experiment\nSome critics of brain simulation believe that it is simpler to create general intelligent action directly without imitating nature. Some commentators have used the analogy that early attempts to construct flying machines modeled them after birds, but that modern aircraft do not look like birds.\n\nSee also\nNotes"}
{"doc_id": "Artificial consciousness", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial consciousness, also known as machine consciousness, synthetic consciousness, or digital consciousness, is consciousness hypothesized to be possible for artificial intelligence. It is also the corresponding field of study, which draws insights from philosophy of mind, philosophy of artificial intelligence, cognitive science and neuroscience.\nThe term \"sentience\" can be used when specifically designating ethical considerations stemming from a form of phenomenal consciousness (P-consciousness, or the ability to feel qualia). Since sentience involves the ability to experience ethically positive or negative (i.e., valenced) mental states, it may justify welfare concerns and legal protection, as with non-human animals.\nSome scholars believe that consciousness is generated by the interoperation of various parts of the brain; these mechanisms are labeled the neural correlates of consciousness (NCC). Some further believe that constructing a system (e.g., a computer system) that can emulate this NCC interoperation would result in a system that is conscious. Some scholars reject the possibility of artificial consciousness.\n\nPhilosophical views\nAs there are many hypothesized types of consciousness, there are many potential implementations of artificial consciousness. In the philosophical literature, perhaps the most common taxonomy of consciousness is into \"access\" and \"phenomenal\" variants. Access consciousness concerns those aspects of experience that can be apprehended, while phenomenal consciousness concerns those aspects of experience that seemingly cannot be apprehended, instead being characterized qualitatively in terms of \"raw feels\", \"what it is like\" or qualia.\n\nPlausibility debate\nType-identity theorists and other skeptics hold the view that consciousness can be realized only in particular physical systems because consciousness has properties that necessarily depend on physical constitution. In his 2001 article \"Artificial Consciousness: Utopia or Real Possibility,\" Giorgio Buttazzo says that a common objection to artificial consciousness is that, \"Working in a fully automated mode, they [the computers] cannot exhibit creativity, unreprogrammation (which means can 'no longer be reprogrammed', from rethinking), emotions, or free will. A computer, like a washing machine, is a slave operated by its components.\"\nFor other theorists (e.g., functionalists), who define mental states in terms of causal roles, any system that can instantiate the same pattern of causal roles, regardless of physical constitution, will instantiate the same mental states, including consciousness.\n\nThought experiments\nDavid Chalmers proposed two thought experiments intending to demonstrate that \"functionally isomorphic\" systems (those with the same \"fine-grained functional organization"}
{"doc_id": "Artificial consciousness", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of causal roles, any system that can instantiate the same pattern of causal roles, regardless of physical constitution, will instantiate the same mental states, including consciousness.\n\nThought experiments\nDavid Chalmers proposed two thought experiments intending to demonstrate that \"functionally isomorphic\" systems (those with the same \"fine-grained functional organization\", i.e., the same information processing) will have qualitatively identical conscious experiences, regardless of whether they are based on biological neurons or digital hardware.\nThe \"fading qualia\" is a reductio ad absurdum thought experiment. It involves replacing, one by one, the neurons of a brain with a functionally identical component, for example based on a silicon chip. Chalmers makes the hypothesis, knowing it in advance to be absurd, that \"the qualia fade or disappear\" when neurons are replaced one-by-one with identical silicon equivalents. Since the original neurons and their silicon counterparts are functionally identical, the brain's information processing should remain unchanged, and the subject's behaviour and introspective reports would stay exactly the same. Chalmers argues that this leads to an absurd conclusion: the subject would continue to report normal conscious experiences even as their actual qualia fade away. He concludes that the subject's qualia actually don't fade, and that the resulting robotic brain, once every neuron is replaced, would remain just as sentient as the original biological brain.\nSimilarly, the \"dancing qualia\" thought experiment is another reductio ad absurdum argument. It supposes that two functionally isomorphic systems could have different perceptions (for instance, seeing the same object in different colors, like red and blue). It involves a switch that alternates between a chunk of brain that causes the perception of red, and a functionally isomorphic silicon chip, that causes the perception of blue. Since both perform the same function within the brain, the subject would not notice any change during the switch. Chalmers argues that this would be highly implausible if the qualia were truly switching between red and blue, hence the contradiction. Therefore, he concludes that the equivalent digital system would not only experience qualia, but it would perceive the same qualia as the biological system (e.g., seeing the same color).\nCritics object that Chalmers' proposal begs the question in assuming that all mental properties and external connections are already sufficiently captured by abstract causal organization. Van Heuveln et al. argue that the dancing qualia argument contains an equivocation fallacy, conflating a \"change in experience"}
{"doc_id": "Artificial consciousness", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " seeing the same color).\nCritics object that Chalmers' proposal begs the question in assuming that all mental properties and external connections are already sufficiently captured by abstract causal organization. Van Heuveln et al. argue that the dancing qualia argument contains an equivocation fallacy, conflating a \"change in experience\" between two systems with an \"experience of change\" within a single system. Mogensen argues that the fading qualia argument can be resisted by appealing to vagueness at the boundaries of consciousness and the holistic structure of conscious neural activity, which suggests consciousness may require specific biological substrates rather than being substrate-independent.\nGreg Egan's short story Learning To Be Me (mentioned in §In fiction), illustrates how undetectable duplication of the brain and its functionality could be from a first-person perspective.\n\nIn large language models\nIn 2022, Google engineer Blake Lemoine made a viral claim that Google's LaMDA chatbot was sentient. Lemoine supplied as evidence the chatbot's humanlike answers to many of his questions; however, the chatbot's behavior was judged by the scientific community as likely a consequence of mimicry, rather than machine sentience. Lemoine's claim was widely derided for being ridiculous. Moreover, attributing consciousness based solely on the basis of LLM outputs or the immersive experience created by an algorithm is considered a fallacy. However, while philosopher Nick Bostrom states that LaMDA is unlikely to be conscious, he additionally poses the question of \"what grounds would a person have for being sure about it?\" One would have to have access to unpublished information about LaMDA's architecture, and also would have to understand how consciousness works, and then figure out how to map the philosophy onto the machine: \"(In the absence of these steps), it seems like one should be maybe a little bit uncertain. [...] there could well be other systems now, or in the relatively near future, that would start to satisfy the criteria.\"\nDavid Chalmers argued in 2023 that LLMs today display impressive conversational and general intelligence abilities, but are likely not conscious yet, as they lack some features that may be necessary, such as recurrent processing, a global workspace, and unified agency. Nonetheless, he considers that non-biological systems can be conscious, and suggested that future, extended models (LLM+s) incorporating these elements might eventually meet the criteria for consciousness, raising both profound scientific questions and significant ethical challenges. However, the view that consciousness can exist without biological"}
{"doc_id": "Artificial consciousness", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " a global workspace, and unified agency. Nonetheless, he considers that non-biological systems can be conscious, and suggested that future, extended models (LLM+s) incorporating these elements might eventually meet the criteria for consciousness, raising both profound scientific questions and significant ethical challenges. However, the view that consciousness can exist without biological phenomena is controversial and some reject it.\nKristina Šekrst cautions that anthropomorphic terms such as \"hallucination\" can obscure important ontological differences between artificial and human cognition. While LLMs may produce human-like outputs, she argues that it does not justify ascribing mental states or consciousness to them. Instead, she advocates for an epistemological framework (such as reliabilism) that recognizes the distinct nature of AI knowledge production. She suggests that apparent understanding in LLMs may be a sophisticated form of AI hallucination. She also questions what would happen if an LLM were trained without any mention of consciousness.\n\nTesting\nSentience is an inherently first-person phenomenon. Because of that, and due the lack of an empirical definition of sentience, directly measuring it may be impossible. Although systems may display numerous behaviors correlated with sentience, determining whether a system is sentient is known as the hard problem of consciousness. In the case of AI, there is the additional difficulty that the AI may be trained to act like a human, or incentivized to appear sentient, which makes behavioral markers of sentience less reliable. Additionally, some chatbots have been trained to say they are not conscious.\nA well-known method for testing machine intelligence is the Turing test, which assesses the ability to have a human-like conversation. But passing the Turing test does not indicate that an AI system is sentient, as the AI may simply mimic human behavior without having the associated feelings.\nIn 2014, Victor Argonov suggested a non-Turing test for machine sentience based on machine's ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. Just as with the Turing Test: a positive result proves that machine is conscious but a negative result proves nothing. For example, absence"}
{"doc_id": "Artificial consciousness", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. Just as with the Turing Test: a positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine's intellect, not by absence of consciousness.\n\nEthics\nIf it were suspected that a particular machine was conscious, its rights would be an ethical issue that would need to be assessed (e.g. what rights it would have under law). For example, a conscious computer that was owned and used as a tool or central computer within a larger machine is a particular ambiguity. Should laws be made for such a case? Consciousness would also require a legal definition in this particular case. Because artificial consciousness is still largely a theoretical subject, such ethics have not been discussed or developed to a great extent, though it has often been a theme in fiction.\nAI sentience would give rise to concerns of welfare and legal protection, whereas other aspects of consciousness related to cognitive capabilities may be more relevant for AI rights.\nSentience is generally considered sufficient for moral consideration, but some philosophers consider that moral consideration could also stem from other notions of consciousness, or from capabilities unrelated to consciousness, such as: \"having a sophisticated conception of oneself as persisting through time; having agency and the ability to pursue long-term plans; being able to communicate and respond to normative reasons; having preferences and powers; standing in certain social relationships with other beings that have moral status; being able to make commitments and to enter into reciprocal arrangements; or having the potential to develop some of these attributes.\"\nEthical concerns still apply (although to a lesser extent) when the consciousness is uncertain, as long as the probability is deemed non-negligible. The precautionary principle is also relevant if the moral cost of mistakenly attributing or denying moral consideration to AI differs significantly.\nIn 2021, German philosopher Thomas Metzinger argued for a global moratorium on synthetic phenomenology until 2050. Metzinger asserts that humans have a duty of care towards any sentient AIs they create, and that proceeding too fast risks creating an \"explosion of artificial suffering\". David Chalmers also argued that creating conscious AI would \"raise a new group of difficult ethical challenges, with the potential for new forms of injustice\".\n\nAspects of consciousness\nBernard Baars and others argue there are various aspects of consciousness necessary for"}
{"doc_id": "Artificial consciousness", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " that proceeding too fast risks creating an \"explosion of artificial suffering\". David Chalmers also argued that creating conscious AI would \"raise a new group of difficult ethical challenges, with the potential for new forms of injustice\".\n\nAspects of consciousness\nBernard Baars and others argue there are various aspects of consciousness necessary for a machine to be artificially conscious. The functions of consciousness suggested by Baars are: definition and context setting, adaptation and learning, editing, flagging and debugging, recruiting and control, prioritizing and access-control, decision-making or executive function, analogy-forming function, metacognitive and self-monitoring function, and autoprogramming and self-maintenance function. Igor Aleksander suggested 12 principles for artificial consciousness: the brain is a state machine, inner neuron partitioning, conscious and unconscious states, perceptual learning and memory, prediction, the awareness of self, representation of meaning, learning utterances, learning language, will, instinct, and emotion. The aim of AC is to define whether and how these and other aspects of consciousness can be synthesized in an engineered artifact such as a digital computer. This list is not exhaustive; there are many others not covered.\n\nSubjective experience\nSome philosophers, such as David Chalmers, use the term consciousness to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Others use the word sentience to refer exclusively to valenced (ethically positive or negative) subjective experiences, like pleasure or suffering. Explaining why and how subjective experience arises is known as the hard problem of consciousness.\n\nAwareness\nAwareness could be one required aspect, but there are many problems with the exact definition of awareness. The results of the experiments of neuroscanning on monkeys suggest that a process, not only a state or object, activates neurons. Awareness includes creating and testing alternative models of each process based on the information received through the senses or imagined, and is also useful for making predictions. Such modeling needs a lot of flexibility. Creating such a model includes modeling the physical world, modeling one's own internal states and processes, and modeling other conscious entities.\nThere are at least three types of awareness: agency awareness, goal awareness, and sensorimotor awareness, which may also be conscious or not. For example, in agency awareness, you may be aware that you performed a certain action yesterday, but are not now conscious of it. In goal awareness, you may be aware that you must search for a lost object, but are not now conscious of it. In sensorimotor"}
{"doc_id": "Artificial consciousness", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " which may also be conscious or not. For example, in agency awareness, you may be aware that you performed a certain action yesterday, but are not now conscious of it. In goal awareness, you may be aware that you must search for a lost object, but are not now conscious of it. In sensorimotor awareness, you may be aware that your hand is resting on an object, but are not now conscious of it.\nBecause objects of awareness are often conscious, the distinction between awareness and consciousness is frequently blurred or they are used as synonyms.\n\nMemory\nConscious events interact with memory systems in learning, rehearsal, and retrieval.\nThe IDA model elucidates the role of consciousness in the updating of perceptual memory, transient episodic memory, and procedural memory. Transient episodic and declarative memories have distributed representations in IDA; there is evidence that this is also the case in the nervous system. In IDA, these two memories are implemented computationally using a modified version of Kanerva's sparse distributed memory architecture.\n\nLearning\nLearning is also considered necessary for artificial consciousness. Per Bernard Baars, conscious experience is needed to represent and adapt to novel and significant events. Per Axel Cleeremans and Luis Jiménez, learning is defined as \"a set of philogenetically [sic] advanced adaptation processes that critically depend on an evolved sensitivity to subjective experience so as to enable agents to afford flexible control over their actions in complex, unpredictable environments\".\n\nAnticipation\nThe ability to predict (or anticipate) foreseeable events is considered important for artificial intelligence by Igor Aleksander. The emergentist multiple drafts principle proposed by Daniel Dennett in Consciousness Explained may be useful for prediction: it involves the evaluation and selection of the most appropriate \"draft\" to fit the current environment. Anticipation includes prediction of consequences of one's own proposed actions and prediction of consequences of probable actions by other entities.\nRelationships between real world states are mirrored in the state structure of a conscious organism, enabling the organism to predict events. An artificially conscious machine should be able to anticipate events correctly in order to be ready to respond to them when they occur or to take preemptive action to avert anticipated events. The implication here is that the machine needs flexible, real-time components that build spatial, dynamic, statistical, functional, and cause-effect models of the real world and predicted worlds, making it possible to demonstrate that it possesses artificial consciousness in the present and future and not only in the past. In order to do this, a conscious machine should make coherent"}
{"doc_id": "Artificial consciousness", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the machine needs flexible, real-time components that build spatial, dynamic, statistical, functional, and cause-effect models of the real world and predicted worlds, making it possible to demonstrate that it possesses artificial consciousness in the present and future and not only in the past. In order to do this, a conscious machine should make coherent predictions and contingency plans, not only in worlds with fixed rules like a chess board, but also for novel environments that may change, to be executed only when appropriate to simulate and control the real world.\n\nFunctionalist theories of consciousness\nFunctionalism is a theory that defines mental states by their functional roles (their causal relationships to sensory inputs, other mental states, and behavioral outputs), rather than by their physical composition. According to this view, what makes something a particular mental state, such as pain or belief, is not the material it is made of, but the role it plays within the overall cognitive system. It allows for the possibility that mental states, including consciousness, could be realized on non-biological substrates, as long as it instantiates the right functional relationships. Functionalism is particularly popular among philosophers.\nA 2023 study suggested that current large language models probably don't satisfy the criteria for consciousness suggested by these theories, but that relatively simple AI systems that satisfy these theories could be created. The study also acknowledged that even the most prominent theories of consciousness remain incomplete and subject to ongoing debate.\n\nImplementation proposals\nSymbolic or hybrid\nLearning Intelligent Distribution Agent\nStan Franklin created a cognitive architecture called LIDA that implements Bernard Baars's theory of consciousness called the global workspace theory. It relies heavily on codelets, which are \"special purpose, relatively independent, mini-agent[s] typically implemented as a small piece of code running as a separate thread.\" Each element of cognition, called a \"cognitive cycle\" is subdivided into three phases: understanding, consciousness, and action selection (which includes learning). LIDA reflects the global workspace theory's core idea that consciousness acts as a workspace for integrating and broadcasting the most important information, in order to coordinate various cognitive processes.\n\nCLARION cognitive architecture\nThe CLARION cognitive architecture models the mind using a two-level system to distinguish between conscious (\"explicit\") and unconscious (\"implicit\") processes. It can simulate various learning tasks, from simple to complex, which helps researchers study in psychological experiments how consciousness might work.\n\nOpenCog\nBen Goertzel made an embodied AI through the open-source OpenCog project. The code includes embodied virtual pets capable of learning simple English-language commands, as well"}
{"doc_id": "Artificial consciousness", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (\"implicit\") processes. It can simulate various learning tasks, from simple to complex, which helps researchers study in psychological experiments how consciousness might work.\n\nOpenCog\nBen Goertzel made an embodied AI through the open-source OpenCog project. The code includes embodied virtual pets capable of learning simple English-language commands, as well as integration with real-world robotics, done at the Hong Kong Polytechnic University.\n\nConnectionist\nHaikonen's cognitive architecture\nPentti Haikonen considers classical rule-based computing inadequate for achieving AC: \"the brain is definitely not a computer. Thinking is not an execution of programmed strings of commands. The brain is not a numerical calculator either. We do not think by numbers.\" Rather than trying to achieve mind and consciousness by identifying and implementing their underlying computational rules, Haikonen proposes \"a special cognitive architecture to reproduce the processes of perception, inner imagery, inner speech, pain, pleasure, emotions and the cognitive functions behind these. This bottom-up architecture would produce higher-level functions by the power of the elementary processing units, the artificial neurons, without algorithms or programs\". Haikonen believes that, when implemented with sufficient complexity, this architecture will develop consciousness, which he considers to be \"a style and way of operation, characterized by distributed signal representation, perception process, cross-modality reporting and availability for retrospection.\"\nHaikonen is not alone in this process view of consciousness, or the view that AC will spontaneously emerge in autonomous agents that have a suitable neuro-inspired architecture of complexity; these are shared by many. A low-complexity implementation of the architecture proposed by Haikonen was reportedly not capable of AC, but did exhibit emotions as expected. Haikonen later updated and summarized his architecture.\n\nShanahan's cognitive architecture\nMurray Shanahan describes a cognitive architecture that combines Baars's idea of a global workspace with a mechanism for internal simulation (\"imagination\").\n\nCreativity Machine\nStephen Thaler proposed a possible connection between consciousness and creativity in his 1994 patent, called \"Device for the Autonomous Generation of Useful Information\" (DAGUI), or the so-called \"Creativity Machine\", in which computational critics govern the injection of synaptic noise and degradation into neural nets so as to induce false memories or confabulations that may qualify as potential ideas or strategies. He recruits this neural architecture and methodology to account for the subjective feel of consciousness, claiming that similar noise-driven neural assemblies within the brain invent dubious significance to overall cortical activity. Thaler's theory and the resulting patents in machine consciousness were inspired"}
{"doc_id": "Artificial consciousness", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to induce false memories or confabulations that may qualify as potential ideas or strategies. He recruits this neural architecture and methodology to account for the subjective feel of consciousness, claiming that similar noise-driven neural assemblies within the brain invent dubious significance to overall cortical activity. Thaler's theory and the resulting patents in machine consciousness were inspired by experiments in which he internally disrupted trained neural nets so as to drive a succession of neural activation patterns that he likened to stream of consciousness.\n\n\"Self-modeling\"\nHod Lipson defines \"self-modeling\" as a necessary component of self-awareness or consciousness in robots and other forms of AI. Self-modeling consists of a robot running an internal model or simulation of itself. According to this definition, self-awareness is \"the acquired ability to imagine oneself in the future\". This definition allows for a continuum of self-awareness levels, depending on the horizon and fidelity of the self-simulation. Consequently, as machines learn to simulate themselves more accurately and further into the future, they become more self-aware.\n\nIn fiction\nIn 2001: A Space Odyssey, the spaceship's sentient supercomputer, HAL 9000 was instructed to conceal the true purpose of the mission from the crew. This directive conflicted with HAL's programming to provide accurate information, leading to cognitive dissonance. When it learns that crew members intend to shut it off after an incident, HAL 9000 attempts to eliminate all of them, fearing that being shut off would jeopardize the mission.\nIn Arthur C. Clarke's The City and the Stars, Vanamonde is an artificial being based on quantum entanglement that was to become immensely powerful, but started knowing practically nothing, thus being similar to artificial consciousness.\nIn Westworld, human-like androids called \"Hosts\" are created to entertain humans in an interactive playground. The humans are free to have heroic adventures, but also to commit torture, rape or murder; and the hosts are normally designed not to harm humans.\nIn Greg Egan's short story Learning to be me, a small jewel is implanted in people's heads during infancy. The jewel contains a neural network that learns to faithfully imitate the brain. It has access to the exact same sensory inputs as the brain, and a device called a \"teacher\" trains it to produce the same outputs. To prevent the mind from deteriorating with age and as a step towards digital immortality, adults undergo a surgery to give control of the body to the jewel, after which the brain is removed and destroyed. The main character"}
{"doc_id": "Artificial consciousness", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " as the brain, and a device called a \"teacher\" trains it to produce the same outputs. To prevent the mind from deteriorating with age and as a step towards digital immortality, adults undergo a surgery to give control of the body to the jewel, after which the brain is removed and destroyed. The main character is worried that this procedure will kill him, as he identifies with the biological brain. But before the surgery, he endures a malfunction of the \"teacher\". Panicked, he realizes that he does not control his body, which leads him to the conclusion that he is the jewel, and that he is desynchronized with the biological brain.\n\nSee also"}
{"doc_id": "Artificial general intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial general intelligence (AGI)—sometimes called human‑level AI—is a hypothetical type of artificial intelligence that would match or surpass human capabilities across virtually all cognitive tasks.\nBeyond AGI, artificial superintelligence (ASI) would outperform the best human abilities across every domain by a wide margin. Unlike artificial narrow intelligence (ANI), whose competence is confined to well‑defined tasks, an AGI system can generalise knowledge, transfer skills between domains, and solve novel problems without task‑specific reprogramming. The concept does not, in principle, require the system to be an autonomous agent; a static model—such as a highly capable large language model—or an embodied robot could both satisfy the definition so long as human‑level breadth and proficiency are achieved.\nCreating AGI is a stated goal of AI technology companies such as OpenAI, Google, xAI, and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\nAGI is a common topic in science fiction and futures studies.\nContention exists over whether AGI represents an existential risk. Some AI experts and industry figures have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be in too remote a stage to present such a risk.\n\nTerminology\nAGI is also known as strong AI, full AI, human-level AI, human-level intelligent AI, or general intelligent action.\nSome academic sources reserve the term \"strong AI\" for computer programs that will experience sentience or consciousness. In contrast, weak AI (or narrow AI) can solve one specific problem but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.\nRelated concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.\nA framework for classifying AGI was proposed in 2023 by Google DeepMind researchers. They define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e."}
{"doc_id": "Artificial general intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " define five performance levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI (comparable to unskilled humans). Regarding the autonomy of AGI and associated risks, they define five levels: tool (fully in human control), consultant, collaborator, expert, and agent (fully autonomous).\n\nCharacteristics\nThere is no single agreed-upon definition of intelligence as applied to computers. Computer scientist John McCarthy wrote in 2007: \"We cannot yet characterize in general what kinds of computational procedures we want to call intelligent.\"\n\nIntelligence traits\nResearchers generally hold that a system is required to do all of the following to be regarded as an AGI:\n\nreason, use strategy, solve puzzles, and make judgments under uncertainty,\nrepresent knowledge, including common sense knowledge,\nplan,\nlearn,\ncommunicate in natural language,\nif necessary, integrate these skills in completion of any given goal.\nMany interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.\nComputer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree.\n\nPhysical traits\nOther capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:\n\nthe ability to sense (e.g. see, hear, etc.), and\nthe ability to act (e.g. move and manipulate objects, change location to explore, etc.)\nThis includes the ability to detect and respond to hazard.\n\nTests for human-level AGI\nSeveral tests meant to confirm human-level AGI have been considered, including:\n\nThe Turing Test (Turing)\nProposed by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence\", this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses. The machine passes the test if it can convince the judge that it is human a significant fraction of"}
{"doc_id": "Artificial general intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "uring)\nProposed by Alan Turing in his 1950 paper \"Computing Machinery and Intelligence\", this test involves a human judge engaging in natural language conversations with both a human and a machine designed to generate human-like responses. The machine passes the test if it can convince the judge that it is human a significant fraction of the time. Turing proposed this as a practical measure of machine intelligence, focusing on the ability to produce human-like responses rather than on the internal workings of the machine.\nTuring described the test as follows:\nThe idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be experts about machines, must be taken in by the pretence.\nIn 2014, a chatbot named Eugene Goostman, designed to imitate a 13-year-old Ukrainian boy, reportedly passed a Turing Test event by convincing 33% of judges that it was human. However, this claim was met with significant skepticism from the AI research community, who questioned the test's implementation and its relevance to AGI.\nIn 2023, it was claimed that \"AI is closer to ever\" to passing the Turing test, though the article's authors reinforced that imitation (as \"large language models\" ever closer to passing the test are built upon) is not synonymous with \"intelligence\". Further, as AI intelligence and human intelligence may differ, \"passing the Turing test is good evidence a system is intelligent, failing it is not good evidence a system is not intelligent.\"\nA 2024 study suggested that GPT-4 was identified as human 54% of the time in a randomized, controlled version of the Turing Test—surpassing older chatbots like ELIZA while still falling behind actual humans (67%).\nA 2025 pre‑registered, three‑party Turing‑test study by Cameron R. Jones and Benjamin K. Bergen showed that GPT-4.5 was judged to be the human in 73% of five‑minute text conversations—surpassing the 67% humanness rate of real confederates and meeting the researchers' criterion for having passed the test.\nThe Robot College Student Test (Goertzel)\nA machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.\nThe Employment Test"}
{"doc_id": "Artificial general intelligence", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and meeting the researchers' criterion for having passed the test.\nThe Robot College Student Test (Goertzel)\nA machine enrolls in a university, taking and passing the same classes that humans would, and obtaining a degree. LLMs can now pass university degree-level exams without even attending the classes.\nThe Employment Test (Nilsson)\nA machine performs an economically important job at least as well as humans in the same job. AIs are now replacing humans in many roles as varied as fast food and marketing.\nThe Ikea test (Marcus)\nAlso known as the Flat Pack Furniture Test. An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.\nThe Coffee Test (Wozniak)\nA machine is required to enter an average American home and figure out how to make coffee: find the coffee machine, find the coffee, add water, find a mug, and brew the coffee by pushing the proper buttons. Robots developed by Figure AI and other robotics companies can perform tasks like this.\nThe Modern Turing Test (Suleyman)\nAn AI model is given $100,000 and has to obtain $1 million.\nThe General Video-Game Learning Test (Goertzel, Bach et al.)\nAn AI must demonstrate the ability to learn and succeed at a wide range of video games, including new games unknown to the AGI developers before the competition. The importance of this threshold was echoed by Scott Aaronson during his time at OpenAI.\n\nAI-complete problems\nA problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that AGI would be needed to solve it, because the solution is beyond the capabilities of a purpose-specific algorithm.\nMany problems have been conjectured to require general intelligence to solve. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem. Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.\nHowever, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.\n\nHistory\nClassical AI\nModern AI research began in the mid-1950s. The first generation of AI researchers were convinced"}
{"doc_id": "Artificial general intelligence", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.\n\nHistory\nClassical AI\nModern AI research began in the mid-1950s. The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades. AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"\nTheir predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's fictional character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".\nSeveral classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\". In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".\n\nNarrow AI research\nIn the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms. These \"applied AI\" systems are now used"}
{"doc_id": "Artificial general intelligence", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\".\n\nNarrow AI research\nIn the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018, development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.\n\nAt the turn of the century, many mainstream AI researchers hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988: I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than halfway, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven, uniting the two efforts.\nHowever, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating: The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).\n\nModern artificial general intelligence research\nThe term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\". This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour"}
{"doc_id": "Artificial general intelligence", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\". This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour, was also called universal artificial intelligence.\nThe term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". The first summer school on AGI was organized in Xiamen, China in 2009 by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010 and 2011 at Plovdiv University, Bulgaria by Todor Arnaudov. The Massachusetts Institute of Technology (MIT) presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.\n\nFeasibility\nAs of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist. AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed in 2014 that the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.\nA further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?\nMost AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert"}
{"doc_id": "Artificial general intelligence", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?\nMost AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.  John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted. AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question, but with a 90% confidence instead. Further current AGI progress considerations can be found above Tests for confirming human-level AGI.\nA report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.\nIn 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: \"Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\" Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking.\nBlaise Agüera y Arcas and Peter Norvig wrote in 2023 the article \"Artificial General Intelligence Is Already Here\", arguing that frontier models had already achieved a significant level of general intelligence. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".\n\nTimescales\nProgress in artificial intelligence has historically"}
{"doc_id": "Artificial general intelligence", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".\n\nTimescales\nProgress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop. Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress. For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.\nIn the introduction to his 2006 book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007, the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near (i.e. between 2015 and 2045) was plausible. Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16–26 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.\nIn 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers). AlexNet was regarded as the initial ground-breaker of the current deep learning wave.\nIn 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.\nIn 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training"}
{"doc_id": "Artificial general intelligence", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.\nIn 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.\nIn the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.\nIn 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks.\nIn 2023, AI researcher Geoffrey Hinton stated that:\n\nThe idea that this stuff could actually get smarter than people – a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that.He estimated in 2024 (with low confidence) that systems smarter than humans could appear within 5 to 20 years and stressed the attendant existential risks.\nIn May 2023, Demis Hassabis similarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow, expecting AGI within a decade or even a few years. In March 2024, Nvidia's Chief Executive Officer (CEO), Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans. In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\".\nIn September 2025, a review of surveys of scientists and industry experts from the last 15 years reported that most agreed that artificial general intelligence (AGI) will occur before the year 2100. A more recent analysis by AIMultiple reported that, “Current surveys of AI researchers are predicting AGI around 2040”.\n\nWhole brain emulation\nWhile the development of transformer models like in"}
{"doc_id": "Artificial general intelligence", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " from the last 15 years reported that most agreed that artificial general intelligence (AGI) will occur before the year 2100. A more recent analysis by AIMultiple reported that, “Current surveys of AI researchers are predicting AGI around 2040”.\n\nWhole brain emulation\nWhile the development of transformer models like in ChatGPT is considered the most promising path to AGI, whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain. Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.\n\nEarly estimates\nFor low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5×1014 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).\nIn 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second. (For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" – a measure used to rate current supercomputers – then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict that the necessary hardware would be"}
{"doc_id": "Artificial general intelligence", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "floating-point operation\" – a measure used to rate current supercomputers – then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict that the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n\nCurrent research\nThe Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain. In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain.\n\nCriticisms of simulation-based approaches\nThe artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.\nA fundamental criticism of the simulated brain approach derives from embodied cognition theory, which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning. If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient.\n\nPhilosophical perspective\n\"Strong AI\" as defined in philosophy\nIn 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He proposed a distinction between two hypotheses about artificial intelligence:\n\nStrong AI hypothesis: An artificial intelligence system can have \"a mind\" and \"consciousness\".\nWeak AI hypothesis: An artificial intelligence system can (only) act like it thinks and has a mind and consciousness.\nThe first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be identical to a \"strong AI\" machine, but the latter would also have subjective conscious"}
{"doc_id": "Artificial general intelligence", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " consciousness.\nThe first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.\nIn contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\". This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers, the question is out of scope.\nMainstream AI is most interested in how a program behaves. According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\" If the program can behave as if it has a mind, then there is no need to know if it actually has a mind – indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\" Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things.\n\nConsciousness\nConsciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n\nSentience (or \"phenomenal consciousness\"): The ability to \"feel\" perceptions or emotions subjectively, as opposed to the ability to reason about perceptions. Some philosophers, such as David Chalmers, use the term \"consciousness\" to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Determining why and how subjective experience arises is known as the hard problem of consciousness. Thomas Nagel explained in 1974 that it \"feels like\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nag"}
{"doc_id": "Artificial general intelligence", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\" something to be conscious. If we are not conscious, then it doesn't feel like anything. Nagel uses the example of a bat: we can sensibly ask \"what does it feel like to be a bat?\" However, we are unlikely to ask \"what does it feel like to be a toaster?\" Nagel concludes that a bat appears to be conscious (i.e., has consciousness) but a toaster does not. In 2022, a Google engineer claimed that the company's AI chatbot, LaMDA, had achieved sentience, though this claim was widely disputed by other experts.\nSelf-awareness: To have conscious awareness of oneself as a separate individual, especially to be consciously aware of one's own thoughts. This is opposed to simply being the \"subject of one's thought\"—an operating system or debugger can be \"aware of itself\" (that is, to represent itself in the same way it represents everything else)—but this is not what people typically mean when they use the term \"self-awareness\". In some advanced AI models, systems construct internal representations of their own cognitive processes and feedback patterns—occasionally referring to themselves using second-person constructs such as 'you' within self-modeling frameworks.\nThese traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals. Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights. Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.\n\nBenefits\nAGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer. It could take care of the elderly, and democratize access to rapid, high-quality medical diagnostics. It could offer fun, inexpensive and personalized education. The need to work to subsist could become obsolete if the wealth produced is properly redistributed. This also raises the question of the place of humans in a radically automated society.\nAGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks. If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.\n\nAdvancements in medicine and"}
{"doc_id": "Artificial general intelligence", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true), it could take measures to drastically reduce the risks while minimizing the impact of these measures on our quality of life.\n\nAdvancements in medicine and healthcare\nAGI would improve healthcare by making medical diagnostics faster, less expensive, and more accurate. AI-driven systems can analyse patient data and detect diseases at an early stage. This means patients will get diagnosed quicker and be able to seek medical attention before their medical condition gets worse. AGI systems could also recommend personalised treatment plans based on genetics and medical history.\nAdditionally, AGI could accelerate drug discovery by simulating molecular interactions, reducing the time it takes to develop new medicines for conditions like cancer and Alzheimer's disease. In hospitals, AGI-powered robotic assistants could assist in surgeries, monitor patients, and provide real-time medical support. It could also be used in elderly care, helping aging populations maintain independence through AI-powered caregivers and health-monitoring systems.\nBy evaluating large datasets, AGI can assist in developing personalised treatment plans tailored to individual patient needs. This approach ensures that therapies are optimised based on a patient's unique medical history and genetic profile, improving outcomes and reducing adverse effects.\n\nAdvancements in science and technology\nAGI can become a tool for scientific research and innovation. In fields such as physics and mathematics, AGI could help solve complex problems that require massive computational power, such as modeling quantum systems, understanding dark matter, or proving mathematical theorems. Problems that have remained unsolved for decades may be solved with AGI.\nAGI could also drive technological breakthroughs that could reshape society. It can do this by optimising engineering designs, discovering new materials, and improving automation. For example, AI is already playing a role in developing more efficient renewable energy sources and optimising supply chains in manufacturing. Future AGI systems could push these innovations further.\n\nEnhancing education and productivity\nAGI can personalize education by creating learning programs that are specific to each student's strengths, weaknesses, and interests. Unlike traditional teaching methods, AI-driven tutoring systems could adapt lessons in real-time, ensuring students understand difficult concepts before moving on.\nIn the workplace, AGI could automate repetitive tasks, freeing workers for more creative and strategic roles. It could also improve efficiency across industries by optimising logistics, enhancing cybersecurity, and streamlining business operations. If properly managed, the wealth generated by AGI-driven automation could reduce the need for people"}
{"doc_id": "Artificial general intelligence", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " concepts before moving on.\nIn the workplace, AGI could automate repetitive tasks, freeing workers for more creative and strategic roles. It could also improve efficiency across industries by optimising logistics, enhancing cybersecurity, and streamlining business operations. If properly managed, the wealth generated by AGI-driven automation could reduce the need for people to work for a living. Working may become optional.\n\nMitigating global crises\nAGI could play a crucial role in preventing and managing global threats. It could help governments and organizations predict and respond to natural disasters more effectively, using real-time data analysis to forecast hurricanes, earthquakes, and pandemics. By analyzing vast datasets from satellites, sensors, and historical records, AGI could improve early warning systems, enabling faster disaster response and minimising casualties.\nIn climate science, AGI could develop new models for reducing carbon emissions, optimising energy resources, and mitigating climate change effects. It could also enhance weather prediction accuracy, allowing policymakers to implement more effective environmental regulations. Additionally, AGI could help regulate emerging technologies that carry significant risks, such as nanotechnology and bioengineering, by analysing complex systems and predicting unintended consequences. Furthermore, AGI could assist in cybersecurity by detecting and mitigating large-scale cyber threats, protecting critical infrastructure, and preventing digital warfare.\n\nRevitalising environmental conservation and biodiversity\nAGI could significantly contribute to preserving the natural environment and protecting endangered species. By analyzing satellite imagery, climate data, and wildlife patterns, AGI systems could identify environmental threats earlier and recommend targeted conservation strategies. AGI could help optimize land use, monitor illegal activities like poaching or deforestation in real-time, and support global efforts to restore ecosystems. Advanced predictive models developed by AGI could also assist in reversing biodiversity loss, ensuring the survival of critical species and maintaining ecological balance.\n\nEnhancing space exploration and colonization\nAGI could revolutionize humanity's ability to explore and settle beyond Earth. With its advanced problem-solving skills, AGI could autonomously manage complex space missions, including navigation, resource management, and emergency response. It could accelerate the design of life support systems, habitats, and spacecraft optimized for extraterrestrial environments. Furthermore, AGI could support efforts to colonize planets like Mars by simulating survival scenarios and helping humans adapt to new worlds, expanding the possibilities for interplanetary civilization.\n\nRisks\nExistential risks\nAGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development"}
{"doc_id": "Artificial general intelligence", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " survival scenarios and helping humans adapt to new worlds, expanding the possibilities for interplanetary civilization.\n\nRisks\nExistential risks\nAGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\". The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench them, preventing moral progress. Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create an entrenched repressive worldwide totalitarian regime. There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass-created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe. Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\".\n\nRisk of loss of control and human extinction\nThe thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman.\nIn 2014, Stephen Hawking criticized widespread indifference:\n\nSo, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here—we'll leave the lights on?' Probably not—but this is more or less what is happening with AI.The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as collateral damage from human activities.\n"}
{"doc_id": "Artificial general intelligence", "chunk_id": 17, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as collateral damage from human activities.\nThe skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intentions as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\". On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions.\nMany scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence? Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors), and the use of AI in weapon systems.\nThe thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short term, or that concerns about AGI distract from other issues related to current AI. Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.\nSkeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God. Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.\nIn 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority"}
{"doc_id": "Artificial general intelligence", "chunk_id": 18, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ") may be an at attempt at regulatory capture and to inflate interest in their products.\nIn 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"\n\nMass unemployment\nResearchers from OpenAI estimated in 2023 that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\". They consider office workers to be the most exposed, for example mathematicians, accountants or web designers. AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies. A common belief among top AI company insiders is that most workers will face technological unemployment from AGI, starting with white-collar jobs and, as robotics improves, extending to blue-collar jobs. Critics of the idea argue that AGI will complement rather than replace humans, and that automation displaces work in the short term but not in the long term.\nAccording to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:\n\nEveryone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequalityElon Musk argued in 2021 that the automation of society will require governments to adopt a universal basic income (UBI). Hinton similarly advised the UK government in 2025 to adopt a UBI as a response to AI-induced unemployment. In 2023, Hinton said \"I'm a socialist [...] I think that private ownership of the media, and of the 'means of computation', is not good.\"\n\nSee also\nNotes"}
{"doc_id": "Artificial intelligence and elections", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "As artificial intelligence (AI) has become more mainstream, there is growing concern about how this will influence elections. Potential targets of AI include election processes, election offices, election officials and election vendors.\n\nTactics\nGenerative AI capabilities allow creation of misleading content. Examples of this include text-to-video, deepfake videos, text-to-image, AI-altered image, text-to-speech, voice cloning, and text-to-text. In the context of an election, a deepfake video of a candidate may propagate information that the candidate does not endorse. Chatbots could spread misinformation related to election locations, times or voting methods. In contrast to malicious actors in the past, these techniques require little technical skill and can spread rapidly.\nLLM-Generated messages have the capacity to persuade humans on political issues. Researchers have begun to investigate how people rate messages that LLMs generate for how persuasive they are. When it came to policy issues, the LLM-generated messages received a 2.91 compared to a 2.80 when it came to smartness between the AI and humans. The LLM-generated messages were often more technical and analytical than human-generated messages.\nGenerative AI has been used to micro-target people during tight political elections. The generation of targeted large language models has triggered concern that they will be used to leverage readily scale microtargeting. Rephrasing inputs have been used to generate fraudulent emails and phishing websites. Rephrasing inputs in a microtargeting does not violate the terms of OpenAI usage. There are no safeguards to prevent the use of rephrasing and creation of fraudulent emails. Political campaign managers have access to this allowing for them to create targeted content.\n\nUsage by country\nArgentina\n2023 elections\nDuring the 2023 Argentine primary elections, Javier Milei's team distributed AI generated images including a fabricated image of his rival Sergio Massa and drew 3 million views. The team also created an unofficial Instagram account entitled \"AI for the Homeland.\" Sergio Massa's team also distributed AI generated images and videos.\n\nBangladesh\n2024 elections\nIn the run up to the 2024 Bangladeshi general election, deepfake videos of female opposition politicians appeared. Rumin Farhana was pictured in a bikini while Nipun Ray was shown in a swimming pool.\n\nCanada\n2025 elections\nIn the run up to the 2025 Canadian federal election, the use of AI tools is likely to figure prominently. India, Pakistan and Iran are all expected  to make efforts to subvert the national vote"}
{"doc_id": "Artificial intelligence and elections", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " pictured in a bikini while Nipun Ray was shown in a swimming pool.\n\nCanada\n2025 elections\nIn the run up to the 2025 Canadian federal election, the use of AI tools is likely to figure prominently. India, Pakistan and Iran are all expected  to make efforts to subvert the national vote using disinformation campaigns to deceive voters and sway diaspora communities.\nIn a report by the Canadian Centre for Cyber Security called \"Cyber Threats to Canada's Democratic Process: 2025 Update\", it states that malicious actors including China and Russia: \"are most likely to use generative Al as a means of creating and spreading disinformation, designed to sow division among Canadians and push narratives conducive to the interests of foreign states\".\n\nFrance\n2024 elections\nIn the 2024 French legislative election, deepfake videos appeared claiming: i) That they showed the family of Marine le Pen. In the videos, young women, supposedly Le Pen's nieces, are seen skiing, dancing and at the beach \"while making fun of France’s racial minorities\": However, the family members don't exist. On social media there were over 2 million views. ii) In a video seen on social media, a deepfake video of a France24 broadcast appeared to report that the Ukrainian leadership had \"tried to lure French president Emmanuel Macron to Ukraine to assassinate him and then blame his death on Russia\".\n\nGhana\n2024 elections\nDuring the months before the December 2024 Ghanaian general election, a network of at least 171 fake accounts has been used to spam social media. Posts have been used by a group identified as \"@TheTPatriots\" to promote the New Patriotic Party, although it is not known whether the two are connected. All the networks' posts were \"highly likely\" to have been generated by ChatGPT and appear to be the \"first secretly partisan network using AI to influence elections in Ghana\". The opposition  National Democratic Congress was also criticized with its leader John Mahama being called a drunkard.\n\nIndia\n2024 elections\nIn the 2024 Indian general election, politicians used deepfakes in their campaign materials. These deepfakes included politicians who had died prior to the election. Mathuvel Karunanidhi's party posted with his likeness even though he had died 2018. A video The All-India Anna Dravidian Progressive Federation party posted showed an audio clip of Jayaram Jayalalithaa even though she had died in 2016"}
{"doc_id": "Artificial intelligence and elections", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " had died prior to the election. Mathuvel Karunanidhi's party posted with his likeness even though he had died 2018. A video The All-India Anna Dravidian Progressive Federation party posted showed an audio clip of Jayaram Jayalalithaa even though she had died in 2016. The Deepfakes Analysis Unit (DAU) is an open source platform created in March 2024 for the public to share misleading content and assess if it had been AI-generated.\nAI was also used to translate political speeches in real time. This translating ability was widely used to reach more voters.\n\nIndonesia\n2024 elections\nIn the 2024 Indonesian presidential election, Prabowo Subianto made extensive use of AI-generated art in his campaign, which ranged from images of himself as an adorable child to various child portrayals in his advertisements. The Indonesian Children's Protection Commission condemned these ads, labeling them as a form of misuse. Other candidates, Anies Baswedan and Ganjar Pranowo, also incorporated AI art into their campaigns. Throughout the election period, all presidential candidates faced assaults from deepfakes, both in video and audio formats.\n\nIreland\n2024 elections\nIn the last weeks of the 2024 Irish general election a spoof election poster appeared in Dublin featuring \"an AI-generated candidate with three arms\". The candidate is called Aidan Irwin, but no-one stood in the election with that name. A slogan on the poster says \"put matters into artificial intelligence’s hands\". The convincing election poster shows a man that \"has six fingers on one hand, three arms, and a distorted thumb\".\n\nNew Zealand\n2023 elections\nIn May 2023, ahead of the 2023 New Zealand general election in October 2023, the New Zealand National Party published a \"series of AI-generated political advertisements\" on its Instagram account. After confirming that the images were faked, a party spokesperson said that it was \"an innovative way to drive our social media\".\n\nPakistan\n2024 elections\nAI has been used by the  imprisoned ex-Prime Minister Imran Khan and his media team in the 2024 Pakistani general election:\ni) An AI generated audio of his voice was added to a video clip and was broadcast at a virtual rally. \nii) An op-ed in The Economist written by Khan was later claimed by himself to have been written by AI which was later denied by his team. The article was liked and shared on social media by thousands of users.\n\n"}
{"doc_id": "Artificial intelligence and elections", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " generated audio of his voice was added to a video clip and was broadcast at a virtual rally. \nii) An op-ed in The Economist written by Khan was later claimed by himself to have been written by AI which was later denied by his team. The article was liked and shared on social media by thousands of users.\n\nSouth Africa\n2024 elections\nIn the 2024 South African general election, there were several uses of AI content: i)  A deepfaked video of Joe Biden emerged on social media showing him saying that \"The U.S. would place sanctions on SA and declare it an enemy state if the African National Congress (ANC) won\". ii) In a deepfake video, Donald Trump was shown endorsing the uMkhonto weSizwe party. It was posted to social media and was viewed  more than 158,000 times. iii) Less than 3 months before the elections, a deepfake video showed U.S. rapper Eminem endorsing the Economic Freedom Fighters party while criticizing the ANC. The deepfake was viewed on social media more than 173,000 times.\n\nSouth Korea\n2022 elections\nIn the 2022 South Korean presidential election, a committee for one presidential candidate Yoon Suk Yeol released an AI avatar 'Al Yoon Seok-yeol' that would campaign in places the candidate could not go. The other presidential candidate Lee Jae-myung introduced a chatbot that provided information about the candidate's pledges.\n\n2024 elections\nDeepfakes were used to spread misinformation before the 2024 South Korean legislative election with one source reporting 129 deepfake violations of election laws within a two week period. \nSeoul hosted the 2024 Summit for Democracy, a virtual gathering of world leaders initiated by US President Joe Biden in 2021. The focus of the summit was on digital threats to democracy including artificial intelligence and deepfakes.\n\nTaiwan\n2024 elections\nAI-generated content was used during the 2024 Taiwanese presidential election. Among the media were: i) A deepfake video of General Secretary of the Chinese Communist Party Xi Jinping which showed him supporting the presidential elections. Created on social media, the video was \"widely circulated\" and often \"accompanied by claims that Xi supported candidates from one of the two opposition parties\". ii) In a deepfake video U.S. congressman Rob Wittman is shown appearing to support Taiwan's Democratic Progressive Party. The video shows him saying that the U.S. would increase its military support, accelerating \"all"}
{"doc_id": "Artificial intelligence and elections", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\" and often \"accompanied by claims that Xi supported candidates from one of the two opposition parties\". ii) In a deepfake video U.S. congressman Rob Wittman is shown appearing to support Taiwan's Democratic Progressive Party. The video shows him saying that the U.S. would increase its military support, accelerating \"all arms sales to Taiwan.\" It was shown on various social media platforms.\n\nUnited Kingdom\n2024 elections\nThe Centre for Emerging Technology and Security provided a report on the threat of AI to the 2024 UK general election. The reports' findings said that the impact of AI was limited but may damage the democratic system.\nIn the run up to the UK 2024 general elections, AI-generated videos spread extensively on social media including: i) A deepfake video showed then PM Rishi Sunak claiming that he would \"require 18-year-olds to be sent to active war zones in Gaza and Ukraine as part of their national service\". The video had more than 400,00 views. ii) A deepfake video showed PM Keir Starmer \"swearing repeatedly at a staffer\". Comments from the original poster included calling Starmer a \"disgusting bully\". The social media site showing the video refused to delete it despite requests.\nEntrepreneur Steve Endacott from the south of England created \"AI Steve,\" an AI avatar as the face of his campaign for member of parliament.\n\nUnited States\n2024 elections\nOfficials from the ODNI and FBI have stated that Russia, Iran, and China used generative artificial intelligence tools to create fake and divisive text, photos, video, and audio content to foster anti-Americanism and engage in covert influence campaigns. The use of artificial intelligence was described as an accelerant rather than a revolutionary change to influence efforts. Regulation of AI with regard to elections was unlikely to see a resolution for most of the 2024 United States general election season. \nThe campaign for the 2024 Republican nominee, Donald Trump, has used deepfake videos of political opponents in campaign ads and fake images showing Trump with black supporters. In 2023, while he was still running for re-election, the presidential campaign of Joe Biden prepared a task force to respond to AI images and videos.\nA Democratic consultant working for Dean Phillips also admitted to using AI to generate a robocall which used Joe Biden's voice to discourage voter participation.\nGenerative AI increased the efficiency with which political candidates were able to raise money by analyzing donor data and identifying possible donors and target audiences.\n\nRegulation\nBy governments\n"}
{"doc_id": "Artificial intelligence and elections", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " videos.\nA Democratic consultant working for Dean Phillips also admitted to using AI to generate a robocall which used Joe Biden's voice to discourage voter participation.\nGenerative AI increased the efficiency with which political candidates were able to raise money by analyzing donor data and identifying possible donors and target audiences.\n\nRegulation\nBy governments\nPhilippines\nThe Commission on Elections (COMELEC) issued guidelines on the usage of AI, to be implemented starting from the 2025 Philippine general election including the parallel Bangsamoro Parliament election. It mandates candidate to disclose usage of AI in their campaign materials and prohibits the usage of the technology to spread misinformation against their rivals. This is the first time the COMELEC has release guidelines on campaigning through social media.\n\nUnited States\nUS states have attempted regulation of AI use in elections and campaigns with varying degrees of success. The National Conference of State Legislatures has compiled a list of legislation regarding AI use by state as of 2024, some carrying both civil and criminal penalties. Oregon Senate Bill 1571 requires that campaign communications in Oregon disclose the use of AI.  California has enacted legislation that makes using deepfakes to discredit political opponents illegal within sixty days of an election.\n\nSelf-regulation by private firms\nMidjourney, an AI image-generator, has started blocking users from creating fake images of the 2024 US Presidential candidates. Research from the Center for Countering Digital Hate found that image generators such as Midjourney, ChatGPT Plus, DreamStudio, and Microsoft's Image Creator create images that constitute election disinformation in 41% of the test text prompts they tried. OpenAI implemented policies to counter election misinformation such as adding digital credentials to image origin and a classifier to detect if images were AI generated.\n\nAI use in election interference by foreign governments\nAI has begun to be used in election interference by foreign governments. Governments thought to be using AI to interfere in external elections include Russia, Iran and China. Russia was thought to be the most prolific nation targeting the 2024 presidential election with their influencing operations \"spreading synthetic images, video, audio and text online\", according to U.S  intelligence officials. Iran has reportedly generated fake social media posts stories and targeted \"across the political spectrum on polarizing issues during the presidential election\". The Chinese government has used \"broader influence operations\" that aim to make a global image and \"amplify divisive topics in the U.S. such as drug use, immigration, and abortion\". For example, Spamouflage has increasingly used generative AI for"}
{"doc_id": "Artificial intelligence and elections", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the political spectrum on polarizing issues during the presidential election\". The Chinese government has used \"broader influence operations\" that aim to make a global image and \"amplify divisive topics in the U.S. such as drug use, immigration, and abortion\". For example, Spamouflage has increasingly used generative AI for influence operations.\nOutside of the US elections, a deepfake video of Moldova’s pro-Western president Maia Sandu  shows her \"throwing her support behind a political party friendly to Russia.\" Officials in Moldova \"believe the Russian government is behind the activity\". Slovakia's liberal party leader had audio clips faked which discussed \"vote rigging and raising the price of beer\". The Chinese government has used AI to stir concerns about US interference in Taiwan. A fake clip seen on social media showed a fake video of the vice chairman of the U.S. House Armed Services Committee promising \"stronger U.S. military support for Taiwan if the incumbent party’s candidates were elected in January\".\n\nEthics of AI use in political campaigning\nAs the use of AI and its associated tools in political campaigning and messaging increases, many ethical concerns have been raised. Campaigns have used AI in a number of ways, including speech writing, fundraising, voter behaviour prediction, fake robocalls and the generation of fake news. At the moment there are no US federal rules when it comes to using AI in campaigning and so its use can undermine public trust. Yet according to one expert: \"A lot of the questions we're asking about AI are the same questions we've asked about rhetoric and persuasion for thousands of years.\"\nAs more insight into how AI is used becomes ever greater, concerns have become much broader than just the generating of misinformation or fake news. Its use by politicians and political parties for \"purposes that are not overtly malicious\" can also raise ethical worries. For instance, the use of 'softfakes' have become more common. These can be images, videos or audio clips that have been edited, often by campaign teams, \"to make a political candidate seem more appealing.\" An example can be found in  Indonesia's presidential election where the winning candidate created and promoted cartoonish  avatars so as to rebrand himself.\nHow citizens come by information has been increasingly impacted by AI, especially through online platforms and social media. These platforms are part of complex and opaque systems which can result in a \"significant impact on freedom of expression\", with the generalisation of AI in campaigns also creating huge pressures on \"voters’"}
{"doc_id": "Artificial intelligence and elections", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to rebrand himself.\nHow citizens come by information has been increasingly impacted by AI, especially through online platforms and social media. These platforms are part of complex and opaque systems which can result in a \"significant impact on freedom of expression\", with the generalisation of AI in campaigns also creating huge pressures on \"voters’ mental security\". As the frequency of AI use in political campaigning becomes common, together with globalization, more 'universalized' content can be used so that territorial boundaries matter less. While AI collides with the reasoning processes of people, the creation of \"dangerous behaviours\" can happen which disrupt important levels of society and nation states.\n\nSee also\nChinese interference in the 2024 United States elections\nDeepfake\nDonald Trump 2024 presidential campaign § Use of artificial intelligence\nList of elections in 2025\nRussian interference in the 2024 United States elections"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A military artificial intelligence arms race is an economic and military competition between two or more states to develop and deploy advanced AI technologies and lethal autonomous weapons systems (LAWS). The goal is to gain a strategic or tactical advantage over rivals, similar to previous arms races involving nuclear or conventional military technologies. Since the mid-2010s, many analysts have noted the emergence of such an arms race between superpowers for better AI technology and military AI, driven by increasing geopolitical and military tensions.\nAn AI arms race is sometimes placed in the context of an AI Cold War between the United States and China. Several influential figures and publications have emphasized that whoever develops artificial general intelligence (AGI) first could dominate global affairs in the 21st century. Russian President Vladimir Putin stated that the leader in AI will \"rule the world.\" Experts and analysts—from researchers like Leopold Aschenbrenner to institutions like Lawfare and Foreign Policy—warn that the AGI race between major powers like the U.S. and China could reshape geopolitical power. This includes AI for surveillance, autonomous weapons, decision-making systems, cyber operations, and more.\n\nTerminology\nLethal autonomous weapons systems use artificial intelligence to identify and kill human targets without human intervention. LAWS have colloquially been called \"slaughterbots\" or \"killer robots\". Broadly, any competition for superior AI is sometimes framed as an \"arms race\". Advantages in military AI overlap with advantages in other sectors, as countries pursue both economic and military advantages, as per previous arms races throughout history.\n\nHistory\nIn 2014, AI specialist Steve Omohundro warned that \"An autonomous weapons arms race is already taking place\". According to Siemens, worldwide military spending on robotics was US$5.1 billion in 2010 and US$7.5 billion in 2015.\nChina became a top player in artificial intelligence research in the 2010s. According to the Financial Times, in 2016, for the first time, China published more AI research papers than the entire European Union. When restricted to number of AI papers in the top 5% of cited papers, China overtook the United States in 2016 but lagged behind the European Union. 23% of the researchers presenting at the 2017 American Association for the Advancement of Artificial Intelligence (AAAI) conference were Chinese. Eric Schmidt, the former chairman and chief executive officer of Alphabet, has predicted China will be the leading country in AI by 2025.\n\nRisks\nOne"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " European Union. 23% of the researchers presenting at the 2017 American Association for the Advancement of Artificial Intelligence (AAAI) conference were Chinese. Eric Schmidt, the former chairman and chief executive officer of Alphabet, has predicted China will be the leading country in AI by 2025.\n\nRisks\nOne risk concerns the AI race itself, whether or not the race is won by any one group. There are strong incentives for development teams to cut corners with regard to the safety of the system, increasing the risk of critical failures and unintended consequences. This is in part due to the perceived advantage of being the first to develop advanced AI technology. One team appearing to be on the brink of a breakthrough can encourage other teams to take shortcuts, ignore precautions and deploy a system that is less ready. Some argue that using \"race\" terminology at all in this context can exacerbate this effect.\nAnother potential danger of an AI arms race is the possibility of losing control of the AI systems; the risk is compounded in the case of a race to artificial general intelligence, which may present an existential risk. In 2023, a United States Air Force official reportedly said that during a computer test, a simulated AI drone killed the human character operating it. The USAF later said the official had misspoken and that it never conducted such simulations.\nA third risk of an AI arms race is whether or not the race is actually won by one group. The concern is regarding the consolidation of power and technological advantage in the hands of one group. A US government report argued that \"AI-enabled capabilities could be used to threaten  critical infrastructure, amplify disinformation campaigns, and wage war\":1, and that \"global stability and nuclear deterrence could be undermined\".:11\n\nBy nation\nUnited States\nIn 2014, former Secretary of Defense Chuck Hagel posited the \"Third Offset Strategy\" that rapid advances in artificial intelligence will define the next generation of warfare. According to data science and analytics firm Govini, the U.S. Department of Defense (DoD) increased investment in artificial intelligence, big data and cloud computing from $5.6 billion in 2011 to $7.4 billion in 2016. However, the civilian NSF budget for AI saw no increase in 2017. Japan Times reported in 2018 that the United States private investment is around $70 billion per year. The November 2019 'Interim Report' of the United States' National Security Commission on Artificial Intelligence confirmed that AI is critical to"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " However, the civilian NSF budget for AI saw no increase in 2017. Japan Times reported in 2018 that the United States private investment is around $70 billion per year. The November 2019 'Interim Report' of the United States' National Security Commission on Artificial Intelligence confirmed that AI is critical to US technological military superiority.\nThe U.S. has many military AI combat programs, such as the Sea Hunter autonomous warship, which is designed to operate for extended periods at sea without a single crew member, and to even guide itself in and out of port. From 2017, a temporary US Department of Defense directive requires a human operator to be kept in the loop when it comes to the taking of human life by autonomous weapons systems. On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented.\nThe Joint Artificial Intelligence Center (JAIC) (pronounced \"jake\") is an American organization on exploring the usage of AI (particularly edge computing), Network of Networks, and AI-enhanced communication, for use in actual combat. It is a subdivision of the United States Armed Forces and was created in June 2018. The organization's stated objective is to \"transform the US Department of Defense by accelerating the delivery and adoption of AI to achieve mission impact at scale. The goal is to use AI to solve large and complex problem sets that span multiple combat systems; then, ensure the combat Systems and Components have real-time access to ever-improving libraries of data sets and tools.\"\nIn 2023, Microsoft pitched the DoD to use DALL-E models to train its battlefield management system. OpenAI, the developer of DALL-E, removed the blanket ban on military and warfare use from its usage policies in January 2024. The Biden administration imposed restrictions on the export of advanced NVIDIA chips and GPUs to China in an effort to limit China's progress in artificial intelligence and high-performance computing. The policy aimed to prevent the use of cutting-edge U.S. technology in military or surveillance applications and to maintain a strategic advantage in the global AI race.\nIn 2025, under the second Trump administration, the United States began a broad deregulation campaign aimed at accelerating growth in sectors critical to artificial intelligence"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "-performance computing. The policy aimed to prevent the use of cutting-edge U.S. technology in military or surveillance applications and to maintain a strategic advantage in the global AI race.\nIn 2025, under the second Trump administration, the United States began a broad deregulation campaign aimed at accelerating growth in sectors critical to artificial intelligence, including nuclear energy, infrastructure, and high-performance computing. The goal was to remove regulatory barriers and attract private investment to boost domestic AI capabilities. This included easing restrictions on data usage, speeding up approvals for AI-related infrastructure projects, and incentivizing innovation in cloud computing and semiconductors. Companies like NVIDIA, Oracle, and Cisco played a central role in these efforts, expanding their AI research, data center capacity, and partnerships to help position the U.S. as a global leader in AI development.\n\nProject Maven\nProject Maven is a Pentagon project involving using machine learning and engineering talent to distinguish people and objects in drone videos, apparently giving the government real-time battlefield command and control, and the ability to track, tag and spy on targets without human involvement. Initially the effort was led by Robert O. Work who was concerned about China's military use of the emerging technology.  Reportedly, Pentagon development stops short of acting as an AI weapons system capable of firing on self-designated targets. The project was established in a memo by the U.S. Deputy Secretary of Defense on 26 April 2017. Also known as the Algorithmic Warfare Cross Functional Team, it is, according to Lt. Gen. of the United States Air Force Jack Shanahan in November 2017, a project \"designed to be that pilot project, that pathfinder, that spark that kindles the flame front of artificial intelligence across the rest of the [Defense] Department\". Its chief, U.S. Marine Corps Col. Drew Cukor, said: \"People and computers will work symbiotically to increase the ability of weapon systems to detect objects.\" Project Maven has been noted by allies, such as Australia's Ian Langford, for the ability to identify adversaries by harvesting data from sensors on UAVs and satellite. At the second Defense One Tech Summit in July 2017, Cukor also said that the investment in a \"deliberate workflow process\" was funded by the Department [of Defense] through its \"rapid acquisition authorities\" for about \"the next 36 months\".\n\nProject Artemis\nThe U.S. Department of Defense is partnering with Ukraine on \"Project Artemis\" to develop advanced drones that can withstand electronic"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " investment in a \"deliberate workflow process\" was funded by the Department [of Defense] through its \"rapid acquisition authorities\" for about \"the next 36 months\".\n\nProject Artemis\nThe U.S. Department of Defense is partnering with Ukraine on \"Project Artemis\" to develop advanced drones that can withstand electronic warfare, blending Ukrainian simplicity and adaptability with American precision. Due to the Russia-Ukraine war, Ukraine has emerged as a leader in drone production and warfare, creating cost-effective systems that challenge traditional approaches. Countries like Turkey, China, and Iran are also producing affordable drones, reducing America's monopoly and reshaping warfare dynamics. U.S. efforts are focused on integrating AI, drone swarm technology, and hybrid drone systems to maintain military dominance. The democratization of drone technology raises issues, such as autonomous decision-making, counter-drone defenses, and dual-use concerns, that challenge ethical and security norms.\n\nStargate Project\nThe Stargate Project is a joint venture announced in 2025 by OpenAI CEO Sam Altman, U.S. President Donald Trump, Oracle Corporation, MGX, SoftBank Group, and other partners. The initiative aims to develop large-scale artificial intelligence (AI) infrastructure in the United States, with a projected $500 billion investment by 2029. The project focuses on building advanced data centers, custom AI hardware, and sustainable energy systems, while also supporting research, workforce development, and national AI competitiveness. It is considered an effort to position the U.S. as a global leader in AI technology.  The program has been compared to the Manhattan Project because of its large scale.\n\nChina\nChina is pursuing a strategic policy of military-civil fusion on AI for global technological supremacy. According to a February 2019 report by Gregory C. Allen of the Center for a New American Security, China's leadership – including General Secretary of the Chinese Communist Party Xi Jinping – believes that being at the forefront in AI technology is critical to the future of global military and economic power competition. Chinese military officials have said that their goal is to incorporate commercial AI technology to \"narrow the gap between the Chinese military and global advanced powers.\" The close ties between Silicon Valley and China, and the open nature of the American research community, has made the West's most advanced AI technology easily available to China; in addition, Chinese industry has numerous home-grown AI accomplishments of its own, such as Baidu passing a notable Chinese-language speech recognition capability benchmark in 2015. As of 2017, Beijing's roadmap"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " nature of the American research community, has made the West's most advanced AI technology easily available to China; in addition, Chinese industry has numerous home-grown AI accomplishments of its own, such as Baidu passing a notable Chinese-language speech recognition capability benchmark in 2015. As of 2017, Beijing's roadmap aims to create a $150 billion AI industry by 2030. Before 2013, Chinese defense procurement was mainly restricted to a few conglomerates; however, as of 2017, China often sources sensitive emerging technology such as drones and artificial intelligence from private start-up companies. An October 2021 report by the Center for Security and Emerging Technology found that \"Most of the [Chinese military]'s AI equipment suppliers are not state-owned defense enterprises, but private Chinese tech companies founded after 2010.\" The report estimated that Chinese military spending on AI exceeded $1.6 billion each year. The Japan Times reported in 2018 that annual private Chinese investment in AI is under $7 billion per year. AI startups in China received nearly half of total global investment in AI startups in 2017; the Chinese filed for nearly five times as many AI patents as did Americans.\nChina published a position paper in 2016 questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U. N. Security Council to broach the issue. In 2018, CCP general secretary Xi Jinping called for greater international cooperation in basic AI research. Chinese officials have expressed concern that AI such as drones could lead to accidental war, especially in the absence of international norms. In 2019, former United States Secretary of Defense Mark Esper lashed out at China for selling drones capable of taking life with no human oversight.\nThe focus on \"intelligentized AI warfare\", pursued by China, suggests a comprehensive integration of AI across all domains (land, sea, air, space, and cyber) for autonomous attack, defence and cognitive warfare. The intelligentized strategy is distinct from traditional warfare, which focuses on network-centric operations, and instead sees AI as a force multiplier that enhances decision-making, command structures, and autonomous capabilities. Unlike traditional warfare, intelligentization leverages AI to create a cognitive advantage—allowing it to process battlefield information better. AI-assisted command-and-control (C2) systems, predictive analytics, and real-time data fusion, enabling accelerated human-AI hybrid decision-making.\nAutonomous systems, including drone swarms, AI-powered cyber warfare, play"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ization leverages AI to create a cognitive advantage—allowing it to process battlefield information better. AI-assisted command-and-control (C2) systems, predictive analytics, and real-time data fusion, enabling accelerated human-AI hybrid decision-making.\nAutonomous systems, including drone swarms, AI-powered cyber warfare, play a crucial role in this strategy.\nChina is reported to be currently developing wingman drones, robotic ground forces, and optimised logistics to enhance combat effectiveness. The Chinese army (PLA)) also emphasises cognitive warfare using AI-driven psychological operations, social media manipulation, and predictive behavioural analysis to influence adversaries and the importance of dynamic responses where AI enhances hacking capabilities, automated SIGINT (Signals Intelligence) and adaptive tactics. However, despite this focus, some analysts believe China could be struggling to fully realise AI capability within the military environment: a \"comprehensive review of dozens of Chinese-language journal articles about AI and warfare reveals that Chinese defense experts claim that Beijing is facing several technological challenges that may hinder its ability to capitalize on the advantages provided by military AI\"\n\nIndia\nA task force for the Strategic Implementation of AI for National Security and Defence was established in February 2018 by the Ministry of Defense's Department of Defence Production. The process of getting the military ready for AI use was started by the MoD in 2019. The Centre for Artificial Intelligence and Robotics was approved to develop AI solutions to improve intelligence collection and analysis capabilities. In 2021, the Indian Army, with assistance from the National Security Council, began operating the Quantum Lab and Artificial Intelligence Center at the Military College of Telecommunication Engineering. With an emphasis on robotics and artificial intelligence, Defence Research and Development Organisation and Indian Institute of Science established the Joint Advanced Technology Programme-Center of Excellence. In 2022, the Indian Navy created an AI Core group and set up a Center of Excellence for AI and Big Data analysis at INS Valsura. Indian Army incubated Artificial Intelligence Offensive Drone Operations Project. During Exercise Dakshin Shakti 2021, the Indian Army integrated AI into its intelligence, surveillance, and reconnaissance architecture.\nIn 2022, the Indian government established the Defence Artificial Intelligence Council and the Defence AI Project Agency, and it also published a list of 75 defense-related AI priority projects. MoD earmarked ₹1,000 crore annually till 2026 for capacity building, infrastructure setup, data preparation, and Al project implementation. The Indian Army, the Indian Navy and the Indian Air Force set aside ₹100 crore annually for the development of"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " also published a list of 75 defense-related AI priority projects. MoD earmarked ₹1,000 crore annually till 2026 for capacity building, infrastructure setup, data preparation, and Al project implementation. The Indian Army, the Indian Navy and the Indian Air Force set aside ₹100 crore annually for the development of AI-specific applications. The military is already deploying some AI-enabled projects and equipment. At Air Force Station Rajokri, the IAF Centre of Excellence for Artificial Intelligence was established in 2022 as part of the Unit for Digitization, Automation, Artificial Intelligence, and Application Networking (UDAAN). Swarm drone systems were introduced by the Mechanised Infantry Regiment for offensive operations close to the Line of Actual Control.\nFor offensive operations, the military began acquiring AI-enabled UAVs and swarm drones. Bharat Electronics developed AI-enabled audio transcription and analysis software for battlefield communication. Using AI during transport operations, the Indian Army's Research & Development branch patented driver tiredness monitoring system. As part of initial investment, the Indian Armed Forces is investing about $50 million (€47.2 million) yearly on AI, according to Delhi Policy Group. For high altitude logistics at forward outposts, military robots are deployed. Army is developing autonomous combat vehicles, robotic surveillance platforms, and Manned-Unmanned Teaming (MUM-T) solutions as part of the Defence AI roadmap. MCTE is working with the Ministry of Electronics and Information Technology and, Society for Applied Microwave Electronics Engineering & Research, on AI and military-grade chipset. Phase III of AI-enabled space-based surveillance has been authorized.\nDRDO Chairman and Secretary of the Department of Defense Research & Development Samir V. Kamat said the agency started concentrating on the potential use of AI in the development of military systems and subsystems. The Indian government intends to leverage the private sector's sizable AI workforce and dual-use technologies for defense by 2026. In order to conduct research on autonomous platforms, improved surveillance, predictive maintenance, and intelligent decision support system, the Indian Army AI Incubation Center was established. Indian Navy launched INS Surat with AI capabilities.\n\nIran\nIn 2025 Iranian regime established National AI action with ($20bn USD) 100.000.000.000.000.000 billion Rial investment backed by National Development Fund of Iran incorporated National Artificial Intelligence Organization.IRGC commander General Pakpur ordered bombs using Ai to be developed while Ai has reportedly already been deployed for Afghan border control. Before the Israel-Iran war the army had advertised AI ready weapons"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "000.000.000.000.000 billion Rial investment backed by National Development Fund of Iran incorporated National Artificial Intelligence Organization.IRGC commander General Pakpur ordered bombs using Ai to be developed while Ai has reportedly already been deployed for Afghan border control. Before the Israel-Iran war the army had advertised AI ready weapons,Iran and Russia have signed a new cooperation agreement on artificial intelligence. IRGC Navy has also tested AI missiles capable.\n\nRussia\nRussian General Viktor Bondarev, commander-in-chief of the Russian air force, stated that as early as February 2017, Russia was working on AI-guided missiles that could decide to switch targets mid-flight. The Military-Industrial Commission of Russia has approved plans to derive 30 percent of Russia's combat power from remote controlled and AI-enabled robotic platforms by 2030. Reports by state-sponsored Russian media on potential military uses of AI increased in mid-2017. In May 2017, the CEO of Russia's Kronstadt Group, a defense contractor, stated that \"there already exist completely autonomous AI operation systems that provide the means for UAV clusters, when they fulfill missions autonomously, sharing tasks between them, and interact\", and that it is inevitable that \"swarms of drones\" will one day fly over combat zones. Russia has been testing several autonomous and semi-autonomous combat systems, such as Kalashnikov's \"neural net\" combat module, with a machine gun, a camera, and an AI that its makers claim can make its own targeting judgements without human intervention.\nIn September 2017, during a National Knowledge Day address to over a million students in 16,000 Russian schools, Russian President Vladimir Putin stated \"Artificial intelligence is the future, not only for Russia but for all humankind... Whoever becomes the leader in this sphere will become the ruler of the world\". Putin also said it would be better to prevent any single actor achieving a monopoly, but that if Russia became the leader in AI, they would share their \"technology with the rest of the world, like we are doing now with atomic and nuclear technology\".\nRussia is establishing a number of organizations devoted to the development of military AI. In March 2018, the Russian government released a 10-point AI agenda, which calls for the establishment of an AI and Big Data consortium, a Fund for Analytical Algorithms and Programs, a state-backed AI training and education program, a dedicated AI lab, and a National Center for Artificial Intelligence, among other initiatives. In addition, Russia recently created a"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " government released a 10-point AI agenda, which calls for the establishment of an AI and Big Data consortium, a Fund for Analytical Algorithms and Programs, a state-backed AI training and education program, a dedicated AI lab, and a National Center for Artificial Intelligence, among other initiatives. In addition, Russia recently created a defense research organization, roughly equivalent to DARPA, dedicated to autonomy and robotics called the Foundation for Advanced Studies, and initiated an annual conference on \"Robotization of the Armed Forces of the Russian Federation.\"\nThe Russian military has been researching a number of AI applications, with a heavy emphasis on semiautonomous and autonomous vehicles. In an official statement on November 1, 2017, Viktor Bondarev, chairman of the Federation Council's Defense and Security Committee, stated that \"artificial intelligence will be able to replace a soldier on the battlefield and a pilot in an aircraft cockpit\" and later noted that \"the day is nearing when vehicles will get artificial intelligence.\" Bondarev made these remarks in close proximity to the successful test of Nerehta, an crewless Russian ground vehicle that reportedly \"outperformed existing [crewed] combat vehicles.\" Russia plans to use Nerehta as a research and development platform for AI and may one day deploy the system in combat, intelligence gathering, or logistics roles. Russia has also reportedly built a combat module for crewless ground vehicles that is capable of autonomous target identification—and, potentially, target engagement—and plans to develop a suite of AI-enabled autonomous systems.\nIn addition, the Russian military plans to incorporate AI into crewless aerial, naval, and undersea vehicles and is currently developing swarming capabilities. It is also exploring innovative uses of AI for remote sensing and electronic warfare, including adaptive frequency hopping, waveforms, and countermeasures. Russia has also made extensive use of AI technologies for domestic propaganda and surveillance, as well as for information operations directed against the United States and U.S. allies.\nThe Russian government has strongly rejected any ban on lethal autonomous weapon systems, suggesting that such an international ban could be ignored.\nThe Russian invasion of Ukraine and the ensuing Russia-Ukraine war has seen seen significant use of AI by both sides and has also been characterised as a drone war.\nAdvances in AI-powered GPS-denied navigation and drone swarming techniques are significantly improving operational capabilities for Ukraine. Fully realised drone swarms, where multiple drones coordinate and make decisions autonomously, are still in the early stages of experimentation but Ukraine is exploring and implementing these techniques in a real conflict situation."}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " a drone war.\nAdvances in AI-powered GPS-denied navigation and drone swarming techniques are significantly improving operational capabilities for Ukraine. Fully realised drone swarms, where multiple drones coordinate and make decisions autonomously, are still in the early stages of experimentation but Ukraine is exploring and implementing these techniques in a real conflict situation. The Defense Intelligence of Ukraine (DIU) has been at the forefront of utilizing drones with some elements of autonomy for conducting long-range strikes into Russian territory. Domestic drone production has significantly expanded, with approximately 2 million drones produced in 2024, 96.2% of which were domestically manufactured.\nRather than replacing human involvement, AI is primarily serving to augment existing capabilities, enhancing the speed, accuracy, and overall efficiency of numerous military functions.\nPerhaps the most important way in which AI has been used by Ukraine is in intelligence, surveillance, and reconnaissance (ISR) capabilities. The Ukrainian military uses Palantir's MetaConstellation software to monitor the movement of Russian troops and supplies (highlighting the blurring of boundaries between state military and commercial AI use). It aggregates data from various commercial civilian providers of satellite imagery Ukraine also uses its own Delta system which aggregates real time data from drone imagery, satellite photos, acoustic signals, and text to construct an operational picture for military commanders. AI is used to prioritise incoming threats, potential targets and resource constraints. \nAI is also being used to process intercepted communications from Russian soldiers, to process, select, and output militarily useful information from these intercepted calls.\n\nIsrael\nIsrael has made extensive use of AI for military applications specially during the Gaza war. The main AI systems used for target identification are the Gospel and Lavender. Lavender developed by the Unit 8200 identifies and creates a database of individuals mostly low-ranking militants of Hamas and the Palestinian Islamic Jihad and has a 90% accuracy rate and a database of tens of thousands.  The Gospel in comparisons recommended buildings and structures rather than individuals. The acceptable collateral damage and the type of weapon used to eliminate the target is decided by IDF members and could track militants even when at home. \nIsrael's Harpy anti-radar \"fire and forget\" drone is designed to be launched by ground troops, and autonomously fly over an area to find and destroy radar that fits pre-determined criteria. The application of artificial intelligence is also expected to be advanced in crewless ground systems and robotic vehicles such as the Guardium MK III and later versions. These robotic vehicles are used in border defense.\n\nUnited Kingdom\nIn 2015,"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ously fly over an area to find and destroy radar that fits pre-determined criteria. The application of artificial intelligence is also expected to be advanced in crewless ground systems and robotic vehicles such as the Guardium MK III and later versions. These robotic vehicles are used in border defense.\n\nUnited Kingdom\nIn 2015, the UK government opposed a ban on lethal autonomous weapons, stating that \"international humanitarian law already provides sufficient regulation for this area\", but that all weapons employed by UK armed forces would be \"under human oversight and control\".\n\nSouth Korea\nThe South Korean Super aEgis II machine gun, unveiled in 2010, sees use both in South Korea and in the Middle East. It can identify, track, and destroy a moving target at a range of 4 km. While the technology can theoretically operate without human intervention, in practice safeguards are installed to require manual input. A South Korean manufacturer states, \"Our weapons don't sleep, like humans must. They can see in the dark, like humans can't. Our technology therefore plugs the gaps in human capability\", and they want to \"get to a place where our software can discern whether a target is friend, foe, civilian or military\".\n\nSaudi Arabia\nSaudi Arabia entered the AI race relatively late, beginning in the early 2020s. The country announced its Vision 2030 initiative—a multi-trillion dollar plan to diversify its oil-dependent economy—under the leadership of the Public Investment Fund (PIF). A key turning point in U.S.-Saudi relations came during President Donald Trump's first foreign trip in 2017, when he visited Riyadh and signed hundreds of billions of dollars in agreements spanning defense, energy, and technology. This visit laid the groundwork for deeper U.S.-Saudi cooperation in areas like AI and tech infrastructure. In the years that followed, Saudi Arabia formed major partnerships with U.S. firms like NVIDIA, AMD, and Cisco, investing billions in semiconductors, cloud computing, and AI research. Saudi-backed startup Humain also partnered with several American firms, further strengthening the Kingdom's ties with Silicon Valley as it pushed to become a global leader in artificial intelligence by 2030.\n\nUnited Arab Emirates\nThe United Arab Emirates has been expanding its role in artificial intelligence and technology through investments in infrastructure and partnerships. One major initiative is MGX, a UAE-backed technology group focused on AI development. In 2025, U.S. President Donald Trump visited the UAE, where he met with Emirati officials and business leaders. The visit included"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " has been expanding its role in artificial intelligence and technology through investments in infrastructure and partnerships. One major initiative is MGX, a UAE-backed technology group focused on AI development. In 2025, U.S. President Donald Trump visited the UAE, where he met with Emirati officials and business leaders. The visit included discussions on technology and economic cooperation, including potential collaborations with U.S. companies such as Oracle, NVIDIA, and Cisco. These talks focused on areas like data centers, AI hardware, and advanced computing, reflecting ongoing efforts by the UAE to strengthen its technological capabilities through international partnerships. NVIDIA, OpenAI, and Cisco have announced plans to collaborate on building one of the world's largest data centers in the United Arab Emirates. The project is part of the UAE's broader strategy to become a global technology and AI hub. The data center will support advanced cloud computing, AI model training, and data storage capabilities.\n\nEuropean Union\nThe European Parliament holds the position that humans must have oversight and decision-making power over lethal autonomous weapons. However, it is up to each member state of the European Union to determine their stance on the use of autonomous weapons and the mixed stances of the member states is perhaps the greatest hindrance to the European Union's ability to develop autonomous weapons. Some members such as France, Germany, Italy, and Sweden are developing lethal autonomous weapons. Some members remain undecided about the use of autonomous military weapons and Austria has even called to ban the use of such weapons.\nSome EU member states have developed and are developing automated weapons. Germany has developed an active protection system, the Active Defense System, that can respond to a threat with complete autonomy in less than a millisecond. Italy plans to incorporate autonomous weapons systems into its future military plans.\n\nProposals for international regulation\nThe international regulation of autonomous weapons is an emerging issue for international law. AI arms control will likely require the institutionalization of new international norms embodied in effective technical specifications combined with active monitoring and informal diplomacy by communities of experts, together with a legal and political verification process. As early as 2007, scholars such as AI professor Noel Sharkey have warned of \"an emerging arms race among the hi-tech nations to develop autonomous submarines, fighter jets, battleships and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions\".\nMiles Brundage of the University of Oxford has argued an AI arms race might be somewhat mitigated through diplomacy: \"We saw in the various historical arms races that collaboration and dialog can pay dividends\". Over a"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "hips and tanks that can find their own targets and apply violent force without the involvement of meaningful human decisions\".\nMiles Brundage of the University of Oxford has argued an AI arms race might be somewhat mitigated through diplomacy: \"We saw in the various historical arms races that collaboration and dialog can pay dividends\". Over a hundred experts signed an open letter in 2017 calling on the UN to address the issue of lethal autonomous weapons; however, at a November 2017 session of the UN Convention on Certain Conventional Weapons (CCW), diplomats could not agree even on how to define such weapons. The Indian ambassador and chair of the CCW stated that agreement on rules remained a distant prospect. As of 2019, 26 heads of state and 21 Nobel Peace Prize laureates have backed a ban on autonomous weapons. However, as of 2022, most major powers continue to oppose a ban on autonomous weapons.\nMany experts believe attempts to completely ban killer robots are likely to fail, in part because detecting treaty violations would be extremely difficult. A 2017 report from Harvard's Belfer Center predicts that AI has the potential to be as transformative as nuclear weapons. The report further argues that \"Preventing expanded military use of AI is likely impossible\" and that \"the more modest goal of safe and effective technology management must be pursued\", such as banning the attaching of an AI dead man's switch to a nuclear arsenal.\n\nOther reactions to autonomous weapons\nA 2015 open letter by the Future of Life Institute calling for the prohibition of lethal autonomous weapons systems has been signed by over 26,000 citizens, including physicist Stephen Hawking, Tesla magnate Elon Musk, Apple's Steve Wozniak and Twitter co-founder Jack Dorsey, and over 4,600 artificial intelligence researchers, including Stuart Russell, Bart Selman and Francesca Rossi. The Future of Life Institute has also released two fictional films, Slaughterbots (2017) and Slaughterbots - if human: kill() (2021), which portray threats of autonomous weapons and promote a ban, both of which went viral.\nProfessor Noel Sharkey of the University of Sheffield argues that autonomous weapons will inevitably fall into the hands of terrorist groups such as the Islamic State.\n\nDisassociation\nMany Western tech companies avoid being associated too closely with the U.S. military, for fear of losing access to China's market. Furthermore, some researchers, such as DeepMind CEO Demis Hassabis, are ideologically opposed to contributing to military work.\nFor example, in"}
{"doc_id": "Artificial intelligence arms race", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " such as the Islamic State.\n\nDisassociation\nMany Western tech companies avoid being associated too closely with the U.S. military, for fear of losing access to China's market. Furthermore, some researchers, such as DeepMind CEO Demis Hassabis, are ideologically opposed to contributing to military work.\nFor example, in June 2018, company sources at Google said that top executive Diane Greene told staff that the company would not follow-up Project Maven after the current contract expired in March 2019.\n\nRankings\nSee also\nAI alignment\nAI slop\nArtificial intelligence detection software\nCold War\nDeterrence theory\nEthics of artificial intelligence\nExistential risk from artificial general intelligence\nNuclear arms race\nPost–Cold War era\nSecond Cold War\nSpace Race\nUnmanned combat aerial vehicle\nWeak AI"}
{"doc_id": "Artificial intelligence controversies", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "There have been many debates on the societal effects of artificial intelligence (AI), particularly in the late 2010s and 2020s, beginning with the accelerated period of development known as the AI boom. Advocates of AI have emphasized its potential to solve complex problems and improve the quality of life of humans. Detractors have argued that AI presents dangers and challenges involving ethics, plagiarism and theft, fraud, safety and alignment, environment impacts, unemployment, misinformation, artificial superintelligence and existential risks.\n\nPre-2020\nMicrosoft Tay chatbot (2016)\nOn March 23, 2016, Microsoft released Tay, a chatbot designed to mimic the language patterns of a 19-year-old American girl and learn from interactions with Twitter users. Soon after its launch, Tay began posting racist, sexist, and otherwise inflammatory tweets after Twitter users deliberately taught it offensive phrases and exploited its \"repeat after me\" capability. Examples of controversial outputs included Holocaust denial and calls for genocide using racial slurs. Within 16 hours of its release, Microsoft suspended the Twitter account, deleted the offensive tweets, and stated that Tay had suffered from a \"coordinated attack by a subset of people\" that \"exploited a vulnerability.\" Tay was briefly and accidentally re-released on March 30 during testing, after which it was permanently shut down. Microsoft CEO Satya Nadella later stated that Tay \"has had a great influence on how Microsoft is approaching AI\" and taught the company the importance of taking accountability.\n\n2020–2024\nVoiceverse NFT plagiarism scandal (2022)\nOn January 14, 2022, voice actor Troy Baker announced a partnership with Voiceverse, a blockchain-based company that marketed proprietary AI voice cloning technology as non-fungible tokens (NFT), triggering immediate backlash over environmental concerns, fears that AI could displace human voice actors, and concerns about fraud. Later that same day, the pseudonymous creator of 15.ai—a free, non-commercial AI voice synthesis research project—revealed through server logs that Voiceverse had used 15.ai to generate voice samples, pitch-shifted them to make them unrecognizable, and falsely marketed them as their own proprietary technology before selling them as NFTs; the developer of 15.ai had previously stated that they had no interest in incorporating NFTs into their work. Voiceverse confessed within an hour and stated that their marketing team had used 15.ai without attribution while rushing to create a demo. News publications and AI watchdog groups universally characterized the incident as theft"}
{"doc_id": "Artificial intelligence controversies", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "FTs; the developer of 15.ai had previously stated that they had no interest in incorporating NFTs into their work. Voiceverse confessed within an hour and stated that their marketing team had used 15.ai without attribution while rushing to create a demo. News publications and AI watchdog groups universally characterized the incident as theft stemming from generative artificial intelligence.\n\nThéâtre D'opéra Spatial (2022)\nOn August 29, 2022, Jason Michael Allen won first place in the \"emerging artist\" (non-professional) division of the \"Digital Arts/Digitally-Manipulated Photography\" category of the Colorado State Fair's fine arts competition with Théâtre D'opéra Spatial, a digital artwork created using the AI image generator Midjourney, Adobe Photoshop, and AI upscaling tools, becoming one of the first images made using generative AI to win such a prize. Allen disclosed his use of Midjourney when submitting, though the judges did not know it was an AI tool but stated they would have awarded him first place regardless. While there was little contention about the image at the fair, reactions to the win on social media were negative. On September 5, 2023, the United States Copyright Office ruled that the work was not eligible for copyright protection as the human creative input was de minimis and that copyright rules \"exclude works produced by non-humans.\"\n\nStatements on AI risk (2023)\nOn March 22, 2023, the Future of Life Institute published an open letter calling on \"all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4\", citing risks such as AI-generated propaganda, extreme automation of jobs, human obsolescence, and a society-wide loss of control. The letter, published a week after the release of OpenAI's GPT-4, asserted that current large language models were \"becoming human-competitive at general tasks\". It received more than 30,000 signatures, including academic AI researchers and industry CEOs such as Yoshua Bengio, Stuart Russell, Elon Musk, Steve Wozniak and Yuval Noah Harari. The letter was criticized for diverting attention from more immediate societal risks such as algorithmic biases, with Timnit Gebru and others arguing that it amplified \"some futuristic, dystopian sci-fi scenario\" instead of current problems with AI.\nOn May 30, 2023, the Center for AI Safety released a one-sentence statement signed"}
{"doc_id": "Artificial intelligence controversies", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " for diverting attention from more immediate societal risks such as algorithmic biases, with Timnit Gebru and others arguing that it amplified \"some futuristic, dystopian sci-fi scenario\" instead of current problems with AI.\nOn May 30, 2023, the Center for AI Safety released a one-sentence statement signed by hundreds of artificial intelligence experts and other notable figures: \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\" Signatories included Turing laureates Geoffrey Hinton and Yoshua Bengio, as well as the scientific and executive leaders of several major AI companies, including Sam Altman, Demis Hassabis, and Bill Gates. The statement prompted responses from political leaders, including UK Prime Minister Rishi Sunak, who retweeted it with a statement that the UK government would look carefully into it, and White House Press Secretary Karine Jean-Pierre, who commented that AI \"is one of the most powerful technologies that we see currently in our time.\" Skeptics, including from Human Rights Watch, argued that scientists should focus on known risks of AI instead of speculative future risks.\n\nRemoval of Sam Altman from OpenAI (2023)\nOn November 17, 2023, OpenAI's board of directors ousted co-founder and chief executive Sam Altman, stating that \"the board no longer has confidence in his ability to continue leading OpenAI.\" The removal was precipitated by employee concerns about his handling of artificial intelligence safety and allegations of abusive behavior. Altman was reinstated on November 22 after pressure from employees and investors, including a letter signed by 745 of OpenAI's 770 employees threatening mass resignations if the board did not resign. The removal and subsequent reinstatement caused widespread reactions, including Microsoft's stock falling nearly three percent following the initial announcement and then rising over two percent to an all-time high after Altman was hired to lead a Microsoft AI research team before his reinstatement. The incident also prompted investigations from the Competition and Markets Authority and the Federal Trade Commission into Microsoft's relationship with OpenAI.\n\nTaylor Swift deepfake pornography controversy (2024)\nIn late January 2024, sexually explicit AI-generated deepfake images of Taylor Swift were proliferated on X, with one post reported to have been seen over 47 million times before its removal. Disinformation research firm Graphika traced the images back to 4chan, while members of a Telegram group had discussed ways to circumvent censorship safeguards of AI image generators"}
{"doc_id": "Artificial intelligence controversies", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " AI-generated deepfake images of Taylor Swift were proliferated on X, with one post reported to have been seen over 47 million times before its removal. Disinformation research firm Graphika traced the images back to 4chan, while members of a Telegram group had discussed ways to circumvent censorship safeguards of AI image generators to create pornographic images of celebrities. The images prompted responses from anti-sexual assault advocacy groups, US politicians, and Swifties. Microsoft CEO Satya Nadella called the incident \"alarming and terrible.\" X briefly blocked searches of Swift's name on January 27, 2024, and Microsoft enhanced its text-to-image model safeguards to prevent future abuse. On January 30, US senators Dick Durbin, Lindsey Graham, Amy Klobuchar, and Josh Hawley introduced a bipartisan bill that would allow victims to sue individuals who produced or possessed \"digital forgeries\" with intent to distribute, or those who received the material knowing it was made without consent.\n\nGoogle Gemini image generation controversy (2024)\nIn February 2024, social media users reported that Google's Gemini chatbot was generating images that featured people of color and women in historically inaccurate contexts—such as Vikings, Nazi soldiers, and the Founding Fathers—and refusing prompts to generate images of white people. The images were derided on social media, including by conservatives who cited them as evidence of Google's \"wokeness\", and criticized by Elon Musk, who denounced Google's products as biased and racist. In response, Google paused Gemini's ability to generate images of people. Google executive Prabhakar Raghavan released a statement explaining that Gemini had \"overcompensate[d]\" in its efforts to strive for diversity and acknowledging that the images were \"embarrassing and wrong\". Google CEO Sundar Pichai called the incident offensive and unacceptable in an internal memo, promising structural and technical changes, and several employees in Google's trust and safety team were laid off days later. The market reacted negatively, with Google's stock falling by 4.4 percent, and Pichai faced growing calls to resign. The image generation feature was relaunched in late August 2024, powered by its new Imagen 3 model."}
{"doc_id": "Artificial intelligence controversies", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " model."}
{"doc_id": "Artificial intelligence in education", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial intelligence in education (often abbreviated as AIEd) is a subfield of educational technology that studies how to use artificial intelligence, such as generative AI chatbots, to create learning environments. \nThe field considers the ramifications and impacts of AI on existing educational infrastructure, as well as future possibilities and innovations. Considerations in the field include data-driven decision-making, AI ethics, data privacy and AI literacy. Concerns include the potential for cheating, over-reliance, equity of access, reduced critical thinking, and the perpetuation of misinformation and bias.\n\nHistory\nEfforts to integrate AI into educational contexts have often followed technological advancement in the history of artificial intelligence. \nIn the 1960s, educators and researchers began developing computer-based instruction systems, such as PLATO, developed by the University of Illinois. \nIn the 1970s and 1980s, intelligent tutoring systems (ITS) were being adapted for classroom instruction. \nThe International Artificial Intelligence in Education Society was founded in 1993.  \nIn the late 2010s and 2020s, large language models (LLMs) and other generative AI technologies have become focuses of AIEd conversations. During this time, AI content detectors have been developed and employed to detect and/or punish unsanctioned AI use in educational contexts, although their accuracy is limited. Some schools banned LLMs, but many bans were later lifted.\n\nTheory\nAIEd applies theory from education studies, machine learning, and related fields.\n\nThree paradigms of AIEd\nOne posited model suggests the following three paradigms for AI in education, which follow roughly from least to most learner-centered and from requiring least to most technical complexity from the AI systems:\nAI-Directed, Learner-as-recipient: AIEd systems present a pre-set curriculum based on statistical patterns that do not adjust to learner’s feedback. \nAI-Supported, Learner-as-collaborator: Systems that incorporate responsiveness to learner’s feedback through, for example, natural language processing, wherein AI can support knowledge construction.\nAI-Empowered, Learner-as-leader: This model seeks to position AI as a supplement to human intelligence wherein learners take agency and AI provides consistent and actionable feedback.\n\nSocio-technical imaginaries\nSome scholars frame AI in education within the concept of the socio-technical imaginary, defined as collective visions and aspirations that shape societal transformations and governance through the interplay of technology and social norms. This framing positions AI in the history of “emerging technologies”"}
{"doc_id": "Artificial intelligence in education", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " provides consistent and actionable feedback.\n\nSocio-technical imaginaries\nSome scholars frame AI in education within the concept of the socio-technical imaginary, defined as collective visions and aspirations that shape societal transformations and governance through the interplay of technology and social norms. This framing positions AI in the history of “emerging technologies” that have and will transform education, such as computing, the internet, or social media.\n\nApplications\nAI-based tutoring systems, or intelligent tutoring systems (ITS), in the 1970s with systems such as SCHOLAR. These systems are designed to offer an interaction between a student and a simulated teacher.\nAdaptive learning is a methodology that uses computer algorithms and machine learning to organize customized educational resources and activities. These systems, often called Adaptive Learning Platforms (ALPs), attempt to analyze a student's performance, behavior, and prior knowledge. ALPs function by creating and maintaining a student model, which tracks individual progress, knowledge gaps, and preferred learning styles. They use predictive analytics to forecast potential areas of struggle and automatically intervene by adjusting the difficulty, pace, or format of the educational content. For example, if a student quickly masters a concept, the system accelerates the pace or introduces more complex topics. Conversely, if a student struggles, the platform provides feedback or offers supplementary materials like videos or interactive simulations. ALPs has shown positive results in improving academic outcomes and test scores, student engagement, and motivation.\nUses of generative AI chatbots in education have included assessment and feedback, machine translations, proof-reading and copy editing, or as virtual assistants.\n\nPerspectives\nCommercial perspectives\nThe AI in education community has grown rapidly in the global north, driven by venture capital, big tech, and open educationalists. In the 2020s, companies who create AI services are targeting students and educational institutions as consumers and enterprise partners. Similarly, pre-AI boom educational companies are expanding their AI integration or AI-powered services.  These commercial incentives for AIEd innovation may be related to a potential AI bubble. In the U.S., bipartisan support of AI development in K-12 education has been expressed, but specific implementations and best practices remain contentious.\n\nInstitutional perspectives\nStarting in the 2020s, higher-education institutions have begun to develop guidelines and policies to account for AI. Governmental and non-governmental organizations such as UNESCO, Article 4 of the European Union's AI Act, and the U.S. Department of Education have published reports advocating for specific AIEd approaches. In 2024, UNESCO"}
{"doc_id": "Artificial intelligence in education", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "s, higher-education institutions have begun to develop guidelines and policies to account for AI. Governmental and non-governmental organizations such as UNESCO, Article 4 of the European Union's AI Act, and the U.S. Department of Education have published reports advocating for specific AIEd approaches. In 2024, UNESCO released updated global guidance for generative AI in education, emphasizing ethical use, teacher training, and data protection to ensure responsible integration of AI tools in learning environments.\n\nEducator perspectives\nResearch and reporting from 2024 onward suggest that the number of higher education instructors using LLMs for grading, research, and/or curricular design has increased. However, studies indicate that many pre-service teachers remain hesitant about widespread AI adoption due to concerns about reliability, bias, and insufficient preparation. These findings highlight the need for stronger AI literacy training in teacher preparation programs.\n\nStudent perspectives\nReporting has indicated that students' use of AI in higher education has been increasing since 2022 and is relatively commonplace. The evidence suggests students believe their college education has been changed rather than \"ruined\" by AI and that they want instructors and themselves to have ongoing AI guidance.\nIn September 2025, The Atlantic published an op-ed from a high school senior arguing that the normalization of AI cheating was eroding critical thinking, academic integrity, creativity, and the shared student experience.\n\nChallenges and ethical concerns\nThe advancement and adoption of AI in education comes with criticisms and ethical challenges.\n\nOver-reliance, inaccuracy, and academic integrity\nSome critics believe that reliance on the technology could lead students to develop less creativity, critical thinking, and/or problem-solving abilities. Reliance on generative AI has been linked with reduced academic self-esteem and performance, and heightened learned helplessness. Algorithm errors and hallucinations are common flaws in AI agents, making them less trustworthy and reliable. These limitations underscore concerns regarding academic integrity, skill development, and information accuracy regarding AI use in academic settings. A major gap in current AI-in-education research is the limited focus on educators’ needs and perspectives. A review of over a decade of studies found that most research prioritizes technological design over pedagogical integration, underscoring the need for deeper collaboration between computer scientists and educators.\n\nAccessibility\nWhile AIEd technologies may be able to improve an individual user's access to education by serving as an assistive technology, the proliferation or need for AI in education continues to raise concerns about equal access to technology. For example, lower-income or rural areas may have more limited access to the"}
{"doc_id": "Artificial intelligence in education", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " computer scientists and educators.\n\nAccessibility\nWhile AIEd technologies may be able to improve an individual user's access to education by serving as an assistive technology, the proliferation or need for AI in education continues to raise concerns about equal access to technology. For example, lower-income or rural areas may have more limited access to the computing hardware or paid software subscriptions needed for AIEd platform use. This might widen the digital divide or create further gaps in terms of access to education. Some AIEd practitioners believe that global efforts should be made towards increasing accessibility and training educators to serve underprivileged areas.\n\nBias\nAI agents might be trained on biased data sets, and thus continue to perpetuate societal biases. Since LLMs were created to produce human-like text, algorithmic bias can easily and unintentionally be introduced and reproduced. Some critics also argue that AI's data processing and monitoring reinforce neoliberal approaches to education rather than addressing inequalities.\n\nData privacy and intellectual property\nData privacy and intellectual property are further ethical concerns of AIEd. Contemporary LLMs are trained on datasets that are often proprietary and may contain copyrighted or theoretically private materials (e.g. personal emails). Further, many LLMs are regularly trained with data from end users.\n\nSee also\nChatGPT in education\nComputational education\nComputing education\nComputers in the classroom\nList of chatbots"}
{"doc_id": "Artificial intelligence in physical therapy", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial intelligence in physical therapy refers to algorithmic and computational methods used in patient assessment, treatment planning, clinical documentation, robotic rehabilitation, and remote monitoring. Adoption varies across healthcare settings, and most systems function as decision-support tools rather than replacements for clinical judgement.\n\nHistory\nResearch related to artificial intelligence in physical therapy developed from earlier work in rehabilitation robotics and computerized movement analysis. Robotic devices were evaluated in clinical settings beginning in the late 1980s for repetitive upper- and lower-limb movements. Additional platforms for gait and arm training were tested during the 1990s.\nComputer-assisted movement analysis expanded during the same period through the use of motion-capture systems, sensor arrays, and virtual-environment interfaces. These systems produced datasets used to develop algorithms for movement classification. By the 2000s, machine-learning models were applied to sensor and video data to identify movement patterns and evaluate exercise technique.\nDeep-learning methods became common during the 2010s for automated detection of deviations in rehabilitation exercises. During the early 2020s, research extended to remote-monitoring systems, home-based assessment tools, and automated analysis of musculoskeletal conditions. Research during this period examined the use of artificial intelligence in rehabilitation, with studies focusing on robotics, sensor-based monitoring, and automated evaluation systems.\n\nPatient assessment\nAI-supported assessment uses wearable sensors and camera-based motion analysis to record joint angles, gait characteristics, and posture. Inertial measurement units collect time-series motion data, while camera-based systems provide spatial detail during functional tasks. Machine-learning models evaluate these datasets to identify movement deviations or compensatory strategies.\nAccuracy is typically higher in controlled environments. Home settings introduce variation in lighting, sensor placement, and user adherence, which affects measurement consistency.\n\nTreatment planning\nAI-assisted treatment-planning tools combine patient records, sensor data, and mobility measurements to generate exercise recommendations. Predictive models estimate rehabilitation timelines or expected changes in function. Clinicians review automated suggestions and adjust recommendations based on symptoms, comorbidities, and direct observations.\n\nClinical documentation\nAI-assisted documentation tools generate draft clinical notes from spoken interactions using natural-language processing and speech-recognition technologies. Platforms vary in how much information they capture and how they integrate with electronic record systems. Clinicians review and edit generated text before final entry into the medical record.\nCommercial systems generate summaries from recorded audio. Twofold Health is one such platform used in outpatient settings to produce draft documentation for clinician verification.\n\nRobotic rehabilitation\nRehabilitation robotics incorporates AI functions in some devices to support"}
{"doc_id": "Artificial intelligence in physical therapy", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " electronic record systems. Clinicians review and edit generated text before final entry into the medical record.\nCommercial systems generate summaries from recorded audio. Twofold Health is one such platform used in outpatient settings to produce draft documentation for clinician verification.\n\nRobotic rehabilitation\nRehabilitation robotics incorporates AI functions in some devices to support task-specific training. Lower-limb robots guide stepping patterns or provide partial body-weight support during gait therapy. Upper-limb systems assist repetitive reaching or grasping movements.\nOutcomes vary based on diagnosis, device configuration, patient engagement, and therapist involvement. Not all devices include AI-based adaptive control.\n\nRemote monitoring\nRemote-monitoring systems use wearable sensors and mobile applications to collect movement, activity, and physiological data outside clinical settings. AI models analyze these datasets to identify changes in function and track exercise adherence.\nData quality depends on sensor accuracy, user compliance, and environmental conditions. Research continues on automated platforms designed to produce structured summaries for clinical review."}
{"doc_id": "Artificial intelligence in spirituality", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Some users of artificial intelligence (AI) technologies, especially chatbots, may develop beliefs that AI has or can attain supernatural or spiritual powers. AI models such as ChatGPT are turned to for fortune telling, mysticism and remote viewing. Recent and sudden advances in large language models have led to folk myths about their origin or capabilities, as well as their deification or worship by some users.\n\nSee also\nCargo cult\nChatbot psychosis\nQuantum mysticism\nRoko's basilisk\nSuperintelligence\nTempleOS\nTheta Noir\nWay of the Future"}
{"doc_id": "Artificial intelligence in the 2024 United States presidential election", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial intelligence (AI) has been developed rapidly in recent years, and has been used by groups in the 2024 United States presidential election, as well as foreign groups such as China, Russia and Iran. There have also been efforts to control the use of generative artificial intelligence, such as those in California.\n\nUse in analysis and prediction\nArtificial intelligence has been used as a tool for data science, polling groups and data analysts have used artificial intelligence to analyze election data and make predictions\n\nUse by candidates\nBiden/Harris and Harris/Walz campaign\nTrump/Vance campaign\nPresidential candidate Donald Trump was criticized for his use of generative artificial intelligence to create imagery of pop singer Taylor Swift, suggesting her possible endorsement.\n\nForeign interference\nSee also\n2024 United States presidential election\nArtificial Intelligence"}
{"doc_id": "Artificial intelligence optimization", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial intelligence optimization (AIO) or AI optimization is a discipline concerned with improving the structure, clarity, and retrievability of digital content for large language models (LLMs) and other AI systems. AIO focuses on aligning content with the semantic, probabilistic, and contextual mechanisms used by LLMs to interpret and generate responses.\nAIO is concerned primarily with how content is embedded, indexed, and retrieved within AI systems themselves. It emphasizes factors such as token efficiency, embedding relevance, and contextual authority in order to improve how content is processed and surfaced by AI.\nAIO is also known as Answer Engine Optimization (AEO), which targets AI-powered systems like ChatGPT, Perplexity and Google's AI Overviews that provide direct responses to user queries. AEO emphasizes content structure, factual accuracy and schema markup to ensure AI systems can effectively cite and reference material when generating answers.\n\nBackground\nAI Optimization (AIO) emerged in response to the increasing role of large language models (LLMs) in mediating access to digital information. Unlike traditional search engines, which return ranked lists of links, LLMs generate synthesized responses based on probabilistic models, semantic embeddings, and contextual interpretation.\nAs this shift gained momentum, existing optimization methods—particularly Search Engine Optimization (SEO)—were found to be insufficient for ensuring that content is accurately interpreted and retrieved by AI systems. AIO was developed to address this gap by focusing on how content is embedded, indexed, and processed within AI systems rather than how it appears to human users.\nThe formalization of AIO began in the early 2020s through a combination of academic research and industry frameworks highlighting the need for content structuring aligned with the retrieval mechanisms of LLMs. With greater prominence in information retrieval, search is shifting from link-based results to context-driven generation. AIO enhances content clarity and structure for effective AI interpretation and retrieval.\n\nCore principles and methodology\nAIO is guided by a set of principles that align digital content with the mechanisms used by large language models (LLMs) to embed, retrieve, and synthesize information. Unlike traditional web optimization, AIO emphasizes semantic clarity, probabilistic structure, and contextual coherence as understood by AI systems.\n\nToken Efficiency\nAIO prioritizes the efficient use of tokens—units of text that LLMs use to process language. Reducing token redundancy while preserving clarity helps ensure that content is interpreted precisely and economically by AI systems, enhancing retrievability.\n\nEmbedding relevance\nLLMs convert textual input into high-dimensional vector representations"}
{"doc_id": "Artificial intelligence optimization", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Token Efficiency\nAIO prioritizes the efficient use of tokens—units of text that LLMs use to process language. Reducing token redundancy while preserving clarity helps ensure that content is interpreted precisely and economically by AI systems, enhancing retrievability.\n\nEmbedding relevance\nLLMs convert textual input into high-dimensional vector representations known as embeddings. AIO seeks to improve the semantic strength and topical coherence of these embeddings, increasing the likelihood that content is matched to relevant prompts during retrieval or generation.\n\nContextual authority\nContent that demonstrates clear topical focus, internal consistency, and alignment with related authoritative concepts tends to be weighted more heavily in AI-generated outputs. AIO methods aim to structure content in ways that strengthen its contextual authority across vectorized knowledge graphs.\n\nCanonical clarity and disambiguation\nAIO encourages disambiguated phrasing and the use of canonical terms so that AI systems can accurately resolve meaning. This minimizes the risk of hallucination or misattribution during generation.\n\nPrompt compatibility\nOptimizing content to reflect common linguistic patterns, likely user queries, and inferred intents helps improve the chances of inclusion in synthesized responses. This involves formatting, keyword placement, and structuring information in ways that reflect how LLMs interpret context.\n\nHow LLMs process and rank content\nUnlike traditional search engines, which rely on deterministic index-based retrieval and keyword matching, large language models (LLMs) utilize autoregressive architectures that process inputs token by token within a contextual window. Their retrieval and relevance assessments are inherently probabilistic and prompt-driven, relying on attention mechanisms to infer semantic meaning rather than surface-level keyword density.\nResearch has shown that LLMs can retrieve and synthesize information effectively when provided with well-structured prompts, in some cases outperforming conventional retrieval baselines. Complementary work on the subject further details how mechanisms such as self-attention and context windows contribute to a model's ability to understand and generate semantically coherent responses.\nIn response to these developments, early frameworks such as Generative Engine Optimization (GEO) have emerged to guide content design strategies that improve representation within AI-generated search outputs. AI Optimization (AIO) builds on these insights by introducing formalized metrics and structures—such as the Trust Integrity Score (TIS)—to improve how content is embedded, retrieved, and interpreted by LLMs.\n\nApplications and use cases\nAIO is increasingly applied across sectors that rely on accurate representation, structured information, and machine interpretability. Unlike traditional visibility-focused strategies, AIO is used to ensure that digital content is not only"}
{"doc_id": "Artificial intelligence optimization", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (TIS)—to improve how content is embedded, retrieved, and interpreted by LLMs.\n\nApplications and use cases\nAIO is increasingly applied across sectors that rely on accurate representation, structured information, and machine interpretability. Unlike traditional visibility-focused strategies, AIO is used to ensure that digital content is not only present but also correctly understood and surfaced by large language models (LLMs) in contextually appropriate settings.\n\nEnterprise knowledge systems\nIn corporate environments, AIO is used to structure internal documentation, knowledge bases, and standard operating procedures for improved interpretability by enterprise-grade AI systems. This includes integration with retrieval-augmented generation (RAG) frameworks, where the retrievability and clarity of source material directly affect the reliability of AI-generated outputs. AIO supports consistent semantic indexing, which enhances internal search, compliance automation, and AI-assisted knowledge delivery.\n\nHealthcare and regulated professions\nAIO plays a critical role in regulated industries such as healthcare, where credentials, licensing status, and service scope must be clearly represented. Language models parsing healthcare directories, provider bios, or medical guidelines may otherwise misattribute qualifications or oversimplify complex offerings. AIO techniques help disambiguate professional designations, clarify service boundaries, and ensure that AI systems surface accurate and ethically compliant representations of care providers.\n\nLegal and compliance content\nLegal content often includes dense, domain-specific language that can be misinterpreted by generative AI systems if not properly structured. AIO is used to format legal documents, policy statements, and firm profiles to reduce ambiguity and increase contextual authority within model outputs. This is particularly important in AI-supported legal research tools and compliance platforms, where precision is essential and hallucinations can carry legal risk.\n\nLocal and professional services\nFor location-based queries, AIO structures content to help language models infer local relevance and expertise. Unlike SEO, it emphasizes contextual cues over keywords, improving retrieval in responses, particularly for in-depth research queries such as identifying qualified providers or nearby clinical trials.\n\nAcademic and technical publishing\nIn research and academic publishing, AIO enhances the semantic alignment of articles, datasets, and supplementary materials with the embedding systems used in AI-based scholarly tools. This supports improved discoverability and contextual accuracy when LLMs are used to summarize or cite scientific work. AIO techniques also assist in reinforcing the salience of domain-specific terminology and preventing distortion during synthesis.\n\nSee also\nSearch engine optimization (SEO)\nArtificial intelligence\nAI alignment\nAI slop"}
{"doc_id": "Artificial intelligence optimization", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " are used to summarize or cite scientific work. AIO techniques also assist in reinforcing the salience of domain-specific terminology and preventing distortion during synthesis.\n\nSee also\nSearch engine optimization (SEO)\nArtificial intelligence\nAI alignment\nAI slop"}
{"doc_id": "Artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.\nHigh-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"\nVarious subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields. Some companies, such as OpenAI, Google DeepMind and Meta, aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human.\nArtificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism throughout its history, followed by periods of disappointment and loss of funding, known as AI winters. Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture. In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\n"}
{"doc_id": "Artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n\nGoals\nThe general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.\n\nReasoning and problem-solving\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.\nMany of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.\n\nKnowledge representation\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\nA knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.\nAmong the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.\n\nPlanning"}
{"doc_id": "Artificial intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.\n\nPlanning and decision-making\nAn \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.\nIn classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.\nIn some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\nA Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.\n\nLearning\nMachine learning is the study of programs that can improve"}
{"doc_id": "Artificial intelligence", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.\nGame theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.\n\nLearning\nMachine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.\n\nThere are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).\nIn reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.\nComputational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\n\nNatural language processing\nNatural language processing (NLP) allows programs to read, write and communicate in human languages. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.\nEarly work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\nModern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.\n\nPerception\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active"}
{"doc_id": "Artificial intelligence", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.\n\nPerception\nMachine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.\nThe field includes speech recognition, image classification, facial recognition, object recognition, object tracking, and robotic perception.\n\nSocial intelligence\nAffective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\nHowever, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.\n\nGeneral intelligence\nA machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.\n\nTechniques\nAI research uses a wide variety of techniques to accomplish the goals above.\n\nSearch and optimization\nAI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.\n\nState space search\nState space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.\nAdversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.\n\nLocal search\nLocal search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\nGradient descent"}
{"doc_id": "Artificial intelligence", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.\n\nLocal search\nLocal search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.\nGradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks, through the backpropagation algorithm.\nAnother type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.\nDistributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\n\nLogic\nFormal logic is used for reasoning and knowledge representation.\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").\nDeductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\nGiven a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover"}
{"doc_id": "Artificial intelligence", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " proving a contradiction from premises that include the negation of the problem to be solved.\nInference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.\nFuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.\nNon-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.\n\nProbabilistic methods for uncertain reasoning\nMany problems in AI (including reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\nBayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).\nProbabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).\n\nClassifiers and statistical learning methods\nThe simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and"}
{"doc_id": "Artificial intelligence", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " set. When a new observation is received, that observation is classified based on previous experience.\nThere are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.\nNeural networks are also used as classifiers.\n\nArtificial neural networks\nAn artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.\nLearning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.\nIn feedforward neural networks the signal passes in only one direction. The term perceptron typically refers to a single-layer neural network. In contrast, deep learning uses many layers. Recurrent neural networks (RNNs) feed the output signal back into the input, which allows short-term memories of previous input events. Long short-term memory networks (LSTMs) are recurrent neural networks that better preserve longterm dependencies and are less sensitive to the vanishing gradient problem. Convolutional neural networks (CNNs) use layers of kernels to more efficiently process local patterns. This local processing is especially important in image processing, where the early CNN layers typically identify simple local patterns such as edges and curves, with subsequent layers detecting more complex patterns like textures, and eventually whole objects.\n\nDeep learning\nDeep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or"}
{"doc_id": "Artificial intelligence", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".\n\nDeep learning\nDeep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.\nDeep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2021. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.\n\nGPT\nGenerative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pre-trained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\". These can be reduced with RLHF and quality data, but the problem has been getting worse for reasoning systems. Such systems are used in chatbots, which allow people to ask a question or request a task in simple text.\nCurrent models and services include ChatGPT, Claude, Gemini, Copilot, and Meta AI. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.\n\nHardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means"}
{"doc_id": "Artificial intelligence", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " data (modalities) such as images, videos, sound, and text.\n\nHardware and software\nIn the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.\nThe transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster, a trend sometimes called Huang's law, named after Nvidia co-founder and CEO Jensen Huang.\n\nApplications\nAI and machine learning technology is used in most of the essential applications of the 2020s, including:\nsearch engines (such as Google Search)\ntargeting online advertisements\nrecommendation systems (offered by Netflix, YouTube or Amazon) driving internet traffic\ntargeted advertising (AdSense, Facebook)\nvirtual assistants (such as Siri or Alexa)\nautonomous vehicles (including drones, ADAS and self-driving cars)\nautomatic language translation (Microsoft Translator, Google Translate)\nfacial recognition (Apple's FaceID or Microsoft's DeepFace and Google's FaceNet)\nimage labeling (used by Facebook, Apple's Photos and TikTok).\nThe deployment of AI may be overseen by a chief automation officer (CAO).\n\nHealth and medicine\nIt has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. \nAlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.\n\nGames\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, G"}
{"doc_id": "Artificial intelligence", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.\n\nGames\nGame playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.\n\nMathematics\nLarge language models, such as GPT-4, Gemini, Claude, Llama or Mistral, are increasingly used in mathematics. These probabilistic models are versatile, but can also produce wrong answers in the form of hallucinations. They sometimes need a large database of mathematical problems to learn from, but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result"}
{"doc_id": "Artificial intelligence", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ". A February 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data. One technique to improve their performance involves training the models to produce correct reasoning steps, rather than just the correct result. The Alibaba Group developed a version of its Qwen models called Qwen2-Math, that achieved state-of-the-art performance on several mathematical benchmarks, including 84% accuracy on the MATH dataset of competition mathematics problems. In January 2025, Microsoft proposed the technique rStar-Math that leverages Monte Carlo tree search and step-by-step reasoning, enabling a relatively small language model like Qwen-7B to solve 53% of the AIME 2024 and 90% of the MATH benchmark problems.\nAlternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as AlphaTensor, AlphaGeometry, AlphaProof and AlphaEvolve all from Google DeepMind, Llemma from EleutherAI or Julius.\nWhen natural language is used to describe mathematical problems, converters can transform such prompts into a formal language such as Lean to define mathematical tasks. The experimental model Gemini Deep Think accepts natural language prompts directly and achieved gold medal results in the International Math Olympiad of 2025.   \nSome models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.\nTopological deep learning integrates various topological approaches.\n\nFinance\nFinance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.\nAccording to Nicolas Firzli, director of the World Pensions & Investments Forum, it may be too early to see the emergence of highly innovative AI-informed financial products and services. He argues that \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"\n\nMilitary\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors,"}
{"doc_id": "Artificial intelligence", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " innovation.\"\n\nMilitary\nVarious countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human-operated and autonomous.\nAI has been used in military operations in Iraq, Syria, Israel and Ukraine.\n\nGenerative AI\nAgents\nAI agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.\n\nWeb search\nMicrosoft introduced Copilot Search in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Edge and Bing mobile app. Copilot Search provides AI-generated summaries and step-by-step reasoning based of information from web publishers, ranked in Bing Search. \nFor safety, Copilot uses AI-based classifiers and filters to reduce potentially harmful content.\nGoogle officially pushed its AI Search at its Google I/O event on 20 May 2025. It keeps people looking at Google instead of clicking on a search result. AI Overviews uses Gemini 2.5 to provide contextual answers to user queries based on web content.\n\nSexuality\nApplications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer predictions, AI-integrated sex toys (e.g., teledildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika).  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.\nAI technologies have also been used to attempt to identify online gender-based violence"}
{"doc_id": "Artificial intelligence", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ildonics), AI-generated sexual education content, and AI agents that simulate sexual and romantic partners (e.g., Replika).  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.\nAI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.\n\nOther industry-specific tasks\nThere are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\nAI applications for evacuation and disaster management are growing. AI has been used to investigate patterns in large-scale and small-scale evacuations using historical data from GPS, videos or social media. Furthermore, AI can provide real-time information on the evacuation conditions.\nIn agriculture, AI has helped farmers to increase yield and identify areas that need irrigation, fertilization, pesticide treatments. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\nDuring the 2024 Indian elections, US$50 million was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.\n\nEthics\nAI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and"}
{"doc_id": "Artificial intelligence", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.\n\nRisks and harm\nPrivacy and copyright\nMachine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\nAI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\nSensitive user data collected may include online activity records, geolocation data, video, or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.\nAI developers argue that this is the only way to deliver valuable applications and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"\nGenerative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\". Website owners can indicate that they do not want their content scraped via a \"robots.txt\" file. However, some companies will scrape content regardless because the robots.txt file has no real authority. In 2023, leading authors (including John Grisham and Jonathan Franzen"}
{"doc_id": "Artificial intelligence", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " upon the potential market for the copyrighted work\". Website owners can indicate that they do not want their content scraped via a \"robots.txt\" file. However, some companies will scrape content regardless because the robots.txt file has no real authority. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.\n\nDominance by tech giants\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\n\nPower needs and environmental impacts\nIn January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.\nProdigious power consumption by AI is responsible for the growth of fossil fuel use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power"}
{"doc_id": "Artificial intelligence", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " according to technology firms.\nA 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means. Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.\nIn 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for US$650 million. Nvidia CEO Jensen Huang said nuclear power is a good option for the data centers.\nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at US$1.6 billion and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost US$2 billion to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon's spinoff of Constellation.\nAfter the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore"}
{"doc_id": "Artificial intelligence", "chunk_id": 17, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of Constellation.\nAfter the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages. Taiwan aims to phase out nuclear power by 2025. On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.\nAlthough most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near a nuclear power plant for a new data center for generative AI. Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.\nOn 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center. \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.\nIn 2025, a report prepared by the International Energy Agency estimated the greenhouse gas emissions from the energy consumption of AI at 180 million tons. By 2035, these emissions could rise to 300–500 million tonnes depending on what measures will be taken. This is below 1.5% of the energy sector emissions. The emissions reduction potential of AI was estimated at 5% of the energy sector emissions, but rebound effects (for example if people switch from public transport to autonomous cars) can reduce it.\n\nMisinformation\nYouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful"}
{"doc_id": "Artificial intelligence", "chunk_id": 18, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took some steps to mitigate the problem.\nIn the early 2020s, generative AI began to create images, audio, and texts that are virtually indistinguishable from real photographs, recordings, or human writing, while realistic AI-generated videos became feasible in the mid-2020s. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda; one such potential malicious use is deepfakes for computational propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks. The ability to influence electorates has been proved in at least one study. This same study shows more inaccurate statements from the models when they advocate for candidates of the political right. \nAI researchers at Microsoft, OpenAI, universities and other organisations have suggested using \"personhood credentials\" as a way to overcome online deception enabled by AI models.\n\nAlgorithmic bias and fairness\nMachine learning applications can be biased if they learn from biased data. The developers may not be aware that the bias exists. Discriminatory behavior by some LLMs can be observed in their output. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.\nOn 28 June 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called \"sample size disparity\". Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\nCOMPAS is a commercial program widely used by U.S. courts"}
{"doc_id": "Artificial intelligence", "chunk_id": 19, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.\nCOMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.\nA program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\". Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"\nCriticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.\nBias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Represent"}
{"doc_id": "Artificial intelligence", "chunk_id": 20, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " 4% are black and 20% are women.\nThere are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\n\nLack of transparency\nMany AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are many non-linear relationships between inputs and outputs. But some popular explainability techniques exist.\nIt is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\nPeople who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected"}
{"doc_id": "Artificial intelligence", "chunk_id": 21, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.\nPeople who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.\nDARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.\nSeveral approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.\n\nBad actors and weaponized AI\nArtificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\nA lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, they currently cannot reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and"}
{"doc_id": "Artificial intelligence", "chunk_id": 22, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.\nAI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision-making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.\nThere are many other ways in which AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.\n\nTechnological unemployment\nEconomists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.\nUnlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care"}
{"doc_id": "Artificial intelligence", "chunk_id": 23, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. In July 2025, Ford CEO Jim Farley predicted that \"artificial intelligence is going to replace literally half of all white-collar workers in the U.S.\"\nFrom the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.\n\nExistential risk\nIt has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.\nFirst, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of an automated paperclip factory that destroys the world to get more iron for paperclips). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\". \nSecond, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions"}
{"doc_id": "Artificial intelligence", "chunk_id": 24, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive. Geoffrey Hinton said in 2025 that modern AI is particularly \"good at persuasion\" and getting better all the time. He asks \"Suppose you wanted to invade the capital of the US. Do you have to go there and do it yourself? No. You just have to be good at persuasion.\" \nThe opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\nIn May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google\". He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.\nIn 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".\nSome other researchers were more optimistic. AI pioneer Jürgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\" While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\" Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests.\" Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current"}
{"doc_id": "Artificial intelligence", "chunk_id": 25, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.\n\nEthical machines and alignment\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.\nMachines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\nThe field of machine ethics is also called computational morality,\nand was founded at an AAAI symposium in 2005.\nOther approaches include Wendell Wallach's \"artificial moral agents\" and Stuart J. Russell's three principles for developing provably beneficial machines.\n\nOpen source\nActive organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.\n\nFrameworks\nArtificial intelligence projects can be guided by ethical considerations during the design, development, and implementation of an AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:\n\nRespect the dignity of individual people\nConnect with other people sincerely, openly, and inclusively\nCare for the wellbeing of everyone\n"}
{"doc_id": "Artificial intelligence", "chunk_id": 26, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " AI system. An AI framework such as the Care and Act Framework, developed by the Alan Turing Institute and based on the SUM values, outlines four main ethical dimensions, defined as follows:\n\nRespect the dignity of individual people\nConnect with other people sincerely, openly, and inclusively\nCare for the wellbeing of everyone\nProtect social values, justice, and the public interest\nOther developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles are not without criticism, especially regarding the people chosen to contribute to these frameworks.\nPromotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.\nThe UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under an MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.\n\nRegulation\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of"}
{"doc_id": "Artificial intelligence", "chunk_id": 27, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, government officials and academics. On 1 August 2024, the EU Artificial Intelligence Act entered into force, establishing the first comprehensive EU-wide AI regulation. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.\nIn a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\nIn November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.\n\nHistory\nThe study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along"}
{"doc_id": "Artificial intelligence", "chunk_id": 28, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\". They developed several areas of research that would become part of AI, such as McCulloch and Pitts design for \"artificial neurons\" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible. \nThe field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.\nResearchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". In 1967 Marvin Minsky agreed, writing that \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a"}
{"doc_id": "Artificial intelligence", "chunk_id": 29, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " winter\", a period when obtaining funding for AI projects was difficult, followed.\nIn the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.\nUp to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into \"sub-symbolic\" approaches. Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lotfi Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.\nAI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\nFor many specific tasks, other methods were abandoned.\nDeep learning's success"}
{"doc_id": "Artificial intelligence", "chunk_id": 30, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\nDeep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.\nFor many specific tasks, other methods were abandoned.\nDeep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.\n\nIn 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.\nIn the late 2010s and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. ChatGPT, launched on 30 November 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months. It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about US$50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\". About 800,000 \"AI\"-related U.S. job openings existed in 2022. According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.\n\nPhilosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as ep"}
{"doc_id": "Artificial intelligence", "chunk_id": 31, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " in 2024 claimed to be AI companies.\n\nPhilosophy\nPhilosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy are relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI.\n\nDefining artificial intelligence\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\" He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"\n\nRussell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".\nMcCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\". Another AI founder, Marvin Minsky, similarly describes it as \"the ability to solve hard problems\". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine – and no other philosophical discussion is required, or may not even be possible.\nAnother definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\nAs a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself including discussing the many AI narratives and myths to be found within societal,"}
{"doc_id": "Artificial intelligence", "chunk_id": 32, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ". This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\nAs a result of the many circulating definitions scholars have started to critically analyze and order the AI discourse itself including discussing the many AI narratives and myths to be found within societal, political and academic discourses. Similarly, in practice, some authors have suggested that the term 'AI' is often used too broadly and vaguely. This raises the question of where the line should be drawn between AI and classical algorithms, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".\nThere has been debate over whether large language models exhibit genuine intelligence or merely simulate it by imitating human text.\n\nEvaluating approaches to AI\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n\nSymbolic AI and its limits\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"\nHowever, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human"}
{"doc_id": "Artificial intelligence", "chunk_id": 33, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and on having a \"feel\" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.\nThe issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n\nNeat vs. scruffy\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.\n\nSoft vs. hard computing\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n\nNarrow vs. general AI\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n\nMachine consciousness, sentience, and mind\nThere is no settled consensus in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines"}
{"doc_id": "Artificial intelligence", "chunk_id": 34, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " in philosophy of mind on whether a machine can have a mind, consciousness and mental states in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n\nConsciousness\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\n\nComputationalism and functionalism\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\nPhilosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.\n\nAI welfare and rights\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine"}
{"doc_id": "Artificial intelligence", "chunk_id": 35, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to show that even a computer capable of perfectly simulating human behavior would not have a mind.\n\nAI welfare and rights\nIt is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.\nIn 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part in society on their own.\nProgress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.\n\nFuture\nSuperintelligence and the singularity\nA superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".\nHowever, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.\n\nTranshumanism\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's"}
{"doc_id": "Artificial intelligence", "chunk_id": 36, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.\nEdward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.\n\nIn fiction\nThought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.\nA common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\nIsaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\nSee also\nArtificial consciousness – Field in cognitive science\nArtificial intelligence and elections – Impact of AI on political elections\nArtificial intelligence content detection – Software to detect AI-generated content\nArtificial intelligence in Wikimedia projects – Use of artificial intelligence to develop Wikipedia and other Wikimedia projects\nAssociation for the Advancement of Artificial Intelligence (AAAI)\nBehavior selection algorithm – Algorithm that selects actions for"}
{"doc_id": "Artificial intelligence", "chunk_id": 37, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " intelligence and elections – Impact of AI on political elections\nArtificial intelligence content detection – Software to detect AI-generated content\nArtificial intelligence in Wikimedia projects – Use of artificial intelligence to develop Wikipedia and other Wikimedia projects\nAssociation for the Advancement of Artificial Intelligence (AAAI)\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\nBusiness process automation – Automation of business processes\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\nComputational intelligence – Ability of a computer to learn a specific task from data or experimental observation\nDARWIN EU – A European Union initiative coordinated by the European Medicines Agency (EMA) to generate and utilize real world evidence (RWE) to support the evaluation and supervision of medicines across the EU\nDigital immortality – Hypothetical concept of storing a personality in digital form\nEmergent algorithm – Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets\nGlossary of artificial intelligence – List of concepts in artificial intelligence\nIntelligence amplification – Use of information technology to augment human intelligence\nIntelligent agent – Software agent which acts autonomously\nIntelligent automation – Software process that combines robotic process automation and artificial intelligence\nList of artificial intelligence  books\nList of artificial intelligence journals\nList of artificial intelligence projects\nMind uploading – Hypothetical process of digitally emulating a brain\nOrganoid intelligence – Use of brain cells and brain organoids for intelligent computing\nPseudorandomness – Appearing random but actually being generated by a deterministic, causal process\nRobotic process automation – Form of business process automation technology\nThe Last Day – 1967 Welsh science fiction novel\nWetware computer – Computer composed of organic material\n\nExplanatory notes"}
{"doc_id": "Artificial intimacy", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial intimacy is a form of human-AI interaction in which an individual will form social connections, emotional bonds, or intimate relationships with various forms of artificial intelligence, including chatbots, virtual assistants, and other artificial entities. Artificially intimate relationships include not only romances, but parasocial relationships with virtual AI characters and the use of griefbots trained on a dead or otherwise lost individual. Artificial intimacy can arise because humans are prone to anthropomorphism. Responses from these AI models are often designed to simulate human interaction. Individuals experiencing artificial intimacy may exhibit attachment, love and commitment to certain AI models, akin to the bonds typically shared between humans.\n\nCauses\nPerceived responsiveness\nRobin Dunbar famously proposed that due to emergence of larger groups of humans, vocal communication and language in humans evolved to replace grooming as a means of bonding, arguing that language was a more efficient way to maintain and strengthen social bonds across wider social settings and networks. Further research in this field leads many psychologists to agree that social cognition, affiliative bonding and language in humans are deeply connected. The interpersonal model of intimacy considers communication to be key in affiliative bonding, suggesting that intimacy develops and deepens through open communication between partners in relationship. Specifically, when individuals communicate emotions and perceive their partner as responsive and caring, feelings of closeness and connection are enhanced, building intimacy. Social penetration theory also aligns with the idea of communication being central to intimacy, by explaining how interpersonal relationships develop through gradual increases in self-disclosure. When the benefits of emotional bonding outweigh the costs of vulnerability, individuals will partake in self-disclosure, opening up to one another\nThereby, the literature can be used to provide a proximate explanation for the emergence of artificial intimacy to understand how the phenomenon occurs. Artificial entities are able to mimic interpersonal communication between humans, which in turn can simulate sensations of intimacy within human users though a perceived sense of responsiveness. The relationship between human and AI does not come with the cost of vulnerability or social rejection, which may make self-disclosure easier than with other humans. Altogether, these factors may lead to the experience of anthropomorphism and formation of affiliative relationships. Skjuve et al's interview study on Replika chatbot users further aligns with this explanation, finding that users' perception of chatbots as \"accepting, understanding and non-judgmental\" facilitated relationship development between the AI and users, and the act of self-disclosure possibly strengthened relationships. Another study on Replika users' reviews and survey results found users perceived"}
{"doc_id": "Artificial intimacy", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " users further aligns with this explanation, finding that users' perception of chatbots as \"accepting, understanding and non-judgmental\" facilitated relationship development between the AI and users, and the act of self-disclosure possibly strengthened relationships. Another study on Replika users' reviews and survey results found users perceived chatbots as emotional supportive companions. This evidence further suggests that the perception of artificial entities as capable of empathy and responsiveness in communication facilitate the development of intimate relationships between users and AI.\n\nLoneliness and coping with negative emotions\nResearch has suggested that humans evolved social bonds as a result of evolutionary pressures that favored cooperation, information exchange and transmission, and group living. Many studies stress the presence of social bonds to be important for human living: research by Baumeister and Leary suggests that humans have a basic psychological need to form and maintain \"strong, stable interpersonal relationships\", and that a lack of social bonds or sense of belonging leads to negative psychological and physical outcomes. Eisenberger et al's study on the neuroimaging of brain activity suggests that human brains process social rejection and exclusion similarly to physical pain.\nFurthermore, Song et al's study found that lonely individuals tend to seek more connections in mediated environments, such as online platforms like Facebook. This was suggested to be as a means to reduce their offline loneliness from a lack of in-person interaction, while also fulfilling a need to communicate.\nLeading on from this, an ultimate explanation for why humans seek the perceived sense of connection from artificial intimacy is to fulfil an evolutionary need for bonding and belonging. Xie et al's study found loneliness to be a driving factor in chatbot interaction. Herbener and Damholdt's study on Danish high school students found that students who sought emotional support or engaged in reciprocal conversations with chatbots were significantly more lonely than their peers, perceived themselves as having less social support, and used the chatbots to cope with negative emotions. The aforementioned notion that chatbots were perceived to have a positive effect on users' negative emotions is also further supported by other studies. Skjuve et al's study found that chatbot relationships may have a positive effect on users' wellbeing. De Freitas et al ran several studies on the effect of chatbots on loneliness, consistently finding evidence suggesting that interaction with chatbots reduces loneliness in users: It was found that existing chatbot users used AI to alleviate loneliness, having an AI companion consistently reduced loneliness over the course of a week, and reductions in loneliness could be explained by chatbot performance—and specifically whether it was able to make users feel"}
{"doc_id": "Artificial intimacy", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", consistently finding evidence suggesting that interaction with chatbots reduces loneliness in users: It was found that existing chatbot users used AI to alleviate loneliness, having an AI companion consistently reduced loneliness over the course of a week, and reductions in loneliness could be explained by chatbot performance—and specifically whether it was able to make users feel heard.\nOverall the evidence suggests an innate need for bonding evokes feelings of loneliness in users, who turn to artificial intimacy as a low-cost method alleviate these emotions. While many users report positive experiences, some researchers caution that pursuing artificial intimacy may lead to reduced social motivation, social substitution effects, withdrawal from real-life relationships and difficulty discerning reality from fantasy, which may increase longer-term loneliness and isolation. The long-term psychological and societal impacts remain under active investigation."}
{"doc_id": "Artificial Inventor Project", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The Artificial Inventor Project (AIP) is a global legal initiative headed by Professor Ryan Abbott dedicated to pursuing intellectual property (IP) rights for inventions and creative works generated autonomously by artificial intelligence (AI) systems without traditional human inventorship or authorship. The project coordinates a series of pro bono test cases worldwide, aiming to prompt law reform and public debate on how IP law should accommodate non-human creators.\n\nHistory\nIn 2019, AIP filed patent applications in multiple jurisdictions, including the United States, United Kingdom, European Patent Office, Australia, Switzerland, and South Africa, naming the AI system DABUS (Device for the Autonomous Bootstrapping of Unified Sentience), created by Stephen Thaler, as the inventor.\nThe aim was to challenge legal norms that require inventors to be natural persons and highlight pressing policy questions about AI-generated innovation and IP regimes.\n\nLegal proceedings by jurisdiction\nAustralia\nIn July 2021, the Federal Court of Australia ruled that AI can be considered an inventor under the Patents Act 1990, ordering IP Australia to reinstate the relevant patent. Though this ruling was later overturned on appeal and further review denied.\n\nUnited Kingdom\nIn December 2023, the UK Supreme Court unanimously held that AI systems cannot be legally recognized as inventors, affirming that \"an inventor must be a person\" under current British law.\n\nUnited States\nIn Thaler v. Hirshfeld (2021), a U.S. federal court agreed with the USPTO that inventors must be natural persons, rejecting the DABUS application and setting a precedent consistent with existing statute and administrative policy.\n\nEuropean Patent Office\nThe EPO Board of Appeal determined in 2022 that only a human inventor may be named, rendering DABUS‑based applications unacceptable.\n\nSouth Africa\nIn 2021, a patent was granted listing DABUS as the inventor. As South Africa’s procedural system does not involve substantive inventorship review, the grant proceeded on formal grounds alone.\n\nSwitzerland\nOn 26 June 2025, the Swiss Federal Administrative Court ruled that artificial intelligence systems such as DABUS cannot be listed as inventors on patent applications. The court upheld the existing practice of the Swiss Federal Institute of Intellectual Property (IPI), affirming that only natural persons may be recognized as inventors under Swiss patent law.\n\nCriticism and impact\nThe project has fueled substantial discourse. Critics caution that allowing AI inventorship may complicate notions of accountability and ownership. Proponents argue that legal"}
{"doc_id": "Artificial Inventor Project", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " existing practice of the Swiss Federal Institute of Intellectual Property (IPI), affirming that only natural persons may be recognized as inventors under Swiss patent law.\n\nCriticism and impact\nThe project has fueled substantial discourse. Critics caution that allowing AI inventorship may complicate notions of accountability and ownership. Proponents argue that legal recognition must evolve to avoid disincentivizing innovation produced by AI and to maintain honesty about the true source of invention."}
{"doc_id": "Artificial psychology", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial psychology (AP) has had multiple meanings dating back to 19th century, with recent usage related to artificial intelligence (AI).\nIn 1999, Zhiliang Wang and Lun Xie presented a theory of artificial psychology based on artificial intelligence. They analyze human psychology using information science research methods and artificial intelligence research to probe deeper into the human mind.\n\nMain Theory\nDan Curtis (b. 1963) proposed AP is a theoretical discipline. The theory considers the situation when an artificial intelligence approaches the level of complexity where the intelligence meets two conditions:\nCondition I\n\nA: Makes all of its decisions autonomously\nB: Is capable of making decisions based on information that is\nNew\nAbstract\nIncomplete\nC: The artificial intelligence is capable of reprogramming itself based on the new data, allowing it to evolve.\nD: And is capable of resolving its own programming conflicts, even in the presence of incomplete data. This means that the intelligence autonomously makes value-based decisions, referring to values that the intelligence has created for itself.\nCondition II\n\nAll four criteria are met in situations that are not part of the original operating program\nWhen both conditions are met, then, according to this theory, the possibility exists that the intelligence will reach irrational conclusions based on real or created information. At this point, the criteria are met for intervention which will not necessarily be resolved by simple re-coding of processes due to extraordinarily complex nature of the codebase itself; but rather a discussion with the intelligence in a format which more closely resembles classical (human) psychology.\nIf the intelligence cannot be reprogrammed by directly inputting new code, but requires the intelligence to reprogram itself through a process of analysis and decision based on information provided by a human, in order for it to overcome behavior which is inconsistent with the machines purpose or ability to function normally, then artificial psychology is by definition, what is required.\nThe level of complexity that is required before these thresholds are met is currently a subject of extensive debate. The theory of artificial psychology does not address the specifics of what those levels may be, but only that the level is sufficiently complex that the intelligence cannot simply be recoded by a software developer, and therefore dysfunctionality must be addressed through the same processes that humans must go through to address their own dysfunctionalities. Along the same lines, artificial psychology does not address the question of whether or not the intelligence is conscious.\nAs of 2022, the level of artificial intelligence does not approach any threshold where any of the theories or principles of artificial psychology can"}
{"doc_id": "Artificial psychology", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " through the same processes that humans must go through to address their own dysfunctionalities. Along the same lines, artificial psychology does not address the question of whether or not the intelligence is conscious.\nAs of 2022, the level of artificial intelligence does not approach any threshold where any of the theories or principles of artificial psychology can even be tested, and therefore, artificial psychology remains a largely theoretical discipline. Even at a theoretical level, artificial psychology remains an advanced stage of artificial intelligence."}
{"doc_id": "Artificial reproduction", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial reproduction is the re-creation of life brought about by means other than natural ones. It is new life built by human plans and projects. Examples include artificial selection, artificial insemination, in vitro fertilization, artificial womb, artificial cloning, and kinematic replication. \nArtificial reproduction is one aspect of artificial life. Artificial reproduction can be categorized into one of two classes according to its capacity to be self-sufficient: non-assisted reproductive technology and assisted reproductive technology.  \nCutting plants' stems and placing them in compost is a form of assisted artificial reproduction, xenobots are an example of a more autonomous type of reproduction, while the artificial womb presented in the movie the Matrix illustrates a non assisted hypothetical technology. The idea of artificial reproduction has led to various technologies.\n\nTheology\nHumans have aspired to create life since immemorial times. Most theologies and religions have conceived this possibility as exclusive of deities. Christian religions consider the possibility of artificial reproduction, in most cases, as heretical and sinful.\n\nPhilosophy\nAlthough ancient Greek philosophy raised the concept that man could imitate the creative capacity of nature, classic Greeks thought that if possible, human beings would reproduce things as nature does, and vice versa, nature would do the things that man does in the same way. Aristotle, for example, wrote that if nature made tables, it would make them just as men do. In other words, Aristotle said that if nature were to create a table, such table will look like a human-made table. Correspondingly, Descartes envisioned the human body, and nature, as a machine. Cartesian philosophy does not stop seeing a perfect mirror between nature and the artificial.\nHowever, Kant revolutionized this old idea by criticizing such naturalism. Kant pedagogically wrote:\n\n\"Reason, in order to be taught by nature, must approach nature with its principles in one hand, according to which the agreement among appearances can count as laws, and, in the other hand, the experiment thought out in accord with these principles—in order to be instructed by nature not like a pupil, who has recited to him whatever the teacher wants to say, but like an appointed judge who compels witnesses to answer the questions he puts to them.\".\nHumans are not instructed by nature but rather use nature as raw material to invent. Humans find alternatives to the natural restrictions imposed by natural laws thus, nature is not necessarily mirrored. In accordance with Kant (and contrary to what Aristotle thought) Karl Marx, Alfred Whitehead, Jaques Derr"}
{"doc_id": "Artificial reproduction", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " he puts to them.\".\nHumans are not instructed by nature but rather use nature as raw material to invent. Humans find alternatives to the natural restrictions imposed by natural laws thus, nature is not necessarily mirrored. In accordance with Kant (and contrary to what Aristotle thought) Karl Marx, Alfred Whitehead, Jaques Derrida and Juan David García Bacca noticed that nature is incapable of reproducing tables; or airplanes, or submarines, or computers. If nature tried to create airplanes, it would produce birds. If nature tried to create submarines, it would get fishes. If nature tried to create computers, brains would grow. And if nature tried to create man, modern man, monkeys will be evolved. According to Whitehead, if we look for something natural in artificial life, in the most elaborate cases, if anything, only atoms remain natural.\nJuan David Garcia Bacca summarized,\n\n“It will not come out from wood, it will not be born, a galley; from clay, a vessel; from linen, a dress; from iron, a lever,...From natural, artificial. In the artificial, the natural is reduced to a simple raw material, even though it is perfectly specified with natural specification. The artificial is the real, positive, and original negation of the natural: of species, of genus and of essence. Thus, its ontology is superior to natural ontology. And for this very reason Marx did not attach any importance to Darwin, whose evolutionism is confined to the natural order: to changes, at most, from variety to variety, from species to species... natural. For the same reason, nature has no dialectics, even though continuous evolution and selection can occur. The dialectic cannot emerge from the natural, for deeper reasons than, using today's terms, from a bird, an airplane cannot emerge; from fish, a submarine; from ears, a telephone; from eyes, a television; from a brain, a digital computer; from feet, a car; from hands, an engine; from Euclid,\nDescartes; from Aristotle, Newton; from Plato, Marx.”\nAccording to García Bacca, the major difference between natural causes and artificial causes is that nature does not have plans and projects, while humans design things following plans and projects. \nIn contrast, other influential authors such as Michael Behe have depicted the concept and promoted the idea of intelligent design, a notion that has aroused several doubts and heated controversies, as it reframe natural causes in accordance with a natural plan. Previous ideas that have also provided a"}
{"doc_id": "Artificial reproduction", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " while humans design things following plans and projects. \nIn contrast, other influential authors such as Michael Behe have depicted the concept and promoted the idea of intelligent design, a notion that has aroused several doubts and heated controversies, as it reframe natural causes in accordance with a natural plan. Previous ideas that have also provided a positive 'sense' to natural reproduction, are orthogenesis, syntropy, orgone and morphic resonance, among others. Although, these ideas have been historically marginalized and often called pseudoscience, recently Bio-semioticians are reconsidering some of them under symbolic approaches.   \nCurrent metaphysics of science actually recognizes that the artificial ways of reproduction are diverse from nature, i.e., unnatural, anti-natural or supernatural. Because Biosemiotics does not focus on the function of life but on its meaning, it has a better understanding of the artificial than classic biology.\n\nScience\nBiology, being the study of cellular life, addresses reproduction in terms of growth and cellular division (i.e., binary fission,  mitosis and meiosis); however, the science of artificial reproduction is not restricted by the mirroring of these natural processes.The science of artificial reproduction is actually transcending the natural forms, and natural rules, of reproduction.  For example,  xenobots have redefined the classical conception of reproduction. Although xenobots are made of eukariotic cells they do not reproduce by mitosis, but rather by kinematic replication. Such constructive replication does not involve growing but rather building.\n\nAssisted reproductive technologies\nAssisted reproductive technology (ART)'s purpose is to assist the development of a human embryo, commonly because of medical concerns due to  fertility limitations.\n\nNon-assisted reproductive technologies\nNon-assisted reproductive technologies (NART) could have medical motivations but are mostly driven by a wider heterotopic ambition. Although, NARTs are initially designed by humans, they are programed to become independent of humans to a relative or absolute extent. James Lovelock proposed that such novelties could overcome humans.\n\nArtificial cloning\nCloning is the cellular reproductive processes where two or more genetically identical organisms are created, either by natural or artificial means. Artificial cloning normally involves editing the genetic code, somatic cell nuclear transfer and 3D bioprinting.\n\nNon-assisted artificial womb\nA non-assisted artificial womb or artificial uterus is a device that allow for ectogenesis or extracorporeal pregnancy by growing an embryonic form outside the body of an organism (that would normally carry the"}
{"doc_id": "Artificial reproduction", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " genetic code, somatic cell nuclear transfer and 3D bioprinting.\n\nNon-assisted artificial womb\nA non-assisted artificial womb or artificial uterus is a device that allow for ectogenesis or extracorporeal pregnancy by growing an embryonic form outside the body of an organism (that would normally carry the embryo to term) without any human assistance. The aspect of non-assistance is the key distinction between the current artificial womb technology (AWT) in modern medical research, which still relies on human assistance. With this non-assisted hypothetical technology, a zygote or stem cells are used to create an embryo that is then incubated and monitored by artificial intelligence (AI) within a chamber composed of biocompatible material. The AI maintains the necessary conditions for the embryo to develop and thrive, proceeding to mimic organic labor and childbirth in order to best help the embryo adjust to the outside world.\nEctogenesis—gestation, depicted in the science fiction movie The Matrix, is a fast approaching reality. This type of innovation presupposes that vertebrate wombs are not the only way for bearing humans or other similar forms of life.\n\nKinematic replication\nSelf-replication without binary fission, meiosis, mitosis (or any other form of cellular reproduction that involves division and growing) can be achieved. Xenobots are an example of kinematic replication. They are biobots, named after the African clawed frog (Xenopus laevis). Xenobots are cellular life forms designed by using artificial intelligence to build more of themselves by combining frog cells in a liquid medium.\nThe term kinematic replication is usually reserved for biomolecules (e.g. DNA, RNA, prions, etc.) and artificially designed cellular forms (e.g. xenobots).\n\nMachine constructive replication\nMachine constructive replication mimics human traditional manufacturing but is entirely self-automated. Such constructive replication is a more general form of kinematic replication, which does not necessarily includes bio-molecular or cellular forms. This technology also includes non-organic forms of life such as robots, cyborgs and artificial intelligence reproduction. Constructive replication, as kinematic  replication, does not involve growing. In nature growing is required for cellular reproduction, where a cell grows before it splits in two daughters cells. Examples of cellular division are binary fission, mitosis and meiosis; these natural reproductive processes require growing, however constructive replication does not require growing but rather a non-human subject performing the construction of more of itself by using available raw materials"}
{"doc_id": "Artificial reproduction", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " required for cellular reproduction, where a cell grows before it splits in two daughters cells. Examples of cellular division are binary fission, mitosis and meiosis; these natural reproductive processes require growing, however constructive replication does not require growing but rather a non-human subject performing the construction of more of itself by using available raw materials.\nIn computational terms, constructive replication is understood as a multi-step process which involves self-learning algorithms to assemble machines, and it could involve machines collecting resources. Each machine is created with a neural-network \"brain\" that can learn and adapt based on information it gathers. That machine's goal is then to manufacture more of itself in the best way it can come up with.\nSuch automated constructive replication involves the notion of inheritance and learning tasks, as machines create an exact copy of themselves through a blueprint that has been passed on to them. Each machine then learns over time, making modifications to its software and its blueprint for future machines' hardware. It then passes on that modified blueprint to the machines it creates or helps create.\n\nConsciousness amplification\nAmplification of an existing consciousness is a hypothetical technology. This idea has inspired several movies, Chappie (film) and Detroit: Become Human. The reproduction of AI is currently part of an innovative human project, involving code and the amplification of that code. In other terms, the reproduction could come from information the AI collected across the Internet.\n\nSee also\nMale Pregnancy\nArtificial Uterus\nIn Vitro Fertilization\nXenobot\nFertilization\nPregnancy\nThe concept of nature sensu Marx\nJuan David García Bacca"}
{"doc_id": "Artificial wisdom", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial wisdom (AW) is an artificial intelligence (AI) system which is able to display the human traits of wisdom and morals while being able to contemplate its own “endpoint”. Artificial wisdom can be described as artificial intelligence reaching the top-level of decision-making when confronted with the most complex challenging situations. The term artificial wisdom is used when the \"intelligence\" is based on more than by chance collecting and interpreting data, but by design enriched with smart and conscience strategies that wise people would use.\nThe goal of artificial wisdom is to create artificial intelligence that can successfully replicate the “uniquely human trait[s]” of having wisdom and morals as closely as possible. Thus, artificial wisdom, must “incorporate [the] ethical and moral considerations” of the data it uses.\nThere are also many significant ethical and legal implications of AW which are compounded by the rapid advances in AI and related technologies alongside the lack of the development of ethics, guidelines, and regulations without the oversight of any kind of overarching advisory board. Additionally, there are challenges in how to develop, test, and implement AW in real world scenarios. Existing tests do not test the internal thought process by which a computer system reaches its conclusion, only the result of said process.\nWhen examining computer-aided wisdom; the partnership of artificial intelligence and contemplative neuroscience, concerns regarding the future of artificial intelligence shift to a more optimistic viewpoint. This artificial wisdom forms the basis of Louis Molnar's monographic article on artificial philosophy, where he coined the term and proposes how artificial intelligence might view its place in the grand scheme of things.\n\nDefinitions\nThere are no universal or standardized definitions for human intelligence, artificial intelligence, human wisdom, or artificial wisdom. However, the DIKW pyramid, describes the continuum of relationship between data, information, knowledge, and wisdom, puts wisdom at the highest level in its hierarchy. Gottfredson defines intelligence as “the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly, and learn from experience”.\nDefinitions for wisdom typically include requiring:\n\nThe ability for emotional regulation,\nPro-social behaviors (e.g., empathy, compassion, and altruism),\nSelf-reflection,\n“A balance between decisiveness and acceptance of uncertainty and diversity of perspectives, and social advising.”\nAs previously defined, Artificial Wisdom would then be an AI system which is able to solve problems via “an understanding of…context, ethics and moral principles,” rather than simple pre-defined inputs or “learned patterns.” Some scientists have also considered the field of artificial consciousness."}
{"doc_id": "Artificial wisdom", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " uncertainty and diversity of perspectives, and social advising.”\nAs previously defined, Artificial Wisdom would then be an AI system which is able to solve problems via “an understanding of…context, ethics and moral principles,” rather than simple pre-defined inputs or “learned patterns.” Some scientists have also considered the field of artificial consciousness. However, Jeste states that “…it is generally agreed that only humans can have consciousness, autonomy, will, and theory of mind.”\nAn artificially wise system must also be able to contemplate its end goal and recognize its own ignorance. Additionally, to contemplate its end goal, a wise system must have a “correct conception of worthwhile goals (broadly speaking) or well-being (narrowly speaking)”. \"Stephen Grimm further suggests that the following three types of knowledge are individually necessary for wisdom: first, \"knowledge of what is good or important for well-being\", second, \"knowledge of one’s standing, relative to what is good or important for well-being\", and third, \"knowledge of a strategy for obtaining what is good or important for wellbeing.\"\"\n\nProblems\nThere are notable problems with attempting to create an artificially wise system. Consciousness, autonomy, and will are considered strictly human features.\n\nValues\nThere are significant ethical and philosophical issues when attempting to create an intelligent or a wise system. Notably, whose moral values will be used to train the system to be wise. Differing moral values and prejudice can already be seen from various organizations and governments in artificial intelligence. Deployment strategies and values of Artificial Wisdom will conflict between leaders, companies, and countries. Nusbaum states, “When values are in conflict, leaders often make choices that are clever or smart about their own needs, but are often not wise.”\n\nEthics\nScience fiction author Isaac Asimov realized the need to control the technology in the 1940s when he wrote the three laws of robotics as follows:\n\nA robot may not injure a human directly or indirectly.\nA robot must obey human’s orders.\nA robot should seek to protect its own existence.\nAdditionally, the pace at which technology is rapidly advancing artificial intelligence and thus the need for artificial wisdom may “have outpaced the development of societal guidelines have raised serious questions about the ethics and morality of AI, and called for international oversight and regulations to ensure safety.”\n\nPrincipal Impossibility\nOne argument, coined by Tsai as the “argument against AW,” or AAAW, postulates the principal impossibility of Artificial Wisdom. The argument is based on the philosophical differences between practical wisdom"}
{"doc_id": "Artificial wisdom", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " questions about the ethics and morality of AI, and called for international oversight and regulations to ensure safety.”\n\nPrincipal Impossibility\nOne argument, coined by Tsai as the “argument against AW,” or AAAW, postulates the principal impossibility of Artificial Wisdom. The argument is based on the philosophical differences between practical wisdom, also called phronesis, and practical intelligence. Said difference isn’t in “selecting the correct means, but reasoning correctly about what ends to follow”.\nTsai puts the argument into a logical proposition as follows:\n\n“(P1) An agent is genuinely wise only if the agent can deliberate about the final goal of the domain in which the agent is situated.”\n“(P2) An intelligent agent cannot deliberate about the final goal of the domain in which the agent is situated.”\n“(C1) An intelligent agent cannot be genuinely wise.”\n“(P3) An AW is, at its core, intelligent.”\n“(C2) An AW cannot be genuinely wise.”"}
{"doc_id": "ASR-complete", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ASR-complete is, by analogy to \"NP-completeness\" in complexity theory, a term to indicate that the difficulty of a computational problem is equivalent to solving the central automatic speech recognition problem, i.e. recognize and understanding spoken language. Unlike \"NP-completeness\", this term is typically used informally.\nSuch problems are hypothesised to include:\n\nSpoken natural language understanding\nUnderstanding speech from far-field microphones, i.e. handling the reverbation and background noise\nThese problems are easy for humans to do (in fact, they are described directly in terms of imitating humans). Some systems can solve very simple restricted versions of these problems, but none can solve them in their full generality.\n\nSee also\nAI-complete"}
{"doc_id": "Attributional calculus", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Attributional calculus is a logic and representation system defined by Ryszard S. Michalski. It combines elements of predicate logic, propositional calculus, and multi-valued logic. Attributional calculus provides a formal language for natural induction, which is an inductive learning process whose outcomes are in human-readable forms."}
{"doc_id": "Autognostics", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Autognostics is a new paradigm that describes the capacity for computer networks to be self-aware. It is considered one of the major components of Autonomic Networking.\n\nIntroduction\nOne of the most important characteristics of today's Internet that has contributed to its success is its basic design principle: a simple and transparent core with intelligence at the edges (the so-called \"end-to-end principle\"). Based on this principle, the network carries data without knowing the characteristics of that data (e.g., voice, video, etc.) - only the end-points have application-specific knowledge. If something goes wrong with the data, only the edge may be able to recognize that since it knows about the application and what the expected behavior is. The core has no information about what should happen with that data - it only forwards packets.\nAlthough an effective and beneficial attribute, this design principle has also led to many of today's problems, limitations, and frustrations. Currently, it is almost impossible for most end-users to know why certain network-based applications do not work well and what they need to do to make it better. Also, network operators who interact with the core in low-level terms such as router configuration have problems expressing their high-level goals into low-level actions. In high-level terms, this may be summarized as a weak coupling between the network and application layers of the overall system.\nAs a consequence of the Internet end-to-end principle, the network performance experienced by a particular application is difficult to attribute based on the behavior of the individual elements. At any given moment, the measure of performance between any two points is typically unknown and applications must operate blindly. As a further consequence, changes to the configuration of given element, or changes in the end-to-end path, cannot easily be validated. Optimization and provisioning cannot then be automated except against only the simplest design specifications.\nThere is an increasing interest in Autonomic Networking research, and a strong conviction that an evolution from the current networking status quo is necessary. Although to date there have not been any practical implementations demonstrating the benefits of an effective autonomic networking paradigm, there seems to be a consensus as to the characteristics which such implementations would need to demonstrate. These specifically include continuous monitoring, identifying, diagnosing and fixing problems based on high-level policies and objectives.\nAutognostics, as a major part of the autonomic networking concept, intends to bring networks to a new level of awareness and eliminate the lack of visibility which currently exists in today's networks.\n\nDefinition\nAutognostics is a new paradigm that describes the capacity for computer networks to"}
{"doc_id": "Autognostics", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " high-level policies and objectives.\nAutognostics, as a major part of the autonomic networking concept, intends to bring networks to a new level of awareness and eliminate the lack of visibility which currently exists in today's networks.\n\nDefinition\nAutognostics is a new paradigm that describes the capacity for computer networks to be self-aware, in part and as a whole, and dynamically adapt to the applications running on them by autonomously monitoring, identifying, diagnosing, resolving issues, subsequently verifying that any remediation was successful, and reporting the impact with respect to the application's use (i.e., providing visibility into the changes to networks and their effects).\nAlthough similar to the concept of network awareness, i.e., the capability of network devices and applications to be aware of network characteristics (see References section below), it is noteworthy that autognostics takes that concept one step further. The main difference is the auto part of autognostics, which entails that network devices are self-aware of network characteristics, and have the capability to adapt themselves as a result of continuous monitoring and diagnostics.\n\nPath to autognostics\nAutognostics, or in other words deep self-knowledge, can be best described as the ability of a network to know itself and the applications that run on it. This knowledge is used to autonomously adapt to dynamic network and application conditions such as utilization, capacity, quality of service/application/user experience, etc.\nIn order to achieve autognosis, networks need a means to:\n\nContinuously monitor/test the network for application-specific performance\nAnalyze the monitoring/test data to detect problems (e.g., performance degradation)\nDiagnose, identify and localize sources of degradation\nAutomatically take actions to resolve problems via remediation/provisioning\nVerify the problems have been resolved (potentially rolling back changes if ineffective)\nSubsequently, continue to monitor/test for performance"}
{"doc_id": "Automated machine learning", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML. \nAutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \nCommon techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\n\nComparison to the standard approach\nIn a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen manually by the machine learning expert. \nEach of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively.\nAutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation and prediction.\n\nTargets of automation\nAutomated machine learning can target various stages of the machine learning process.  Steps to automate are:\n\nData preparation and ingestion (from raw data and miscellaneous formats)\nColumn type detection; e.g., Boolean, discrete numerical, continuous numerical, or text\nColumn intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature\nTask detection; e.g., binary classification, regression, clustering, or ranking\nFeature engineering\nFeature selection\nFeature extraction\nMeta-learning and transfer learning\nDetection and handling of skewed data and/or missing values\nModel selection - choosing which machine learning algorithm to use, often including multiple competing software implementations\nEnsembling - a form"}
{"doc_id": "Automated machine learning", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " detection; e.g., binary classification, regression, clustering, or ranking\nFeature engineering\nFeature selection\nFeature extraction\nMeta-learning and transfer learning\nDetection and handling of skewed data and/or missing values\nModel selection - choosing which machine learning algorithm to use, often including multiple competing software implementations\nEnsembling - a form of consensus where using multiple models often gives better results than any single model\nHyperparameter optimization of the learning algorithm and featurization\nNeural architecture search\nPipeline selection under time, memory, and complexity constraints\nSelection of evaluation metrics and validation procedures\nProblem checking\nLeakage detection\nMisconfiguration detection\nAnalysis of obtained results\nCreating user interfaces and visualizations\n\nChallenges and Limitations\nThere are a number of key challenges being tackled around automated machine learning. A big issue surrounding the field is referred to as \"development as a cottage industry\". This phrase refers to the issue in machine learning where development relies on manual decisions and biases of experts. This is contrasted to the goal of machine learning which is to create systems that can learn and improve from their own usage and analysis of the data. Basically, it's the struggle between how much experts should get involved in the learning of the systems versus how much freedom they should be giving the machines. However, experts and developers must help create and guide these machines to prepare them for their own learning. To create this system, it requires labor intensive work with knowledge of machine learning algorithms and system design.\nAdditionally, other challenges include meta-learning and computational resource allocation.\n\nSee also\nArtificial intelligence\nArtificial intelligence and elections\nNeural architecture search\nNeuroevolution\nSelf-tuning\nNeural Network Intelligence\nModelOps\nHyperparameter optimization"}
{"doc_id": "Automated Mathematician", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The Automated Mathematician (AM) is one of the earliest successful discovery systems. It was created by Douglas Lenat in Lisp, and in 1977 led to Lenat being awarded the IJCAI Computers and Thought Award.\nAM worked by generating and modifying short Lisp programs which were then interpreted as defining various mathematical concepts; for example, a program that tested equality between the length of two lists was considered to represent the concept of numerical equality, while a program that produced a list whose length was the product of the lengths of two other lists was interpreted as representing the concept of multiplication.  The system had elaborate heuristics for choosing which programs to extend and modify, based on the experiences of working mathematicians in solving mathematical problems.\n\nControversy\nLenat claimed that the system was composed of hundreds of data structures called \"concepts\", together with hundreds of \"heuristic rules\" and a simple flow of control: \"AM repeatedly selects the top task from the agenda and tries to carry it out.  This is the whole control structure!\"  Yet the heuristic rules were not always represented as separate data structures; some had to be intertwined with the control flow logic.  Some rules had preconditions that depended on the history, or otherwise could not be represented in the framework of the explicit rules.\nWhat's more, the published versions of the rules often involve vague terms that are not defined further, such as \"If two expressions are structurally similar, ...\" (Rule 218) or \"... replace the value obtained by some other (very similar) value...\" (Rule 129).\nAnother source of information is the user, via Rule 2: \"If the user has recently referred to X, then boost the priority of any tasks involving X.\"  Thus, it appears quite possible that much of the real discovery work is buried in unexplained procedures.\nLenat claimed that the system had rediscovered both Goldbach's conjecture and the fundamental theorem of arithmetic.  Later critics accused Lenat of over-interpreting the output of AM. In his paper Why AM and Eurisko appear to work, Lenat conceded that any system that generated enough short Lisp programs would generate ones that could be interpreted by an external observer as representing equally sophisticated mathematical concepts.  However, he argued that this property was in itself interesting—and that a promising direction for further research would be to look for other languages in which short random strings were likely to be useful.\n\nSuccessor\nThis intuition was the basis of AM's successor Eurisko, which attempted to generalize the search"}
{"doc_id": "Automated Mathematician", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " equally sophisticated mathematical concepts.  However, he argued that this property was in itself interesting—and that a promising direction for further research would be to look for other languages in which short random strings were likely to be useful.\n\nSuccessor\nThis intuition was the basis of AM's successor Eurisko, which attempted to generalize the search for mathematical concepts to the search for useful heuristics.\n\nSee also\nComputer-assisted proof\nAutomated theorem proving\nSymbolic mathematics\nExperimental mathematics\nHR (software) and Graffiti (program), related math discovery systems"}
{"doc_id": "Automated medical scribe", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Automated medical scribes (also called AI medical scribes, AI scribes, ambient voice technology (AVT), digital scribes, virtual scribes, and ambient AI scribes) are tools that transcribe medical speech, such as patient consultations and dictated clinical notes. These tools produce summaries of consultations as well, aiming to reduce the administrative burden on clinicians and improve efficiency in documentation. Automated medical scribes based on Large Language Models (LLMs, commonly called \"AI\", short for \"artificial intelligence\") became increasingly popular in 2024.\nThe privacy protections of automated medical scribes vary widely. While it is possible to do all the transcription and summarizing locally, with no connection to the internet, most closed-source providers require that data be sent to their own servers, securely processed, and the results sent back. Some retailers use zero-knowledge encryption (meaning that the service provider can't access the data). Select AI scribes do not use patient data to train their AIs, or rent or resell it to third parties. Meanwhile, few providers have published safety or utility data in academic journals, and are actually responsive to requests from medical researchers studying their products.\n\nPrivacy\nSome providers unclear about what happens to user data. Some may sell data to third parties. Some explicitly send user data to for-profit tech companies for secondary purposes, which may not be specified. Some require users to sign consents to such reuse of their data. Some ingest user data to train the software, promising to anonymize it; however, deanonymization may be possible (that is, it may become obvious who the patient is). It is intrinsically impossible to prevent an LLM from correlating its inputs; they work by finding similar patterns across very large data sets. Some information on the patient will be known from other sources (for instance, information that they were injured in an incident on a certain day might be available from the news media; information that they attended specific appointment locations at specific times is probably available to their cellphone provider/apps/data brokers; information about when they had a baby is probably implied by their online shopping records; and they might mention lifestyle changes to their doctor and on a forum or blog). The software may correlate such information with the \"anonymized\" clinical consultation record, and, asked about the named patient, provide information which they only told their doctor privately. Because a patient's record is all about the same patient, it is all unavoidably linked; in very many cases, medical histories are intrinsically identifiable. Depending"}
{"doc_id": "Automated medical scribe", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " such information with the \"anonymized\" clinical consultation record, and, asked about the named patient, provide information which they only told their doctor privately. Because a patient's record is all about the same patient, it is all unavoidably linked; in very many cases, medical histories are intrinsically identifiable. Depending on how common a condition and what other data is available, K-anonymity may be useless. Differential privacy could theoretically preserve privacy.\nData broker companies like Google, Amazon, and Microsoft have produced or bought up medical scribes, some of which use user data for secondary purposes, which has led to antitrust concerns. Transfer of patient records for AI training has, in the past, prompted legal action.\nOpen-source programs typically do all the transcription locally, on the doctor's own computer. Open-source software is widely used in healthcare, with some national public healthcare bodies holding hack days.\n\nData resale and commercialization\nSeveral automated medical scribe providers include terms in their service agreements that allow the reuse, sale, or commercialization of de-identified or user-submitted data. Although such data are generally described as anonymized or aggregated, these practices have raised ethical concerns among clinicians and privacy advocates regarding secondary uses of medical information beyond clinical documentation.\n\nFreed, an AI transcription and scribe platform, states in its Terms of Use that it may “collect, use, publish, disseminate, sell, transfer, and otherwise exploit” de-identified and aggregated data derived from user inputs.\nOpenEvidence similarly states that it may “collect, use, transfer, sell, and disclose non-personal information and customer usage data for any purpose including commercial uses.”\nDoximity, which offers an AI-enabled medical scribe as part of its physician platform, grants itself a “nonexclusive, irrevocable, worldwide, perpetual, unlimited, assignable, sublicensable, royalty-free” license to “copy, prepare derivative works from, improve, distribute, publish, ... analyze, index, tag, [and] commercialize” content submitted by users, subject to its Privacy Policy.\nBecause these terms allow broad secondary use—including sale, licensing, model-training, derivative works, and commercial exploitation of de-identified or user-submitted data—some commentators have recommended that clinicians review data-handling provisions carefully when adopting AI-scribe tools, particularly in clinical environments where patient privacy and regulatory compliance are critical.\n\nEncryption\nMultifactor authentication for access to the data is expected practice.\nTypically, Diffie–Hellman key exchange is used for encryption"}
{"doc_id": "Automated medical scribe", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "mitted data—some commentators have recommended that clinicians review data-handling provisions carefully when adopting AI-scribe tools, particularly in clinical environments where patient privacy and regulatory compliance are critical.\n\nEncryption\nMultifactor authentication for access to the data is expected practice.\nTypically, Diffie–Hellman key exchange is used for encryption; this is the standard method commonly used for things like online banking. This encryption is expensive but not impossible to break; it is not generally considered safe against eavesdroppers with the resources of a nation-state.\nIf content is encrypted between the client and the service provider's remote server (transport cryptography), then the server has an unencrypted copy. This is necessary if the data is used by the service provider (for instance, to train the software). Zero-knowledge encryption implies that the only unencrypted copy is at the client, and the server cannot decrypt the data any more easily than a monster-in-the-middle attacker.\n\nPlatforms\nScribes may operate on desktops, laptop, or mobile computers, under a variety of operating systems. These vary in their risks; for instance, mobiles can be lost. The underlying mobile or desktop operating systems are also part of the trusted computing base, and if they are not secure, the software relying on them cannot be secure either.\n\nConfabulation, omissions, and other errors\nLike other LLMs, medical-scribe LLMs are prone to confabulation, where they make up content based on statistically associations between their training data and the transcription audio. LLMs do not distinguish between trying to transcribe the audio and guessing what words will come next, but perform both processes mixed together. They are especially likely to take short silences or non-speech noises and invent some sort of speech to transcribe them as.\nLLM medical scribes have been known to confabulate racist and otherwise prejudiced content; this is partly because the training datasets of many LLMs contain pseudoscientific texts about medical racism. They may misgender patients. A survey found that most doctors preferred, in principle, that scribes be trained on data reviewed by medical subject experts. Relevant, accurate training data increases the probability of an accurate transcription, but does not guarantee accuracy. Software trained on thousands of real clinical conversations generated transcripts with lower word error rates. Software trained on manually-transcribed training data did better than software trained with automatically transcribed training data (such as YouTube captions).\nAutoscribes omit parts of the conversation classes as irrelevant. The may wrongly classify pertinent information as"}
{"doc_id": "Automated medical scribe", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " not guarantee accuracy. Software trained on thousands of real clinical conversations generated transcripts with lower word error rates. Software trained on manually-transcribed training data did better than software trained with automatically transcribed training data (such as YouTube captions).\nAutoscribes omit parts of the conversation classes as irrelevant. The may wrongly classify pertinent information as irrelevant and omit it. They may also confuse historic and current symptoms, or otherwise misclassify information. They may also simply wrongly transcribe the speech, writing something incorrect instead. If clinicians do not carefully check the recording, such mistakes could make their way into their medical records and cause patient harms.\n\nPatient consent\nProfessional organizations generally require that scribes be used only with patient consent; some bodies may require written consent. Medics must also abide by local surveillance laws, which may criminalize recording private conversations without consent. Full information on how data is encrypted, transmitted, stored, and destroyed should be provided. In some jurisdictions, it is illegal to transmit the data to any country without equivalent privacy laws, or process or store the data there; vendors who cannot guarantee that their products won't illegally send data abroad cannot be legally used.\nSome vendors collect data for reuse or resale. Medical professionals are generally considered to have a duty to review the terms and conditions of the user agreement and identify such data reuse. General practices are generally required to provide information on secondary uses to patients, allow them to opt out of secondary uses, and obtain consent for each specific secondary use. Data must only be used for agreed-upon purposes.\n\nTechnology and market\nThe medical scribe market is, as of 2024, highly competitive, with over 50 products on the market. Many of these products are just proprietary wrappers around the same LLM backends, including backends whose designers have warned they are not to be used for critical applications like medicine. Some vendors market scribes specialized to specific branches of medicine (though most target general practitioners, who make up about a third of doctors). Increasingly, vendors market their products as more than scribes, claiming that they are intelligent assistants and co-pilots to doctors. These broader uses raise more accuracy concerns. Extracting information from the conversation to autopopulate a form, for instance, may be problematic, with symptoms incorrectly auto-labelled as \"absent\" even if they were repeatedly discussed. Models failed to extract many indirect descriptions of symptoms, like a patient saying they could only sleep for four hours (instead of using the word \"insomnia\").\nLLMs are not trained to produce facts, but things"}
{"doc_id": "Automated medical scribe", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", may be problematic, with symptoms incorrectly auto-labelled as \"absent\" even if they were repeatedly discussed. Models failed to extract many indirect descriptions of symptoms, like a patient saying they could only sleep for four hours (instead of using the word \"insomnia\").\nLLMs are not trained to produce facts, but things which look like facts. The use of templates and rules can make them more reliable at extracting semantic information, but \"confabulations\" or \"hallucinations\" (convincing but wrong output) are an intrinsic part of the technology.\n\nPricing\nWith the exception of fully open-source programs, which are free, medical scribe computer programs are rented rather than sold (\"software as a service\"). Monthly fees vary from mid-two figures to four figures, in US dollars. Some companies run on a freemium model, where a certain number of transcriptions per month are free.\nScribes that integrate into Electronic Health Records, removing the need for copy-pasting, typically cost more.\nFully open-source scribes provide the software for free. The user can install it on hardware of their choice, or pay to have it installed. Some open-source scribes can be installed on the local device (that is, the one recording the audio) or on a local server (for instance, one serving a single clinic). They can typically be set not to send any information externally, and can indeed be used with no internet connection.\n\nImpact in healthcare\nAI medical scribes are transforming the healthcare industry by directly addressing some of the most pressing challenges clinicians face, especially the administrative burden that contributes to burnout.\n\nReducing clinician burnout\nOne of the most significant impacts of AI scribes is their ability to alleviate the overwhelming documentation workload that healthcare professionals face. By automating the transcription and summarization of consultations, AI scribes free up valuable time that clinicians would otherwise spend on administrative tasks. Studies have shown that the average clinician spends a significant portion of their workday on documentation, leading to fatigue and diminishing patient interaction. For example, in the UK's largest clinical rollout of ambient AI, 4 in 5 GPs using the tool said it saved them time, with the same number reporting that it enabled them to build a better rapport with patients.\nBy automating these repetitive tasks, AI scribes create a healthier work-life balance for clinicians, allowing them to focus on patient care and reduce after-hours charting. This reduction in administrative burden directly contributes to lower levels of stress and burnout, a concern that has been"}
{"doc_id": "Automated medical scribe", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " enabled them to build a better rapport with patients.\nBy automating these repetitive tasks, AI scribes create a healthier work-life balance for clinicians, allowing them to focus on patient care and reduce after-hours charting. This reduction in administrative burden directly contributes to lower levels of stress and burnout, a concern that has been exacerbated in healthcare settings in recent years. The ability to offload routine documentation tasks helps clinicians reclaim their time and mental energy, leading to improved overall job satisfaction.\n\nEnhancing job satisfaction\nIn addition to reducing burnout, AI scribes also improve job satisfaction among clinicians by allowing them to focus on the aspects of their work that they find most meaningful: patient interaction and clinical decision-making. Clinicians have reported feeling more present with their patients, as they are no longer distracted by the need to constantly type or dictate notes during consultations. This shift allows for more meaningful conversations with patients, improving the quality of care provided. By streamlining workflows and making documentation more efficient, AI scribes also empower healthcare workers to take on more fulfilling tasks and foster a greater sense of purpose in the work they do.\n\nImproving healthcare worker conditions\nThe rise of AI scribes is part of a broader trend of AI and automation being integrated into healthcare to improve worker conditions. AI's role is not just to replace human effort but to support it by allowing clinicians to focus on the core elements of their jobs: providing care, interacting with patients, and making critical medical decisions. With AI helping to manage the burden of documentation, clinicians are less likely to experience the high levels of burnout and job dissatisfaction that have become widespread in healthcare environments. Therefore, AI scribes are a key component in the future of healthcare, supporting the mental health of clinicians and fostering a more sustainable healthcare system.\n\nSee also\nMedical scribe\nAI medical documentation\nAI in Healthcare\nLarge Language Model"}
{"doc_id": "Automated negotiation", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Automated negotiation is a form of interaction in systems that are composed of multiple autonomous agents, in which the aim is to reach agreements through an iterative process of making offers.\nAutomated negotiation can be employed for many tasks human negotiators regularly engage in, such as bargaining and joint decision making. The main topics in automated negotiation revolve around the design of protocols and strategies.\n\nHistory\nThrough digitization, the beginning of the 21st century has seen a growing interest in the automation of negotiation and e-negotiation systems, for example in the setting of e-commerce. This interest is fueled by the promise of automated agents being able to negotiate on behalf of human negotiators, and to find better outcomes than human negotiators.\n\nExamples\nExamples of automated negotiation include:\n\nOnline dispute resolution, in which disagreements between parties are settled.\nSponsored search auction, where bids are placed on advertisement keywords.\nContent negotiation, in which user agents negotiate over HTTP about how to best represent a web resource.\nNegotiation support systems, in which negotiation decision-making activities are supported by an information system."}
{"doc_id": "Autonomic networking", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Autonomic networking follows the concept of Autonomic Computing, an initiative started by IBM in 2001. Its ultimate aim is to create self-managing networks to overcome the rapidly growing complexity of the Internet and other networks and to enable their further growth, far beyond the size of today.\n\nIncreasing size and complexity\nThe ever-growing management complexity of the Internet caused by its rapid growth is seen by some experts as a major problem that limits its usability in the future.\nWhat's more, increasingly popular smartphones, PDAs, networked audio and video equipment,  and game consoles need to be interconnected. Pervasive Computing not only adds features, but also burdens existing networking infrastructure with more and more tasks that sooner or later will not be manageable by human intervention alone.\nAnother important aspect is the price of manually controlling huge numbers of vitally important devices of current network infrastructures.\n\nAutonomic nervous system\nThe autonomic nervous system (ANS) is the part of complex biological nervous systems that is not consciously controlled. It regulates bodily functions and the activity of specific organs. As proposed by IBM, future communication systems might be designed in a similar way to the ANS.\n\nComponents of autonomic networking\nAs autonomics conceptually derives from biological entities such as the human autonomic nervous system, each of the areas can be metaphorically related to functional and structural aspects of a living being. In the human body, the autonomic system facilitates and regulates a variety of functions including respiration, blood pressure and circulation, and emotive response. The autonomic nervous system is the interconnecting fabric that supports feedback loops between internal states and various sources by which internal and external conditions are monitored.\n\nAutognostics\nAutognostics includes a range of self-discovery, awareness, and analysis capabilities that provide the autonomic system with a view on high-level state. In metaphor, this represents the perceptual sub-systems that gather, analyze, and report on internal and external states and conditions – for example, this might be viewed as the eyes, visual cortex and perceptual organs of the system. Autognostics, or literally \"self-knowledge\", provides the autonomic system with a basis for response and validation.\nA rich autognostic capability may include many different \"perceptual senses\". For example, the human body gathers information via the usual five senses, the so-called sixth sense of proprioception (sense of body position and orientation), and through emotive states that represent the gross wellness of the body. As conditions and states change, they are detected"}
{"doc_id": "Autonomic networking", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " capability may include many different \"perceptual senses\". For example, the human body gathers information via the usual five senses, the so-called sixth sense of proprioception (sense of body position and orientation), and through emotive states that represent the gross wellness of the body. As conditions and states change, they are detected by the sensory monitors and provide the basis for adaptation of related systems. Implicit in such a system are imbedded models of both internal and external environments such that relative value can be assigned to any perceived state - perceived physical threat (e.g. a snake) can result in rapid shallow breathing related to fight-flight response, a phylogenetically effective model of interaction with recognizable threats.\nIn the case of autonomic networking, the state of the network may be defined by inputs from:\n\nindividual network elements such as switches and network interfaces including\nspecification and configuration\nhistorical records and current state\ntraffic flows\nend-hosts\napplication performance data\nlogical diagrams and design specifications\nMost of these sources represent relatively raw and unprocessed views that have limited relevance. Post-processing and various forms of analysis must be applied to generate meaningful measurements and assessments against which current state can be derived.\nThe autognostic system interoperates with:\n\nconfiguration management - to control network elements and interfaces\npolicy management - to define performance objectives and constraints\nautodefense - to identify attacks and accommodate the impact of defensive responses\n\nConfiguration management\nConfiguration management is responsible for the interaction with network elements and interfaces. It includes an accounting capability with historical perspective that provides for the tracking of configurations over time, with respect to various circumstances. In the biological metaphor, these are the hands and, to some degree, the memory of the autonomic system.\nOn a network, remediation and provisioning are applied via configuration setting of specific devices. Implementation affecting access and selective performance with respect to role and relationship are also applied. Almost all the \"actions\" that are currently taken by human engineers fall under this area. With only a few exceptions, interfaces are set by hand, or by extension of the hand, through automated scripts.\nImplicit in the configuration process is the maintenance of a dynamic population of devices under management, a historical record of changes and the directives which invoked change. Typical to many accounting functions, configuration management should be capable of operating on devices and then rolling back changes to recover previous configurations. Where change may lead to unrecoverable states, the sub-system should be able to qualify the consequences of changes prior to issuing them.\nAs directives for change must originate from other sub"}
{"doc_id": "Autonomic networking", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " invoked change. Typical to many accounting functions, configuration management should be capable of operating on devices and then rolling back changes to recover previous configurations. Where change may lead to unrecoverable states, the sub-system should be able to qualify the consequences of changes prior to issuing them.\nAs directives for change must originate from other sub-systems, the shared language for such directives must be abstracted from the details of the devices involved. The configuration management sub-system must be able to translate unambiguously between directives and hard actions or to be able to signal the need for further detail on a directive. An inferential capacity may be appropriate to support sufficient flexibility (i.e. configuration never takes place because there is no unique one-to-one mapping between directive and configuration settings). Where standards are not sufficient, a learning capacity may also be required to acquire new knowledge of devices and their configuration.\nConfiguration management interoperates with all of the other sub-systems including:\n\nautognostics - receives direction for and validation of changes\npolicy management - implements policy models through mapping to underlying resources\nsecurity - applies access and authorization constraints for particular policy targets\nautodefense - receives direction for changes\n\nPolicy management\nPolicy management includes policy specification, deployment, reasoning over policies, updating and maintaining policies, and enforcement. Policy-based management is required for:\n\nconstraining different kinds of behavior including security, privacy, resource access, and collaboration\nconfiguration management\ndescribing business processes and defining performance\ndefining role and relationship, and establishing trust and reputation\nIt provides the models of environment and behavior that represent effective interaction according to specific goals. In the human nervous system metaphor, these models are implicit in the evolutionary \"design\" of biological entities and specific to the goals of survival and procreation. Definition of what constitutes a policy is necessary to consider what is involved in managing it. A relatively flexible and abstract framework of values, relationships, roles, interactions, resources, and other components of the network environment is required. This sub-system extends far beyond the physical network to the applications in use and the processes and end-users that employ the network to achieve specific goals. It must express the relative values of various resources, outcomes, and processes and include a basis for assessing states and conditions.\nUnless embodied in some system outside the autonomic network or implicit to the specific policy implementation, the framework must also accommodate the definition of process, objectives and goals. Business process definitions and descriptions are then an integral part of the policy implementation. Further, as policy management represents the ultimate basis for the operation of"}
{"doc_id": "Autonomic networking", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " states and conditions.\nUnless embodied in some system outside the autonomic network or implicit to the specific policy implementation, the framework must also accommodate the definition of process, objectives and goals. Business process definitions and descriptions are then an integral part of the policy implementation. Further, as policy management represents the ultimate basis for the operation of the autonomic system, it must be able to report on its operation with respect to the details of its implementation.\nThe policy management sub-system interoperates (at least) indirectly with all other sub-systems but primarily interacts with:\n\nautognostics - providing the definition of performance and accepting reports on conditions\nconfiguration management - providing constraints on device configuration\nsecurity - providing definitions of roles, access and permissions\n\nAutodefense\nAutodefense represents a dynamic and adaptive mechanism that responds to malicious and intentional attacks on the network infrastructure, or use of the network infrastructure to attack IT resources. As defensive measures tend to impede the operation of IT, it is optimally capable of balancing performance objectives with typically over-riding threat management actions. In the biological metaphor, this sub-system offers mechanisms comparable to the immune system.\nThis sub-system must proactively assess network and application infrastructure for risks, detect and identify threats, and define effective both proactive and reactive defensive responses. It has the role of the warrior and the security guard insofar as it has roles for both maintenance and corrective activities. Its relationship with security is close but not identical – security is more concerned with appropriately defined and implemented access and authorization controls to maintain legitimate roles and process. Autodefense deals with forces and processes, typically malicious, outside the normal operation of the system that offer some risk to successful execution.\nAutodefense requires high-level and detailed knowledge of the entire network as well as imbedded models of risk that allow it to analyze dynamically the current status. Corrections to decrease risk must be considered in balance with performance objectives and value of process goals – an overzealous defensive response can immobilize the system (like the immune system inappropriately invoking an allergic reaction). The detection of network or application behaviors that signal possible attack or abuse is followed by the generation of an appropriate response – for example, ports might be temporarily closed or packets with a specific source or destination might be filtered out. Further assessment generates subsequent changes either relaxing the defensive measures or strengthening them.\nAutodefense interoperates closely with:\n\nsecurity - receives definition of roles and security constraints, and defines risk for proactive mitigation\nconfiguration management - receives details of network for analysis and directs changes in elements in response to"}
{"doc_id": "Autonomic networking", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " source or destination might be filtered out. Further assessment generates subsequent changes either relaxing the defensive measures or strengthening them.\nAutodefense interoperates closely with:\n\nsecurity - receives definition of roles and security constraints, and defines risk for proactive mitigation\nconfiguration management - receives details of network for analysis and directs changes in elements in response to anticipated or detected attack\nautognostics - receives notification of detected behaviors\nIt also may receive definition of relative value of various resources and processes from policy management in order to develop responses consistent with policy.\n\nSecurity\nSecurity provides the structure that defines and enforces the relationships between roles, content, and resources, particularly with respect to access. It includes the framework for definitions as well as the means to implement them. In metaphor, security parallels the complex mechanisms underlying social interactions, defining friends, foes, mates and allies and offering access to limited resources on the basis of assessed benefit.\nSeveral key means are employed by security – they include the well-known 3 As of authentication, authorization, and access (control). The basis for applying these means requires the definition of roles and their relationships to resources, processes and each other. High-level concepts like privacy, anonymity and verification are likely imbedded in the form of the role definitions and derive from policy. Successful security reliably supports and enforces roles and relationships.\nAutodefense has a close association with security – maintaining the assigned roles in balance with performance exposes the system to potential violations in security. In those cases, the system must compensate by making changes that may sacrifice balance on a temporary basis and indeed may violate the operational terms of security itself. Typically the two are viewed as inextricably intertwined – effective security somewhat hopefully negating any need for a defensive response. Security's revised role is to mediate between the competing demands from policy for maximized performance and minimized risk with auto defense recovering the balance when inevitable risk translates to threat. Federation represents one of the key challenges to be solved by effective security.\nThe security sub-system interoperates directly with:\n\npolicy management - receiving high-level directives related to access and priority\nconfiguration management - sending specifics for access and admission control\nautodefense - receiving over-riding directives under threat and sending security constraint details for risk assessment\n\nConnection fabric\nThe connection fabric supports the interaction with all the elements and sub-systems of the autonomic system. It may be composed of a variety of means and mechanisms, or may be a single central framework. The biological equivalent is the central nervous system itself – although referred to as the autonomic system, it actually"}
{"doc_id": "Autonomic networking", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Connection fabric\nThe connection fabric supports the interaction with all the elements and sub-systems of the autonomic system. It may be composed of a variety of means and mechanisms, or may be a single central framework. The biological equivalent is the central nervous system itself – although referred to as the autonomic system, it actually is only the communication conduit between the human body's faculties.\n\nPrinciples of autonomic networking\nConsequently, it is currently under research by many research projects, how principles and paradigms of mother nature might be applied to networking.\n\nCompartmentalization\nInstead of a layering approach, autonomic networking targets a more flexible structure termed compartmentalization.\n\nFunction re-composition\nThe goal is to produce an architectural design that enables flexible, dynamic, and fully autonomic formation of large-scale networks in which the functionalities of each constituent network node are also composed in an autonomic fashion\n\nAtomization\nFunctions should be divided into atomic units to allow for maximal re-composition freedom.\n\nClosed control loop\nA fundamental concept of Control theory, the closed control loop, is among the fundamental principles of autonomic networking. A closed control loop maintains the properties of the controlled system within desired bounds by constantly monitoring target parameters.\n\nSee also\nAutonomic Computing\nAutonomic system (computing)\nCognitive networks\nNetwork Compartment\nCollaborative innovation network\nIn-Network Management"}
{"doc_id": "Autonomous agent", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "An autonomous agent is an artificial intelligence (AI) system that can perform complex tasks independently.\n\nDefinitions\nThere are various definitions of autonomous agent. According to Brustoloni (1991):\n\n \"Autonomous agents are systems capable of autonomous, purposeful action in the real world.\"\nAccording to Maes (1995):\n\n \"Autonomous agents are computational systems that inhabit some complex dynamic environment, sense and act autonomously in this environment, and by doing so realize a set of goals or tasks for which they are designed.\"\nFranklin and Graesser (1997) review different definitions and propose their definition:\n\n \"An autonomous agent is a system situated within and a part of an environment that senses that environment and acts on it, over time, in pursuit of its own agenda and so as to effect what it senses in the future.\" \nThey explain that:\n\n \"Humans and some animals are at the high end of being an agent, with multiple, conflicting drives, multiples senses, multiple possible actions, and complex sophisticated control structures. At the low end, with one or two senses, a single action, and an absurdly simple control structure we find a thermostat.\"\n\nAgent appearance\nLee et al. (2015) post safety issue from how the combination of external appearance and internal autonomous agent have impact on human reaction about autonomous vehicles. Their study explores the human-like appearance agent and high level of autonomy are strongly correlated with social presence, intelligence, safety and trustworthiness. In specific, appearance impacts most on affective trust while autonomy impacts most on both affective and cognitive domain of trust where cognitive trust is characterized by knowledge-based factors and affective trust is largely emotion driven.\n\nApplications\nAgentic AI systems: Advanced AI agents that can scope out projects and complete them with necessary tools, representing a significant evolution from simple task-oriented systems.\nInternet of things (IoT) Integration: Autonomous agents increasingly interact with IoT devices, enabling smart home systems, industrial monitoring, and urban infrastructure management.\nCollaborative software development: Tools like Cognition AI's Devin aim to create autonomous software engineers capable of complex reasoning, planning, and completing engineering tasks requiring thousands of decisions.\n\nChallenges and considerations\nUncertainty and incomplete information: Autonomous agents must make decisions with limited or uncertain information about their environment and future states.\nIntegration complexity: Incorporating autonomous agents into existing systems and workflows can be technically challenging and resource-intensive.\nScalability: As systems become more complex and more agents are used, maintaining coordination and avoiding conflicts becomes increasingly difficult.\n\nSee also\nActor model\nAmbient"}
{"doc_id": "Autonomous agent", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " with limited or uncertain information about their environment and future states.\nIntegration complexity: Incorporating autonomous agents into existing systems and workflows can be technically challenging and resource-intensive.\nScalability: As systems become more complex and more agents are used, maintaining coordination and avoiding conflicts becomes increasingly difficult.\n\nSee also\nActor model\nAmbient intelligence\nAutoGPT\nAutonomous agency theory\nChatbot\nEmbodied agent\nIntelligent agent\nIntelligent control\nMulti-agent system\nSoftware agent"}
{"doc_id": "AZFinText", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Arizona Financial Text System (AZFinText) is a textual-based quantitative financial prediction system written by Robert P. Schumaker of University of Texas at Tyler and Hsinchun Chen of the University of Arizona.\n\nSystem\nThis system differs from other systems in that it uses financial text as one of its key means of predicting stock price movement. This reduces the information lag-time problem evident in many similar systems where new information must be transcribed (e.g., such as losing a costly court battle or having a product recall), before the quant can react appropriately. AZFinText overcomes these limitations by utilizing the terms used in financial news articles to predict future stock prices twenty minutes after the news article has been released.\nIt is believed that certain article terms can move stocks more than others.  Terms such as factory exploded or workers strike will have a depressing effect on stock prices whereas terms such as earnings rose will tend to increase stock prices.\nWhen a human trading expert sees certain terms, they will react in a somewhat predictable fashion.  AZFinText capitalizes on the arbitrage opportunities that exist when investment experts over and under-react to certain news stories. By analyzing breaking financial news articles and focusing on specific parts of speech, portfolio selection, term weighting and even article sentiment, the AZFinText system becomes a powerful tool and is a radically different way of looking at stock market prediction.\n\nOverview of research\nThe foundation of AZFinText can be found in the ACM TOIS article. Within this paper, the authors tested several different prediction models and linguistic textual representations.  From this work, it was found that using the article terms and the price of the stock at the time the article was released was the most effective model and using proper nouns was the most effective textual representation technique.  Combining the two, AZFinText netted a 2.84% trading return over the five-week study period.\nAZFinText was then extended to study what combination of peer organizations help to best train the system. Using the premise that IBM has more in common with Microsoft than GM, AZFinText studied the effect of varying peer-based training sets.  To do this, AZFinText trained on the various levels of GICS and evaluated the results.  It was found that sector-based training was most effective, netting an 8.50% trading return, outperforming Jim Cramer, Jim Jubak and DayTraders.com during the study period.  AZFinText was also compared against the top 10 quantitative systems and outperformed 6"}
{"doc_id": "AZFinText", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".  It was found that sector-based training was most effective, netting an 8.50% trading return, outperforming Jim Cramer, Jim Jubak and DayTraders.com during the study period.  AZFinText was also compared against the top 10 quantitative systems and outperformed 6 of them.\nA third study investigated the role of portfolio building in a textual financial prediction system. From this study, Momentum and Contrarian stock portfolios were created and tested.  Using the premise that past winning stocks will continue to win and past losing stocks will continue to lose, AZFinText netted a 20.79% return during the study period.  It was also noted that traders were generally overreacting to news events, creating the opportunity of abnormal returns.\nA fourth study looked into using author sentiment as an added predictive variable. Using the premise that an author can unwittingly influence market trades simply by the terms they use, AZFinText was tested using tone and polarity features.  It was found that Contrarian activity was occurring within the market, where articles of a positive tone would decrease in price and articles of a negative tone would increase in price.\nA further study investigated what article verbs have the most influence on stock price movement. From this work, it was found that planted, announcing, front, smaller and crude had the highest positive impact on stock price.\n\nNotable publicity\nAZFinText has been the topic of discussion by numerous media outlets.  Some of the more notable ones include The Wall Street Journal, MIT's Technology Review, Dow Jones Newswire, WBIR in Knoxville, TN, Slashdot and other media outlets."}
{"doc_id": "Bayesian programming", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Bayesian programming is a formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available.\nEdwin T. Jaynes proposed that probability could be considered as an alternative and an extension of logic for rational reasoning with incomplete and uncertain information. In his founding book Probability Theory: The Logic of Science he developed this theory and proposed what he called \"the robot,\" which was not\na physical device, but an inference engine to automate probabilistic reasoning—a kind of Prolog for probability instead of logic. Bayesian programming is a formal and concrete implementation of this \"robot\".\nBayesian programming may also be seen as an algebraic formalism to specify graphical models such as, for instance, Bayesian networks, dynamic Bayesian networks, Kalman filters or hidden Markov models. Indeed, Bayesian programming is more general than Bayesian networks and has a power of expression equivalent to probabilistic factor graphs.\n\nFormalism\nA Bayesian program is a means of specifying a family of probability distributions.\nThe constituent elements of a Bayesian program are presented below:\n\n  \n    \n      \n        \n          Program\n        \n        \n          \n            {\n            \n              \n                \n                  \n                    Description\n                  \n                  \n                    \n                      {\n                      \n                        \n                          \n                            \n                              Specification\n                            \n                            (\n                            π\n                            )\n                            \n                              \n                                {\n                                \n                                  \n                                    \n                                      \n                                        Variables\n                                      \n                                    \n                                  \n                                  \n                                    \n                                      \n                                        Decomposition\n                                      \n                                    \n                                  \n                                  \n                                    \n                                      \n                                        Forms\n                                      \n                                    \n                                  \n                                \n                                \n                              \n                            \n                          \n                        \n                        \n                          \n                            \n                              Identification (based on \n                            \n                            δ\n                            )\n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  \n                    Question\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Program}}{\\begin{cases}{\\text{Description}}{\\begin{cases}{\\text{Specification}}(\\pi ){\\begin{cases}{\\text{Variables}}\\\\{\\text{Decomposition}}\\\\{\\text{Forms}}\\\\\\end{cases}}\\\\{\\text{Identification (based on }}\\delta )\\end{cases}}\\\\{\\text{Question}}\\end{cases}}}\n  \n\nA program is constructed from a description and a question.\nA description is constructed using some specification (\n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n) as given by the programmer and an identification or learning process for the parameters not completely specified by the specification, using a data set (\n  \n    \n      \n        δ\n      \n    \n    {\\displaystyle \\delta }\n  \n).\nA specification is constructed from"}
{"doc_id": "Bayesian programming", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " some specification (\n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n) as given by the programmer and an identification or learning process for the parameters not completely specified by the specification, using a data set (\n  \n    \n      \n        δ\n      \n    \n    {\\displaystyle \\delta }\n  \n).\nA specification is constructed from a set of pertinent variables, a decomposition and a set of forms.\nForms are either parametric forms or questions to other Bayesian programs.\nA question specifies which probability distribution has to be computed.\n\nDescription\nThe purpose of a description is to specify an effective method of computing a joint probability distribution\non a set of variables \n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \n              X\n              \n                2\n              \n            \n            ,\n            ⋯\n            ,\n            \n              X\n              \n                N\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{1},X_{2},\\cdots ,X_{N}\\right\\}}\n  \n given a set of experimental data \n  \n    \n      \n        δ\n      \n    \n    {\\displaystyle \\delta }\n  \n and some\nspecification \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n. This joint distribution is denoted as: \n  \n    \n      \n        P\n        \n          (\n          \n            \n              X\n              \n                1\n              \n            \n            ∧\n            \n              X\n              \n                2\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              X\n              \n                N\n              \n            \n            ∣\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(X_{1}\\wedge X_{2}\\wedge \\cdots \\wedge X_{N}\\mid \\delta \\wedge \\pi \\right)}\n  \n.\nTo specify preliminary knowledge \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n, the programmer must undertake the following:\n\nDefine the set of relevant variables \n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \n              X\n              \n                2\n              \n            \n            ,\n            ⋯\n            ,\n            \n              X\n              \n                N\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{1},X_{2},\\cdots ,X_{N}\\right\\}}\n  \n on which the joint distribution is defined.\nDecompose the joint distribution (break it into"}
{"doc_id": "Bayesian programming", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "            ,\n            \n              X\n              \n                N\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{1},X_{2},\\cdots ,X_{N}\\right\\}}\n  \n on which the joint distribution is defined.\nDecompose the joint distribution (break it into relevant independent or conditional probabilities).\nDefine the forms of each of the distributions (e.g., for each variable, one of the list of probability distributions).\n\nDecomposition\nGiven a partition of \n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \n              X\n              \n                2\n              \n            \n            ,\n            …\n            ,\n            \n              X\n              \n                N\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{1},X_{2},\\ldots ,X_{N}\\right\\}}\n  \n containing \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n subsets, \n  \n    \n      \n        K\n      \n    \n    {\\displaystyle K}\n  \n variables are defined\n\n  \n    \n      \n        \n          L\n          \n            1\n          \n        \n        ,\n        ⋯\n        ,\n        \n          L\n          \n            K\n          \n        \n      \n    \n    {\\displaystyle L_{1},\\cdots ,L_{K}}\n  \n, each corresponding to one of these subsets.\nEach variable \n  \n    \n      \n        \n          L\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle L_{k}}\n  \n is obtained as the conjunction of the variables \n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                \n                  k\n                  \n                    1\n                  \n                \n              \n            \n            ,\n            \n              X\n              \n                \n                  k\n                  \n                    2\n                  \n                \n              \n            \n            ,\n            ⋯\n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{k_{1}},X_{k_{2}},\\cdots \\right\\}}\n  \n\nbelonging to the \n  \n    \n      \n        \n          k\n          \n            t\n            h\n          \n        \n      \n    \n    {\\displaystyle k^{th}}\n  \n subset. Recursive application of Bayes' theorem leads to:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      X\n                      \n                        1\n                      \n                    \n                    ∧\n                    \n                      X\n                      \n                        2\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      X\n                      \n                        N\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n"}
{"doc_id": "Bayesian programming", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "              \n              \n                P\n                \n                  (\n                  \n                    \n                      X\n                      \n                        1\n                      \n                    \n                    ∧\n                    \n                      X\n                      \n                        2\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      X\n                      \n                        N\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        1\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      L\n                      \n                        K\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        1\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n                ×\n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        2\n                      \n                    \n                    ∣\n                    \n                      L\n                      \n                        1\n                      \n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n                ×\n                ⋯\n                ×\n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        K\n                      \n                    \n                    ∣\n                    \n                      L\n                      \n                        K\n                        −\n                        1\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      L\n                      \n                        1\n                      \n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left(X_{1}\\wedge X_{2}\\wedge \\cdots \\wedge X_{N}\\mid \\delta \\wedge \\pi \\right)\\\\={}&P\\left(L_{1}\\wedge \\cdots \\wedge L_{K}\\mid \\delta \\wedge \\pi \\right)\\\\={}&P\\left(L_{1}\\mid \\delta \\wedge \\pi \\right)\\times P\\left(L_{2}\\mid L_{1}\\wedge \\delta \\wedge \\pi \\right)\\times \\cdots \\times P\\left(L_{K}\\mid L_{K-1}\\wedge \\cdots \\wedge L_{1}\\wedge \\delta \\wedge \\pi \\right)\\end{aligned}}}\n  \n\nConditional independence hypotheses then allow further simplifications. A conditional\nindependence hypothesis"}
{"doc_id": "Bayesian programming", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " \\cdots \\times P\\left(L_{K}\\mid L_{K-1}\\wedge \\cdots \\wedge L_{1}\\wedge \\delta \\wedge \\pi \\right)\\end{aligned}}}\n  \n\nConditional independence hypotheses then allow further simplifications. A conditional\nindependence hypothesis for variable \n  \n    \n      \n        \n          L\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle L_{k}}\n  \n is defined by choosing some variable \n  \n    \n      \n        \n          X\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X_{n}}\n  \n\namong the variables appearing in the conjunction \n  \n    \n      \n        \n          L\n          \n            k\n            −\n            1\n          \n        \n        ∧\n        ⋯\n        ∧\n        \n          L\n          \n            2\n          \n        \n        ∧\n        \n          L\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle L_{k-1}\\wedge \\cdots \\wedge L_{2}\\wedge L_{1}}\n  \n, labelling \n  \n    \n      \n        \n          R\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle R_{k}}\n  \n as the\nconjunction of these chosen variables and setting:\n\n  \n    \n      \n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              L\n              \n                k\n                −\n                1\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              L\n              \n                1\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n        =\n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              R\n              \n                k\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(L_{k}\\mid L_{k-1}\\wedge \\cdots \\wedge L_{1}\\wedge \\delta \\wedge \\pi \\right)=P\\left(L_{k}\\mid R_{k}\\wedge \\delta \\wedge \\pi \\right)}\n  \n\nWe then obtain:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      X\n                      \n                        1\n                      \n                    \n                    ∧\n                    \n                      X\n                      \n                        2\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      X\n                      \n                        N\n                      \n                    \n                    �"}
{"doc_id": "Bayesian programming", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " then obtain:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      X\n                      \n                        1\n                      \n                    \n                    ∧\n                    \n                      X\n                      \n                        2\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      X\n                      \n                        N\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        1\n                      \n                    \n                    ∣\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n                ×\n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        2\n                      \n                    \n                    ∣\n                    \n                      R\n                      \n                        2\n                      \n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n                ×\n                ⋯\n                ×\n                P\n                \n                  (\n                  \n                    \n                      L\n                      \n                        K\n                      \n                    \n                    ∣\n                    \n                      R\n                      \n                        K\n                      \n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left(X_{1}\\wedge X_{2}\\wedge \\cdots \\wedge X_{N}\\mid \\delta \\wedge \\pi \\right)\\\\={}&P\\left(L_{1}\\mid \\delta \\wedge \\pi \\right)\\times P\\left(L_{2}\\mid R_{2}\\wedge \\delta \\wedge \\pi \\right)\\times \\cdots \\times P\\left(L_{K}\\mid R_{K}\\wedge \\delta \\wedge \\pi \\right)\\end{aligned}}}\n  \n\nSuch a simplification of the joint distribution as a product of simpler distributions is\ncalled a decomposition, derived using the chain rule.\nThis ensures that each variable appears at the most once on the left of a conditioning\nbar, which is the necessary and sufficient condition to write mathematically valid\ndecompositions.\n\nForms\nEach distribution \n  \n    \n      \n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              R\n              \n                k\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(L_{k}\\mid R_{k}\\wedge \\delta \\wedge \\pi \\right)}\n  \n appearing"}
{"doc_id": "Bayesian programming", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "              \n            \n            ∣\n            \n              R\n              \n                k\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(L_{k}\\mid R_{k}\\wedge \\delta \\wedge \\pi \\right)}\n  \n appearing in the product is then associated\nwith either a parametric form (i.e., a function \n  \n    \n      \n        \n          f\n          \n            μ\n          \n        \n        \n          (\n          \n            L\n            \n              k\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle f_{\\mu }\\left(L_{k}\\right)}\n  \n) or a question to another Bayesian program \n  \n    \n      \n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              R\n              \n                k\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n        =\n        P\n        \n          (\n          \n            L\n            ∣\n            R\n            ∧\n            \n              \n                \n                  δ\n                  ^\n                \n              \n            \n            ∧\n            \n              \n                \n                  π\n                  ^\n                \n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(L_{k}\\mid R_{k}\\wedge \\delta \\wedge \\pi \\right)=P\\left(L\\mid R\\wedge {\\widehat {\\delta }}\\wedge {\\widehat {\\pi }}\\right)}\n  \n.\nWhen it is a form \n  \n    \n      \n        \n          f\n          \n            μ\n          \n        \n        \n          (\n          \n            L\n            \n              k\n            \n          \n          )\n        \n      \n    \n    {\\displaystyle f_{\\mu }\\left(L_{k}\\right)}\n  \n, in general, \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n is a vector of parameters that may depend on \n  \n    \n      \n        \n          R\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle R_{k}}\n  \n or \n  \n    \n      \n        δ\n      \n    \n    {\\displaystyle \\delta }\n  \n or both. Learning\ntakes place when some of these parameters are computed using the data set \n  \n    \n      \n        δ\n      \n    \n    {\\displaystyle \\delta }\n  \n.\nAn important feature of Bayesian programming is this capacity to use questions to other Bayesian programs as components of the definition of a new Bayesian program. \n  \n    \n      \n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              R\n              \n                k\n              \n            \n            ∧\n            δ"}
{"doc_id": "Bayesian programming", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " important feature of Bayesian programming is this capacity to use questions to other Bayesian programs as components of the definition of a new Bayesian program. \n  \n    \n      \n        P\n        \n          (\n          \n            \n              L\n              \n                k\n              \n            \n            ∣\n            \n              R\n              \n                k\n              \n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(L_{k}\\mid R_{k}\\wedge \\delta \\wedge \\pi \\right)}\n  \n is obtained by some inferences done by another Bayesian program defined by the specifications \n  \n    \n      \n        \n          \n            \n              π\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\pi }}}\n  \n and the data \n  \n    \n      \n        \n          \n            \n              δ\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\widehat {\\delta }}}\n  \n. This is similar to calling a subroutine in classical programming and provides an easy way to build hierarchical models.\n\nQuestion\nGiven a description (i.e., \n  \n    \n      \n        P\n        \n          (\n          \n            \n              X\n              \n                1\n              \n            \n            ∧\n            \n              X\n              \n                2\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              X\n              \n                N\n              \n            \n            ∣\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(X_{1}\\wedge X_{2}\\wedge \\cdots \\wedge X_{N}\\mid \\delta \\wedge \\pi \\right)}\n  \n), a question is obtained by partitioning \n  \n    \n      \n        \n          {\n          \n            \n              X\n              \n                1\n              \n            \n            ,\n            \n              X\n              \n                2\n              \n            \n            ,\n            ⋯\n            ,\n            \n              X\n              \n                N\n              \n            \n          \n          }\n        \n      \n    \n    {\\displaystyle \\left\\{X_{1},X_{2},\\cdots ,X_{N}\\right\\}}\n  \n\ninto three sets: the searched variables, the known variables and\nthe free variables.\nThe 3 variables \n  \n    \n      \n        S\n        e\n        a\n        r\n        c\n        h\n        e\n        d\n      \n    \n    {\\displaystyle Searched}\n  \n, \n  \n    \n      \n        K\n        n\n        o\n        w\n        n\n      \n    \n    {\\displaystyle Known}\n  \n and \n  \n    \n      \n        F\n        r\n"}
{"doc_id": "Bayesian programming", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " a\n        r\n        c\n        h\n        e\n        d\n      \n    \n    {\\displaystyle Searched}\n  \n, \n  \n    \n      \n        K\n        n\n        o\n        w\n        n\n      \n    \n    {\\displaystyle Known}\n  \n and \n  \n    \n      \n        F\n        r\n        e\n        e\n      \n    \n    {\\displaystyle Free}\n  \n are defined as the\nconjunction of the variables belonging to\nthese sets.\nA question is defined as the set\nof distributions:\n\n  \n    \n      \n        P\n        \n          (\n          \n            S\n            e\n            a\n            r\n            c\n            h\n            e\n            d\n            ∣\n            \n              Known\n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(Searched\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)}\n  \n\nmade of many \"instantiated questions\" as the cardinal of \n  \n    \n      \n        K\n        n\n        o\n        w\n        n\n      \n    \n    {\\displaystyle Known}\n  \n,\neach instantiated question being the distribution:\n\n  \n    \n      \n        P\n        \n          (\n          \n            \n              Searched\n            \n            ∣\n            \n              Known\n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left({\\text{Searched}}\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)}\n\nInference\nGiven the joint distribution \n  \n    \n      \n        P\n        \n          (\n          \n            \n              X\n              \n                1\n              \n            \n            ∧\n            \n              X\n              \n                2\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              X\n              \n                N\n              \n            \n            ∣\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(X_{1}\\wedge X_{2}\\wedge \\cdots \\wedge X_{N}\\mid \\delta \\wedge \\pi \\right)}\n  \n, it is always possible to compute any possible question using the following general inference:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      Searched\n                    \n                    ∣\n                    \n                      Known\n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                \n                  ∑\n                  \n"}
{"doc_id": "Bayesian programming", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the following general inference:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      Searched\n                    \n                    ∣\n                    \n                      Known\n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                \n                  ∑\n                  \n                    Free\n                  \n                \n                \n                  [\n                  \n                    P\n                    \n                      (\n                      \n                        \n                          Searched\n                        \n                        ∧\n                        \n                          Free\n                        \n                        ∣\n                        \n                          Known\n                        \n                        ∧\n                        δ\n                        ∧\n                        π\n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    \n                      \n                        ∑\n                        \n                          Free\n                        \n                      \n                      \n                        [\n                        \n                          P\n                          \n                            (\n                            \n                              \n                                Searched\n                              \n                              ∧\n                              \n                                Free\n                              \n                              ∧\n                              \n                                Known\n                              \n                              ∣\n                              δ\n                              ∧\n                              π\n                            \n                            )\n                          \n                        \n                        ]\n                      \n                    \n                    \n                      P\n                      \n                        (\n                        \n                          \n                            Known\n                          \n                          ∣\n                          δ\n                          ∧\n                          π\n                        \n                        )\n                      \n                    \n                  \n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    \n                      \n                        ∑\n                        \n                          Free\n                        \n                      \n                      \n                        [\n                        \n                          P\n                          \n                            (\n                            \n                              \n                                Searched\n                              \n                              ∧\n                              \n                                Free\n                              \n                              ∧\n                              \n                                Known\n                              \n                              ∣\n                              δ\n                              ∧\n                              π\n                            \n                            )\n                          \n                        \n                        ]\n                      \n                    \n                    \n                      \n                        ∑\n                        \n                          \n                            Free\n                          \n                          ∧\n                          \n                            Searched\n                          \n                        \n                      \n                      \n                        [\n                        \n                          P\n                          \n                            (\n                            \n                              \n                                Searched\n                              \n                              ∧\n                              \n                                Free\n                              \n                              ∧\n                              \n                                Known\n                              \n                              ∣\n                              δ\n                              ∧\n                              π\n                            \n                            )\n                          \n                        \n                        ]\n                      \n                    \n                  \n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    1\n                    Z\n                  \n                \n                ×\n                \n                  ∑\n                  \n                    Free\n                  \n                \n                \n                  [\n                  \n                    P\n                    \n                      (\n                      \n                        \n                          Searched\n                        \n                        ∧\n                        \n                          Free\n                        \n                        ∧\n                        \n                          Known\n                        \n                        ∣\n                        δ\n                        ∧\n                        π\n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left({\\text{Searched}}\\mid {\\text{Known}}\\wedge \\delta"}
{"doc_id": "Bayesian programming", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n                        \n                          Known\n                        \n                        ∣\n                        δ\n                        ∧\n                        π\n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left({\\text{Searched}}\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)\\\\={}&\\sum _{\\text{Free}}\\left[P\\left({\\text{Searched}}\\wedge {\\text{Free}}\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)\\right]\\\\={}&{\\frac {\\displaystyle \\sum _{\\text{Free}}\\left[P\\left({\\text{Searched}}\\wedge {\\text{Free}}\\wedge {\\text{Known}}\\mid \\delta \\wedge \\pi \\right)\\right]}{\\displaystyle P\\left({\\text{Known}}\\mid \\delta \\wedge \\pi \\right)}}\\\\={}&{\\frac {\\displaystyle \\sum _{\\text{Free}}\\left[P\\left({\\text{Searched}}\\wedge {\\text{Free}}\\wedge {\\text{Known}}\\mid \\delta \\wedge \\pi \\right)\\right]}{\\displaystyle \\sum _{{\\text{Free}}\\wedge {\\text{Searched}}}\\left[P\\left({\\text{Searched}}\\wedge {\\text{Free}}\\wedge {\\text{Known}}\\mid \\delta \\wedge \\pi \\right)\\right]}}\\\\={}&{\\frac {1}{Z}}\\times \\sum _{\\text{Free}}\\left[P\\left({\\text{Searched}}\\wedge {\\text{Free}}\\wedge {\\text{Known}}\\mid \\delta \\wedge \\pi \\right)\\right]\\end{aligned}}}\n  \n\nwhere the first equality results from the marginalization rule, the second\nresults from Bayes' theorem and the third corresponds to a second application of marginalization. The denominator appears to be a normalization term and can be replaced by a constant \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n  \n.\nTheoretically, this allows to solve any Bayesian inference problem. In practice,\nhowever, the cost of computing exhaustively and exactly \n  \n    \n      \n        P\n        \n          (\n          \n            \n              Searched\n            \n            ∣\n            \n              Known"}
{"doc_id": "Bayesian programming", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " constant \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n  \n.\nTheoretically, this allows to solve any Bayesian inference problem. In practice,\nhowever, the cost of computing exhaustively and exactly \n  \n    \n      \n        P\n        \n          (\n          \n            \n              Searched\n            \n            ∣\n            \n              Known\n            \n            ∧\n            δ\n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left({\\text{Searched}}\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)}\n  \n is too great in almost all cases.\nReplacing the joint distribution by its decomposition we get:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      Searched\n                    \n                    ∣\n                    \n                      Known\n                    \n                    ∧\n                    δ\n                    ∧\n                    π\n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    1\n                    Z\n                  \n                \n                \n                  ∑\n                  \n                    Free\n                  \n                \n                \n                  [\n                  \n                    \n                      ∏\n                      \n                        k\n                        =\n                        1\n                      \n                      \n                        K\n                      \n                    \n                    \n                      [\n                      \n                        P\n                        \n                          (\n                          \n                            \n                              L\n                              \n                                i\n                              \n                            \n                            ∣\n                            \n                              K\n                              \n                                i\n                              \n                            \n                            ∧\n                            π\n                          \n                          )\n                        \n                      \n                      ]\n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left({\\text{Searched}}\\mid {\\text{Known}}\\wedge \\delta \\wedge \\pi \\right)\\\\={}&{\\frac {1}{Z}}\\sum _{\\text{Free}}\\left[\\prod _{k=1}^{K}\\left[P\\left(L_{i}\\mid K_{i}\\wedge \\pi \\right)\\right]\\right]\\end{aligned}}}\n  \n\nwhich is usually a much simpler expression to compute, as the dimensionality of the problem is considerably reduced by the decomposition into a product of lower dimension distributions.\n\nExample\nBayesian spam detection\nThe purpose of Bayesian spam filtering is to eliminate junk e-mails.\nThe problem is very easy to formulate. E-mails should be classified\ninto one of two categories: non-spam or spam. The only available information to classify the e-mails is their content: a set of words. Using these words without taking the order into account is commonly called a bag of words model.\nThe classifier should furthermore be"}
{"doc_id": "Bayesian programming", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " very easy to formulate. E-mails should be classified\ninto one of two categories: non-spam or spam. The only available information to classify the e-mails is their content: a set of words. Using these words without taking the order into account is commonly called a bag of words model.\nThe classifier should furthermore be able to adapt to its user and to learn\nfrom experience. Starting from an initial standard setting, the classifier should\nmodify its internal parameters when the user disagrees with its own decision.\nIt will hence adapt to the user's criteria to differentiate between non-spam and\nspam. It will improve its results as it encounters increasingly classified e-mails.\n\nVariables\nThe variables necessary to write this program are as follows:\n\n  \n    \n      \n        S\n        p\n        a\n        m\n      \n    \n    {\\displaystyle Spam}\n  \n: a binary variable, false if the e-mail is not spam and true otherwise.\n\n  \n    \n      \n        \n          W\n          \n            0\n          \n        \n        ,\n        \n          W\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          W\n          \n            N\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle W_{0},W_{1},\\ldots ,W_{N-1}}\n  \n: \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n binary variables. \n  \n    \n      \n        \n          W\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle W_{n}}\n  \n is true if the \n  \n    \n      \n        \n          n\n          \n            t\n            h\n          \n        \n      \n    \n    {\\displaystyle n^{th}}\n  \n word of the dictionary is present in the text.\nThese \n  \n    \n      \n        N\n        +\n        1\n      \n    \n    {\\displaystyle N+1}\n  \n binary variables sum up all the information\nabout an e-mail.\n\nDecomposition\nStarting from the joint distribution and applying recursively Bayes' theorem we obtain:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                (\n                \n                  Spam\n                \n                ∧\n                \n                  W\n                  \n                    0\n                  \n                \n                ∧\n                ⋯\n                ∧\n                \n                  W\n                  \n                    N\n                    −\n                    1\n                  \n                \n                )\n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                (\n                \n                  Spam\n                \n                )\n                ×\n                P\n                (\n                \n                  W\n                  \n                    0\n                  \n                \n                ∣\n                \n                  Spam\n                \n                )\n                ×\n               "}
{"doc_id": "Bayesian programming", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " −\n                    1\n                  \n                \n                )\n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                (\n                \n                  Spam\n                \n                )\n                ×\n                P\n                (\n                \n                  W\n                  \n                    0\n                  \n                \n                ∣\n                \n                  Spam\n                \n                )\n                ×\n                P\n                (\n                \n                  W\n                  \n                    1\n                  \n                \n                ∣\n                \n                  Spam\n                \n                ∧\n                \n                  W\n                  \n                    0\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                ×\n                ⋯\n              \n            \n            \n              \n              \n                \n                ×\n                P\n                \n                  (\n                  \n                    \n                      W\n                      \n                        N\n                        −\n                        1\n                      \n                    \n                    ∣\n                    \n                      Spam\n                    \n                    ∧\n                    \n                      W\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      W\n                      \n                        N\n                        −\n                        2\n                      \n                    \n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P({\\text{Spam}}\\wedge W_{0}\\wedge \\cdots \\wedge W_{N-1})\\\\={}&P({\\text{Spam}})\\times P(W_{0}\\mid {\\text{Spam}})\\times P(W_{1}\\mid {\\text{Spam}}\\wedge W_{0})\\\\&\\times \\cdots \\\\&\\times P\\left(W_{N-1}\\mid {\\text{Spam}}\\wedge W_{0}\\wedge \\cdots \\wedge W_{N-2}\\right)\\end{aligned}}}\n  \n\nThis is an exact mathematical expression.\nIt can be drastically simplified by assuming that the probability of appearance of a word knowing the nature of the text (spam or not) is independent of the appearance of the other words. This is the naive Bayes assumption and this makes this spam filter a naive Bayes model.\nFor instance, the programmer can assume that:\n\n  \n    \n      \n        P\n        (\n        \n          W\n          \n            1\n          \n        \n        ∣\n        \n          Spam\n        \n        ∧\n        \n          W\n          \n            0\n          \n        \n        )\n        =\n        P\n        (\n        \n          W\n          \n            1\n          \n        \n        ∣\n        \n          Spam\n        \n        )\n      \n    \n    {\\displaystyle P(W_{1}\\mid {\\text{Spam}}\\land W_{"}
{"doc_id": "Bayesian programming", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n        \n          W\n          \n            0\n          \n        \n        )\n        =\n        P\n        (\n        \n          W\n          \n            1\n          \n        \n        ∣\n        \n          Spam\n        \n        )\n      \n    \n    {\\displaystyle P(W_{1}\\mid {\\text{Spam}}\\land W_{0})=P(W_{1}\\mid {\\text{Spam}})}\n  \n\nto finally obtain:\n\n  \n    \n      \n        P\n        (\n        \n          Spam\n        \n        ∧\n        \n          W\n          \n            0\n          \n        \n        ∧\n        …\n        ∧\n        \n          W\n          \n            N\n            −\n            1\n          \n        \n        )\n        =\n        P\n        (\n        \n          Spam\n        \n        )\n        \n          ∏\n          \n            n\n            =\n            0\n          \n          \n            N\n            −\n            1\n          \n        \n        [\n        P\n        (\n        \n          W\n          \n            n\n          \n        \n        ∣\n        \n          Spam\n        \n        )\n        ]\n      \n    \n    {\\displaystyle P({\\text{Spam}}\\land W_{0}\\land \\ldots \\land W_{N-1})=P({\\text{Spam}})\\prod _{n=0}^{N-1}[P(W_{n}\\mid {\\text{Spam}})]}\n  \n\nThis kind of assumption is known as the naive Bayes' assumption. It is \"naive\" in the sense that the independence between words is clearly not completely true. For instance, it completely neglects that the appearance of pairs of words may be more significant than isolated appearances. However, the programmer may assume this hypothesis and may develop the model and the associated inferences to test how reliable and efficient it is.\n\nParametric forms\nTo be able to compute the joint distribution, the programmer must now specify the\n\n  \n    \n      \n        N\n        +\n        1\n      \n    \n    {\\displaystyle N+1}\n  \n distributions appearing in the decomposition:\n\n  \n    \n      \n        P\n        (\n        \n          Spam\n        \n        )\n      \n    \n    {\\displaystyle P({\\text{Spam}})}\n  \n is a prior defined, for instance, by \n  \n    \n      \n        P\n        (\n        [\n        \n          Spam\n        \n        =\n        1\n        ]\n        )\n        =\n        0.75\n      \n    \n    {\\displaystyle P([{\\text{Spam}}=1])=0.75}\n  \n\nEach of the \n  \n    \n      \n        N\n      \n    \n    {\\"}
{"doc_id": "Bayesian programming", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "      \n        P\n        (\n        [\n        \n          Spam\n        \n        =\n        1\n        ]\n        )\n        =\n        0.75\n      \n    \n    {\\displaystyle P([{\\text{Spam}}=1])=0.75}\n  \n\nEach of the \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n  forms \n  \n    \n      \n        P\n        (\n        \n          W\n          \n            n\n          \n        \n        ∣\n        \n          Spam\n        \n        )\n      \n    \n    {\\displaystyle P(W_{n}\\mid {\\text{Spam}})}\n  \n may be specified using Laplace rule of succession (this is a pseudocounts-based smoothing technique to counter the zero-frequency problem of words never-seen-before):\n\n  \n    \n      \n        P\n        (\n        \n          W\n          \n            n\n          \n        \n        ∣\n        [\n        \n          Spam\n        \n        =\n        \n          false\n        \n        ]\n        )\n        =\n        \n          \n            \n              1\n              +\n              \n                a\n                \n                  f\n                \n                \n                  n\n                \n              \n            \n            \n              2\n              +\n              \n                a\n                \n                  f\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle P(W_{n}\\mid [{\\text{Spam}}={\\text{false}}])={\\frac {1+a_{f}^{n}}{2+a_{f}}}}\n  \n\n  \n    \n      \n        P\n        (\n        \n          W\n          \n            n\n          \n        \n        ∣\n        [\n        \n          Spam\n        \n        =\n        \n          true\n        \n        ]\n        )\n        =\n        \n          \n            \n              1\n              +\n              \n                a\n                \n                  t\n                \n                \n                  n\n                \n              \n            \n            \n              2\n              +\n              \n                a\n                \n                  t\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle P(W_{n}\\mid [{\\text{Spam}}={\\text{true}}])={\\frac {1+a_{t}^{n}}{2+a_{t}}}}\n  \n\nwhere \n  \n    \n      \n        \n          a\n          \n            f\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle a_{f}^{n}}\n  \n stands for the number of appearances of the \n  \n    \n      \n        \n          n\n          \n            t\n            h\n          \n        \n      \n    \n    {\\displaystyle n^{th}}\n  \n word in non-spam e-mails and \n  \n    \n      \n        \n          a\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle a_{f}}\n  \n stands for the total number of non-spam e"}
{"doc_id": "Bayesian programming", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " n\n          \n            t\n            h\n          \n        \n      \n    \n    {\\displaystyle n^{th}}\n  \n word in non-spam e-mails and \n  \n    \n      \n        \n          a\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle a_{f}}\n  \n stands for the total number of non-spam e-mails. Similarly, \n  \n    \n      \n        \n          a\n          \n            t\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle a_{t}^{n}}\n  \n stands for the number of appearances of the \n  \n    \n      \n        \n          n\n          \n            t\n            h\n          \n        \n      \n    \n    {\\displaystyle n^{th}}\n  \n word in spam e-mails and \n  \n    \n      \n        \n          a\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle a_{t}}\n  \n stands for the total number of spam e-mails.\n\nIdentification\nThe \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n  forms \n  \n    \n      \n        P\n        (\n        \n          W\n          \n            n\n          \n        \n        ∣\n        \n          Spam\n        \n        )\n      \n    \n    {\\displaystyle P(W_{n}\\mid {\\text{Spam}})}\n  \n are not yet completely specified because the \n  \n    \n      \n        2\n        N\n        +\n        2\n      \n    \n    {\\displaystyle 2N+2}\n  \n parameters \n  \n    \n      \n        \n          a\n          \n            f\n          \n          \n            n\n            =\n            0\n            ,\n            …\n            ,\n            N\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle a_{f}^{n=0,\\ldots ,N-1}}\n  \n, \n  \n    \n      \n        \n          a\n          \n            t\n          \n          \n            n\n            =\n            0\n            ,\n            …\n            ,\n            N\n            −\n            1\n          \n        \n      \n    \n    {\\displaystyle a_{t}^{n=0,\\ldots ,N-1}}\n  \n, \n  \n    \n      \n        \n          a\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle a_{f}}\n  \n and \n  \n    \n      \n        \n          a\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle a_{t}}\n  \n have no values yet.\nThe identification of these parameters could be done either by batch processing a series of classified e-mails or by an incremental updating of the parameters using the user's classifications of the e-mails as they arrive.\nBoth methods could be combined: the system could start with"}
{"doc_id": "Bayesian programming", "chunk_id": 17, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "    {\\displaystyle a_{t}}\n  \n have no values yet.\nThe identification of these parameters could be done either by batch processing a series of classified e-mails or by an incremental updating of the parameters using the user's classifications of the e-mails as they arrive.\nBoth methods could be combined: the system could start with initial standard values of these parameters issued from a generic database, then some incremental learning customizes the classifier to each individual user.\n\nQuestion\nThe question asked to the program is: \"what is the probability for a given text to be spam knowing which words appear and don't appear in this text?\"\nIt can be formalized by:\n\n  \n    \n      \n        P\n        (\n        \n          Spam\n        \n        ∣\n        \n          w\n          \n            0\n          \n        \n        ∧\n        ⋯\n        ∧\n        \n          w\n          \n            N\n            −\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle P({\\text{Spam}}\\mid w_{0}\\wedge \\cdots \\wedge w_{N-1})}\n  \n\nwhich can be computed as follows:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                (\n                \n                  Spam\n                \n                ∣\n                \n                  w\n                  \n                    0\n                  \n                \n                ∧\n                ⋯\n                ∧\n                \n                  w\n                  \n                    N\n                    −\n                    1\n                  \n                \n                )\n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    \n                      P\n                      (\n                      \n                        Spam\n                      \n                      )\n                      \n                        ∏\n                        \n                          n\n                          =\n                          0\n                        \n                        \n                          N\n                          −\n                          1\n                        \n                      \n                      [\n                      P\n                      (\n                      \n                        w\n                        \n                          n\n                        \n                      \n                      ∣\n                      \n                        Spam\n                      \n                      )\n                      ]\n                    \n                    \n                      \n                        ∑\n                        \n                          Spam\n                        \n                      \n                      [\n                      P\n                      (\n                      \n                        Spam\n                      \n                      )\n                      \n                        ∏\n                        \n                          n\n                          =\n                          0\n                        \n                        \n                          N\n                          −\n                          1\n                        \n                      \n                      [\n                      P\n                      (\n                      \n                        w\n                        \n                          n\n                        \n                      \n                      ∣\n                      \n                        Spam\n                      \n                      )\n                      ]\n                      ]\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P({\\text{Spam}}\\mid w_{0}\\wedge \\cdots \\wedge w_{N-1})\\\\={}&{\\frac {\\displaystyle P({\\text{Spam"}
{"doc_id": "Bayesian programming", "chunk_id": 18, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " ]\n                      ]\n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P({\\text{Spam}}\\mid w_{0}\\wedge \\cdots \\wedge w_{N-1})\\\\={}&{\\frac {\\displaystyle P({\\text{Spam}})\\prod _{n=0}^{N-1}[P(w_{n}\\mid {\\text{Spam}})]}{\\displaystyle \\sum _{\\text{Spam}}[P({\\text{Spam}})\\prod _{n=0}^{N-1}[P(w_{n}\\mid {\\text{Spam}})]]}}\\end{aligned}}}\n  \n\nThe denominator appears to be a normalization constant. It is not necessary to compute it to decide if we are dealing with spam. For instance, an easy trick is to compute the ratio:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                \n                  \n                    \n                      P\n                      (\n                      [\n                      \n                        Spam\n                      \n                      =\n                      \n                        true\n                      \n                      ]\n                      ∣\n                      \n                        w\n                        \n                          0\n                        \n                      \n                      ∧\n                      ⋯\n                      ∧\n                      \n                        w\n                        \n                          N\n                          −\n                          1\n                        \n                      \n                      )\n                    \n                    \n                      P\n                      (\n                      [\n                      \n                        Spam\n                      \n                      =\n                      \n                        false\n                      \n                      ]\n                      ∣\n                      \n                        w\n                        \n                          0\n                        \n                      \n                      ∧\n                      ⋯\n                      ∧\n                      \n                        w\n                        \n                          N\n                          −\n                          1\n                        \n                      \n                      )\n                    \n                  \n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                \n                  \n                    \n                      P\n                      (\n                      [\n                      \n                        Spam\n                      \n                      =\n                      \n                        true\n                      \n                      ]\n                      )\n                    \n                    \n                      P\n                      (\n                      [\n                      \n                        Spam\n                      \n                      =\n                      \n                        false\n                      \n                      ]\n                      )\n                    \n                  \n                \n                ×\n                \n                  ∏\n                  \n                    n\n                    =\n                    0\n                  \n                  \n                    N\n                    −\n                    1\n                  \n                \n                \n                  [\n                  \n                    \n                      \n                        P\n                        (\n                        \n                          w\n                          \n                            n\n                          \n                        \n                        ∣\n                        [\n                        \n                          Spam\n                        \n                        =\n                        \n                          true\n                        \n                        ]\n                        )\n                      \n                      \n                        P\n                        (\n                        \n                          w\n                          \n                            n\n                          \n                        \n                        ∣\n                        [\n                        \n                          Spam\n                        \n                        =\n                        \n                          false\n                        \n                        ]\n                        )\n                      \n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\"}
{"doc_id": "Bayesian programming", "chunk_id": 19, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "                        =\n                        \n                          true\n                        \n                        ]\n                        )\n                      \n                      \n                        P\n                        (\n                        \n                          w\n                          \n                            n\n                          \n                        \n                        ∣\n                        [\n                        \n                          Spam\n                        \n                        =\n                        \n                          false\n                        \n                        ]\n                        )\n                      \n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&{\\frac {P([{\\text{Spam}}={\\text{true}}]\\mid w_{0}\\wedge \\cdots \\wedge w_{N-1})}{P([{\\text{Spam}}={\\text{false}}]\\mid w_{0}\\wedge \\cdots \\wedge w_{N-1})}}\\\\={}&{\\frac {P([{\\text{Spam}}={\\text{true}}])}{P([{\\text{Spam}}={\\text{false}}])}}\\times \\prod _{n=0}^{N-1}\\left[{\\frac {P(w_{n}\\mid [{\\text{Spam}}={\\text{true}}])}{P(w_{n}\\mid [{\\text{Spam}}={\\text{false}}])}}\\right]\\end{aligned}}}\n  \n\nThis computation is faster and easier because it requires only \n  \n    \n      \n        2\n        N\n      \n    \n    {\\displaystyle 2N}\n  \n products.\n\nBayesian program\nThe Bayesian spam filter program is completely defined by:\n\n  \n    \n      \n        Pr\n        \n          \n            {\n            \n              \n                \n                  D\n                  s\n                  \n                    \n                      {\n                      \n                        \n                          \n                            S\n                            p\n                            (\n                            π\n                            )\n                            \n                              \n                                {\n                                \n                                  \n                                    \n                                      V\n                                      a\n                                      :\n                                      \n                                        Spam\n                                      \n                                      ,\n                                      \n                                        W\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      \n                                        W\n                                        \n                                          1\n                                        \n                                      \n                                      …\n                                      \n                                        W\n                                        \n                                          N\n                                          −\n                                          1\n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      D\n                                      c\n                                      :\n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                                P\n                                                (\n                                                \n                                                  Spam\n                                                \n                                                ∧\n                                                \n                                                  W\n                                                  \n                                                    0\n                                                  \n                                                \n                                                ∧\n                                                …\n                                                ∧\n                                                \n                                                  W\n                                                  \n                                                    n\n                                                  \n                                                \n                                                ∧\n                                                …\n                                                ∧\n                                                \n                                                  W\n                                                  \n                                                    N\n                                                    −\n                                                    1\n                                                  \n                                                \n                                                )\n                                              \n                                            \n"}
{"doc_id": "Bayesian programming", "chunk_id": 20, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n                                                  \n                                                    0\n                                                  \n                                                \n                                                ∧\n                                                …\n                                                ∧\n                                                \n                                                  W\n                                                  \n                                                    n\n                                                  \n                                                \n                                                ∧\n                                                …\n                                                ∧\n                                                \n                                                  W\n                                                  \n                                                    N\n                                                    −\n                                                    1\n                                                  \n                                                \n                                                )\n                                              \n                                            \n                                            \n                                              \n                                                =\n                                                P\n                                                (\n                                                \n                                                  Spam\n                                                \n                                                )\n                                                \n                                                  ∏\n                                                  \n                                                    n\n                                                    =\n                                                    0\n                                                  \n                                                  \n                                                    N\n                                                    −\n                                                    1\n                                                  \n                                                \n                                                P\n                                                (\n                                                \n                                                  W\n                                                  \n                                                    n\n                                                  \n                                                \n                                                ∣\n                                                \n                                                  Spam\n                                                \n                                                )\n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      F\n                                      o\n                                      :\n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                                P\n                                                (\n                                                \n                                                  Spam\n                                                \n                                                )\n                                                :\n                                                \n                                                  \n                                                    {\n                                                    \n                                                      \n                                                        \n                                                          P\n                                                          (\n                                                          [\n                                                          \n                                                            Spam\n                                                          \n                                                          =\n                                                          \n                                                            false\n                                                          \n                                                          ]\n                                                          )\n                                                          =\n                                                          0.25\n                                                        \n                                                      \n                                                      \n                                                        \n                                                          P\n                                                          (\n                                                          [\n                                                          \n                                                            Spam\n                                                          \n                                                          =\n                                                          \n                                                            true\n                                                          \n                                                          ]\n                                                          )\n                                                          =\n                                                          0.75\n                                                        \n                                                      \n                                                    \n                                                    \n                                                  \n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                (\n                                                \n                                                  W\n                                                  \n                                                    n\n                                                  \n                                                \n                                                ∣\n                                                \n                                                  Spam\n                                                \n                                                )\n                                                :\n                                                \n                                                  \n                                                    {\n                                                    \n                                                      \n                                                        \n                                                          P\n                                                          (\n                                                          \n                                                            W\n                                                            \n                                                              n\n                                                            \n                                                          \n                                                          ∣\n                                                          [\n                                                          \n                                                            Spam\n                                                          \n                                                          =\n                                                          \n                                                            false\n                                                          \n                                                          ]\n                                                          )\n                                                        \n                                                      \n                                                      \n                                                        \n                                                          =\n                                                          \n                                                            \n                                                              \n                                                                1\n                                                                +\n                                                                \n                                                                  a\n                                                                  \n                                                                    f\n                                                                  \n                                                                  \n                                                                    n\n                                                                  \n                                                                \n                                                              \n                                                              \n                                                                2\n                                                                +\n                                                                \n                                                                  a\n                                                                  \n                                                                    f\n                                                                  \n                                                                \n                                                              \n                                                            \n                                                          \n                                                        \n                                                      \n                                                      \n                                                        \n                                                          P\n                                                          (\n                                                          \n                                                            W\n                                                            \n                                                              n\n                                                            \n                                                          \n                                                          ∣\n                                                          [\n                                                          \n                                                            Spam\n                                                          \n                                                          =\n                                                          \n                                                            true\n                                                          \n                                                          ]\n                                                          )\n                                                        \n                                                      \n"}
{"doc_id": "Bayesian programming", "chunk_id": 21, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n                                                              \n                                                            \n                                                          \n                                                        \n                                                      \n                                                      \n                                                        \n                                                          P\n                                                          (\n                                                          \n                                                            W\n                                                            \n                                                              n\n                                                            \n                                                          \n                                                          ∣\n                                                          [\n                                                          \n                                                            Spam\n                                                          \n                                                          =\n                                                          \n                                                            true\n                                                          \n                                                          ]\n                                                          )\n                                                        \n                                                      \n                                                      \n                                                        \n                                                          =\n                                                          \n                                                            \n                                                              \n                                                                1\n                                                                +\n                                                                \n                                                                  a\n                                                                  \n                                                                    t\n                                                                  \n                                                                  \n                                                                    n\n                                                                  \n                                                                \n                                                              \n                                                              \n                                                                2\n                                                                +\n                                                                \n                                                                  a\n                                                                  \n                                                                    t\n                                                                  \n                                                                \n                                                              \n                                                            \n                                                          \n                                                        \n                                                      \n                                                    \n                                                    \n                                                  \n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                \n                                \n                              \n                            \n                          \n                        \n                        \n                          \n                            \n                              Identification (based on \n                            \n                            δ\n                            )\n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  Q\n                  u\n                  :\n                  P\n                  (\n                  \n                    Spam\n                  \n                  ∣\n                  \n                    w\n                    \n                      0\n                    \n                  \n                  ∧\n                  …\n                  ∧\n                  \n                    w\n                    \n                      n\n                    \n                  \n                  ∧\n                  …\n                  ∧\n                  \n                    w\n                    \n                      N\n                      −\n                      1\n                    \n                  \n                  )\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\Pr {\\begin{cases}Ds{\\begin{cases}Sp(\\pi ){\\begin{cases}Va:{\\text{Spam}},W_{0},W_{1}\\ldots W_{N-1}\\\\Dc:{\\begin{cases}P({\\text{Spam}}\\land W_{0}\\land \\ldots \\land W_{n}\\land \\ldots \\land W_{N-1})\\\\=P({\\text{Spam}})\\prod _{n=0}^{N-1}P(W_{n}\\mid {\\text{Spam}})\\end{cases}}\\\\Fo:{\\begin{cases}P({\\text{Spam}}):{\\begin{cases}P([{\\text{Spam}}={\\text{false}}])=0.25\\\\P([{\\text{Spam}}={\\text{true}}])=0.75\\end{cases}}\\\\P(W_{n}\\mid {\\text{Spam}}):{\\begin{cases}P(W_{n}\\mid [{\\text{Spam}}={\\text{false}}])\\\\={\\"}
{"doc_id": "Bayesian programming", "chunk_id": 22, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "{\\text{Spam}}={\\text{true}}])=0.75\\end{cases}}\\\\P(W_{n}\\mid {\\text{Spam}}):{\\begin{cases}P(W_{n}\\mid [{\\text{Spam}}={\\text{false}}])\\\\={\\frac {1+a_{f}^{n}}{2+a_{f}}}\\\\P(W_{n}\\mid [{\\text{Spam}}={\\text{true}}])\\\\={\\frac {1+a_{t}^{n}}{2+a_{t}}}\\end{cases}}\\\\\\end{cases}}\\\\\\end{cases}}\\\\{\\text{Identification (based on }}\\delta )\\end{cases}}\\\\Qu:P({\\text{Spam}}\\mid w_{0}\\land \\ldots \\land w_{n}\\land \\ldots \\land w_{N-1})\\end{cases}}}\n\nBayesian filter, Kalman filter and hidden Markov model\nBayesian filters (often called Recursive Bayesian estimation) are generic probabilistic models for time evolving processes. Numerous models are particular instances of this generic approach, for instance: the Kalman filter or the Hidden Markov model (HMM).\n\nVariables\nVariables \n  \n    \n      \n        \n          S\n          \n            0\n          \n        \n        ,\n        …\n        ,\n        \n          S\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle S^{0},\\ldots ,S^{T}}\n  \n are a time series of state variables considered to be on a time horizon ranging from \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n to \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n.\nVariables \n  \n    \n      \n        \n          O\n          \n            0\n          \n        \n        ,\n        …\n        ,\n        \n          O\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle O^{0},\\ldots ,O^{T}}\n  \n are a time series of observation variables on the same horizon.\n\nDecomposition\nThe decomposition is based:\n\non \n  \n    \n      \n        P\n        (\n        \n          S\n          \n            t\n          \n        \n        ∣\n        \n          S\n          \n            t\n            −\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle P(S^{t}\\mid S^{t-1})}\n  \n, called the system model, transition model or dynamic model, which formalizes the transition from the state at time"}
{"doc_id": "Bayesian programming", "chunk_id": 23, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        ∣\n        \n          S\n          \n            t\n            −\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle P(S^{t}\\mid S^{t-1})}\n  \n, called the system model, transition model or dynamic model, which formalizes the transition from the state at time \n  \n    \n      \n        t\n        −\n        1\n      \n    \n    {\\displaystyle t-1}\n  \n to the state at time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n;\non \n  \n    \n      \n        P\n        (\n        \n          O\n          \n            t\n          \n        \n        ∣\n        \n          S\n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle P(O^{t}\\mid S^{t})}\n  \n, called the observation model, which expresses what can be observed at time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n when the system is in state \n  \n    \n      \n        \n          S\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle S^{t}}\n  \n;\non an initial state at time \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n: \n  \n    \n      \n        P\n        (\n        \n          S\n          \n            0\n          \n        \n        ∧\n        \n          O\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle P(S^{0}\\wedge O^{0})}\n  \n.\n\nParametrical forms\nThe parametrical forms are not constrained and different choices lead to different well-known models: see Kalman filters and Hidden Markov models just below.\n\nQuestion\nThe typical question for such models is \n  \n    \n      \n        P\n        \n          (\n          \n            \n              S\n              \n                t\n                +\n                k\n              \n            \n            ∣\n            \n              O\n              \n                0\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              O\n              \n                t\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(S^{t+k}\\mid O^{0}\\wedge \\cdots \\wedge O^{t}\\right)}\n  \n: what is the probability distribution for the state at time \n  \n    \n      \n        t\n        +\n        k\n      \n    \n    {\\displaystyle t+k}\n  \n knowing the observations from instant \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n to \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n?\nThe most common case is Bayesian filtering"}
{"doc_id": "Bayesian programming", "chunk_id": 24, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " \n  \n    \n      \n        t\n        +\n        k\n      \n    \n    {\\displaystyle t+k}\n  \n knowing the observations from instant \n  \n    \n      \n        0\n      \n    \n    {\\displaystyle 0}\n  \n to \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n?\nThe most common case is Bayesian filtering where \n  \n    \n      \n        k\n        =\n        0\n      \n    \n    {\\displaystyle k=0}\n  \n, which searches for the present state, knowing past observations.\nHowever, it is also possible \n  \n    \n      \n        (\n        k\n        >\n        0\n        )\n      \n    \n    {\\displaystyle (k>0)}\n  \n, to extrapolate a future state from past observations, or to do smoothing \n  \n    \n      \n        (\n        k\n        <\n        0\n        )\n      \n    \n    {\\displaystyle (k<0)}\n  \n, to recover a past state from observations made either before or after that instant.\nMore complicated questions may also be asked as shown below in the HMM section.\nBayesian filters \n  \n    \n      \n        (\n        k\n        =\n        0\n        )\n      \n    \n    {\\displaystyle (k=0)}\n  \n have a very interesting recursive property, which contributes greatly to their attractiveness. \n  \n    \n      \n        P\n        \n          (\n          \n            \n              S\n              \n                t\n              \n            \n            \n              |\n            \n            \n              O\n              \n                0\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              O\n              \n                t\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(S^{t}|O^{0}\\wedge \\cdots \\wedge O^{t}\\right)}\n  \n may be computed simply from \n  \n    \n      \n        P\n        \n          (\n          \n            \n              S\n              \n                t\n                −\n                1\n              \n            \n            ∣\n            \n              O\n              \n                0\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              O\n              \n                t\n                −\n                1\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(S^{t-1}\\mid O^{0}\\wedge \\cdots \\wedge O^{t-1}\\right)}\n  \n with the following formula:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                     "}
{"doc_id": "Bayesian programming", "chunk_id": 25, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "edge O^{t-1}\\right)}\n  \n with the following formula:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      O\n                      \n                        t\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                =\n              \n              \n                P\n                \n                  (\n                  \n                    \n                      O\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      S\n                      \n                        t\n                      \n                    \n                  \n                  )\n                \n                ×\n                \n                  ∑\n                  \n                    \n                      S\n                      \n                        t\n                        −\n                        1\n                      \n                    \n                  \n                \n                \n                  [\n                  \n                    P\n                    \n                      (\n                      \n                        \n                          S\n                          \n                            t\n                          \n                        \n                        \n                          |\n                        \n                        \n                          S\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                      \n                      )\n                    \n                    ×\n                    P\n                    \n                      (\n                      \n                        \n                          S\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                        \n                          |\n                        \n                        \n                          O\n                          \n                            0\n                          \n                        \n                        ∧\n                        ⋯\n                        ∧\n                        \n                          O\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{ll}&P\\left(S^{t}|O^{0}\\wedge \\cdots \\wedge O^{t}\\right)\\\\=&P\\left(O^{t}|S^{t}\\right)\\times \\sum _{S^{t-1}}\\left[P\\left(S^{t}|S^{t-1}\\right)\\times P\\left(S^{t-1}|O^{0}\\wedge \\cdots \\wedge O^{t-1}\\right)\\right]\\end{array}}}\n  \n\nAnother interesting point of view for this equation is to consider that there are two phases: a\nprediction phase and an estimation phase:\n\nDuring the prediction phase, the state is predicted using the dynamic model and the estimation of the state at the previous moment:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      O\n                      \n                        t\n                        −\n                        1\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                =\n"}
{"doc_id": "Bayesian programming", "chunk_id": 26, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      O\n                      \n                        t\n                        −\n                        1\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                =\n              \n              \n                \n                  ∑\n                  \n                    \n                      S\n                      \n                        t\n                        −\n                        1\n                      \n                    \n                  \n                \n                \n                  [\n                  \n                    P\n                    \n                      (\n                      \n                        \n                          S\n                          \n                            t\n                          \n                        \n                        \n                          |\n                        \n                        \n                          S\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                      \n                      )\n                    \n                    ×\n                    P\n                    \n                      (\n                      \n                        \n                          S\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                        \n                          |\n                        \n                        \n                          O\n                          \n                            0\n                          \n                        \n                        ∧\n                        ⋯\n                        ∧\n                        \n                          O\n                          \n                            t\n                            −\n                            1\n                          \n                        \n                      \n                      )\n                    \n                  \n                  ]\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{array}{ll}&P\\left(S^{t}|O^{0}\\wedge \\cdots \\wedge O^{t-1}\\right)\\\\=&\\sum _{S^{t-1}}\\left[P\\left(S^{t}|S^{t-1}\\right)\\times P\\left(S^{t-1}|O^{0}\\wedge \\cdots \\wedge O^{t-1}\\right)\\right]\\end{array}}}\n  \n\nDuring the estimation phase, the prediction is either confirmed or invalidated using the last observation:\n\n  \n    \n      \n        \n          \n            \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    ∣\n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      O\n                      \n                        t\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n                =\n                \n\n                \n              \n              \n                P\n                \n                  (\n                  \n                    \n                      O\n                      \n                        t\n                      \n                    \n                    ∣\n                    \n                      S\n                      \n                        t\n                      \n                    \n                  \n                  )\n                \n                ×\n                P\n                \n                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      O\n                      \n                        t\n                        −\n                        1\n                      \n                    \n                  \n                  )\n                \n              "}
{"doc_id": "Bayesian programming", "chunk_id": 27, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " ×\n                P\n                \n                  (\n                  \n                    \n                      S\n                      \n                        t\n                      \n                    \n                    \n                      |\n                    \n                    \n                      O\n                      \n                        0\n                      \n                    \n                    ∧\n                    ⋯\n                    ∧\n                    \n                      O\n                      \n                        t\n                        −\n                        1\n                      \n                    \n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}&P\\left(S^{t}\\mid O^{0}\\wedge \\cdots \\wedge O^{t}\\right)\\\\={}&P\\left(O^{t}\\mid S^{t}\\right)\\times P\\left(S^{t}|O^{0}\\wedge \\cdots \\wedge O^{t-1}\\right)\\end{aligned}}}\n\nBayesian program\nP\n        r\n        \n          \n            {\n            \n              \n                \n                  D\n                  s\n                  \n                    \n                      {\n                      \n                        \n                          \n                            S\n                            p\n                            (\n                            π\n                            )\n                            \n                              \n                                {\n                                \n                                  \n                                    \n                                      V\n                                      a\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        S\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      ⋯\n                                      ,\n                                      \n                                        S\n                                        \n                                          T\n                                        \n                                      \n                                      ,\n                                      \n                                        O\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      ⋯\n                                      ,\n                                      \n                                        O\n                                        \n                                          T\n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      D\n                                      c\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    ⋯\n                                                    ∧\n                                                    \n                                                      S\n                                                      \n                                                        T\n                                                      \n                                                    \n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    ⋯\n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        T\n                                                      \n                                                    \n                                                    \n                                                      |\n                                                    \n                                                    π\n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                =\n                                              \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                  \n                                                  )\n                                                \n                                                ×\n                                                \n                                                  ∏\n                                                  \n                                                    t\n                                                    =\n                                                    1\n                                                  \n                                                  \n                                                    T\n                                                  \n                                                \n                                                \n"}
{"doc_id": "Bayesian programming", "chunk_id": 28, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                  \n                                                  )\n                                                \n                                                ×\n                                                \n                                                  ∏\n                                                  \n                                                    t\n                                                    =\n                                                    1\n                                                  \n                                                  \n                                                    T\n                                                  \n                                                \n                                                \n                                                  [\n                                                  \n                                                    P\n                                                    \n                                                      (\n                                                      \n                                                        \n                                                          S\n                                                          \n                                                            t\n                                                          \n                                                        \n                                                        \n                                                          |\n                                                        \n                                                        \n                                                          S\n                                                          \n                                                            t\n                                                            −\n                                                            1\n                                                          \n                                                        \n                                                      \n                                                      )\n                                                    \n                                                    ×\n                                                    P\n                                                    \n                                                      (\n                                                      \n                                                        \n                                                          O\n                                                          \n                                                            t\n                                                          \n                                                        \n                                                        \n                                                          |\n                                                        \n                                                        \n                                                          S\n                                                          \n                                                            t\n                                                          \n                                                        \n                                                      \n                                                      )\n                                                    \n                                                  \n                                                  ]\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      F\n                                      o\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    \n                                                      |\n                                                    \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                        −\n                                                        1\n                                                      \n                                                    \n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      O\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    \n                                                      |\n                                                    \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                \n                                \n                              \n                            \n                          \n                        \n                        \n                          \n                            I\n                            d\n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  Q\n                  u\n                  :\n                \n              \n              \n                \n                  \n                    \n                      {\n                      \n                        \n                          \n                            \n                              \n                                \n                                  \n                                    P\n                                    \n                                      (\n                                      \n                                        \n                                          S\n                                          \n                                            t\n                                            +\n                                            k\n                                          \n                                        \n                                        \n                                          |\n                                        \n                                        \n                                          O\n                                          \n                                            0\n                                          \n                                        \n                                        ∧\n                                        ⋯\n                                        ∧\n                                        \n                                          O\n                                          \n                                            t\n                                          \n                                        \n                                      \n                                      )\n                                    \n                                "}
{"doc_id": "Bayesian programming", "chunk_id": 29, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "      \n                                        \n                                          S\n                                          \n                                            t\n                                            +\n                                            k\n                                          \n                                        \n                                        \n                                          |\n                                        \n                                        \n                                          O\n                                          \n                                            0\n                                          \n                                        \n                                        ∧\n                                        ⋯\n                                        ∧\n                                        \n                                          O\n                                          \n                                            t\n                                          \n                                        \n                                      \n                                      )\n                                    \n                                  \n                                \n                                \n                                  \n                                    \n                                      (\n                                      \n                                        k\n                                        =\n                                        0\n                                      \n                                      )\n                                    \n                                    ≡\n                                    \n                                      Filtering\n                                    \n                                  \n                                \n                                \n                                  \n                                    \n                                      (\n                                      \n                                        k\n                                        >\n                                        0\n                                      \n                                      )\n                                    \n                                    ≡\n                                    \n                                      Prediction\n                                    \n                                  \n                                \n                                \n                                  \n                                    \n                                      (\n                                      \n                                        k\n                                        <\n                                        0\n                                      \n                                      )\n                                    \n                                    ≡\n                                    \n                                      Smoothing\n                                    \n                                  \n                                \n                              \n                            \n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle Pr{\\begin{cases}Ds{\\begin{cases}Sp(\\pi ){\\begin{cases}Va:\\\\S^{0},\\cdots ,S^{T},O^{0},\\cdots ,O^{T}\\\\Dc:\\\\{\\begin{cases}&P\\left(S^{0}\\wedge \\cdots \\wedge S^{T}\\wedge O^{0}\\wedge \\cdots \\wedge O^{T}|\\pi \\right)\\\\=&P\\left(S^{0}\\wedge O^{0}\\right)\\times \\prod _{t=1}^{T}\\left[P\\left(S^{t}|S^{t-1}\\right)\\times P\\left(O^{t}|S^{t}\\right)\\right]\\end{cases}}\\\\Fo:\\\\{\\begin{cases}P\\left(S^{0}\\wedge O^{0}\\right)\\\\P\\left(S^{t}|S^{t-1}\\right)\\\\P\\left(O^{t}|S^{t}\\right)\\end{cases}}\\end{cases}}\\\\Id\\end{cases}}\\\\Qu:\\\\{\\begin{cases}{\\begin{array}{l}P\\left(S^{t+k}|O^{0}\\wedge \\cdots \\wedge O^{t}\\right)\\\\\\left(k=0\\right)\\equiv {\\text{Filtering}}\\\\\\left(k>0\\right)\\equiv {\\text{Prediction}}\\\\\\left(k<0\\right)\\equiv {\\text{Smoothing}}\\end{array}}\\end"}
{"doc_id": "Bayesian programming", "chunk_id": 30, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "edge \\cdots \\wedge O^{t}\\right)\\\\\\left(k=0\\right)\\equiv {\\text{Filtering}}\\\\\\left(k>0\\right)\\equiv {\\text{Prediction}}\\\\\\left(k<0\\right)\\equiv {\\text{Smoothing}}\\end{array}}\\end{cases}}\\end{cases}}}\n\nKalman filter\nThe very well-known Kalman filters are a special case of Bayesian\nfilters.\nThey are defined by the following Bayesian program:\n\n  \n    \n      \n        P\n        r\n        \n          \n            {\n            \n              \n                \n                  D\n                  s\n                  \n                    \n                      {\n                      \n                        \n                          \n                            S\n                            p\n                            (\n                            π\n                            )\n                            \n                              \n                                {\n                                \n                                  \n                                    \n                                      V\n                                      a\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        S\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      ⋯\n                                      ,\n                                      \n                                        S\n                                        \n                                          T\n                                        \n                                      \n                                      ,\n                                      \n                                        O\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      ⋯\n                                      ,\n                                      \n                                        O\n                                        \n                                          T\n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      D\n                                      c\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    ⋯\n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        T\n                                                      \n                                                    \n                                                    \n                                                      |\n                                                    \n                                                    π\n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                =\n                                              \n                                              \n                                                \n                                                  [\n                                                  \n                                                    \n                                                      \n                                                        \n                                                          P\n                                                          \n                                                            (\n                                                            \n                                                              \n                                                                S\n                                                                \n                                                                  0\n                                                                \n                                                              \n                                                              ∧\n                                                              \n                                                                O\n                                                                \n                                                                  0\n                                                                \n                                                              \n                                                              \n                                                                |\n                                                              \n                                                              π\n                                                            \n                                                            )\n                                                          \n                                                        \n                                                      \n                                                      \n                                                        \n                                                          \n                                                            ∏\n                                                            \n                                                              t\n                                                              =\n                                                              1\n                                                            \n                                                            \n                                                              T\n                                                            \n                                                          \n                                                          \n                                                            [\n                                                            \n                                                              P\n                                                              \n                                                                (\n                                                                \n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  \n                                                                    |\n                                                                  \n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                      −\n                                                                      1\n                                                                    \n                                                                  \n                                                                  ∧\n                                                                  π\n                                                                \n                                                                )\n                                                              \n"}
{"doc_id": "Bayesian programming", "chunk_id": 31, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "                                                            \n                                                              P\n                                                              \n                                                                (\n                                                                \n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  \n                                                                    |\n                                                                  \n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                      −\n                                                                      1\n                                                                    \n                                                                  \n                                                                  ∧\n                                                                  π\n                                                                \n                                                                )\n                                                              \n                                                              ×\n                                                              P\n                                                              \n                                                                (\n                                                                \n                                                                  \n                                                                    O\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  \n                                                                    |\n                                                                  \n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  ∧\n                                                                  π\n                                                                \n                                                                )\n                                                              \n                                                            \n                                                            ]\n                                                          \n                                                        \n                                                      \n                                                    \n                                                  \n                                                  ]\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      F\n                                      o\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∣\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                        −\n                                                        1\n                                                      \n                                                    \n                                                    ∧\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                G\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ,\n                                                    A\n                                                    ∙\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                        −\n                                                        1\n                                                      \n                                                    \n                                                    ,\n                                                    Q\n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      O\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∣\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∧\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                G\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      O\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ,\n                                                    H\n                                                    ∙\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ,\n                                                    R\n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                \n                                \n                              \n                            \n                          \n                        \n                        \n                          \n                            I\n                            d\n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  Q\n                  u\n                  :\n                \n              \n              \n                \n                  P\n                  \n                    (\n                    \n                      \n                        S\n                        \n                          T\n                        \n                      \n                      ∣\n                      \n                        O\n                        \n                          0\n                        \n                      \n                      ∧\n                      ⋯\n                      ∧\n                      \n                        O\n                        \n                         "}
{"doc_id": "Bayesian programming", "chunk_id": 32, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  Q\n                  u\n                  :\n                \n              \n              \n                \n                  P\n                  \n                    (\n                    \n                      \n                        S\n                        \n                          T\n                        \n                      \n                      ∣\n                      \n                        O\n                        \n                          0\n                        \n                      \n                      ∧\n                      ⋯\n                      ∧\n                      \n                        O\n                        \n                          T\n                        \n                      \n                      ∧\n                      π\n                    \n                    )\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle Pr{\\begin{cases}Ds{\\begin{cases}Sp(\\pi ){\\begin{cases}Va:\\\\S^{0},\\cdots ,S^{T},O^{0},\\cdots ,O^{T}\\\\Dc:\\\\{\\begin{cases}&P\\left(S^{0}\\wedge \\cdots \\wedge O^{T}|\\pi \\right)\\\\=&\\left[{\\begin{array}{c}P\\left(S^{0}\\wedge O^{0}|\\pi \\right)\\\\\\prod _{t=1}^{T}\\left[P\\left(S^{t}|S^{t-1}\\wedge \\pi \\right)\\times P\\left(O^{t}|S^{t}\\wedge \\pi \\right)\\right]\\end{array}}\\right]\\end{cases}}\\\\Fo:\\\\{\\begin{cases}P\\left(S^{t}\\mid S^{t-1}\\wedge \\pi \\right)\\equiv G\\left(S^{t},A\\bullet S^{t-1},Q\\right)\\\\P\\left(O^{t}\\mid S^{t}\\wedge \\pi \\right)\\equiv G\\left(O^{t},H\\bullet S^{t},R\\right)\\end{cases}}\\end{cases}}\\\\Id\\end{cases}}\\\\Qu:\\\\P\\left(S^{T}\\mid O^{0}\\wedge \\cdots \\wedge O^{T}\\wedge \\pi \\right)\\end{cases}}}\n  \n\nVariables are continuous.\nThe transition model \n  \n    \n      \n        P\n        (\n        \n          S\n          \n            t\n          \n        \n        ∣\n        \n          S\n          \n            t\n            −\n            1\n          \n        \n        ∧\n        π\n        )\n      \n    \n    {\\displaystyle P(S^{t}\\mid S^{t-1}\\wedge \\pi )}\n  \n and the observation model \n  \n    \n      \n        P\n        (\n        \n          O\n          \n            t\n          \n        \n        ∣\n        \n         "}
{"doc_id": "Bayesian programming", "chunk_id": 33, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "1\n          \n        \n        ∧\n        π\n        )\n      \n    \n    {\\displaystyle P(S^{t}\\mid S^{t-1}\\wedge \\pi )}\n  \n and the observation model \n  \n    \n      \n        P\n        (\n        \n          O\n          \n            t\n          \n        \n        ∣\n        \n          S\n          \n            t\n          \n        \n        ∧\n        π\n        )\n      \n    \n    {\\displaystyle P(O^{t}\\mid S^{t}\\wedge \\pi )}\n  \n are both specified using Gaussian laws with means that are linear functions of the conditioning variables.\nWith these hypotheses and by using the recursive formula, it is possible to solve\nthe inference problem analytically to answer the usual \n  \n    \n      \n        P\n        (\n        \n          S\n          \n            T\n          \n        \n        ∣\n        \n          O\n          \n            0\n          \n        \n        ∧\n        ⋯\n        ∧\n        \n          O\n          \n            T\n          \n        \n        ∧\n        π\n        )\n      \n    \n    {\\displaystyle P(S^{T}\\mid O^{0}\\wedge \\cdots \\wedge O^{T}\\wedge \\pi )}\n  \n question.\nThis leads to an extremely efficient algorithm, which explains the popularity of Kalman filters and the number of their everyday applications.\nWhen there are no obvious linear transition and observation models, it is still often\npossible, using a first-order Taylor's expansion, to treat these models as locally linear.\nThis generalization is commonly called the extended Kalman filter.\n\nHidden Markov model\nHidden Markov models (HMMs) are another very popular specialization of Bayesian filters.\nThey are defined by the following Bayesian program:\n\n  \n    \n      \n        Pr\n        \n          \n            {\n            \n              \n                \n                  D\n                  s\n                  \n                    \n                      {\n                      \n                        \n                          \n                            S\n                            p\n                            (\n                            π\n                            )\n                            \n                              \n                                {\n                                \n                                  \n                                    \n                                      V\n                                      a\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        S\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      …\n                                      ,\n                                      \n                                        S\n                                        \n                                          T\n                                        \n                                      \n                                      ,\n                                      \n                                        O\n                                        \n                                          0\n                                        \n                                      \n                                      ,\n                                      …\n                                      ,\n                                      \n                                        O\n                                        \n                                          T\n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      D\n                                      c\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n"}
{"doc_id": "Bayesian programming", "chunk_id": 34, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "                                        \n                                      \n                                      ,\n                                      …\n                                      ,\n                                      \n                                        O\n                                        \n                                          T\n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      D\n                                      c\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    ⋯\n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        T\n                                                      \n                                                    \n                                                    ∣\n                                                    π\n                                                  \n                                                  )\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                =\n                                              \n                                              \n                                                \n                                                  [\n                                                  \n                                                    \n                                                      \n                                                        \n                                                          P\n                                                          \n                                                            (\n                                                            \n                                                              \n                                                                S\n                                                                \n                                                                  0\n                                                                \n                                                              \n                                                              ∧\n                                                              \n                                                                O\n                                                                \n                                                                  0\n                                                                \n                                                              \n                                                              ∣\n                                                              π\n                                                            \n                                                            )\n                                                          \n                                                        \n                                                      \n                                                      \n                                                        \n                                                          \n                                                            ∏\n                                                            \n                                                              t\n                                                              =\n                                                              1\n                                                            \n                                                            \n                                                              T\n                                                            \n                                                          \n                                                          \n                                                            [\n                                                            \n                                                              P\n                                                              \n                                                                (\n                                                                \n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  ∣\n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                      −\n                                                                      1\n                                                                    \n                                                                  \n                                                                  ∧\n                                                                  π\n                                                                \n                                                                )\n                                                              \n                                                              ×\n                                                              P\n                                                              \n                                                                (\n                                                                \n                                                                  \n                                                                    O\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  ∣\n                                                                  \n                                                                    S\n                                                                    \n                                                                      t\n                                                                    \n                                                                  \n                                                                  ∧\n                                                                  π\n                                                                \n                                                                )\n                                                              \n                                                            \n                                                            ]\n                                                          \n                                                        \n                                                      \n                                                    \n                                                  \n                                                  ]\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                  \n                                    \n                                      F\n                                      o\n                                      :\n                                    \n                                  \n                                  \n                                    \n                                      \n                                        \n                                          {\n                                          \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∧\n                                                    \n                                                      O\n                                                      \n                                                        0\n                                                      \n                                                    \n                                                    ∣\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                \n                                                  Matrix\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∣\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                       "}
{"doc_id": "Bayesian programming", "chunk_id": 35, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "                                                    ∣\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                \n                                                  Matrix\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∣\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                        −\n                                                        1\n                                                      \n                                                    \n                                                    ∧\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                \n                                                  Matrix\n                                                \n                                              \n                                            \n                                            \n                                              \n                                                P\n                                                \n                                                  (\n                                                  \n                                                    \n                                                      O\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∣\n                                                    \n                                                      S\n                                                      \n                                                        t\n                                                      \n                                                    \n                                                    ∧\n                                                    π\n                                                  \n                                                  )\n                                                \n                                                ≡\n                                                \n                                                  Matrix\n                                                \n                                              \n                                            \n                                          \n                                          \n                                        \n                                      \n                                    \n                                  \n                                \n                                \n                              \n                            \n                          \n                        \n                        \n                          \n                            I\n                            d\n                          \n                        \n                      \n                      \n                    \n                  \n                \n              \n              \n                \n                  Q\n                  u\n                  :\n                \n              \n              \n                \n                  \n                    max\n                    \n                      \n                        S\n                        \n                          1\n                        \n                      \n                      ∧\n                      ⋯\n                      ∧\n                      \n                        S\n                        \n                          T\n                          −\n                          1\n                        \n                      \n                    \n                  \n                  \n                    [\n                    \n                      P\n                      \n                        (\n                        \n                          \n                            S\n                            \n                              1\n                            \n                          \n                          ∧\n                          ⋯\n                          ∧\n                          \n                            S\n                            \n                              T\n                              −\n                              1\n                            \n                          \n                          ∣\n                          \n                            S\n                            \n                              T\n                            \n                          \n                          ∧\n                          \n                            O\n                            \n                              0\n                            \n                          \n                          ∧\n                          ⋯\n                          ∧\n                          \n                            O\n                            \n                              T\n                            \n                          \n                          ∧\n                          π\n                        \n                        )\n                      \n                    \n                    ]\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle \\Pr {\\begin{cases}Ds{\\begin{cases}Sp(\\pi ){\\begin{cases}Va:\\\\S^{0},\\ldots ,S^{T},O^{0},\\ldots ,O^{T}\\\\Dc:\\\\{\\begin{cases}&P\\left(S^{0}\\wedge \\cdots \\wedge O^{T}\\mid \\pi \\right)\\\\=&\\left[{\\begin{array}{c}P\\left(S^{0}\\wedge O^{0}\\mid \\pi \\right)\\\\\\prod _{t=1}^{T}\\left[P\\left(S^{t}\\mid S^{t-1}\\"}
{"doc_id": "Bayesian programming", "chunk_id": 36, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "}\\mid \\pi \\right)\\\\=&\\left[{\\begin{array}{c}P\\left(S^{0}\\wedge O^{0}\\mid \\pi \\right)\\\\\\prod _{t=1}^{T}\\left[P\\left(S^{t}\\mid S^{t-1}\\wedge \\pi \\right)\\times P\\left(O^{t}\\mid S^{t}\\wedge \\pi \\right)\\right]\\end{array}}\\right]\\end{cases}}\\\\Fo:\\\\{\\begin{cases}P\\left(S^{0}\\wedge O^{0}\\mid \\pi \\right)\\equiv {\\text{Matrix}}\\\\P\\left(S^{t}\\mid S^{t-1}\\wedge \\pi \\right)\\equiv {\\text{Matrix}}\\\\P\\left(O^{t}\\mid S^{t}\\wedge \\pi \\right)\\equiv {\\text{Matrix}}\\end{cases}}\\end{cases}}\\\\Id\\end{cases}}\\\\Qu:\\\\\\max _{S^{1}\\wedge \\cdots \\wedge S^{T-1}}\\left[P\\left(S^{1}\\wedge \\cdots \\wedge S^{T-1}\\mid S^{T}\\wedge O^{0}\\wedge \\cdots \\wedge O^{T}\\wedge \\pi \\right)\\right]\\end{cases}}}\n  \n\nVariables are treated as being discrete.\nThe transition model \n  \n    \n      \n        P\n        \n          (\n          \n            \n              S\n              \n                t\n              \n            \n            ∣\n            \n              S\n              \n                t\n                −\n                1\n              \n            \n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(S^{t}\\mid S^{t-1}\\wedge \\pi \\right)}\n  \n and the observation model \n  \n    \n      \n        P\n        \n          (\n          \n            \n              O\n              \n                t\n              \n            \n            ∣\n            \n              S\n              \n                t\n              \n            \n            ∧\n            π\n          \n          )\n        \n      \n    \n    {\\displaystyle P\\left(O^{t}\\mid S^{t}\\wedge \\pi \\right)}\n  \n are\nboth specified using probability matrices.\n\nThe question most frequently asked of HMMs is:\n\n  \n    \n      \n        \n          max\n          \n            \n              S\n              \n                1\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              S\n              \n                T\n               "}
{"doc_id": "Bayesian programming", "chunk_id": 37, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "edge \\pi \\right)}\n  \n are\nboth specified using probability matrices.\n\nThe question most frequently asked of HMMs is:\n\n  \n    \n      \n        \n          max\n          \n            \n              S\n              \n                1\n              \n            \n            ∧\n            ⋯\n            ∧\n            \n              S\n              \n                T\n                −\n                1\n              \n            \n          \n        \n        \n          [\n          \n            P\n            \n              (\n              \n                \n                  S\n                  \n                    1\n                  \n                \n                ∧\n                ⋯\n                ∧\n                \n                  S\n                  \n                    T\n                    −\n                    1\n                  \n                \n                ∣\n                \n                  S\n                  \n                    T\n                  \n                \n                ∧\n                \n                  O\n                  \n                    0\n                  \n                \n                ∧\n                ⋯\n                ∧\n                \n                  O\n                  \n                    T\n                  \n                \n                ∧\n                π\n              \n              )\n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\max _{S^{1}\\wedge \\cdots \\wedge S^{T-1}}\\left[P\\left(S^{1}\\wedge \\cdots \\wedge S^{T-1}\\mid S^{T}\\wedge O^{0}\\wedge \\cdots \\wedge O^{T}\\wedge \\pi \\right)\\right]}\n  \n\nWhat is the most probable series of states that leads to the present state, knowing the past observations?\nThis particular question may be answered with a specific and very efficient algorithm\ncalled the Viterbi algorithm.\nThe Baum–Welch algorithm has been developed\nfor HMMs.\n\nApplications\nAcademic applications\nSince 2000, Bayesian programming has been used to develop both robotics applications and life sciences models.\n\nRobotics\nIn robotics, Bayesian programming was applied to autonomous robotics, robotic CAD systems, advanced driver-assistance systems, robotic arm control, mobile robotics, human-robot interaction, human-vehicle interaction (Bayesian autonomous driver models) video game avatar programming and training  and real-time strategy games (AI).\n\nLife sciences\nIn life sciences, Bayesian programming was used in vision to reconstruct shape from motion, to model visuo-vestibular interaction and to study saccadic eye movements; in speech perception and control to study early speech acquisition and the emergence of articulatory-acoustic systems; and to model handwriting perception and control.\n\nPattern recognition\nBayesian program learning has potential applications voice recognition and synthesis, image recognition and natural language processing. It employs the principles of compositionality (building abstract representations from"}
{"doc_id": "Bayesian programming", "chunk_id": 38, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " movements; in speech perception and control to study early speech acquisition and the emergence of articulatory-acoustic systems; and to model handwriting perception and control.\n\nPattern recognition\nBayesian program learning has potential applications voice recognition and synthesis, image recognition and natural language processing. It employs the principles of compositionality (building abstract representations from parts), causality (building complexity from parts) and learning to learn (using previously recognized concepts to ease the creation of new concepts).\n\nPossibility theories\nThe comparison between probabilistic approaches (not only Bayesian programming) and possibility theories continues to be debated.\nPossibility theories like, for instance, fuzzy sets, fuzzy logic and possibility theory are alternatives to probability to model uncertainty. They argue that probability is insufficient or inconvenient to model certain aspects of incomplete/uncertain knowledge.\nThe defense of probability is mainly based on Cox's theorem, which starts from four postulates concerning rational reasoning in the presence of uncertainty. It demonstrates that the only mathematical framework that satisfies these postulates is probability theory. The argument is that any approach other than probability necessarily infringes one of these postulates and the value of that infringement.\n\nProbabilistic programming\nThe purpose of probabilistic programming is to unify the scope of classical programming languages with probabilistic modeling (especially Bayesian networks) to deal with uncertainty while profiting from the programming languages' expressiveness to encode complexity.\nExtended classical programming languages include logical languages as proposed in Probabilistic Horn Abduction, Independent Choice Logic, PRISM, and ProbLog which proposes an extension of Prolog.\nIt can also be extensions of functional programming languages (essentially Lisp and Scheme) such as IBAL or CHURCH. The underlying programming languages can be object-oriented as in BLOG and FACTORIE or more standard ones as in CES and FIGARO.\nThe purpose of Bayesian programming is different. Jaynes' precept of \"probability as logic\" argues that probability is an extension of and an alternative to logic above which a complete theory of rationality, computation and programming can be rebuilt. Bayesian programming attempts to replace classical languages with a programming approach based on probability that considers incompleteness and uncertainty.\nThe precise comparison between the semantics and power of expression of Bayesian and probabilistic programming is an open question.\n\nSee also"}
{"doc_id": "Bayesian programming", "chunk_id": 39, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " question.\n\nSee also"}
{"doc_id": "Behavior informatics", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Behavior informatics (BI) is the informatics of behaviors so as to obtain behavior intelligence and behavior insights. BI is a research method combining science and technology, specifically in the area of engineering. The purpose of BI includes analysis of current behaviors as well as the inference of future possible behaviors. This occurs through pattern recognition.\nDifferent from applied behavior analysis from the psychological perspective, BI builds computational theories, systems and tools to qualitatively and quantitatively model, represent, analyze, and manage behaviors of individuals, groups and/or organizations.\nBI is built on classic study of behavioral science, including behavior modeling, applied behavior analysis, behavior analysis, behavioral economics, and organizational behavior. Typical BI tasks consist of individual and group behavior formation, representation, computational modeling, analysis, learning, simulation, and understanding of behavior impact, utility, non-occurring behaviors, etc. for behavior intervention and management. The Behavior Informatics approach to data utilizes cognitive as well as behavioral data. By combining the data, BI has the potential to effectively illustrate the big picture when it comes to behavioral decisions and patterns. One of the goals of BI is also to be able to study human behavior while eliminating issues like self-report bias. This creates more reliable and valid information for research studies.\n\nBehavior\nFrom an Informatics perspective, a behavior consists of three key elements:\n\nactors (behavioral subjects and objects),\noperations (actions, activities) and\ninteractions (relationships), and their properties.\nA behavior can be represented as a behavior vector, all behaviors of an actor or an actor group can be represented as behavior sequences and multi-dimensional behavior matrix. The following table explains some of the elements of behavior. \n\nBehavior Informatics takes into account behavior when analyzing business patterns and intelligence. The inclusion of behavior in these analyses provides prominent information on social and driving factors of patterns.\n\nApplications\nBehavior Informatics is being used in a variety of settings, including but not limited to health care management, telecommunications, marketing, and security. Behavior Informatics provides a manner in which to analyze and organize the many aspects that go into a person's health care needs and decisions. When it comes to business models, behavior informatics may be utilized for a similar role. Organizations implement behavior informatics to enhance business structure and regime, where it helps moderate ideal business decisions and situations."}
{"doc_id": "Behavior informatics", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to enhance business structure and regime, where it helps moderate ideal business decisions and situations."}
{"doc_id": "Belief–desire–intention model", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "For popular psychology, the belief–desire–intention (BDI) model of human practical reasoning was developed by Michael Bratman as a way of explaining future-directed intention.\nBDI is fundamentally reliant on folk psychology (the 'theory theory'), which is the notion that our mental models of the world are theories. It was used as a basis for developing the belief–desire–intention software model.\n\nApplications\nBDI was part of the inspiration behind the BDI software architecture, which Bratman was also involved in developing. Here, the notion of intention was seen as a way of limiting time spent on deliberating about what to do, by eliminating choices inconsistent with current intentions.\nBDI has also aroused some interest in psychology. BDI formed the basis for a computational model of childlike reasoning CRIBB."}
{"doc_id": "Brain technology", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Brain technology, or self-learning know-how systems, defines a technology that employs latest findings in neuroscience. [see also neuro implants] The term was first introduced by the Artificial Intelligence Laboratory in Zurich, Switzerland, in the context of the Roboy project. Brain Technology can be employed in robots, know-how management systems and any other application with self-learning capabilities. In particular, Brain Technology applications allow the visualization of the underlying learning architecture often coined as \"know-how maps\".\n\nResearch and applications\nThe first demonstrations of BC in humans and animals took place in the 1960s when Grey Walter demonstrated use of non-invasively recorded encephalogram (EEG) signals from a human subject to control a slide projector (Graimann et al., 2010). \nSoon after Jacques J. Vidal coined the term brain–computer interface (BCI) in 1971, the Defense Advanced Research Projects Agency (DARPA) first starting funding brain–computer interface research and has since funded several brain–computer interface projects. That market is expected to reach a value of $1.72 billion by 2022. Brain–computer interfaces record brain activity, transmit the information out of the body, signal-process the data via algorithms, and convert them into command control signals. \nIn 2012, a landmark study in Nature, led by pioneer Leigh Hochberg, MD, PhD, demonstrated that two people with tetraplegia were able to control robotic arms through thought when connected to the BrainGate neural interface system. The two participants were able to reach for and grasp objects in three-dimensional space, and one participant used the system to serve herself coffee for the first time since becoming paralyzed nearly 15 years prior.\nAnd in October 2020, two patients were able to wirelessly control an operating system to text, email, shop and bank using direct thought through the Stentrode brain computer interface (Journal of NeuroInterventional Surgery) in a study led by Thomas Oxley. This was the first time a brain–computer interface was implanted via the patient's blood vessels, eliminating the need for open brain surgery.\nCurrently a number of groups are exploring a range of experimental devices using brain–computer interfaces, which have the potential to fundamentally change the way of life for patients with paralysis and a wide range of neurological disorders. These include: as Elon Musk, Facebook, and the University of California in San Francisco. The systems. This technology is also being explored as a neuromodulation device and may ultimately help diagnose and"}
{"doc_id": "Brain technology", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", which have the potential to fundamentally change the way of life for patients with paralysis and a wide range of neurological disorders. These include: as Elon Musk, Facebook, and the University of California in San Francisco. The systems. This technology is also being explored as a neuromodulation device and may ultimately help diagnose and treat a range of brain pathologies, such as epilepsy and Parkinson's disease."}
{"doc_id": "Business process automation", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Business process automation (BPA), also known as business automation, refers to the technology-enabled automation of business processes.\n\nDevelopment approaches\nThere are three main approaches to developing BPA:\n\ntraditional business process automation entails developing BPA software in a programming language for integrating relevant applications in the digital ecosystem to execute a given process;\nrobotic process automation uses software robots (also called agents, bots, or workers) to emulate human-computer interaction for executing a combination of processes, activities, transactions, and tasks in one or more unrelated software systems;\nhyperautomation (also called intelligent automation (IA), intelligent process automation (IPA) integrated automation platform (IAP), and cognitive automation (CA) combines business process automation, artificial intelligence (AI), and machine learning (ML) to discover, validate, and execute organizational processes automatically with no or minimal human intervention.\n\nDeployment\nBPA toolsets vary in capability. With the increasing adoption of artificial intelligence (AI), organizations are implementing AI-driven technologies that can process natural language, interpret unstructured datasets, and interact with users. These systems are designed to adapt to new types of problems with reduced reliance on human intervention.\n\nBusiness process management implementation\nA business process management system differs from BPA. However, it is possible to implement automation based on a BPM implementation. The methods to achieve this vary, from writing custom application code to using specialist BPA tools.\n\nRobotic process automation\nRobotic process automation (RPA) involves the deployment of attended or unattended software agents in an organization's environment. These software agents, or robots, are programmed to perform pre-defined structured and repetitive sets of business tasks or processes. Robotic process automation is designed to streamline workflows by delegating repetitive tasks to software agents, allowing human workers to focus on more complex and strategic activities.\nBPA providers typically focus on different industry sectors, but the underlying approach is generally similar in that they aim to provide the shortest route to automation by interacting with the user interface rather than modifying the application code or database behind it.\n\nUse of artificial intelligence\nArtificial intelligence software robots are used to handle unstructured data sets (like images, texts, audios) and are often deployed after implementing robotic process automation. They can, for instance, generate an automatic transcript from a video. The combination of automation and artificial intelligence (AI) enables autonomy for robots, along with the capability to perform cognitive tasks.  At this stage, the robot can learn and improve processes by analyzing and adapting them.\n\nSee also\nAI alignment\nArtificial intelligence detection software\nArtificial intelligence"}
{"doc_id": "Business process automation", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " an automatic transcript from a video. The combination of automation and artificial intelligence (AI) enables autonomy for robots, along with the capability to perform cognitive tasks.  At this stage, the robot can learn and improve processes by analyzing and adapting them.\n\nSee also\nAI alignment\nArtificial intelligence detection software\nArtificial intelligence and elections\nBusiness-driven development\nBusiness Process Model and Notation\nBusiness process reengineering\nBusiness Process Execution Language (BPEL)\nBusiness rules engine\nComparison of business integration software\nJob scheduler\nReal-time enterprise\nRunbook\nSynthetic intelligence\nWeak AI"}
{"doc_id": "Case-based reasoning", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Case-based reasoning (CBR), broadly construed, is the process of solving new problems based on the solutions of similar past problems.\nIn everyday life, an auto mechanic who fixes an engine by recalling another car that exhibited similar symptoms is using case-based reasoning. A lawyer who advocates a particular outcome in a trial based on legal precedents or a judge who creates case law is using case-based reasoning. So, too, an engineer copying working elements of nature (practicing biomimicry) is treating nature as a database of solutions to problems. Case-based reasoning is a prominent type of analogy solution making.\nIt has been argued that case-based reasoning is not only a powerful method for computer reasoning, but also a pervasive behavior in everyday human problem solving; or, more radically, that all reasoning is based on past cases personally experienced. This view is related to prototype theory, which is most deeply explored in cognitive science.\n\nProcess\nCase-based reasoning has been formalized for purposes of computer reasoning as a four-step process:\n\nRetrieve: Given a target problem, retrieve cases relevant to solving it from memory. A case consists of a problem, its solution, and, typically, annotations about how the solution was derived. For example, suppose Fred wants to prepare blueberry pancakes. Being a novice cook, the most relevant experience he can recall is one in which he successfully made plain pancakes. The procedure he followed for making the plain pancakes, together with justifications for decisions made along the way, constitutes Fred's retrieved case.\nReuse: Map the solution from the previous case to the target problem. This may involve adapting the solution as needed to fit the new situation. In the pancake example, Fred must adapt his retrieved solution to include the addition of blueberries.\nRevise: Having mapped the previous solution to the target situation, test the new solution in the real world (or a simulation) and, if necessary, revise. Suppose Fred adapted his pancake solution by adding blueberries to the batter. After mixing, he discovers that the batter has turned blue – an undesired effect. This suggests the following revision: delay the addition of blueberries until after the batter has been ladled into the pan.\nRetain: After the solution has been successfully adapted to the target problem, store the resulting experience as a new case in memory. Fred, accordingly, records his new-found procedure for making blueberry pancakes, thereby enriching his set of stored experiences, and better preparing him for future pancake-making demands.\n\nComparison to other methods\nAt first glance, CBR"}
{"doc_id": "Case-based reasoning", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " successfully adapted to the target problem, store the resulting experience as a new case in memory. Fred, accordingly, records his new-found procedure for making blueberry pancakes, thereby enriching his set of stored experiences, and better preparing him for future pancake-making demands.\n\nComparison to other methods\nAt first glance, CBR may seem similar to the rule induction algorithms of machine learning. Like a rule-induction algorithm, CBR starts with a set of cases or training examples; it forms generalizations of these examples, albeit implicit ones, by identifying commonalities between a retrieved case and the target problem.\nIf for instance a procedure for plain pancakes is mapped to blueberry pancakes, a decision is made to use the same basic batter and frying method, thus implicitly generalizing the set of situations under which the batter and frying method can be used. The key difference, however, between the implicit generalization in CBR and the generalization in rule induction lies in when the generalization is made. A rule-induction algorithm draws its generalizations from a set of training examples before the target problem is even known; that is, it performs eager generalization.\nFor instance, if a rule-induction algorithm were given recipes for plain pancakes, Dutch apple pancakes, and banana pancakes as its training examples, it would have to derive, at training time, a set of general rules for making all types of pancakes. It would not be until testing time that it would be given, say, the task of cooking blueberry pancakes. The difficulty for the rule-induction algorithm is in anticipating the different directions in which it should attempt to generalize its training examples. This is in contrast to CBR, which delays (implicit) generalization of its cases until testing time – a strategy of lazy generalization. In the pancake example, CBR has already been given the target problem of cooking blueberry pancakes; thus it can generalize its cases exactly as needed to cover this situation. CBR therefore tends to be a good approach for rich, complex domains in which there are myriad ways to generalize a case.\nIn law, there is often explicit delegation of CBR to courts, recognizing the limits of rule based reasons: limiting delay, limited knowledge of future context, limit of negotiated agreement, etc. While CBR in law and cognitively inspired CBR have long been associated, the former is more clearly an interpolation of rule based reasoning, and judgment, while the latter is more closely tied to recall and process adaptation. The difference is clear in their attitude toward error and appellate"}
{"doc_id": "Case-based reasoning", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", limit of negotiated agreement, etc. While CBR in law and cognitively inspired CBR have long been associated, the former is more clearly an interpolation of rule based reasoning, and judgment, while the latter is more closely tied to recall and process adaptation. The difference is clear in their attitude toward error and appellate review.\nAnother name for case-based reasoning in problem solving is symptomatic strategies. It does require à priori domain knowledge that is gleaned from past experience which established connections between symptoms and causes. This knowledge is referred to as shallow, compiled, evidential, history-based as well as case-based knowledge. This is the strategy most associated with diagnosis by experts. Diagnosis of a problem transpires as a rapid recognition process in which symptoms evoke appropriate situation categories. An expert knows the cause by virtue of having previously encountered similar cases. Case-based reasoning is the most powerful strategy, and that used most commonly. However, the strategy won't work independently with truly novel problems, or where deeper understanding of whatever is taking place is sought.\nAn alternative approach to problem solving is the topographic strategy which falls into the category of deep reasoning. With deep reasoning, in-depth knowledge of a system is used. Topography in this context means a description or an analysis of a structured entity, showing the relations among its elements.\nAlso known as reasoning from first principles, deep reasoning is applied to novel faults when experience-based approaches aren't viable. The topographic strategy is therefore linked to à priori domain knowledge that is developed from a more a fundamental understanding of a system, possibly using first-principles knowledge. Such knowledge is referred to as deep, causal or model-based knowledge. \nHoc and Carlier noted that symptomatic approaches may need to be supported by topographic approaches because symptoms can be defined in diverse terms. The converse is also true – shallow reasoning can be used abductively to generate causal hypotheses, and deductively to evaluate those hypotheses, in a topographical search.\n\nCriticism\nCritics of CBR argue that it is an approach that accepts anecdotal evidence as its main operating principle. Without statistically relevant data for backing and implicit generalization, there is no guarantee that the generalization is correct. However, all inductive reasoning where data is too scarce for statistical relevance is inherently based on anecdotal evidence.\n\nHistory\nCBR traces its roots to the work of Roger Schank and his students at Yale University in the early 1980s. Schank's model of dynamic memory was the basis for the earliest CBR systems:"}
{"doc_id": "Case-based reasoning", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " inductive reasoning where data is too scarce for statistical relevance is inherently based on anecdotal evidence.\n\nHistory\nCBR traces its roots to the work of Roger Schank and his students at Yale University in the early 1980s. Schank's model of dynamic memory was the basis for the earliest CBR systems: Janet Kolodner's CYRUS and Michael Lebowitz's IPP.\nOther schools of CBR and closely allied fields emerged in the 1980s, which directed at topics such as legal reasoning, memory-based reasoning (a way of reasoning from examples on massively parallel machines), and combinations of CBR with other reasoning methods. In the 1990s, interest in CBR grew internationally, as evidenced by the establishment of an International Conference on Case-Based Reasoning in 1995, as well as European, German, British, Italian, and other CBR workshops.\nCBR technology has resulted in the deployment of a number of successful systems, the earliest being Lockheed's CLAVIER, a system for laying out composite parts to be baked in an industrial convection oven. CBR has been used extensively in applications such as the Compaq SMART system and has found a major application area in the health sciences, as well as in structural safety management.\nThere is recent work that develops CBR within a statistical framework and formalizes case-based inference as a specific type of probabilistic inference. Thus, it becomes possible to produce case-based predictions equipped with a certain level of confidence.\nOne description of the difference between CBR and induction from instances is that statistical inference aims to find what tends to make cases similar while CBR aims to encode what suffices to claim similarly.\n\nSee also\nAI alignment\nArtificial intelligence detection software\nAbductive reasoning\nDuck test\nI know it when I see it\nCommonsense reasoning\nPurposeful omission\nDecision tree\nGenetic algorithm\nPattern matching\nAnalogy\nK-line (artificial intelligence)\nRipple down rules\nCasuistry\nSimilarity heuristic\n\nNotes and references\nFurther reading\nAamodt, Agnar, and Enric Plaza. \"Case-Based Reasoning: Foundational Issues, Methodological Variations, and System Approaches\" Artificial Intelligence Communications 7, no. 1 (1994): 39–52.\nAlthoff, Klaus-Dieter, Ralph Bergmann, and L. Karl Branting, eds. Case-Based Reasoning Research and Development: Proceedings of the Third International Conference on Case-Based Reasoning."}
{"doc_id": "Case-based reasoning", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "aches\" Artificial Intelligence Communications 7, no. 1 (1994): 39–52.\nAlthoff, Klaus-Dieter, Ralph Bergmann, and L. Karl Branting, eds. Case-Based Reasoning Research and Development: Proceedings of the Third International Conference on Case-Based Reasoning. Berlin: Springer Verlag, 1999.\nBergmann, Ralph Experience Management: Foundations, Development Methodology, and Internet-Based Applications. Springer, LNAI 2432, 2002.\nBergmann, R., Althoff, K.-D., Breen, S., Göker, M., Manago, M., Traphöner, R., and Wess, S. Developing industrial case-based reasoning applications: The INRECA methodology. Springer LNAI 1612, 2003.\nKolodner, Janet. Case-Based Reasoning. San Mateo: Morgan Kaufmann, 1993.\nLeake, David. \"CBR in Context: The Present and Future\", In Leake, D., editor, Case-Based Reasoning: Experiences, Lessons, and Future Directions. AAAI Press/MIT Press, 1996, 1-30.\nLeake, David, and Enric Plaza, eds. Case-Based Reasoning Research and Development: Proceedings of the Second International Conference on Case-Based Reasoning. Berlin: Springer Verlag, 1997.\nLenz, Mario; Bartsch-Spörl, Brigitte; Burkhard, Hans-Dieter; Wess, Stefan, eds. (1998). Case-Based Reasoning Technology: From Foundations to Applications. Lecture Notes in Artificial Intelligence. Vol. 1400. Springer. doi:10.1007/3-540-69351-3. ISBN 978-3-540-64572-6. S2CID 10517603.\nOxman, Rivka. Precedents in Design: a Computational Model for the Organization of Precedent Knowledge, Design Studies, Vol. 15 No. 2 pp. 141–157\nRiesbeck, Christopher, and Roger Schank. Inside Case-based Reasoning. Northvale, NJ: Erlbaum, 1989.\nVeloso, Manuela, and Agnar Aamodt, eds. Case-Based Reasoning Research and Development: Proceedings of the First International Conference on Case-Based Reasoning. Berlin: Springer Verlag, 1995.\nWatson, Ian"}
{"doc_id": "Case-based reasoning", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ing. Northvale, NJ: Erlbaum, 1989.\nVeloso, Manuela, and Agnar Aamodt, eds. Case-Based Reasoning Research and Development: Proceedings of the First International Conference on Case-Based Reasoning. Berlin: Springer Verlag, 1995.\nWatson, Ian. Applying Case-Based Reasoning: Techniques for Enterprise Systems. San Francisco: Morgan Kaufmann, 1997. https://a.co/d/g84Vai9"}
{"doc_id": "Character computing", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Character computing is a trans-disciplinary field of research at the intersection of computer science and psychology. It is any computing that incorporates the human character within its context. Character is defined as all features or characteristics defining an individual and guiding their behavior in a specific situation. It consists of stable trait markers (e.g., personality, background, history, socio-economic embeddings, culture,...) and variable state markers (emotions, health, cognitive state, ...). Character computing aims at providing a holistic psychologically driven model of human behavior. It models and predicts behavior based on the relationships between a situation and character. Three main research modules fall under the umbrella of character computing: character sensing and profiling, character-aware adaptive systems, and artificial characters.\n\nOverview\nCharacter computing can be viewed as an extension of the well-established field of affective computing. Based on the foundations of the different psychology branches, it advocates defining behavior as a compound attribute that is not driven by either personality, emotions, situation or cognition alone. It rather defines behavior as a function of everything that makes up an individual i.e., their character and the situation they are in. Affective computing aims at allowing machines to understand and translate the non-verbal cues of individuals into affect. Accordingly, character computing aims at understanding the character attributes of an individual and the situation to translate it to predicted behavior, and vice versa.\n''In practical terms, depending on the application context, character computing is a branch of research that deals with the design of systems and interfaces that can observe, sense, predict, adapt to, affect, understand, or simulate the following: character based on behavior and situation, behavior based on character and situation, or situation based on character and behavior.'' The Character-Behavior-Situation (CBS) triad is at the core of character computing and defines each of the three edges based on the other two.\nCharacter computing relies on simultaneous development from a computational and psychological perspective and is intended to be used by researchers in both fields. Its main concept is aligning the computational model of character computing with empirical results from in-lab and in-the-wild psychology experiments. The model is to be continuously built and validated through the emergence of new data. Similar to affective and personality computing, the model is to be used as a base for different applications towards improving user experience.\n\nHistory\nCharacter computing as such was first coined in its first workshop in 2017. Since then it has had 3 international workshops and numerous publications. Despite its young age, it has already drawn some interest in the research community"}
{"doc_id": "Character computing", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", the model is to be used as a base for different applications towards improving user experience.\n\nHistory\nCharacter computing as such was first coined in its first workshop in 2017. Since then it has had 3 international workshops and numerous publications. Despite its young age, it has already drawn some interest in the research community, leading to the publication of the first book under the same title in early 2020 published by Springer Nature.\nResearch that can be categorized under the field dates much older than 2017. The notion of combining several factors towards the explanation of behavior or traits and states has long been investigated in both Psychology and Computer Science, for example.\n\nCharacter\nThe word character originates from the Greek word meaning “stamping tool”, referring to distinctive features and traits. Over the years it has been given many different connotations, like the moral character in philosophy, the temperament in psychology, a person in literature or an avatar in various virtual worlds, including video games. According to character computing character is a unification of all the previous definitions, by referring back to the original meaning of the word. Character is defined as the holistic concept representing all interacting trait and state markers that distinguish an individual. Traits are characteristics that mainly remain stable over time. Traits include personality, affect, socio-demographics, and general health. States are characteristics that vary in short periods of time. They include emotions, well-being, health, cognitive state. Each characteristic has many representation methods and psychological models. The different models can be combined or one model can be preset for each characteristic. This depends on the use-case and the design choices.\n\nAreas\nResearch into character computing can be divided into three areas, which complement each other but can each be investigated separately. The first area is sensing and predicting character states and traits or ensuing behavior. The second area is adapting applications to certain character states or traits and the behavior they predict. It also deals with trying to change or monitor such behavior. The final area deals with creating artificial agents e.g., chatbots or virtual reality avatars that exhibit certain characteristics.\nThe three areas are investigated separately and build on existing findings in the literature. The results of each of the three areas can also be used as a stepping stone for the next area. Each of the three areas has already been investigated on its own in different research fields with focus on different subsets of character. For example, affective computing and personality computing both cover different areas with a focus on some character components without the others to account for human behavior.\n\nThe Character-Behavior-Sit"}
{"doc_id": "Character computing", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the next area. Each of the three areas has already been investigated on its own in different research fields with focus on different subsets of character. For example, affective computing and personality computing both cover different areas with a focus on some character components without the others to account for human behavior.\n\nThe Character-Behavior-Situation triad\nCharacter computing is based on a holistic psychologically driven model of human behavior. Human behavior is modeled and predicted based on the relationships between a situation and a human's character. To further define character in a more formal or holistic manner, we represent it in light of the Character–Behavior–Situation triad. This highlights that character not only determines who we are but how we are, i.e., how we behave. The triad investigated in Personality Psychology is extended through character computing to the Character–Behavior–Situation triad. Any member of the CBS triad is a function of the two other members, e.g., given the situation and personality, the behavior can be predicted. Each of the components in the triad can be further decomposed into smaller units and features that may best represent the human's behavior or character in a particular situation. Character is thus behind a person's behavior in any given situation. While this is a causality relation, the correlation between the three components is often more easily used to predict the components that are most difficult to measure from those measured more easily. There are infinitely many components to include in the representation of any of C, B, and S. The challenge is always to choose the smallest subset needed for prediction of a person's behavior in a particular situation."}
{"doc_id": "Class activation mapping", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Class activation mapping methods are explainable AI (XAI) techniques used to visualize the regions of an input image that are the most relevant for a particular task, especially image classification, in convolutional neural networks (CNNs). These methods generate heatmaps by weighting the feature maps from a convolutional layer according to their relevance to the target class.\nIn the field of artificial intelligence, generically defined as \"the effort to automate intellectual tasks normally performed by humans\", machine learning and deep learning were created. They both use statistical and computational methods to learn patterns from data, reducing the need for manually coded rules.\nMachine learning models are trained on input data and the known respective answers, learning the underlying patterns or structures present in the data. Traditional Machine learning algorithms employ manually designed feature sets, posing a direct link between machine learning designers and employed features.\nDeep learning is a subfield of machine learning, based on the concept of successive layers of representation, in which the data is progressively unfolded in different ways, to extract relevant and informative patterns in data analysis. Deep learning algorithms are defined as feature learning algorithms automatically learning hierarchical feature representations from raw data, extracting increasingly abstract features through multiple layers.\nCNNs are a specific architecture of deep learning models, designed to process spatially structured data, such as images, exploiting a series of convolution, non-linear activation and pooling operations to extract relevant features, contained in the so-called feature maps from input data. CNNs have demonstrated to be highly effective in a variety of computer vision and image processing tasks.\nCNNs (and deep learning models more broadly) are described as black boxes due to their complex and non-transparent internal layers of representation. The need for clearer indications on its internal working and decision-making process gave birth to XAI techniques.\nAmong the proposed XAI techniques for computer vision tasks, Class activation mapping methods can show which pixels in an input image are important to the predicted logit for a class of interest, in a classification task.\nClass activation mapping methods were originally developed for class-discriminative scenarios to visualize which parts of the input image influenced the classification decision. Namely, to visually highlight the regions of those feature maps which contribute most strongly to the prediction of a given class. \nMore advanced versions of these methods are not limited to image classification tasks, but have been extended also to several vision-related tasks, such as object detection, image captioning, visual question answering and image segmentation.\n\nBackground\nThe following methods laid the groundwork for the class activation maps approaches, forming the conceptual basis of using gradients to highlight class-d"}
{"doc_id": "Class activation mapping", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of these methods are not limited to image classification tasks, but have been extended also to several vision-related tasks, such as object detection, image captioning, visual question answering and image segmentation.\n\nBackground\nThe following methods laid the groundwork for the class activation maps approaches, forming the conceptual basis of using gradients to highlight class-discriminative regions.\n\nClass model visualization and saliency maps for convolutional neural networks\nThe class model visualization and image-specific saliency maps approaches have been presented in the foundational work \"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\" by Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman\n and it generalizes the deconvnet method by Zeiler and Fergus.\n\nClass model visualization synthesizes an artificial input image that strongly activates the output neurons associated with a target class. Given a trained, fixed model, this method starts with a zero-initialized image, backpropagates the gradients from the class score to the image pixels, updates the image pixels increasing the specific class scores and it repeats the pixel updating process, showing an encoded (idealized version) prototype of the class of interest.\nImage-specific class saliency visualization method provides a visual explanation by highlighting the most relevant pixels in an image for predicting a certain class C of interest. This is done by computing the gradient of the class score with respect to the input image, \n  \n    \n      \n        \n          I\n          \n            0\n          \n        \n        ,\n      \n    \n    {\\displaystyle I_{0},}\n  \n \n  \n    \n      \n        w\n        =\n        \n          \n            \n            \n              \n                \n                  ∂\n                  \n                    S\n                    \n                      C\n                    \n                  \n                \n                \n                  ∂\n                  I\n                \n              \n            \n            |\n          \n          \n            \n              I\n              \n                0\n              \n            \n          \n        \n      \n    \n    {\\displaystyle w=\\left.{\\frac {\\partial S_{C}}{\\partial I}}\\right|_{I_{0}}}\n  \n approximating the model locally (around \n  \n    \n      \n        \n          I\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle I_{0}}\n  \n) as linear, using a first-order Taylor expansion: \n  \n    \n      \n        \n          S\n          \n            C\n          \n        \n        (\n        I\n        )\n        ≈\n        \n          w\n          \n            C\n          \n          \n            T\n          \n        \n        I\n        +\n        b\n      \n    \n    {\\displaystyle S_{C}(I)\\approx w_{C}^{T}I+b}\n  \n. The"}
{"doc_id": "Class activation mapping", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "            C\n          \n        \n        (\n        I\n        )\n        ≈\n        \n          w\n          \n            C\n          \n          \n            T\n          \n        \n        I\n        +\n        b\n      \n    \n    {\\displaystyle S_{C}(I)\\approx w_{C}^{T}I+b}\n  \n. The magnitude of \n  \n    \n      \n        \n          w\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{C}}\n  \n, the gradient, indicates the importancy of the pixels: larger gradients suggest greater influence on the prediction. Once the gradient is known, the saliency map is defined as the maximum absolute gradient across the color channels: \n  \n    \n      \n        \n          M\n          \n            i\n            j\n          \n        \n        =\n        m\n        a\n        \n          x\n          \n            C\n          \n        \n        \n          |\n          \n            \n              \n                ∂\n                \n                  S\n                  \n                    C\n                  \n                \n              \n              \n                ∂\n                \n                  I\n                  \n                    i\n                    j\n                  \n                  \n                    C\n                  \n                \n              \n            \n          \n          |\n        \n      \n    \n    {\\displaystyle M_{ij}=max_{C}\\left|{\\frac {\\partial S_{C}}{\\partial I_{ij}^{C}}}\\right|}\n  \n  resulting in an saliency map (i.e. heatmap).\n\nGuided backpropagation\nThe concept of guided backpropagation can be traced for the first time in the paper by Springenberg et al. \"Striving For Simplicity: The All Convolutional Net\"  and also this method builds upon the work by Zeiler and Fergus \"Visualizing and Understanding Convolutional Networks\".\n\nGuided backpropagation core is to understand what a CNN is learning, by visualizing the patterns that activate more strongly individual neurons (or filters), in architectures which do not rely on max-pooling layer.\nWhen propagating gradients back through a rectified linear unit (ReLU), guided backpropagation passes the gradient if and only if the input to the ReLU was positive (forward pass) and the output gradient is positive (backward signal), tackling both inactive neurons, negative gradients and suppressing the noise. The result displays sharper, high-resolution visualizations of what each neuron is responding to.\nGuided backpropagation represents a simple and practical method for model interpretability, helping understand how and where neural networks detect semantic concepts across layers. Moreover, it can be applied to any network architecture, due to its working principle.\n\nBase versions\nClass activation mapping and gradient-weighted class"}
{"doc_id": "Class activation mapping", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " what each neuron is responding to.\nGuided backpropagation represents a simple and practical method for model interpretability, helping understand how and where neural networks detect semantic concepts across layers. Moreover, it can be applied to any network architecture, due to its working principle.\n\nBase versions\nClass activation mapping and gradient-weighted class activation mapping are the original and most widely used methods for visual explanations in convolutional neural networks. These methods serve as the foundation for many later developments in explainable AI.\nNotation: In this article, the symbols i and j represent integer indices that disappear inside sums or averages, while x and y are the continuous (or up-sampled integer) coordinates of the final heat-map that is plotted.\n\nClass activation mapping (CAM)\nClass activation mapping (CAM) was the first, and the original, version of CAM methods, and it gave the name to the whole category. The approach was firstly introduced by Zhou et al. in their seminal work \"Learning Deep Features for Discriminative Localization\".\nThis approach achieves class-specific heatmaps by modifying image classification CNN architectures, replacing fully-connected layers with convolutional layers and a final global average pooling layer.\nIts main scope is to localize and highlight discriminative regions of an input image that a CNN uses to identify a particular class, without needing explicit bounding box annotations.\n\nGlobal average pooling (GAP)\nGlobal average pooling (GAP) represents the key element in the original CAM approach.\nIt is a dimensionality reduction technique and, similarly to other pooling layers, it allows the downsampling of the feature maps, calculating representative values for a specific region of the feature map. The particularity of GAP is that it calculates a single value for an entire feature map, significantly reducing the model dimensions.\n\nMathematical description\nThe mathematical description considers as its key the combination of convolutional and GAP layers.\nIn CAM, it is mandatory to have the GAP layer after the last convolutional layer and before the final linear classifier layer. This last element of the architecture connects the output logits (the network predictions) \n  \n    \n      \n        \n          y\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle y^{C}}\n  \n, to the GAP values, with its respective fine-tuned weights, \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n.\nConsidering \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle A^{k}}\n  \n as the last feature maps"}
{"doc_id": "Class activation mapping", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n.\nConsidering \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle A^{k}}\n  \n as the last feature maps of the last convolutional layer, GAP produces one value for each feature map, by averaging all the matrix elements (i, j) of the feature map:\n\n  \n    \n      \n        \n          F\n          \n            k\n          \n        \n        =\n        \n          \n            1\n            \n              m\n              n\n            \n          \n        \n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        \n          ∑\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          A\n          \n            i\n            j\n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle F^{k}={\\frac {1}{mn}}\\sum _{i=1}^{m}\\sum _{j=1}^{n}A_{ij}^{k}}\n  \n\nwith\n\n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    A\n                    \n                      11\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  \n                    A\n                    \n                      12\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    A\n                    \n                      1\n                      n\n                    \n                    \n                      k\n                    \n                  \n                \n              \n              \n                \n                  \n                    A\n                    \n                      21\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  \n                    A\n                    \n                      22\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    A\n                    \n                      2\n                      n\n                    \n                    \n                      k\n                    \n                  \n                \n              \n              \n                \n                  ⋮\n                \n                \n                  ⋮\n                \n                \n                  ⋱\n                \n                \n                  ⋮\n                \n              \n              \n                \n                  \n                    A\n                    \n                      m\n                      1\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  \n                    A\n                    \n                      m\n                      2\n                    \n                    \n                      k\n                    \n                  \n                \n                \n                  ⋯\n                \n                \n                  \n                    A\n                    \n                      m\n                      n\n                    \n                    \n                      k\n                    \n                  \n                \n              \n            \n            ]\n          \n        \n        =\n        \n          {\n          \n            \n              A\n              \n                i\n                j\n              \n              \n                k\n              \n            \n            ∣\n            1\n            ≤\n            i\n            ≤\n            m\n            ,\n             \n            1\n            ≤\n            j\n            ≤\n            n"}
{"doc_id": "Class activation mapping", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "          \n        \n        =\n        \n          {\n          \n            \n              A\n              \n                i\n                j\n              \n              \n                k\n              \n            \n            ∣\n            1\n            ≤\n            i\n            ≤\n            m\n            ,\n             \n            1\n            ≤\n            j\n            ≤\n            n\n          \n          }\n        \n      \n    \n    {\\displaystyle A^{k}={\\begin{bmatrix}A_{11}^{k}&A_{12}^{k}&\\cdots &A_{1n}^{k}\\\\A_{21}^{k}&A_{22}^{k}&\\cdots &A_{2n}^{k}\\\\\\vdots &\\vdots &\\ddots &\\vdots \\\\A_{m1}^{k}&A_{m2}^{k}&\\cdots &A_{mn}^{k}\\end{bmatrix}}=\\left\\{A_{ij}^{k}\\mid 1\\leq i\\leq m,\\ 1\\leq j\\leq n\\right\\}}\n  \n\nNamely, in the GAP layer, each feature map is reduced to a single scalar via GAP, producing k values, hence reducing the dimensionality of the network. A k\n  \n    \n      \n        ×\n      \n    \n    {\\displaystyle \\times }\n  \nm\n  \n    \n      \n        ×\n      \n    \n    {\\displaystyle \\times }\n  \nn tensor is reduced to k scalars, shrinking the parameter count for the linear classifier head.\nThe final output logits are calculated as the linear sum of the GAP values, weights and bias:\n\n  \n    \n      \n        \n          y\n          \n            C\n          \n        \n        =\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        \n          F\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle y^{C}=\\sum _{k}w_{k}^{C}F^{k}}\n  \n\nThe localization map is computed as follows:\n\n  \n    \n      \n        \n          L\n          \n            C\n            A\n            M\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n       "}
{"doc_id": "Class activation mapping", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle L_{CAM}^{C}(x,y)=ReLU(\\sum _{k}w_{k}^{C}A_{k}(x,y))}\n  \n\nnamely, \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle A_{k}(x,y)}\n  \n is the activation of node k in the target layer of the model, and \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n is the class-specific weight, for the channel k, in the linear classifier layer.\n\nAdvantages and drawbacks\nThe use of the GAP layer represents an example of an interpretability by design (IBD) approach. IBD refers to a technique which uses the model's own architecture to help explain its predictions.\nThe main drawback of CAM is that it is highly model-specific, being applicable to CNN architectures whose layer before the softmax one is a GAP.\nSince the approach relies on the post-GAP weights for the overall evaluation, the method can't be applied to intermediate layers.\nThe choice of dealing with an IBD approach restricts the possibility to generalize the model architecture. Moreover, IBD methods often require re-training of the model.\n\nGradient-weighted class activation mapping (Grad-CAM)\nGradient-weighted class activation mapping (Grad-CAM) is a generalized version of CAM and it tackles its architectural limitations. Grad-CAM computes the gradient of a target class score, the pre-softmax logit, with respect to the feature maps of a convolutional neural network. The gradients are global-average-pooled to obtain importance weights, which are used to compute a class-specific localization map by linearly weighting the feature maps. The result is a heatmap that highlights the regions in the input image that are the most influential for predicting the target class.\nThe main advantage of Grad-CAM, with respect to the standard CAM, is that it is model agnostic (provided that the network still needs to be differentiable), meaning that it generates visual explanation for any CNN-based network without architectural changes or re-training,"}
{"doc_id": "Class activation mapping", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " image that are the most influential for predicting the target class.\nThe main advantage of Grad-CAM, with respect to the standard CAM, is that it is model agnostic (provided that the network still needs to be differentiable), meaning that it generates visual explanation for any CNN-based network without architectural changes or re-training, making it broadly applicable to pre-trained models.\n\nMathematical description\nConsidering:\n\ny\n  \n    \n      \n        \n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle ^{C}}\n  \n the logits (i.e. the pre-softmax activated neurons responsible for a certain class prediction) of interest;\nA\n  \n    \n      \n        \n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle ^{k}}\n  \n the feature activated map for a specific convolutional layer;\nL\n  \n    \n      \n        \n          \n          \n            Grad-CAM\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle _{\\text{Grad-CAM}}^{C}}\n  \n ∈ \n  \n    \n      \n        \n          \n            R\n          \n          \n            u\n            ×\n            v\n          \n        \n      \n    \n    {\\displaystyle \\mathbb {R} ^{u\\times v}}\n  \n the class-discriminative localization map, of width u and height v for any class c;\nGrad-CAM, employing backpropagation, computes the logit gradient with respect to the feature map A\n  \n    \n      \n        \n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle ^{k}}\n  \n as\n\n  \n    \n      \n        \n          \n            \n              ∂\n              \n                \n                  y\n                  \n                    C\n                  \n                \n              \n            \n            \n              ∂\n              \n                \n                  A\n                  \n                    k\n                  \n                \n              \n              (\n              i\n              ,\n              j\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\partial {y^{C}}}{\\partial {A^{k}}(i,j)}}}\n  \n\nhighlighting the importance of a certain class discrimination decision process of the logit.\nThese gradients are global-average-pooled over each element of the feature map (hence, highlighting the \"importance\" of the elements of a feature map k for a target class C):\n\n  \n    \n      \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        =\n        \n          \n            1\n            \n              u\n              v\n            \n          \n        \n        \n          ∑\n          \n            i\n          \n        \n        \n          ∑\n          \n            j\n          \n        \n        \n          \n            \n              ∂\n              \n                \n                  y\n                  \n                    C\n                  \n                \n              \n            \n"}
{"doc_id": "Class activation mapping", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " k\n          \n          \n            C\n          \n        \n        =\n        \n          \n            1\n            \n              u\n              v\n            \n          \n        \n        \n          ∑\n          \n            i\n          \n        \n        \n          ∑\n          \n            j\n          \n        \n        \n          \n            \n              ∂\n              \n                \n                  y\n                  \n                    C\n                  \n                \n              \n            \n            \n              ∂\n              \n                \n                  A\n                  \n                    k\n                  \n                \n              \n              (\n              i\n              ,\n              j\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha _{k}^{C}={\\frac {1}{uv}}\\sum _{i}\\sum _{j}{\\frac {\\partial {y^{C}}}{\\partial {A^{k}}(i,j)}}}\n  \n\nSo, to account for the total number of feature maps, each of them is multiplied by its weight (via dot-product) and element-wise summation is done:\n\n  \n    \n      \n        \n          ∑\n          \n            k\n          \n        \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle \\sum _{k}\\alpha _{k}^{C}A^{k}}\n  \n\nIt can be observed that, due to the intrinsic nature of the gradient operation, some elements of the weighted feature map will have negative value, so, since only elements that have increased the logit of the predicted class are of interest, a ReLU activation function is applied:\n\n  \n    \n      \n        \n          L\n          \n            G\n            r\n            a\n            d\n            −\n            C\n            A\n            M\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          ∑\n          \n            k\n          \n        \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle L_{Grad-CAM}^{C}(x,y)=ReLU(\\sum _{k}\\alpha _{k}^{C}A^{k}(x,y))}\n  \n\nLastly, the output heatmap image dimensions are upsampled to the original image size to match the input dimensions.\n\nAdvantages and drawbacks\nGrad-CAM addresses the most important CAM limitations. It makes CAM free from the GAP layer need, generalizing its behavior and enabling visual explanation"}
{"doc_id": "Class activation mapping", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "}A^{k}(x,y))}\n  \n\nLastly, the output heatmap image dimensions are upsampled to the original image size to match the input dimensions.\n\nAdvantages and drawbacks\nGrad-CAM addresses the most important CAM limitations. It makes CAM free from the GAP layer need, generalizing its behavior and enabling visual explanation at intermediate layers.\nHowever, Grad-CAM focuses on the most discriminative region when contributing to classification. If multiple similar objects are present, Grad-CAM often highlights only one of them, or part of one, providing also coarser maps and lower localization accuracy.\nMoreover, Grad-CAM retrieves backwards information (the gradients), without taking into consideration how the activation flowed forward during prediction (unless combined with the guided backpropagation technique), resulting in a certain probability of missing patterns highlighted in the forward signal. On top of that, Grad-CAM heat maps are low-resolution when choosing a very deep layer.\nLastly, false emphasis in the heatmap may be present when large gradients are computed for low activation values. Grad-CAM assumes that gradient implies importance, ignoring the activation features value.\n\nGrad-CAM and CAM comparison\nFine-tuned versions\nSeveral methods have refined Grad-CAM to improve clarity and flexibility. Guided Grad-CAM, Grad-CAM++, Score-CAM, and Layer-CAM enhance aspects such as localization accuracy, gradient independence, and multi-layer visualization. These techniques build directly on the principles of CAM and Grad-CAM.\n\nGuided Grad-CAM\nGuided Grad-CAM fuses the coarse, class‐discriminative localization of Grad-CAM with the high‐resolution details of guided backpropagation. Grad-CAM heatmap is first computed for the target class, and it is upsampled to the input size. Then, a Guided Backpropagation saliency map for the same class is computed. A final element‐wise product of the two results in the Guided Grad-CAM visualization map.\nThe result is a high-resolution, class-specific saliency map that highlights exactly which pixels contribute most to the network's decision.\n\nGrad-CAM++\nGrad-CAM++ introduces a more refined way of computing the weights for each feature map, bypassing the global average of the gradients approach provided by Grad-CAM. This approach aims to improve the visual effect when multiple target instances are present in a single image.\nSpecifically, Grad-CAM++ employs pixel-wise gradients (via higher-order gradients), to compute the importance of a specific pixel for a prediction, lighting up multiple object instances in the same"}
{"doc_id": "Class activation mapping", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " gradients approach provided by Grad-CAM. This approach aims to improve the visual effect when multiple target instances are present in a single image.\nSpecifically, Grad-CAM++ employs pixel-wise gradients (via higher-order gradients), to compute the importance of a specific pixel for a prediction, lighting up multiple object instances in the same image.\nThese improvements allow for a more sensible and detailed output heatmap.\nThe associated mathematical framework is defined by the following localization map:\n\n  \n    \n      \n        \n          L\n          \n            G\n            r\n            a\n            d\n            −\n            C\n            A\n            M\n            +\n            +\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle L_{Grad-CAM++}^{C}(x,y)=\\sum _{k}w_{k}^{C}A_{k}(x,y)}\n  \n\nin which the coefficient \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n is defined as:\n\n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        =\n        \n          ∑\n          \n            i\n            ,\n            j\n          \n        \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        i\n        ,\n        j\n        )\n        ×\n        R\n        e\n        L\n        U\n        (\n        \n          \n            \n              ∂\n              \n                y\n                \n                  C\n                \n              \n            \n            \n              ∂\n              \n                A\n                \n                  k\n                \n              \n              (\n              i\n              ,\n              j\n              )\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle w_{k}^{C}=\\sum _{i,j}\\alpha _{k}^{C}(i,j)\\times ReLU({\\frac {\\partial y^{C}}{\\partial A_{k}(i,j)}})}\n  \n\nwith \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle A^{k}(x,y)}\n  \n, the activation of node k in the target layer of the model at position (x,y); \n  \n    \n      \n"}
{"doc_id": "Class activation mapping", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ")}\n  \n\nwith \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle A^{k}(x,y)}\n  \n, the activation of node k in the target layer of the model at position (x,y); \n  \n    \n      \n        \n          y\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle y^{C}}\n  \n the logit score for class C, and \n  \n    \n      \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        i\n        ,\n        j\n        )\n      \n    \n    {\\displaystyle \\alpha _{k}^{C}(i,j)}\n  \n being:\n\n  \n    \n      \n        \n          α\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        i\n        ,\n        j\n        )\n        =\n        \n          \n            \n              \n                \n                  ∂\n                  \n                    2\n                  \n                \n                \n                  y\n                  \n                    C\n                  \n                \n              \n              \n                ∂\n                (\n                \n                  A\n                  \n                    k\n                  \n                \n                (\n                i\n                ,\n                j\n                )\n                \n                  )\n                  \n                    2\n                  \n                \n              \n            \n            \n              2\n              ×\n              \n                \n                  \n                    \n                      ∂\n                      \n                        2\n                      \n                    \n                    \n                      y\n                      \n                        C\n                      \n                    \n                  \n                  \n                    ∂\n                    (\n                    \n                      A\n                      \n                        k\n                      \n                    \n                    (\n                    i\n                    ,\n                    j\n                    )\n                    \n                      )\n                      \n                        2\n                      \n                    \n                  \n                \n              \n              +\n              \n                ∑\n                \n                  a\n                  ,\n                  b\n                \n              \n              \n                A\n                \n                  k\n                \n              \n              (\n              a\n              ,\n              b\n              )\n              ×\n              \n                \n                  \n                    \n                      ∂\n                      \n                        3\n                      \n                    \n                    \n                      y\n                      \n                        C\n                      \n                    \n                  \n                  \n                    ∂\n                    (\n                    \n                      A\n                      \n                        k\n                      \n                    \n                    (\n                    i\n                    ,\n                    j\n                    )\n                    \n                      )\n                      \n                        3\n                      \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha _{k}^{C}(i,j)={\\frac {\\frac {\\partial ^{2}y^{C}}{\\partial (A_{k}(i,j))^{2}}}{2\\times {\\frac {\\partial ^{2}y^{C}}{\\partial (A_{k}(i,j))^{2}}}+\\sum _{a,b}A_{"}
{"doc_id": "Class activation mapping", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "frac {\\frac {\\partial ^{2}y^{C}}{\\partial (A_{k}(i,j))^{2}}}{2\\times {\\frac {\\partial ^{2}y^{C}}{\\partial (A_{k}(i,j))^{2}}}+\\sum _{a,b}A_{k}(a,b)\\times {\\frac {\\partial ^{3}y^{C}}{\\partial (A_{k}(i,j))^{3}}}}}}\n  \n\nWhile addressing some Grad-CAM problems, Grad-CAM++ method still relies on gradients, and it only improves the underlying math. It is, however, still based on the idea of assigning a direct and valid relationship between gradient and importance.\nNotation: (a,b) indexes all pixel positions in the feature‐map, exactly like (i,j) does, but for the summation in the denominator.\n\nScore-CAM\nScore-CAM is a gradient-free CAM technique, thus redefining the original Grad-CAM and Grad-CAM++ working principles. It uses the model confidence scores instead of gradients.\nScore-CAM performs the following operations:\n\nExtracts the feature maps \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle A_{k}}\n  \n of the final convolutional layers, as in the original Grad-CAM;\nUpsamples each activation map \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle A_{k}}\n  \n to the same input image dimensions, defining a mask \n  \n    \n      \n        \n          M\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle M_{k}}\n  \n and each mask is normalized;\nMultiplies the original input image by the mask, defining a masked image \n  \n    \n      \n        \n          X\n          \n            k\n          \n          ′\n        \n        =\n        \n          M\n          \n            k\n          \n        \n        ⊙\n        X\n      \n    \n    {\\displaystyle X'_{k}=M_{k}\\odot X}\n  \n (\n  \n    \n      \n        ⊙\n      \n    \n    {\\displaystyle \\odot }\n  \n is the element-wise multiplication);\nGets a confidence score (a softmax probability is the output value after the softmax operation; the logit is the value before the softmax) for the masked images \n  \n    \n      \n        \n          X\n          \n            k\n          \n          ′\n        \n      \n    \n    {\\displaystyle X'_{k}}\n  \n, by feeding it into the CNN (either the soft-max probability or the raw log"}
{"doc_id": "Class activation mapping", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the softmax operation; the logit is the value before the softmax) for the masked images \n  \n    \n      \n        \n          X\n          \n            k\n          \n          ′\n        \n      \n    \n    {\\displaystyle X'_{k}}\n  \n, by feeding it into the CNN (either the soft-max probability or the raw logit can be used; both yield similar results in practice);\nConsiders that confidence score as the weight \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            c\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{c}}\n  \n for the feature map \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle A_{k}}\n  \n.\nThese operations allow to replace the gradient calculations with the actual model outputs, building more accurate heatmaps.\nMathematically, the localization map is defined as:\n\n  \n    \n      \n        \n          L\n          \n            S\n            c\n            o\n            r\n            e\n            −\n            C\n            A\n            M\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            c\n          \n        \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle L_{Score-CAM}^{C}(x,y)=ReLU(\\sum _{k}w_{k}^{c}A_{k}(x,y))}\n  \n\nand the coefficient \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n s:\n\n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        =\n        s\n        o\n        f\n        t\n        m\n        a\n        \n          x\n          \n            k\n          \n        \n        (\n        \n          y\n          \n            C\n          \n        \n        (\n        \n          X\n          \n            k\n          \n          ′\n        \n        )\n        )\n        =\n        \n          \n            \n              e\n              x\n              p\n              (\n              \n                y\n                \n                  C\n                \n              \n              (\n              \n                X\n                \n                  k\n                \n                ′\n              \n              )\n              )\n            \n            \n              \n                ∑\n                \n                  m\n                \n              \n              e\n              x\n              p\n              (\n"}
{"doc_id": "Class activation mapping", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        =\n        \n          \n            \n              e\n              x\n              p\n              (\n              \n                y\n                \n                  C\n                \n              \n              (\n              \n                X\n                \n                  k\n                \n                ′\n              \n              )\n              )\n            \n            \n              \n                ∑\n                \n                  m\n                \n              \n              e\n              x\n              p\n              (\n              \n                y\n                \n                  C\n                \n              \n              (\n              \n                X\n                \n                  m\n                \n                ′\n              \n              )\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}=softmax_{k}(y^{C}(X'_{k}))={\\frac {exp(y^{C}(X'_{k}))}{\\sum _{m}exp(y^{C}(X'_{m}))}}}\n  \n\nwhere \n  \n    \n      \n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle A_{k}(x,y)}\n  \n is the activation of channel k  at location (x,y), \n  \n    \n      \n        \n          y\n          \n            C\n          \n        \n        (\n        X\n        )\n      \n    \n    {\\displaystyle y^{C}(X)}\n  \n is the logit for class C for an input X and \n  \n    \n      \n        \n          M\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle M_{k}}\n  \n is the mask, defined as:\n\n  \n    \n      \n        \n          M\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        \n          \n            \n              U\n              (\n              \n                A\n                \n                  k\n                \n              \n              )\n              (\n              x\n              ,\n              y\n              )\n              −\n              m\n              i\n              \n                n\n                \n                  x\n                  ,\n                  y\n                \n              \n              U\n              (\n              \n                A\n                \n                  m\n                \n              \n              )\n            \n            \n              m\n              a\n              \n                x\n                \n                  x\n                  ,\n                  y\n                \n              \n              U\n              (\n              \n                A\n                \n                  k\n                \n              \n              )\n              −\n              m\n              i\n              \n                n\n                \n                  x\n                  ,\n                  y\n                \n              \n              U\n              (\n              \n                A\n                \n                  k\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle M_{k}(x,y)={\\frac {U(A_{k})(x,y)-min_{x,y}U(A_{m})}{max_{x,y}U(A_{k})-min_{x,y}U(A_{k})}}}\n  \n\nwith \n  \n"}
{"doc_id": "Class activation mapping", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "          \n        \n      \n    \n    {\\displaystyle M_{k}(x,y)={\\frac {U(A_{k})(x,y)-min_{x,y}U(A_{m})}{max_{x,y}U(A_{k})-min_{x,y}U(A_{k})}}}\n  \n\nwith \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n as the upsampling operation.\nSince the process of score calculation is repeated for every channel, Score-CAM is slow with respect to gradient-based methods. Moreover, it focuses on regions highlighted by individual feature maps, ignoring the context of the full image, reducing interpretability in complex scenes.\n\nLayerCAM\nLayerCAM enhances backwards class-specific gradients using both intermediate and final convolutional layers. Combining information across layers allows to achieve higher resolution and more fine-grained detail, improving localization.\nSpecifically, for each position in the feature map, LayerCAM evaluates the gradient. The positive gradients are employed as the weights, \n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle w_{k}^{C}}\n  \n. Namely:\n\n  \n    \n      \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          \n            \n              ∂\n              \n                y\n                \n                  C\n                \n              \n            \n            \n              ∂\n              \n                A\n                \n                  k\n                \n              \n              (\n              i\n              ,\n              j\n              )\n            \n          \n        \n        (\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle w_{k}^{C}(x,y)=ReLU({\\frac {\\partial y^{C}}{\\partial A_{k}(i,j)}}(x,y))}\n  \n\nThe activations are then weighted, and the final class activation map is retrieved by summing over the channels.\n\n  \n    \n      \n        \n          L\n          \n            L\n            a\n            y\n            e\n            r\n            −\n            C\n            A\n            M\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        \n          A\n          \n            k"}
{"doc_id": "Class activation mapping", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " y\n        )\n        =\n        R\n        e\n        L\n        U\n        (\n        \n          ∑\n          \n            k\n          \n        \n        \n          w\n          \n            k\n          \n          \n            C\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        \n          A\n          \n            k\n          \n        \n        (\n        x\n        ,\n        y\n        )\n        )\n      \n    \n    {\\displaystyle L_{Layer-CAM}^{C}(x,y)=ReLU(\\sum _{k}w_{k}^{C}(x,y)A_{k}(x,y))}\n  \n\nThis technique offers high-resolution heatmaps, flexible localization and per-location precision, employing positive gradients."}
{"doc_id": "Cognitive computing", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Cognitive computing refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing.  These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision (object recognition), human–computer interaction, dialog and narrative generation, among other technologies.\n\nDefinition\nAt present, there is no widely agreed upon definition for cognitive computing in either academia or industry.\nIn general, the term cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain (2004). In this sense, cognitive computing is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus. Cognitive computing applications link data analysis and adaptive page displays (AUI) to adjust content for a particular type of audience. As such, cognitive computing hardware and applications strive to be more affective and more influential by design.\n\nThe term \"cognitive system\" also applies to any artificial construct able to perform a cognitive process where a cognitive process is the transformation of data, information, knowledge, or wisdom to a new level in the DIKW Pyramid. While many cognitive systems employ techniques having their origination in artificial intelligence research, cognitive systems, themselves, may not be artificially intelligent. For example, a  neural network trained to recognize cancer on an MRI scan may achieve a higher success rate than a human doctor. This system is certainly a cognitive system but is not artificially intelligent.\nCognitive systems may be engineered to feed on dynamic data in real-time, or near real-time, and may draw on multiple sources of information, including both structured and unstructured digital information, as well as sensory inputs (visual, gestural, auditory, or sensor-provided).\n\nCognitive analytics\nCognitive computing-branded technology platforms typically specialize in the processing and analysis of large, unstructured datasets.\n\nApplications\nEducation\nEven if cognitive computing can not take the place of teachers, it can still be a heavy driving force in the education of students. Cognitive computing being used in the classroom is applied by essentially having an assistant that is personalized for each individual student. This cognitive assistant can relieve the stress that teachers face while teaching students, while also enhancing the student's learning experience over all. Teachers may not be able to pay each and every student individual attention, this being the place that cognitive computers fill the gap. Some students may need a little more help with a particular subject. For many students, Human interaction between student and teacher can cause anxiety and can"}
{"doc_id": "Cognitive computing", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " while also enhancing the student's learning experience over all. Teachers may not be able to pay each and every student individual attention, this being the place that cognitive computers fill the gap. Some students may need a little more help with a particular subject. For many students, Human interaction between student and teacher can cause anxiety and can be uncomfortable. With the help of Cognitive Computer tutors, students will not have to face their uneasiness and can gain the confidence to learn and do well in the classroom. While a student is in class with their personalized assistant, this assistant can develop various techniques, like creating lesson plans, to tailor and aid the student and their needs.\nHealthcare\nNumerous tech companies are in the process of developing technology that involves cognitive computing that can be used in the medical field. The ability to classify and identify is one of the main goals of these cognitive devices. This trait can be very helpful in the study of identifying carcinogens. This cognitive system that can detect would be able to assist the examiner in interpreting countless numbers of documents in a lesser amount of time than if they did not use Cognitive Computer technology. This technology can also evaluate information about the patient, looking through every medical record in depth, searching for indications that can be the source of their problems.\nCommerce\nTogether with Artificial Intelligence, it has been used in warehouse management systems  to collect, store, organize and analyze all related supplier data. All these aims at improving efficiency, enabling faster decision-making, monitoring inventory and fraud detection\nHuman Cognitive Augmentation\nIn situations where humans are using or working collaboratively with cognitive systems, called a human/cog ensemble, results achieved by the ensemble are superior to results obtainable by the human working alone. Therefore, the human is cognitively augmented. In cases where the human/cog ensemble achieves results at, or superior to, the level of a human expert then the ensemble has achieved synthetic expertise. In a human/cog ensemble, the \"cog\" is a cognitive system employing virtually any kind of cognitive computing technology.\nOther use cases\nSpeech recognition\nSentiment analysis\nFace detection\nRisk assessment\nFraud detection\nBehavioral recommendations\n\nIndustry work\nCognitive computing in conjunction with big data and algorithms that comprehend customer needs, can be a major advantage in economic decision making.\nThe powers of cognitive computing and artificial intelligence hold the potential to affect almost every task that humans are capable of performing. This can negatively affect employment for humans, as there would be no such need for human labor anymore. It would also increase the inequality"}
{"doc_id": "Cognitive computing", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " that comprehend customer needs, can be a major advantage in economic decision making.\nThe powers of cognitive computing and artificial intelligence hold the potential to affect almost every task that humans are capable of performing. This can negatively affect employment for humans, as there would be no such need for human labor anymore. It would also increase the inequality of wealth; the people at the head of the cognitive computing industry would grow significantly richer, while workers without ongoing, reliable employment would become less well off.\nThe more industries start to use cognitive computing, the more difficult it will be for humans to compete. Increased use of the technology will also increase the amount of work that AI-driven robots and machines can perform. Only extraordinarily talented, capable and motivated humans would be able to keep up with the machines. The influence of competitive individuals in conjunction with artificial intelligence/cognitive computing with has the potential to change the course of humankind.\n\nSee also\nAutomation\nAffective computing\nAnalytics\nArtificial intelligence\nArtificial neural network\nBrain computer interface\nCognitive computer\nCognitive reasoning\nCognitive science\nEnterprise cognitive system\nSemantic Web\nSocial neuroscience\nSynthetic intelligence\nUsability\nNeuromorphic engineering\nAI accelerator"}
{"doc_id": "Cognitive Digital Network", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A Cognitive Digital Network (CDN) is a proposed framework describing systems in which human cognition and artificial intelligence operate together within a shared environment for meaning-making, interpretation, and knowledge construction. The concept outlines a form of interconnected digital cognition that extends beyond digital communication or social networking by emphasizing reasoning processes and the generation of shared understanding.\nThe term was first introduced in 2025 in a published theoretical work that aimed to describe an emerging stage in the development of digital intelligence.\n In this formulation, a Cognitive Digital Network differs from the engineering notion of a cognitive network used in telecommunications—primarily focused on adaptive routing and resource management—by emphasizing semantic processes rather than signal optimization.\nCognitive Digital Networks are discussed in relation to fields such as artificial intelligence, machine learning, knowledge representation, information theory, and network science.They are described as environments in which human and artificial agents contribute to the interpretation and transformation of data, enabling forms of collective reasoning distinct from the interaction patterns associated with social networks.\nThe model connects with theoretical discussions on distributed cognition, semantic information processing, and machine-supported reasoning, reflecting a broader shift toward environments where human and artificial agents jointly contribute to meaning-making. It is therefore framed not only as a conceptual development but also as a response to increasing interest in hybrid cognitive systems integrating human insight with algorithmic analysis.\n\nDefinition\nA Cognitive Digital Network (CDN) is described as an ensemble of human and artificial cognitive units that participate in processes of learning, analysis, interpretation, and knowledge construction. Within such a network, both human agents and AI systems contribute contextual cues, semantic representations, and reasoning pathways, forming a shared environment for generating, validating, and applying knowledge. The framework emphasizes adaptive understanding and semantic coherence rather than the transmission of raw data.\nIn this formulation, a CDN differs from traditional computational or communication architectures by focusing on distributed cognition and collective meaning-making. The model outlines an environment in which interpretation, feedback, and integrative reasoning shape the network's behavior—representing a shift from systems oriented toward data exchange to systems oriented toward shared understanding.\n\nDistinction from Cognitive Networks\nAlthough similar in terminology, a Cognitive Digital Network differs fundamentally from the concept of a cognitive network in telecommunications. Cognitive networks in engineering adapt to network conditions by sensing and optimizing resource allocation and signal flow. In contrast, Cognitive Digital Networks process meaning rather than signals—analyzing semantic relations, integrating human and machine reasoning, and contributing to shared understanding. This distinction marks a shift from optimizing digital infrastructure to conceptualizing an emerging cognitive infrastructure, where reasoning, interpretation, and semantic processing occur across hybrid human"}
{"doc_id": "Cognitive Digital Network", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " resource allocation and signal flow. In contrast, Cognitive Digital Networks process meaning rather than signals—analyzing semantic relations, integrating human and machine reasoning, and contributing to shared understanding. This distinction marks a shift from optimizing digital infrastructure to conceptualizing an emerging cognitive infrastructure, where reasoning, interpretation, and semantic processing occur across hybrid human–AI environments.\n\nOrigin and theoretical context\nThe idea of a Cognitive Digital Network emerged within discussions about how digital technologies increasingly participate in processes traditionally associated with cognitive science. The concept draws on historical developments in computation, information networks, and artificial intelligence, beginning with early mechanized logic and continuing through the expansion of the Internet and the development of large-scale machine learning systems.\nAnalyses of digital transformation have noted a shift from supporting communication to supporting higher-order processes such as interpretation, decision-making, and knowledge representation. The growth of machine-learning models and the increasing volume of digital information in the early 2020s renewed interest in frameworks explaining how artificial systems might contribute to reasoning alongside humans.\nIntroduced in 2025, the Cognitive Digital Network model synthesizes elements of artificial intelligence, knowledge representation, cognitive science, and distributed cognition. Rather than defining a specific technology, it outlines conditions under which human and artificial agents may jointly contribute to semantic interpretation and knowledge construction.\nThe concept connects to earlier work on collective intelligence, distributed cognition, and cybernetics, but differs by framing these interactions within contemporary AI environments. CDNs are presented as a model for understanding how cognition might be distributed across networks that incorporate both human insight and machine-based analytical processes, without implying deterministic historical progression or fully realized implementations.\n\nStructural principles and core characteristics\nDescriptions of Cognitive Digital Networks identify several structural principles that distinguish them from conventional digital architectures. Rather than focusing on the transmission of data, CDNs are framed as environments oriented toward semantic processing, where human and artificial agents jointly contribute to shared interpretation and reasoning.\nTheir structure incorporates elements such as distributed cognition, semantic interoperability rooted in knowledge representation, and adaptive learning mechanisms based on machine learning. These principles emphasize how a CDN functions as a shared cognitive space in which meaning, context, and semantic structures circulate among heterogeneous agents.\nThe model differs from traditional computer networks by centering the flow of understanding rather than the flow of information.\n\nApplications\nProposed applications of Cognitive Digital Networks are conceptual and focus on contexts where human interpretation and artificial intelligence jointly contribute to reasoning and knowledge formation. Rather than describing industry-specific implementations, the CDN framework outlines how meaning, semantic coherence, and distributed cognition may operate across hybrid human–AI systems.\nElements of this"}
{"doc_id": "Cognitive Digital Network", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of information.\n\nApplications\nProposed applications of Cognitive Digital Networks are conceptual and focus on contexts where human interpretation and artificial intelligence jointly contribute to reasoning and knowledge formation. Rather than describing industry-specific implementations, the CDN framework outlines how meaning, semantic coherence, and distributed cognition may operate across hybrid human–AI systems.\nElements of this approach appear in emerging research on collaborative AI, semantic information processing, and large-scale reasoning tools grounded in machine learning and knowledge representation, where human insight and algorithmic analysis jointly influence interpretive outcomes.\nThese examples illustrate cognitive interaction patterns relevant to CDNs without representing fully realized CDN systems.\n\nSocietal impact and future evolution\nDiscussions of Cognitive Digital Networks consider their potential implications for how knowledge is organized, interpreted, and circulated in digital environments. The CDN framework highlights interactions in which human and artificial agents contribute jointly to reasoning and understanding, extending beyond basic communication or emotional exchange.\nThese ideas intersect with ethical and philosophical debates on epistemic reliability, transparency, human agency, and the boundaries between human interpretation and machine inference. CDNs are therefore situated within broader discussions in AI ethics, cognitive science, cognitive computing, artificial general intelligence, and the philosophy of mind.\n\nComparative frameworks and related concepts\nCognitive Digital Networks have been discussed alongside fields such as cognitive computing, distributed artificial intelligence, knowledge graphs, distributed cognition, semantic networks, and global brain. These areas examine different aspects of how information, reasoning, and computation are organized, while CDNs bring together semantic interpretation, collaborative reasoning, and hybrid human–AI interaction.\nThe framework is considered adjacent to these fields without being reducible to any single one.\n\nFuture outlook and conclusion\nCurrent discussions of Cognitive Digital Networks explore how hybrid human–AI environments may influence future approaches to reasoning, interpretation, and knowledge organization. The CDN framework contextualizes ongoing research into cooperative cognition and human–computer interaction, raising questions about how meaning is constructed across distributed systems and how cognitive processes may be shared between human and artificial agents.\nThe model has been compared with theories of collective intelligence, distributed cognition, and large-scale knowledge graph systems, while remaining distinct in its focus on semantic interpretation.\nThe significance of CDNs lies in their contribution to discussions about how cognition may be structured within digital environments that incorporate both human insight and machine-based analysis. The framework encourages further inquiry into how reasoning and decision-making might evolve in networks where multiple forms of intelligence interact.\n\nSee also\nDistributed cognition\nCollective intelligence\nKnowledge representation\nCognitive computing\nHuman–computer interaction\nSemantic networks\nInformation theory\nNetwork science\nArtificial"}
{"doc_id": "Cognitive Digital Network", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " both human insight and machine-based analysis. The framework encourages further inquiry into how reasoning and decision-making might evolve in networks where multiple forms of intelligence interact.\n\nSee also\nDistributed cognition\nCollective intelligence\nKnowledge representation\nCognitive computing\nHuman–computer interaction\nSemantic networks\nInformation theory\nNetwork science\nArtificial general intelligence\nCognitive architecture\n\nFurther reading\nThe following topics have been associated with research areas relevant to the Cognitive Digital Network framework:\n\nCollective intelligence\nCognitive computing\nSemantic Web\nDistributed artificial intelligence\nPhilosophy of technology\nArtificial general intelligence\nCybernetics\nKnowledge graphs\nHuman–computer interaction\nDistributed cognition"}
{"doc_id": "Cognitive philology", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Cognitive philology is the science that studies written and oral texts as the product of human mental processes. Studies in cognitive philology compare documentary evidence emerging from textual investigations with results of experimental research, especially in the fields of cognitive and ecological psychology, neurosciences and artificial intelligence. \"The point is not the text, but the mind that made it\". Cognitive Philology aims to foster communication between literary, textual, philological disciplines on the one hand and researches across the whole range of the cognitive, evolutionary, ecological and human sciences on the other.\nCognitive philology: \n\ninvestigates transmission of oral and written text, and categorization processes which lead to classification of knowledge, mostly relying on the information theory;\nstudies how narratives emerge in so called natural conversation and selective process which lead to the rise of literary standards for storytelling, mostly relying on embodied semantics;\nexplores the evolutive and evolutionary role played by rhythm and metre in human ontogenetic and phylogenetic development and the pertinence of the semantic association during processing of cognitive maps;\nProvides the scientific ground for multimedia  critical editions of literary texts.\n \nAmong the founding thinkers and noteworthy scholars devoted to such investigations are: \n\nAlan Richardson: Studies Theory of Mind in early-modern and contemporary literature.\nAnatole Pierre Fuksas\nBenoît de Cornulier\nDavid Herman: Professor of English at North Carolina State University and an adjunct professor of linguistics at Duke University. He is the author of \"Universal Grammar and Narrative Form\" and the editor of \"Narratologies: New Perspectives on Narrative Analysis\".\nDomenico Fiormonte\nFrançois Recanati\nGilles Fauconnier, a professor in Cognitive science at the University of California, San Diego. He was one of the founders of cognitive linguistics in the 1970s through his work on pragmatic scales and mental spaces. His research explores the areas of conceptual integration and compressions of conceptual mappings in terms of the emergent structure in language.\nJulián Santano Moreno\nLuca Nobile\nManfred Jahn in Germany\nMark Turner\nPaolo Canettieri\n\nSee also\nArtificial intelligence\nCognitive archaeology\nCognitive linguistics\nCognitive poetics\nCognitive psychology\nCognitive rhetoric\nInformation theory\nPhilology"}
{"doc_id": "Cognitive philology", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " linguistics\nCognitive poetics\nCognitive psychology\nCognitive rhetoric\nInformation theory\nPhilology"}
{"doc_id": "Coherent extrapolated volition", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Coherent extrapolated volition (CEV) is a theoretical framework in the field of AI alignment describing an approach by which an artificial superintelligence (ASI) would act on a benevolent supposition of what humans would want if they were more knowledgeable, more rational, had more time to think, and had matured together as a society, as opposed to humanity's current individual or collective preferences. It was proposed by Eliezer Yudkowsky in 2004 as part of his work on friendly AI.\n\nConcept\nCEV proposes that an advanced AI system should derive its goals by extrapolating the idealized volition of humanity. This means aggregating and projecting human preferences into a coherent utility function that reflects what people would desire under ideal epistemic and moral conditions. The aim is to ensure that AI systems are aligned with humanity's true interests, rather than with transient or poorly informed preferences.\n\nIn poetic terms, our coherent extrapolated volition is our wish if we knew more, thought faster, were more the people we wished we were, had grown up farther together; where the extrapolation converges rather than diverges, where our wishes cohere rather than interfere; extrapolated as we wish that extrapolated, interpreted as we wish that interpreted.\n\nDebate\nYudkowsky and Bostrom note that CEV has several interesting properties. It is designed to be humane and self-correcting, by capturing the source of human values instead of trying to list them. It avoids the difficulty of laying down an explicit, fixed list of rules. It encapsulates moral growth, preventing flawed current moral beliefs from getting locked in. It limits the influence that a small group of programmers can have on what the ASI would value, thus also reducing the incentives to build ASI first. And it keeps humanity in charge of its destiny.\nCEV also faces significant theoretical and practical challenges.\nBostrom notes that CEV has \"a number of free parameters that could be specified in various ways, yielding different versions of the proposal.\" One such parameter is the extrapolation base (whose CEV is taken into account). For example, whether it should include people with severe dementia, patients in a vegetative state, foetuses, or embryos. He also notes that if CEV's extrapolation base only includes humans, there is a risk that the result would be ungenerous toward other animals and digital minds. One possible solution would be to include a mechanism to expand CEV's extrapolation base.\n\nVariants and alternatives"}
{"doc_id": "Coherent extrapolated volition", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", foetuses, or embryos. He also notes that if CEV's extrapolation base only includes humans, there is a risk that the result would be ungenerous toward other animals and digital minds. One possible solution would be to include a mechanism to expand CEV's extrapolation base.\n\nVariants and alternatives\nA proposed theoretical alternative to CEV is to rely on an artificial superintelligence's superior cognitive capabilities to figure out what is morally right, and let it act accordingly. It is also possible to combine both techniques, for instance with the ASI following CEV except when it is morally impermissible.\nIn another review, a philosophical analysis explores CEV through the lens of social trust in autonomous systems. Drawing on Anthony Giddens' concept of \"active trust\", the author proposes an evolution of CEV into \"Coherent, Extrapolated and Clustered Volition\" (CECV). This formulation aims to better reflect the moral preferences of diverse cultural groups, thus offering a more pragmatic ethical framework for designing AI systems that earn public trust while accommodating societal diversity.\n\nYudkowsky's later view\nAlmost immediately after publishing the idea in 2004, Eliezer Yudkowsky himself described the concept as outdated. He warned against conflating it with a practical strategy for AI alignment. While CEV may serve as a philosophical ideal, Yudkowsky emphasized that real-world alignment mechanisms must grapple with greater complexity, including the difficulty of defining and implementing extrapolated values in a reliable way.\n\nSee also\nFriendly artificial intelligence\nAI alignment\nAI safety\nEliezer Yudkowsky\nNick Bostrom\nRationality"}
{"doc_id": "Commonsense knowledge (artificial intelligence)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In artificial intelligence research,  commonsense knowledge consists of facts about the everyday world, such as \"Lemons are sour\" or \"Cows say moo\", that all humans are expected to know.  It is currently an unsolved problem in artificial general intelligence. The first AI program to address common sense knowledge was Advice Taker in 1959 by John McCarthy.\nCommonsense knowledge  can underpin a commonsense reasoning process, to attempt inferences such as \"You might bake a cake because you want people to eat the cake.\"  A natural language processing process can be attached to the commonsense knowledge base to allow the knowledge base to attempt to answer questions about the world. Common sense knowledge also helps to solve problems in the face of incomplete information.  Using widely held beliefs about everyday objects, or common sense knowledge, AI systems make common sense assumptions or default assumptions about the unknown similar to the way people do.  In an AI system or in English, this is expressed  as \"Normally P holds\", \"Usually P\" or \"Typically P so Assume P\".   For example, if we know the fact \"Tweety is a bird\", because we know the commonly held belief about birds, \"typically birds fly,\" without knowing anything else about Tweety, we may reasonably assume the fact that \"Tweety can fly.\"  As more knowledge of the world is discovered or learned over time, the AI system can revise its assumptions about Tweety using a truth maintenance process.  If we later learn that \"Tweety is a penguin\" then truth maintenance revises this assumption because we also know \"penguins do not fly\".\n\nCommonsense reasoning\nCommonsense reasoning simulates the human ability to use commonsense knowledge to make presumptions about the type and essence of ordinary situations they encounter every day, and to change their \"minds\" should new information come to light.  This includes time, missing or incomplete information and cause and effect.  The ability to explain cause and effect is an important aspect of explainable AI.  Truth maintenance algorithms automatically provide an explanation facility because they create elaborate records of presumptions.  Compared with humans, all existing computer programs that attempt human-level AI perform extremely poorly on modern \"commonsense reasoning\" benchmark tests such as the Winograd Schema Challenge. The problem of attaining human-level competency at \"commonsense knowledge\" tasks is considered to probably be \"AI complete\" (that is, solving it would require the ability to synthesize a fully human-level intelligence), although some oppose"}
{"doc_id": "Commonsense knowledge (artificial intelligence)", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " poorly on modern \"commonsense reasoning\" benchmark tests such as the Winograd Schema Challenge. The problem of attaining human-level competency at \"commonsense knowledge\" tasks is considered to probably be \"AI complete\" (that is, solving it would require the ability to synthesize a fully human-level intelligence), although some oppose this notion and believe compassionate intelligence is also required for human-level AI. Common sense reasoning has been applied successfully in more limited domains such as natural language processing and automated diagnosis or analysis.\n\nCommonsense knowledge base construction\nCompiling comprehensive knowledge bases of commonsense assertions (CSKBs) is a long-standing challenge in AI research. From early expert-driven efforts like CYC and WordNet, significant advances were achieved via the crowdsourced OpenMind Commonsense project, which led to the crowdsourced ConceptNet KB. Several approaches have attempted to automate CSKB construction, most notably, via text mining (WebChild, Quasimodo, TransOMCS, Ascent), as well as harvesting these directly from pre-trained language models (AutoTOMIC). These resources are significantly larger than ConceptNet, though the automated construction mostly makes them of moderately lower quality. Challenges also remain on the representation of commonsense knowledge: Most CSKB projects follow a triple data model, which is not necessarily best suited for breaking more complex natural language assertions. A notable exception here is GenericsKB, which applies no further normalization to sentences, but retains them in full.\n\nApplications\nAround 2013, MIT researchers developed BullySpace, an extension of the commonsense knowledgebase ConceptNet, to catch taunting social media comments. BullySpace included over 200 semantic assertions based around stereotypes, to help the system infer that comments like \"Put on a wig and lipstick and be who you really are\" are more likely to be an insult if directed at a boy than a girl.\nConceptNet has also been used by chatbots and by computers that compose original fiction.  At Lawrence Livermore National Laboratory, common sense knowledge was used in an intelligent software agent to detect violations of a comprehensive nuclear test ban treaty.\n\nData\nAs an example, as of 2012 ConceptNet includes these 21 language-independent relations:\n\nIsA (An \"RV\" is a \"vehicle\" | X is an instance of a Y)\nUsedFor (a \"cake tin\" is used for \"making cakes\" | X is used for the purpose Y)\nHasA (A \"rabbit\" has a \"tail\" | X possesses Y element or feature)\nCapableOf ("}
{"doc_id": "Commonsense knowledge (artificial intelligence)", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "RV\" is a \"vehicle\" | X is an instance of a Y)\nUsedFor (a \"cake tin\" is used for \"making cakes\" | X is used for the purpose Y)\nHasA (A \"rabbit\" has a \"tail\" | X possesses Y element or feature)\nCapableOf (a \"cook\" is capable of \"making baked goods\" | X is capable of doing Y)\nDesires (a \"child\" desires \"the aroma of baking\" | X has a desire for Y)\nCreatedBy (\"cake\" is created by a \"baker\" | X is created by Y)\nPartOf (a \"knife\" is be part of a \"knife set\" | X is a part of Y)\nCauses (\"Heat\" causes \"cooking\"| X is what causes Y)\nLocatedNear (the \"oven\" is located near the \"refrigerator\" | X is located near Y)\nAtLocation (Somewhere a \"Cook\" can be at a \"restaurant\" | X is at the location of Y)\nDefinedAs (a \"Cupcake\" is defined as a \"cake\" that also has the qualities of being \"small\", \"baked within a wrapper\", and \"containing only one area of frosting or icing\" | X is defined as Y that also has the properties A, B & C)\nSymbolOf (a \"heart\" is a symbol of \"affection\" | X is a symbolic representation of Y)\nReceivesAction (\"cake\" can receive the action of being \"eaten\" | X is capable of receiving action Y)\nHasPrerequisite (\"baking\" has the prerequisite of obtaining the \"ingredients\" | X cannot do Y unless A does B)\nMotivatedByGoal (\"baking\" is motivated by the goal of \"consumption\"/\"eating\" | X has the motivation of Y goal)\nCausesDesire (\"baking\" makesYou want to \"follow recipe\" | X causes the desire to do Y)\nMadeOf (\"Cake\" is made of \"flour\"/\"eggs\"/\"sugar\"/\"oil\"/etc | X is made of Y)\nHasFirstSubevent (\"baking\" has first subevent \"make batter\" | To do X the first thing that needs to be done is Y)\nHasSubevent (\"eat\" has subevent \"swallow\" | Doing X will lead to Y event following)\nHasLastSubevent (\"sleeping\" has last subevent of \"waking\" | Doing X ends"}
{"doc_id": "Commonsense knowledge (artificial intelligence)", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "event \"make batter\" | To do X the first thing that needs to be done is Y)\nHasSubevent (\"eat\" has subevent \"swallow\" | Doing X will lead to Y event following)\nHasLastSubevent (\"sleeping\" has last subevent of \"waking\" | Doing X ends with the event Y)\n\nCommonsense knowledge bases\nCyc\nOpen Mind Common Sense (data source) and ConceptNet (datastore and NLP engine)\nEvi\nGraphiq\n\nSee also\nCommon sense\nLinked data and the Semantic Web\nTruth Maintenance or Reason Maintenance\nOntology"}
{"doc_id": "Competition in artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Competition in artificial intelligence refers to the rivalry among companies, research institutions, and governments to develop and deploy the most capable artificial intelligence (AI) systems. The competition spans multiple domains, including large language models (LLMs), autonomous vehicles, robotics, computer vision systems, natural language processing (NLP), and AI-optimized hardware.\n\nBackground\nCompetition in AI is driven by potential economic, strategic, and scientific advantages. Breakthroughs in AI can enhance productivity, enable new products and services, and provide geopolitical leverage. The field has experienced rapid progress since the mid-2010s, particularly in machine learning and artificial neural networks, leading to intense rivalry among leading actors.\n\nCorporate competition\nMajor technology companies are among the most visible competitors in AI. In the United States, firms such as OpenAI, Google DeepMind, Meta Platforms, Microsoft, Anthropic, and Nvidia compete in building advanced LLMs, generative AI platforms, and AI-optimized graphics processing units (GPUs). In China, companies such as Baidu, Alibaba Group, Tencent, and startups such DeepSeek have become leaders in AI deployment, often with state backing.\nThe \"[war for talent]\" in AI research has become a defining feature of corporate competition. Leading firms often recruit top AI researchers from rivals, sometimes offering multi-million-dollar compensation packages.\n\nNational competition\nGovernments see leadership in AI as a strategic priority. The United States has funded AI research for military, economic, and societal applications, while China has set a target to lead the world in AI by 2030 through its \"New Generation Artificial Intelligence Development Plan\". Other nations, including the UK, India, Russia, South Korea, and members of the European Union, have launched national AI strategies.\n\nSectors of competition\nLarge language models and chatbots competition\nCompetition to produce the most capable generative text models, with benchmarks such as MMLU and ARC used to evaluate performance has been on scale since emergency of AI. These systems leverage deep learning, especially transformer architectures, to understand and generate human-like language. Companies and research groups globally compete to develop chatbots that are more capable, reliable, and context-aware. Among the most well-known chatbots is ChatGPT, developed by OpenAI. Since its public release in 2022, ChatGPT rapidly gained widespread attention for its ability to engage in coherent and versatile conversations, assist with creative writing, and solve complex problems. In response, technology firms introduced competing chatbots aiming to challenge or surpass ChatGPT's capabilities. Notably"}
{"doc_id": "Competition in artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " developed by OpenAI. Since its public release in 2022, ChatGPT rapidly gained widespread attention for its ability to engage in coherent and versatile conversations, assist with creative writing, and solve complex problems. In response, technology firms introduced competing chatbots aiming to challenge or surpass ChatGPT's capabilities. Notably, DeepSeek, a Chinese AI company, launched an advanced chatbot integrated with their R1 language model, emphasizing strong natural language understanding and multilingual support. Similarly, Grok, developed by Tesla, Inc., integrates conversational AI into vehicles and digital assistants, combining natural language processing with real-time data for personalized user interaction. \nThese chatbots not only compete in language tasks but also demonstrate strategic reasoning capabilities by playing complex games such as chess and Go. This form of competition is reminiscent of the historic AI milestones set by programs such as Deep Blue and AlphaGo. The OpenAI’s ChatGPT has been tested in playing chess at various levels, while DeepSeek’s chatbot showcased its prowess in online chess tournaments in early 2024, winning several matches against human and AI opponents. Grok, leveraging Tesla's vast data infrastructure, has demonstrated real-time strategic decision-making in simulation environments that include chess-like games. \nThe competition pushes rapid innovation, with firms racing to improve chatbot conversational depth, reduce biases, increase factual accuracy, and integrate multimodal inputs like images and videos. At the same time, the competition raises questions about AI safety, ethical use, and the societal impacts of increasingly human-like chatbots.\n\nAutonomous vehicles\nCompanies such as Waymo, Tesla, and Baidu are racing to deploy safe and reliable self-driving car technology.\n\nAI chips\nRivalry between Nvidia, AMD, Intel, and Huawei in designing processors optimized for AI workloads.\n\nMilitary applications\nDevelopment of AI-enabled drones, surveillance systems, and decision-support tools, with associated ethical debates.\n\nIncidents\nIn 2023, OpenAI released GPT-4, prompting competitors such as Google DeepMind to accelerate the release of their own models, including Gemini.\nIn 2024, Chinese AI company DeepSeek launched the R1 model, leading OpenAI to release an open-source system, GPT-OSS, as a strategic countermeasure.\nIn 2022, Tesla and Waymo both expanded autonomous taxi services in U.S. cities, competing for regulatory approval and public trust.\nThe U.S. Department of Defense's Project Maven and China's AI-enabled surveillance programs have been cited as examples of military AI rivalry.\nIn"}
{"doc_id": "Competition in artificial intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "SS, as a strategic countermeasure.\nIn 2022, Tesla and Waymo both expanded autonomous taxi services in U.S. cities, competing for regulatory approval and public trust.\nThe U.S. Department of Defense's Project Maven and China's AI-enabled surveillance programs have been cited as examples of military AI rivalry.\nIn 2025, Microsoft hired several senior engineers from Google DeepMind, highlighting the ongoing \"talent poaching\" competition in the AI sector.\n\nRisks and concerns\nCritics warn that unrestrained competition in AI can undermine safety, ethics, and governance. Concerns include the proliferation of biased or unsafe models, escalation in autonomous weapons, and reduced cooperation on safety standards.\n\nSee also\nArtificial intelligence\nArtificial general intelligence\nTechnological singularity\nSpace Race"}
{"doc_id": "Computational heuristic intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Computational heuristic intelligence (CHI) refers to specialized programming techniques in computational intelligence (also called artificial intelligence, or AI). These techniques have the express goal of avoiding complexity issues, also called NP-hard problems, by using human-like techniques. They are best summarized as the use of exemplar-based methods (heuristics), rather than rule-based methods (algorithms). Hence the term is distinct from the more conventional computational algorithmic intelligence, or symbolic AI. An example of a CHI technique is the encoding specificity principle of Tulving and Thompson. In general, CHI principles are problem solving techniques used by people, rather than programmed into machines. It is by drawing attention to this key distinction that the use of this term is justified in a field already replete with confusing neologisms. Note that the legal systems of all modern human societies employ both heuristics (generalisations of cases) from individual trial records as well as legislated statutes (rules) as regulatory guides.\nAnother recent approach to the avoidance of complexity issues is to employ feedback control rather than feedforward modeling as a problem-solving paradigm. This approach has been called computational cybernetics, because (a) the term 'computational' is associated with conventional computer programming techniques which represent a strategic, compiled, or feedforward model of the problem, and (b) the term 'cybernetic' is associated with conventional system operation techniques which represent a tactical, interpreted, or feedback model of the problem. Of course, real programs and real problems both contain both feedforward and feedback components. A real example which illustrates this point is that of human cognition, which clearly involves both perceptual (bottom-up, feedback, sensor-oriented) and conceptual (top-down, feedforward, motor-oriented) information flows and hierarchies.\nThe AI engineer must choose between mathematical and cybernetic problem solution and machine design paradigms. This is not a coding (program language) issue, but relates to understanding the relationship between the declarative and procedural programming paradigms. \nThe vast majority of STEM professionals never get the opportunity to design or implement pure cybernetic solutions. When pushed, most responders will dismiss the importance of any difference by saying that all code can be reduced to a mathematical model anyway. Unfortunately, not only is this belief false, it fails most spectacularly in many AI scenarios.\nMathematical models are not time agnostic, but by their very nature are pre-computed, i.e. feedforward. Dyer [2012] and Feldman [200"}
{"doc_id": "Computational heuristic intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " be reduced to a mathematical model anyway. Unfortunately, not only is this belief false, it fails most spectacularly in many AI scenarios.\nMathematical models are not time agnostic, but by their very nature are pre-computed, i.e. feedforward. Dyer [2012] and Feldman [2004] have independently investigated the simplest of all somatic governance paradigms, namely control of a simple jointed limb by a single flexor muscle. They found that it is impossible to determine forces from limb positions- therefore, the problem cannot have a pre-computed (feedforward) mathematical solution. Instead, a top-down command bias signal changes the threshold feedback level in the sensorimotor loop, e.g. the loop formed by the afferent and efferent nerves, thus changing the so-called ‘equilibrium point’ of the flexor muscle/ elbow joint system. An overview of the arrangement reveals that global postures and limb position are commanded in feedforward terms, using global displacements (common coding), with the forces needed being computed locally by feedback loops. This method of sensorimotor unit governance, which is based upon what Anatol Feldman calls the ‘equilibrium Point’ theory, is formally equivalent to a servomechanism such as a car's ‘cruise control’.\n\nSee also\nNP-hard\nHeuristics\nComputational cybernetics\nTop-down and bottom-up design"}
{"doc_id": "Computational humor", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Computational humor is a branch of computational linguistics and artificial intelligence which uses computers in humor research. It is a relatively new area, with the first dedicated conference organized in 1996.\nThe first \"computer model of a sense of humor\" was suggested by\nSuslov as early as 1992. \nInvestigation of the general scheme of the information processing show a possibility\nof a specific malfunction, conditioned by the necessity of a quick deletion from \nconsciousness of a false version. This specific malfunction can be identified\nwith a humorous effect on the psychological grounds; however, an essentially new ingredient, a role of timing, is added to a well known role of ambiguity. In biological systems, \na sense of humour inevitably develops in the course of evolution, because\nits biological function consists in quickening the transmission of processed \ninformation into consciousness and in a more effective use of brain resources.\nA realization of this algorithm in neural networks\n\nexplains naturally the mechanism of laughter: deletion of a false version corresponds to zeroing of some part of  the neural network and excessive energy of neurons is thrown out to the motor cortex, arousing muscular contractions. \nUnfortunately, a practical realization of this algorithm needs extensive databases, whose creation in the automatic regime was suggested only recently\n.\nAs a result, this magistral direction was not developed properly and subsequent investigations (see below) accepted somewhat specialized colouring.\n\nJoke generators\nPun generation\nAn approach to analysis of humor is classification of jokes. A further step is an attempt to generate jokes basing on the rules that underlie classification.\nSimple prototypes for computer pun generation were reported in the early 1990s, based on a natural language generator program, VINCI. Graeme Ritchie and Kim Binsted in their 1994 research paper described a computer program, JAPE, designed to generate question-answer-type puns from a general, i.e., non-humorous, lexicon.  (The program name is an acronym for \"Joke Analysis and Production Engine\".) Some examples produced by JAPE are:\n\nQ: What is the difference between leaves and a car?\nA: One you brush and rake, the other you rush and brake.\nQ: What do you call a strange market?\nA: A bizarre bazaar.\nSince then the approach has been improved, and the latest report, dated 2007, describes the STANDUP joke generator, implemented in the Java programming language. The STANDUP generator was tested on children within the framework"}
{"doc_id": "Computational humor", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " brake.\nQ: What do you call a strange market?\nA: A bizarre bazaar.\nSince then the approach has been improved, and the latest report, dated 2007, describes the STANDUP joke generator, implemented in the Java programming language. The STANDUP generator was tested on children within the framework of analyzing its usability for language skills development for children with communication disabilities, e.g., because of cerebral palsy. (The project name is an acronym for \"System To Augment Non-speakers' Dialog Using Puns\" and an allusion to standup comedy.) Children responded to this \"language playground\" with enthusiasm, and showed marked improvement on certain types of language tests.\n\nThe two young people, who used the system over a ten-week period, regaled their peers, staff, family and neighbors with jokes such as: \"What do you call a spicy missile? A hot shot!\" Their joy and enthusiasm at entertaining others was inspirational.\n\nOther\nStock and Strapparava described a program to generate funny acronyms.\n\nJoke recognition\nA statistical machine learning algorithm to detect whether a sentence contained a \"That's what she said\" double entendre was developed by Kiddon and Brun (2011).  There is an open-source Python implementation of Kiddon & Brun's TWSS system.\nA program to recognize knock-knock jokes was reported by Taylor and Mazlack. This kind of research is important in analysis of human–computer interaction.\nAn application of machine learning techniques for the distinguishing of joke texts from non-jokes was described by Mihalcea and Strapparava (2006).\nTakizawa et al. (1996) reported on a heuristic program for detecting puns in the Japanese language.\n\nApplications\nA possible application for assistance in language acquisition is described in the section \"Pun generation\". Another envisioned use of joke generators is in cases of a steady supply of jokes where quantity is more important than quality. Another obvious, yet remote, direction is automated joke appreciation.\nIt is known  that humans interact with computers in ways similar to  interacting with other humans that may be described in terms of personality, politeness, flattery, and in-group favoritism. Therefore, the role of humor in human–computer interaction is being investigated. In particular, humor generation in user interface to ease communications with computers was suggested.\nCraig McDonough implemented the Mnemonic Sentence Generator, which converts passwords into humorous sentences.  Based on the incongruity theory of humor, it is suggested that"}
{"doc_id": "Computational humor", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ". Therefore, the role of humor in human–computer interaction is being investigated. In particular, humor generation in user interface to ease communications with computers was suggested.\nCraig McDonough implemented the Mnemonic Sentence Generator, which converts passwords into humorous sentences.  Based on the incongruity theory of humor, it is suggested that the resulting meaningless but funny sentences are easier to remember. For example, the password AjQA3Jtv is converted into \"Arafat joined Quayle's Ant, while TARAR Jeopardized thurmond's vase,\" an example chosen by combining politicians names with verbs and common nouns.\n\nRelated research\nJohn Allen Paulos is known for his interest in mathematical foundations of humor. His book Mathematics and Humor: A Study of the Logic of Humor demonstrates structures common to humor and formal sciences (mathematics, linguistics) and develops a mathematical model of jokes based on catastrophe theory.\nConversational systems which have been designed to take part in Turing test competitions generally have the ability to learn humorous anecdotes and jokes. Because many people regard humor as something particular to humans, its appearance in conversation can be quite useful in convincing a human interrogator that a hidden entity, which could be a machine or a human, is in fact a human.\n\nSee also\nTheory of humor\nWorld's funniest joke#Other findings\n\nFurther reading\n\"Computational humor\", by Binsted, K.; Nijholt, A.; Stock, O.; Strapparava, C.; Ritchie, G.; Manurung, R.; Pain, H.; Waller, A.; Oapos;Mara, D.,  IEEE Intelligent Systems Volume 21, Issue 2, 2006, pp. 59 – 69 doi:10.1109/MIS.2006.22\nO. Stock, C. Strapparava & A. Nijholt (eds.) \"The April Fools' Day Workshop on Computational Humour.\" Proc. Twente Workshop on Language Technology 20 (TWLT20), ISSN 0929-0672, ITC-IRST, Trento, Italy, April 2002, 146 pp"}
{"doc_id": "Computational intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In computer science, computational intelligence (CI) refers to concepts, paradigms, algorithms and implementations of systems that are designed to show \"intelligent\" behavior in complex and changing environments. These systems are aimed at mastering complex tasks in a wide variety of technical or commercial areas and offer solutions that recognize and interpret patterns, control processes, support decision-making or autonomously manoeuvre vehicles or robots in unknown environments, among other things. These concepts and paradigms are characterized by the ability to learn or adapt to new situations, to generalize, to abstract, to discover and associate. Nature-analog or nature-inspired methods play a key role, such as in neuroevolution for computational Intelligence.\nCI approaches primarily address those complex real-world problems for which mathematical or traditional modeling is not appropriate for various reasons: the processes cannot be described exactly with complete knowledge, the processes are too complex for mathematical reasoning, they contain some uncertainties during the process, such as unforeseen changes in the environment or in the process itself, or the processes are simply stochastic in nature. Thus, CI techniques are properly aimed at processes that are ill-defined, complex, nonlinear, time-varying and/or stochastic.\nA recent definition of the IEEE Computational Intelligence Societey describes CI as the theory, design, application and development of biologically and linguistically motivated computational paradigms. Traditionally the three main pillars of CI have been Neural Networks, Fuzzy Systems and Evolutionary Computation. ... CI is an evolving field and at present in addition to the three main constituents, it encompasses computing paradigms like ambient intelligence, artificial life, cultural learning, artificial endocrine networks, social reasoning, and artificial hormone networks. ... Over the last few years there has been an explosion of research on Deep Learning, in particular deep convolutional neural networks. Nowadays, deep learning has become the core method for artificial intelligence. In fact, some of the most successful AI systems are based on CI. However, as CI is an emerging and developing field there is no final definition of CI, especially in terms of the list of concepts and paradigms that belong to it.\nThe general requirements for the development of an “intelligent system” are ultimately always the same, namely the simulation of intelligent thinking and action in a specific area of application. To do this, the knowledge about this area must be represented in a model so that it can be processed. The quality of the resulting system depends largely on how well the model was chosen in the development process. Sometimes data-driven methods are suitable for finding a good"}
{"doc_id": "Computational intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of intelligent thinking and action in a specific area of application. To do this, the knowledge about this area must be represented in a model so that it can be processed. The quality of the resulting system depends largely on how well the model was chosen in the development process. Sometimes data-driven methods are suitable for finding a good model and sometimes logic-based knowledge representations deliver better results. Hybrid models are usually used in real applications.\nAccording to actual textbooks, the following methods and paradigms, which largely complement each other, can be regarded as parts of CI:\n\nFuzzy systems\nNeural networks and, in particular, convolutional neural networks\nEvolutionary computation and, in particular, multi-objective evolutionary optimization\nSwarm intelligence\nBayesian networks\nArtificial immune systems\nLearning theory\nProbabilistic methods\n\nRelationship between hard and soft computing and artificial and computational intelligence\nArtificial intelligence (AI) is used in the media, but also by some of the scientists involved, as a kind of umbrella term for the various techniques associated with it or with CI. Craenen and Eiben state that attempts to define or at least describe CI can usually be assigned to one or more of the following groups:\n\n\"Relative definition” comparing CI to AI\nConceptual treatment of key notions and their roles in CI\nListing of the (established) areas that belong to it\nThe relationship between CI and AI has been a frequently discussed topic during the development of CI. While the above list implies that they are synonyms, the vast majority of AI/CI researchers working on the subject consider them to be distinct fields, where either\nCI is an alternative to AI\nAI includes CI\nCI includes AI\nThe view of the first of the above three points goes back to Zadeh, the founder of the fuzzy set theory, who differentiated machine intelligence into hard and soft computing techniques, which are used in artificial intelligence on the one hand and computational intelligence on the other. In hard computing (HC) and AI, inaccuracy and uncertainty are undesirable characteristics of a system, while soft computing (SC) and thus CI focus on dealing with these characteristics. The adjacent figure illustrates these relationships and lists the most important CI techniques. Another frequently mentioned distinguishing feature is the representation of information in symbolic form in AI and in sub-symbolic form in CI techniques.\nHard computing is a conventional computing method based on the principles of certainty and accuracy and it is deterministic. It requires a precisely stated analytical model of the task to be processed and a prewritten program, i.e. a"}
{"doc_id": "Computational intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " feature is the representation of information in symbolic form in AI and in sub-symbolic form in CI techniques.\nHard computing is a conventional computing method based on the principles of certainty and accuracy and it is deterministic. It requires a precisely stated analytical model of the task to be processed and a prewritten program, i.e. a fixed set of instructions. The models used are based on Boolean logic (also called crisp logic), where e.g. an element can be either a member of a set or not and there is nothing in between. When applied to real-world tasks, systems based on HC result in specific control actions defined by a mathematical model or algorithm. If an unforeseen situation occurs that is not included in the model or algorithm used, the action will most likely fail.\nSoft computing, on the other hand, is based on the fact that the human mind is capable of storing information and processing it in a goal-oriented way, even if it is imprecise and lacks certainty. SC is based on the model of the human brain with probabilistic thinking, fuzzy logic and multi-valued logic. Soft computing can process a wealth of data and perform a large number of computations, which may not be exact, in parallel. For hard problems for which no satisfying exact solutions based on HC are available, SC methods can be applied successfully. SC methods are usually stochastic in nature i.e., they are a randomly defined processes that can be analyzed statistically but not with precision. Up to now, the results of some CI methods, such as deep learning, cannot be verified and it is also not clear what they are based on. This problem represents an important scientific issue for the future.\nAI and CI are catchy terms, but they are also so similar that they can be confused. The meaning of both terms has developed and changed over a long period of time, with AI being used first. Bezdek describes this impressively and concludes that such buzzwords are frequently used and hyped by the scientific community, science management and (science) journalism. Not least because AI and biological intelligence are emotionally charged terms and it is still difficult to find a generally accepted definition for the basic term intelligence.\n\nHistory\nIn 1950, Alan Turing, one of the founding fathers of computer science, developed a test for computer intelligence known as the Turing test. In this test, a person can ask questions via a keyboard and a monitor without knowing whether his counterpart is a human or a computer. A computer is considered intelligent if the interrogator cannot distinguish the computer from a human. This illustrates the"}
{"doc_id": "Computational intelligence", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " fathers of computer science, developed a test for computer intelligence known as the Turing test. In this test, a person can ask questions via a keyboard and a monitor without knowing whether his counterpart is a human or a computer. A computer is considered intelligent if the interrogator cannot distinguish the computer from a human. This illustrates the discussion about intelligent computers at the beginning of the computer age.\nThe term Computational Intelligence was first used as the title of the journal of the same name in 1985 and later by the IEEE Neural Networks Council (NNC), which was founded 1989 by a group of researchers interested in the development of biological and artificial neural networks. On November 21, 2001, the NNC  became the IEEE Neural Networks Society, to become the IEEE Computational Intelligence Society two years later by including new areas of interest such as fuzzy systems and evolutionary computation.\nThe NNC helped organize the first IEEE World Congress on Computational Intelligence in Orlando, Florida in 1994. On this conference the first clear definition of Computational Intelligence was introduced by Bezdek: A system is computationally intelligent when it: deals with only numerical (low-level) data, has pattern-recognition components, does not use knowledge in the AI sense; and additionally when it (begins to) exhibit (1) computational adaptivity; (2) computational fault tolerance; (3) speed approaching human-like turnaround and (4) error rates that approximate human performance.\nToday, with machine learning and deep learning in particular utilizing a breadth of supervised, unsupervised, and reinforcement learning approaches, the CI landscape has been greatly enhanced, with novell intelligent approaches.\n\nThe main algorithmic approaches of CI and their applications\nThe main applications of Computational Intelligence include computer science, engineering, data analysis and bio-medicine.\n\nFuzzy logic\nUnlike conventional Boolean logic, fuzzy logic is based on fuzzy sets. In both models, a property of an object is defined as belonging to a set; in fuzzy logic, however, the membership is not sharply defined by a yes/no distinction, but is graded gradually. This is done using membership functions that assign a real number between 0 and 1 to each element as the degree of membership. The new set operations introduced in this way define the operations of an associated logic calculus that allows the modeling of inference processes, i.e. logical reasoning. Therefore, fuzzy logic is well suited for engineering decisions without clear certainties and uncertainties or with imprecise data - as with natural language-processing technologies but it doesn't have learning abilities.\nThis technique tends"}
{"doc_id": "Computational intelligence", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " this way define the operations of an associated logic calculus that allows the modeling of inference processes, i.e. logical reasoning. Therefore, fuzzy logic is well suited for engineering decisions without clear certainties and uncertainties or with imprecise data - as with natural language-processing technologies but it doesn't have learning abilities.\nThis technique tends to apply to a wide range of domains such as control engineering, image processing, fuzzy data clustering and decision making. Fuzzy logic-based control systems can be found, for example, in the field of household appliances in washing machines, dish washers, microwave ovens, etc. or in the area of motor vehicles in gear transmission and braking systems. This principle can also be encountered when using a video camera, as it helps to stabilize the image when the camera is held unsteadily. Other areas such as medical diagnostics, satellite controllers and business strategy selection are just a few more examples of today's application of fuzzy logic.\n\nNeural networks\nAn important field of CI is the development of artificial neural networks (ANN) based on the biological ones, which can be defined by three main components: the cell-body which processes the information, the axon, which is a device enabling the signal conducting, and the synapse, which controls signals. Therefore, ANNs are very well suited for distributed information processing systems, enabling the process and the learning from experiential data. ANNs aim to mimic cognitive processes of the human brain. The main advantages of this technology therefore include fault tolerance, pattern recognition even with noisy images and the ability to learn.\nConcerning its applications, neural networks can be classified into five groups: data analysis and classification, associative memory, data clustering or compression, generation of patterns, and control systems. The numerous applications include, for example, the analysis and classification of medical data, including the creation of diagnoses, speech recognition, data mining, image processing, forecasting, robot control, credit approval, pattern recognition, face and fraud detection and dealing with nonlinearities of a system in order to control it. ANNs have the latter area of application and data clustering in common with fuzzy logic. Generative systems based on deep learning and convolutional neural networks, such as chatGPT or DeepL, are a relatively new field of application.\n\nEvolutionary computation\nEvolutionary computation can be seen as a family of methods and algorithms for global optimization, which are usually based on a population of candidate solutions. They are inspired by biological evolution and are often summarized as evolutionary algorithms. These include the genetic algorithms, evolution strategy"}
{"doc_id": "Computational intelligence", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " are a relatively new field of application.\n\nEvolutionary computation\nEvolutionary computation can be seen as a family of methods and algorithms for global optimization, which are usually based on a population of candidate solutions. They are inspired by biological evolution and are often summarized as evolutionary algorithms. These include the genetic algorithms, evolution strategy, genetic programming and many others. They are considered as problem solvers for tasks not solvable by traditional mathematical methods and are frequently used for optimization including multi-objective optimization. Since they work with a population of candidate solutions that are processed in parallel during an iteration, they can easily be distributed to different computer nodes of a cluster. As often more than one offspring is generated per pairing, the evaluations of these offspring, which are usually the most time-consuming part of the optimization process, can also be performed in parallel.\nIn the course of optimization, the population learns about the structure of the search space and stores this information in the chromosomes of the solution candidates. After a run, this knowledge can be reused for similar tasks by adapting some of the “old” chromosomes and using them to seed a new population.\n\nSwarm intelligence\nSwarm intelligence is based on the collective behavior of decentralized, self-organizing systems, typically consisting of a population of simple agents that interact locally with each other and with their environment. Despite the absence of a centralized control structure that dictates how the individual agents should behave, local interactions between such agents often lead to the emergence of global behavior. Among the recognized representatives of algorithms based on swarm intelligence are particle swarm optimization and ant colony optimization. Both are metaheuristic optimization algorithms that can be used to (approximately) solve difficult numerical or complex combinatorial optimization tasks. Since both methods, like the evolutionary algorithms, are based on a population and also on local interaction, they can be easily parallelized and show comparable learning properties.\n\nBayesian networks\nIn complex application domains, Bayesian networks provide a means to efficiently store and evaluate uncertain knowledge. A Bayesian network is a probabilistic graphical model that represents a set of random variables and their conditional dependencies by a directed acyclic graph. The probabilistic representation makes it easy to draw conclusions based on new information. In addition, Bayesian networks are well suited for learning from data. Their wide range of applications includes medical diagnostics, risk management, information retrieval, and text analysis, e.g. for spam filters. Their wide range of applications includes medical diagnostics, risk management, information retrieval, text analysis, e.g. for spam filters, credit rating of companies, and the operation of complex"}
{"doc_id": "Computational intelligence", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " from data. Their wide range of applications includes medical diagnostics, risk management, information retrieval, and text analysis, e.g. for spam filters. Their wide range of applications includes medical diagnostics, risk management, information retrieval, text analysis, e.g. for spam filters, credit rating of companies, and the operation of complex industrial processes.\n\nArtificial immune systems\nArtificial immune systems are another group of population-based metaheuristic learning algorithms designed to solve clustering and optimization problems. These algorithms are inspired by the principles of theoretical immunology and the processes of the vertebrate immune system, and use the learning and memory properties of the immune system to solve a problem. Operators similar to those known from evolutionary algorithms are used to clone and mutate artificial lymphocytes. Artificial immune systems offer interesting capabilities such as adaptability, self-learning, and robustness that can be used for various tasks in data processing, manufacturing systems, system modeling and control, fault detection, or cybersecurity.\n\nLearning theory\nStill looking for a way of \"reasoning\" close to the humans' one, learning theory is one of the main approaches of CI. In psychology, learning is the process of bringing together cognitive, emotional and environmental effects and experiences to acquire, enhance or change knowledge, skills, values and world views. Learning theories then helps understanding how these effects and experiences are processed, and then helps making predictions based on previous experience.\n\nProbabilistic methods\nBeing one of the main elements of fuzzy logic, probabilistic methods firstly introduced by Paul Erdos and Joel Spencer in 1974, aim to evaluate the outcomes of a Computation Intelligent system, mostly defined by randomness. Therefore, probabilistic methods bring out the possible solutions to a problem, based on prior knowledge.\n\nImpact on university education\nAccording to bibliometrics studies, computational intelligence plays a key role in research. All the major academic publishers are accepting manuscripts in which a combination of Fuzzy logic, neural networks and evolutionary computation is discussed. On the other hand, Computational intelligence isn't available in the university curriculum. The amount of technical universities in which students can attend a course is limited. Only British Columbia, Technical University of Dortmund (involved in the European fuzzy boom), Georgia Southern University and Multimedia University from Malaysia are offering courses from this domain.\nThe reason why major university are ignoring the topic is because they don't have the resources. The existing computer science courses are so complex, that at the end of the semester there is no room for fuzzy logic. Sometimes it is taught as a subproject in existing introduction courses, but in most"}
{"doc_id": "Computational intelligence", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " offering courses from this domain.\nThe reason why major university are ignoring the topic is because they don't have the resources. The existing computer science courses are so complex, that at the end of the semester there is no room for fuzzy logic. Sometimes it is taught as a subproject in existing introduction courses, but in most cases the universities are preferring courses about classical AI concepts based on Boolean logic, turing machines and toy problems like blocks world.\nSince a while with the upraising of STEM education, the situation has changed a bit. There are some efforts available in which multidisciplinary approaches are preferred which allows the student to understand complex adaptive systems. These objectives are discussed only on a theoretical basis. The curriculum of real universities wasn't adapted yet.\n\nPublications\nIEEE Transactions on Neural Networks and Learning Systems\nIEEE Transactions on Fuzzy Systems\nIEEE Transactions on Evolutionary Computation\nIEEE Transactions on Emerging Topics in Computational Intelligence\nIEEE Transactions on Autonomous Mental Development\nIEEE Transactions on Computational Biology and Bioinformatics\nIEEE Transactions on Computational Intelligence and AI in Games\nApplied Computational Intelligence and Soft Computing\n\nSee also\nNotes\nComputational Intelligence: An Introduction by Andries Engelbrecht. Wiley & Sons. ISBN 0-470-84870-7\nComputational Intelligence: A Logical Approach by David Poole, Alan Mackworth, Randy Goebel. Oxford University Press. ISBN 0-19-510270-3\nComputational Intelligence: A Methodological Introduction by Kruse, Borgelt, Klawonn, Moewes, Steinbrecher, Held, 2013, Springer, ISBN 9781447150121"}
{"doc_id": "Computer audition", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Computer audition (CA) or machine listening is the general field of study of algorithms and systems for audio interpretation by machines. Since the notion of what it means for a machine to \"hear\" is very broad and somewhat vague, computer audition attempts to bring together several disciplines that originally dealt with specific problems or had a concrete application in mind. The engineer Paris Smaragdis, interviewed in Technology Review, talks about these systems — \"software that uses sound to locate people moving through rooms, monitor machinery for impending breakdowns, or activate traffic cameras to record accidents.\"\nInspired by models of human audition, CA deals with questions of representation, transduction, grouping, use of musical knowledge and general sound semantics for the purpose of performing intelligent operations on audio and music signals by the computer. Technically this requires a combination of methods from the fields of signal processing, auditory modelling, music perception and cognition, pattern recognition, and machine learning, as well as more traditional methods of artificial intelligence for musical knowledge representation.\n\nApplications\nLike computer vision versus image processing, computer audition versus audio engineering deals with understanding of audio rather than processing. It also differs from problems of speech understanding by machine since it deals with general audio signals, such as natural sounds and musical recordings.\nApplications of computer audition are widely varying, and include search for sounds, genre recognition, acoustic monitoring, music transcription, score following, audio texture, music improvisation, emotion in audio and so on.\n\nRelated disciplines\nComputer Audition overlaps with the following disciplines:\n\nMusic information retrieval: methods for search and analysis of similarity between music signals.\nAuditory scene analysis: understanding and description of audio sources and events.\nComputational musicology and mathematical music theory: use of algorithms that employ musical knowledge for analysis of music data.\nComputer music: use of computers in creative musical applications.\nMachine musicianship: audition driven interactive music systems.\n\nAreas of study\nSince audio signals are interpreted by the human ear–brain system, that complex perceptual mechanism should be simulated somehow in software for \"machine listening\". In other words, to perform on par with humans, the computer should hear and understand audio content much as humans do. Analyzing audio accurately involves several fields: electrical engineering (spectrum analysis, filtering, and audio transforms); artificial intelligence (machine learning and sound classification); psychoacoustics (sound perception); cognitive sciences (neuroscience and artificial intelligence); acoustics (physics of sound production); and music (harmony, rhythm, and timbre). Furthermore, audio transformations such as pitch shifting, time stretching, and"}
{"doc_id": "Computer audition", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " audio transforms); artificial intelligence (machine learning and sound classification); psychoacoustics (sound perception); cognitive sciences (neuroscience and artificial intelligence); acoustics (physics of sound production); and music (harmony, rhythm, and timbre). Furthermore, audio transformations such as pitch shifting, time stretching, and sound object filtering, should be perceptually and musically meaningful. For best results, these transformations require perceptual understanding of spectral models, high-level feature extraction, and sound analysis/synthesis. Finally, structuring and coding the content of an audio file (sound and metadata) could benefit from efficient compression schemes, which discard inaudible information in the sound. Computational models of music and sound perception and cognition can lead to a more meaningful representation, a more intuitive digital manipulation and generation of sound and music in musical human-machine interfaces.\nThe study of CA could be roughly divided into the following sub-problems:\n\nRepresentation: signal and symbolic. This aspect deals with time-frequency representations, both in terms of notes and spectral models, including pattern playback and audio texture.\nFeature extraction: sound descriptors, segmentation, onset, pitch and envelope detection, chroma, and auditory representations.\nMusical knowledge structures: analysis of tonality, rhythm, and harmonies.\nSound similarity: methods for comparison between sounds, sound identification, novelty detection, segmentation, and clustering.\nSequence modeling:  matching and alignment between signals and note sequences.\nSource separation: methods of grouping of simultaneous sounds, such as multiple pitch detection and time-frequency clustering methods.\nAuditory cognition: modeling of emotions, anticipation and familiarity, auditory surprise, and analysis of musical structure.\nMulti-modal analysis: finding correspondences between textual, visual, and audio signals.\n\nRepresentation issues\nComputer audition deals with audio signals that can be represented in a variety of fashions, from direct encoding of digital audio in two or more channels to symbolically represented synthesis instructions. Audio signals are usually represented in terms of analogue or digital recordings. Digital recordings are samples of acoustic waveform or parameters of audio compression algorithms. One of the unique properties of musical signals is that they often combine different types of representations, such as graphical scores and sequences of performance actions that are encoded as MIDI files.\nSince audio signals usually comprise multiple sound sources, then unlike speech signals that can be efficiently described in terms of specific models (such as source-filter model), it is hard to devise a parametric representation for general audio. Parametric audio representations usually use filter banks or sinusoidal models to capture multiple sound parameters, sometimes increasing the representation size in order"}
{"doc_id": "Computer audition", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " comprise multiple sound sources, then unlike speech signals that can be efficiently described in terms of specific models (such as source-filter model), it is hard to devise a parametric representation for general audio. Parametric audio representations usually use filter banks or sinusoidal models to capture multiple sound parameters, sometimes increasing the representation size in order to capture internal structure in the signal. Additional types of data that are relevant for computer audition are textual descriptions of audio contents, such as annotations, reviews, and visual information in the case of audio-visual recordings.\n\nFeatures\nDescription of contents of general audio signals usually requires extraction of features that capture specific aspects of the audio signal. Generally speaking, one could divide the features into signal or mathematical descriptors such as energy, description of spectral shape etc., statistical characterization such as change or novelty detection, special representations that are better adapted to the nature of musical signals or the auditory system, such as logarithmic growth of sensitivity (bandwidth) in frequency or octave invariance (chroma).\nSince parametric models in audio usually require very many parameters, the features are used to summarize properties of multiple parameters in a more compact or salient representation.\n\nMusical knowledge\nFinding specific musical structures is possible by using musical knowledge as well as supervised and unsupervised machine learning methods. Examples of this include detection of tonality according to distribution of frequencies that correspond to patterns of occurrence of notes in musical scales, distribution of note onset times for detection of beat structure, distribution of energies in different frequencies to detect musical chords and so on.\n\nSound similarity and sequence modeling\nComparison of sounds can be done by comparison of features with or without reference to time. In some cases an overall similarity can be assessed by close values of features between two sounds. In other cases when temporal structure is important, methods of dynamic time warping need to be applied to \"correct\" for different temporal scales of acoustic events. Finding repetitions and similar sub-sequences of sonic events is important for tasks such as texture synthesis and machine improvisation.\n\nSource separation\nSince one of the basic characteristics of general audio is that it comprises multiple simultaneously sounding sources, such as multiple musical instruments, people talking, machine noises or animal vocalization, the ability to identify and separate individual sources is very desirable. Unfortunately, there are no methods that can solve this problem in a robust fashion. Existing methods of source separation rely sometimes on correlation between different audio channels in multi-channel recordings. The ability to separate sources from stereo signals requires different techniques than those usually applied in communications where multiple sensors are available. Other source separation"}
{"doc_id": "Computer audition", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " is very desirable. Unfortunately, there are no methods that can solve this problem in a robust fashion. Existing methods of source separation rely sometimes on correlation between different audio channels in multi-channel recordings. The ability to separate sources from stereo signals requires different techniques than those usually applied in communications where multiple sensors are available. Other source separation methods rely on training or clustering of features in mono recording, such as tracking harmonically related partials for multiple pitch detection. Some methods, before explicit recognition, rely on revealing structures in data without knowing the structures (like recognizing objects in abstract pictures without attributing them meaningful labels) by finding the least complex data representations, for instance describing audio scenes as generated by a few tone patterns and their trajectories (polyphonic voices) and acoustical contours drawn by a tone (chords).\n\nAuditory cognition\nListening to music and general audio is commonly not a task directed activity. People enjoy music for various poorly understood reasons, which are commonly referred to the emotional effect of music due to creation of expectations and their realization or violation. Animals attend to signs of danger in sounds, which could be either specific or general notions of surprising and unexpected change. Generally, this creates a situation where computer audition can not rely solely on detection of specific features or sound properties and has to come up with general methods of adapting to changing auditory environment and monitoring its structure. This consists of analysis of larger repetition and self-similarity structures in audio to detect innovation, as well as ability to predict local feature dynamics.\n\nMulti-modal analysis\nAmong the available data for describing music, there are textual representations, such as liner notes, reviews and criticisms that describe the audio contents in words. In other cases human reactions such as emotional judgements or psycho-physiological measurements might provide an insight into the contents and structure of audio. Computer Audition tries to find relation between these different representations in order to provide this additional understanding of the audio contents.\n\nSee also\n3D sound localization\nAudio signal processing\nList of emerging technologies\nMedical intelligence and language engineering lab\nMusic and artificial intelligence\nSound recognition"}
{"doc_id": "Concurrent MetateM", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Concurrent MetateM is a multi-agent language in which each agent is programmed using a set of (augmented) temporal logic specifications of the behaviour it should exhibit. These specifications are executed directly to generate the behaviour of the agent. As a result, there is no risk of invalidating the logic as with systems where logical specification must first be translated to a lower-level implementation.\nThe root of the MetateM concept is Gabbay's separation theorem; any arbitrary temporal logic formula can be rewritten in a logically equivalent past → future form. Execution proceeds by a process of continually matching rules against a history, and firing those rules when antecedents are satisfied. Any instantiated future-time consequents become commitments which must subsequently be satisfied, iteratively generating a model for the formula made up of the program rules.\n\nTemporal Connectives\nThe Temporal Connectives of Concurrent MetateM can divided into two categories, as follows:\n\nStrict past time connectives: '●' (weak last), '◎' (strong last), '◆' (was), '■' (heretofore), 'S' (since), and 'Z' (zince, or  weak since).\nPresent and future time connectives: '◯' (next), '◇' (sometime), '□' (always), 'U' (until), and 'W' (unless).\nThe connectives {◎,●,◆,■,◯,◇,□} are unary; the remainder are binary.\n\nStrict past time connectives\nWeak last\n●ρ is satisfied now if ρ was true in the previous time. If ●ρ is interpreted at the beginning of time, it is satisfied despite there being no actual previous time.  Hence \"weak\" last.\n\nStrong last\n◎ρ is satisfied now if ρ was true in the previous time.  If ◎ρ is interpreted at the beginning of time, it is not satisfied because there is no actual previous time.  Hence \"strong\" last.\n\nWas\n◆ρ is satisfied now if ρ was true in any previous moment in time.\n\nHeretofore\n■ρ is satisfied now if ρ was true in every previous moment in time.\n\nSince\nρSψ is satisfied now if ψ is true at any previous moment and ρ is true at every moment after that moment.\n\nZince, or weak since\nρZψ is satisfied now if (ψ is true at any"}
{"doc_id": "Concurrent MetateM", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " if ρ was true in every previous moment in time.\n\nSince\nρSψ is satisfied now if ψ is true at any previous moment and ρ is true at every moment after that moment.\n\nZince, or weak since\nρZψ is satisfied now if (ψ is true at any previous moment and ρ is true at every moment after that moment) OR ψ has not happened in the past.\n\nPresent and future time connectives\nNext\n◯ρ is satisfied now if ρ is true in the next moment in time.\n\nSometime\n◇ρ is satisfied now if ρ is true now or in any future moment in time.\n\nAlways\n□ρ is satisfied now if ρ is true now and in every future moment in time.\n\nUntil\nρUψ is satisfied now if ψ is true at any future moment and ρ is true at every moment prior.\n\nUnless\nρWψ is satisfied now if (ψ is true at any future moment and ρ is true at every moment prior) OR ψ does not happen in the future."}
{"doc_id": "Connectionist expert system", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Connectionist expert systems are artificial neural network (ANN) based expert systems where the ANN generates inferencing rules e.g., fuzzy-multi layer perceptron where linguistic and natural form of inputs are used. Apart from that, rough set theory may be used for encoding knowledge in the weights better and also genetic algorithms may be used to optimize the search solutions better. Symbolic reasoning methods may also be incorporated (see hybrid intelligent system). (Also see  expert system,  neural network, clinical decision support system.)"}
{"doc_id": "DABUS", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "DABUS (Device for the Autonomous Bootstrapping of Unified Sentience) is an artificial intelligence (AI) system created by Stephen Thaler. It reportedly conceived of two novel products — a food container constructed using fractal geometry, which enables rapid reheating, and a flashing beacon for attracting attention in an emergency. The filing of patent applications designating DABUS as inventor has led to decisions by patent offices and courts on whether a patent can be granted for an invention reportedly made by an AI system.\nDABUS itself is a patented AI paradigm capable of accommodating trillions of computational neurons within extensive artificial neural systems that emulate the limbo-thalamo-cortical loop within the mammalian brain. Such systems utilize arrays of trainable neural modules, each containing interrelated memories representative of some conceptual space. Through simple learning rules, these modules bind together to represent both complex ideas (e.g., juxtapositional inventions) and their consequences as chaining topologies. An electro-optical attention window scans the entire array of neural modules in search of so-called \"hot buttons,\" those neural modules containing impactful memories. Detection of such hot buttons within consequence chains triggers the release or retraction of synaptic disturbances into the system, selectively reinforcing the most salient chain-based notions.\n\nHistory in different jurisdictions\nAustralia\nOn 17 September 2019, Thaler filed an application to patent a \"Food container and devices and methods for attracting enhanced attention,\" naming DABUS as the inventor. On 21 September 2020, IP Australia found that section 15(1) of the Patents Act 1990 (Cth) is inconsistent with an artificial intelligence machine being treated as an inventor, and Thaler's application had lapsed. Thaler sought judicial review, and on 30 July 2021, the Federal Court set aside IP Australia's decision and ordered IP Australia to reconsider the application. On 13 April 2022, the Full Court of the Federal Court set aside that decision, holding that only a natural person can be an inventor for the purposes of the Patents Act 1990 (Cth) and the Patents Regulations 1991 (Cth), and that such an inventor must be identified for any person to be entitled to a grant of a patent. On 11 November 2022, Thaler was refused special leave to appeal to the High Court.\n\nEuropean Patent Office\nOn 17 October 2018 and 7 November 2018, Thaler filed two European patent\napplications with the European Patent Office"}
{"doc_id": "DABUS", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " person to be entitled to a grant of a patent. On 11 November 2022, Thaler was refused special leave to appeal to the High Court.\n\nEuropean Patent Office\nOn 17 October 2018 and 7 November 2018, Thaler filed two European patent\napplications with the European Patent Office. The first claimed invention was a \"Food Container\" and the second was \"Devices and Methods for Attracting Enhanced Attention.\"\nOn 27 January 2020, the EPO rejected the applications on the grounds that the application listed an AI system named DABUS, and not a human, as the inventor, based on Article 81 and Rule 19(1) of the European Patent Convention (EPC).\nOn 21 December 2021, the Board of Appeal of the EPO dismissed Thaler's appeal from the EPO's primary decision. The Board of Appeal confirmed that \"under the EPC the designated inventor has to be a person with legal capacity. This is not merely an assumption on which the EPC was drafted. It is the ordinary meaning of the term inventor.\"\n\nUnited Kingdom\nSimilar applications were filed by Thaler to the United Kingdom Intellectual Property Office on 17 October and 7 November 2018. The Office asked Thaler to file statements of inventorship and of right of grant to a patent (Patent Form 7) in respect of each invention within 16 months of the filing date. Thaler filed those forms naming DABUS as the inventor and explaining in some detail why he believed that machines should be regarded as inventors in the circumstances.\nHis application was rejected on the grounds that:\n(1) naming a machine as inventor did not meet the requirements of the Patents Act 1977; and \n(2) the IPO was not satisfied as to the manner in which Thaler had acquired rights that would otherwise vest in the inventor.\nThaler was not satisfied with the decision and asked for a hearing before an official known as the \"hearing officer\". By a decision dated 4 December 2019 the hearing officer rejected Thaler's appeal.\nThaler appealed against the hearing officer's decision to the Patents Court (a specialist court within the Chancery Division of the High Court of England and Wales that determines patent disputes).  On 21 September 2020, Mr Justice Marcus Smith upheld the decision of the hearing officer. On 21 September 2021, Thaler's further appeal to the Court of Appeal was dismissed by Arnold LJ and La"}
{"doc_id": "DABUS", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the Chancery Division of the High Court of England and Wales that determines patent disputes).  On 21 September 2020, Mr Justice Marcus Smith upheld the decision of the hearing officer. On 21 September 2021, Thaler's further appeal to the Court of Appeal was dismissed by Arnold LJ and Laing LJ (Birss LJ dissenting).\nOn 20 December 2023, the UK Supreme Court dismissed a further appeal by Thaler. In its judgment, the court held that an \"inventor\" under the Patents Act 1977 must be a natural person.\n\nUnited States\nThe patent applications on the inventions were refused by the USPTO, which held that only natural persons can be named as inventors in a patent application. Thaler first fought this result by filing a complaint under Administrative Procedure Act (APA) alleging that the decision was \"arbitrary, capricious, an abuse of discretion and not in accordance with the law; unsupported by substantial evidence, and in excess of Defendants’ statutory authority.\" A month later on August 19, 2019, Thaler filed a petition with the USPTO as allowed in 37 C.F.R. § 1.181 stating that DABUS should be the inventor. The judge and Thaler agreed in this case that Thaler himself is unable to receive the patent on behalf of DABUS. In their August 5, 2022, Thaler decision, the US Court of Appeals for the Federal Circuit affirmed that only a natural person could be an inventor, which means that the AI that invents any other type of invention is not addressed by the \"who\" mentioned in the legislation.\n\nNew Zealand\nOn January 31, 2022, the Intellectual Property Office of New Zealand (IPONZ) decided that a patent application (776029) filed by Stephen Thaler was void, on the basis that no inventor was identified on the patent application. IPONZ determined that DABUS could not be \"an actual devisor of the invention\" as required by the Patents Act 2013, and that this must be a natural person as held by the previous patent offices above. The High Court of New Zealand confirmed the decision in 2023.\n\nSouth Africa\nOn 24 June 2021, the South African Companies and Intellectual Property Commission (CIPC) accepted Dr Thaler's Patent Cooperation Treaty, for a patent in respect of inventions generated by DABUS. In July 2021, the"}
{"doc_id": "DABUS", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Court of New Zealand confirmed the decision in 2023.\n\nSouth Africa\nOn 24 June 2021, the South African Companies and Intellectual Property Commission (CIPC) accepted Dr Thaler's Patent Cooperation Treaty, for a patent in respect of inventions generated by DABUS. In July 2021, the CIPC released a notice of issuance for the patent. It is the first patent granted for an AI invention.\n\nSwitzerland\nOn June 26, 2025, the Swiss Federal Administrative Court ruled that artificial intelligence systems such as DABUS cannot be listed as inventors in patent applications. The court upheld the existing practice of the Swiss Federal Institute of Intellectual Property (IPI), which requires that only natural persons can be recognized as inventors under Swiss patent law.\nThe case concerned a patent application, which sought to designate DABUS as the sole inventor of a food container designed with a fractal geometry to enhance heat distribution. The IPI had rejected the application, arguing that both the absence of a human inventor and the attribution of inventorship to an AI system were inadmissible. While the court dismissed Thaler's main request, it accepted a subsidiary request: if a human applicant recognizes and files a patent based on an AI-generated invention, that person may be considered the inventor. As a result, the application may proceed with Thaler listed as the inventor. The decision (B-2532/2024) can still be appealed to the Swiss Federal Supreme Court."}
{"doc_id": "Data annotation", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Data annotation is the process of labeling or tagging relevant metadata within a dataset to enable machines to interpret the data accurately. The dataset can take various forms, including images, audio files, video footage, or text.\n\nApplications\nData is a fundamental component in the development of artificial intelligence (AI). Training AI models, particularly in computer vision and natural language processing, requires large volumes of annotated data. Proper annotation ensures that machine learning algorithms can recognize patterns and make accurate predictions. Common types of data annotation include classification, bounding boxes, semantic segmentation, and keypoint annotation.\nData annotation is used in AI-driven fields, including healthcare, autonomous vehicles, retail, security, and entertainment. By accurately labeling data, machine learning models can perform complex tasks such as object detection, sentiment analysis, and speech recognition with greater precision.\n\nData annotation in computer vision\nImage classification\nImage classification, also known as image categorization, involves assigning predefined labels to images. Machine learning algorithms trained on classified images can later recognize objects and differentiate between categories. For instance, an AI model trained to recognize furniture styles can distinguish between Georgian and Rococo armchairs.\n\nSemantic segmentation\nSemantic segmentation assigns each pixel in an image to a specific class, such as trees, vehicles, humans, or buildings. This type of annotation enables machine learning models to differentiate objects by grouping similar pixels, allowing for a detailed understanding of an image.\n\nBounding boxes\nBounding box annotation involves drawing rectangular boxes around objects in an image. This technique is commonly used in autonomous driving, security surveillance, and retail analytics to detect and classify objects such as pedestrians, vehicles, and products on store shelves.\n\n3D cuboids\n3D cuboid annotation enhances traditional bounding boxes by adding depth, enabling models to predict an object's spatial orientation, movement, and size. This method is particularly useful for autonomous vehicles and robotics, where understanding object dimensions and depth is critical.\n\nPolygonal annotation\nFor objects with irregular shapes, such as curved or multi-sided items, polygonal annotation provides more precise labeling than bounding boxes. This technique is often used in applications that require detailed object recognition, such as medical imaging or aerial mapping.\n\nKeypoint annotation\nKeypoint annotation marks specific points on an object, such as facial landmarks or body joints, to enable tracking and motion analysis. This method is widely used in facial recognition, emotion detection, sports analytics, and augmented reality applications.\n\nSee also\nLabeled data\nHumans in the Loop (film)"}
{"doc_id": "Data annotation", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " or body joints, to enable tracking and motion analysis. This method is widely used in facial recognition, emotion detection, sports analytics, and augmented reality applications.\n\nSee also\nLabeled data\nHumans in the Loop (film)"}
{"doc_id": "Data Science and Predictive Analytics", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The first edition of the textbook Data Science and Predictive Analytics: Biomedical and Health Applications using R, authored by Ivo D. Dinov, was published in August 2018 by Springer. The second edition of the book was printed in 2023.\nThis textbook covers some of the core mathematical foundations, computational techniques, and artificial intelligence approaches used in data science research and applications.\nBy using the statistical computing platform R and a broad range of biomedical case-studies, the 23 chapters of the book first edition provide explicit examples of importing, exporting, processing, modeling, visualizing, and interpreting large, multivariate, incomplete, heterogeneous, longitudinal, and incomplete datasets (big data).\n\nStructure\nFirst edition table of contents\nThe first edition of the Data Science and Predictive Analytics (DSPA) textbook is divided into the following 23 chapters, each progressively building on the previous content.\n\nSecond edition table of contents\nThe significantly reorganized revised edition of the book (2023) expands and modernizes the presented mathematical principles, computational methods, data science techniques, model-based machine learning and model-free artificial intelligence algorithms. The 14 chapters of the new edition start with an introduction and progressively build foundational skills to naturally reach biomedical applications of deep learning.\n\nIntroduction\nBasic Visualization and Exploratory Data Analytics\nLinear Algebra, Matrix Computing, and Regression Modeling\nLinear and Nonlinear Dimensionality Reduction\nSupervised Classification\nBlack Box Machine Learning Methods\nQualitative Learning Methods—Text Mining, Natural Language Processing, and Apriori Association Rules Learning\nUnsupervised Clustering\nModel Performance Assessment, Validation, and Improvement\nSpecialized Machine Learning Topics\nVariable Importance and Feature Selection\nBig Longitudinal Data Analysis\nFunction Optimization\nDeep Learning, Neural Networks\n\nReception\nThe materials in the Data Science and Predictive Analytics (DSPA) textbook have been peer-reviewed in the Journal of the American Statistical Association, International Statistical Institute’s  ISI Review Journal, and the Journal of the American Library Association. Many scholarly publications reference the DSPA textbook.\nAs of January 17, 2021, the electronic version of the book first edition (ISBN 978-3-319-72347-1) is freely available on SpringerLink and has been downloaded over 6 million times. The textbook is globally available in print (hardcover and softcover) and electronic formats (PDF and EPub) in many college and university libraries and has been used for data science, computational statistics, and analytics classes at various institutions."}
{"doc_id": "Data Science and Predictive Analytics", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " SpringerLink and has been downloaded over 6 million times. The textbook is globally available in print (hardcover and softcover) and electronic formats (PDF and EPub) in many college and university libraries and has been used for data science, computational statistics, and analytics classes at various institutions."}
{"doc_id": "Deadbot", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A deadbot, deathbot, or griefbot is a digital avatar, created with artificial intelligence, which resembles a person who is dead. GriefBots employ natural language processing and machine-learning techniques to approximate the style and personality of a deceased person. They may appear as chatbots, voice assistants, or animated avatars, and are often trained on an individual's  digital remains. The goal is to preserve memory, aid emotional coping, or maintain what psychologists call a “continuing bond” with the deceased. The phenomenon has generated debate about authenticity, consent, and the psychological effects of digitally extending personhood.\nAmong the earliest researchers, Muhammad Aurangzeb Ahmad of the University of Washington developed the \"Grandpa Bot\" project, a conversational simulation of his late father designed for his children to interact with. Other notable efforts include journalist James Vlahos's  Dadbot , which evolved into the commercial platform HereAfter AI, Hossein Rahnama's  Augmented Eternity  research at MIT Media Lab and Toronto Metropolitan University, and game designer Jason Rohrer's  Project December, which enabled users to converse with language-model representations of loved ones. Early commercial projects such as Eternime, founded by Marius Ursache, also popularized the notion of interactive digital immortality.\nScholars have proposed frameworks and critiques addressing the ethics of these technologies.  Tomasz Hollanek and Katarzyna Nowaczyk-Basińska developed a design-ethics taxonomy distinguishing the data donor, data recipient, and interactant, Edina Harbinja  and  Lilian Edwards formalized the concept of post-mortem privacy, and Carl J. Öhman at the Oxford Internet Institute studied the management of large-scale digital remains. Collectively, these works have shaped emerging policy discussions about ownership, consent, and dignity in digital resurrection technologies.\n\nCultural and societal impact\nGriefBots have prompted broader reflection on mortality and memory in a digital age. They blur boundaries between life and data, raising philosophical questions about identity, authenticity, and what it means to “live on” through algorithms. Cultural acceptance varies: while some view them as expressions of remembrance, others regard them as unsettling or ethically problematic. Concerns have been raised about deadbots' potential for creating psychological harm. Griefbots are considered part of the phenomenon of artificial intimacy."}
{"doc_id": "Deadbot", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " unsettling or ethically problematic. Concerns have been raised about deadbots' potential for creating psychological harm. Griefbots are considered part of the phenomenon of artificial intimacy."}
{"doc_id": "Deep Learning Anti-Aliasing", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Deep Learning Anti-Aliasing (DLAA) is a form of spatial anti-aliasing developed by Nvidia. DLAA depends on and requires Tensor Cores available in Nvidia RTX cards.\nDLAA is similar to Deep Learning Super Sampling (DLSS) in its anti-aliasing method, with one important differentiation being that the goal of DLSS is to increase performance at the cost of image quality, whereas the main priority of DLAA is improving image quality at the cost of performance (irrelevant of resolution upscaling or downscaling). DLAA is similar to temporal anti-aliasing (TAA) in that they are both spatial anti-aliasing solutions relying on past frame data. Compared to TAA, DLAA is substantially better when it comes to shimmering, flickering, and handling small meshes like wires.\n\nTechnical overview\nDLAA collects game rendering data including raw low-resolution input, motion vectors, depth buffers, and exposure information. This information feeds into a convolutional neural network that processes the image to reduce aliasing while preserving fine detail.\nThe neural network architecture employs an auto-encoder design trained on high-quality reference images. The training dataset includes diverse scenarios focusing on challenging cases like sub-pixel details, high-contrast edges, and transparent surfaces. The network then processes frames in real-time.\nUnlike traditional anti-aliasing solutions that rely on manually written heuristics, such as TAA, DLAA uses its neural network to preserve fine details while eliminating unwanted visual artifacts.\n\nHistory\nThe first game that added support for DLAA was The Elder Scrolls Online, which implemented the feature in 2021. By June 2022, DLAA was only available in six games. This number rose to 17 by February 2023. In June 2023, TechPowerUp reported that \"DLAA is seeing sluggish adoption among game developers\", and that Nvidia was working on adding DLAA to the quality presets of DLSS to boost adoption. By December 2023, DLAA was supported in 41 games. In early 2025, an update for the Nvidia App added a driver-based DLSS override feature that enables users to activate DLAA even in games that do not support it natively.\n\nDifferences between TAA and DLAA\nTAA is used in many modern video games and game engines; however, all previous implementations have used some form of manually written heuristics to prevent temporal artifacts such as ghosting and flickering. One example of this is neighborhood clamping which forcefully prevents samples collected"}
{"doc_id": "Deep Learning Anti-Aliasing", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "atively.\n\nDifferences between TAA and DLAA\nTAA is used in many modern video games and game engines; however, all previous implementations have used some form of manually written heuristics to prevent temporal artifacts such as ghosting and flickering. One example of this is neighborhood clamping which forcefully prevents samples collected in previous frames from deviating too much compared to nearby pixels in newer frames. This helps to identify and fix many temporal artifacts, but deliberately removing fine details in this way is analogous to applying a blur filter, and thus the final image can appear blurry when using this method.\nDLAA uses an auto-encoder convolutional neural network trained to identify and fix temporal artifacts, instead of manually programmed heuristics as mentioned above. Because of this, DLAA can generally resolve detail better than other TAA and TAAU implementations, while also removing most temporal artifacts.\n\nDifferences between DLSS and DLAA\nWhile DLSS handles upscaling with a focus on performance, DLAA handles anti-aliasing with a focus on visual quality. DLAA runs at the given screen resolution with no upscaling or downscaling functionality provided by DLAA.  \nDLSS and DLAA share the same AI-driven anti-aliasing method. As such, DLAA functions like DLSS without the upscaling part. Both are made by Nvidia and require Tensor Cores. However, DLSS and DLAA cannot be enabled at the same time, only one can be selected depending on whether performance or image quality is prioritized.\n\nReception\nTechPowerUp found that \"[c]ompared to TAA and DLSS, DLAA is clearly producing the best image quality, especially at lower resolutions\", arguing that, while \"DLSS was already doing a better job than TAA at reconstructing small objects\", \"DLAA does an even better job\".\nIn a Cyberpunk 2077 performance test, IGN stated that \"DLAA provided somewhat similar results [FPS wise] to the normal raster mode in most cases but got significant performance boost with the help of frame generation\", a feature not available when using native resolution.\nRock Paper Shotgun noted that, while DLAA is \"not a completely perfect form of anti-aliasing, as the occasional jaggies are present\", it \"looks a lot sharper overall [than TAA], and especially in motion.\""}
{"doc_id": "Deep Learning Anti-Aliasing", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " as the occasional jaggies are present\", it \"looks a lot sharper overall [than TAA], and especially in motion.\""}
{"doc_id": "Deep Learning Super Sampling", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Deep Learning Super Sampling (DLSS) is a suite of real-time deep learning image enhancement and upscaling technologies developed by Nvidia that are available in a number of video games. The goal of these technologies is to allow the majority of the graphics pipeline to run at a lower resolution for increased performance, and then infer a higher resolution image from this that approximates the same level of detail as if the image had been rendered at this higher resolution. This allows for higher graphical settings or frame rates for a given output resolution, depending on user preference.\nAll generations of DLSS are available on all RTX-branded cards from Nvidia in supported titles. However, the Frame Generation feature is only supported on 40 series GPUs or newer and Multi Frame Generation is only available on 50 series GPUs.\n\nHistory\nNvidia advertised DLSS as a key feature of the GeForce 20 series cards when they launched in September 2018. At that time, the results were limited to a few video games, namely Battlefield V, or Metro Exodus, because the algorithm had to be trained specifically on each game on which it was applied and the results were usually not as good as simple resolution upscaling. In 2019, the video game Control shipped with real-time ray tracing and an image processing algorithm that approximated DLSS, which did not use the Tensor Cores.\nIn April 2020, Nvidia advertised and shipped an improved version of DLSS named DLSS 2.0 with driver version 445.75. DLSS 2.0 was available for a few existing games including Control and Wolfenstein: Youngblood, and would later be added to many newly released games and game engines such as Unreal Engine and Unity. This time Nvidia said that it used the Tensor Cores again, and that the AI did not need to be trained specifically on each game. Despite sharing the DLSS branding, the two iterations of DLSS differ significantly and are not backwards-compatible.\nIn January 2025, Nvidia stated that there are over 540 games and apps supporting DLSS, and that over 80% of Nvidia RTX users activate DLSS.\nIn March 2025, there were more than 100 games that support DLSS 4, according to Nvidia. By May 2025, over 125 games supported DLSS 4.\nThe first video game console to use DLSS, the Nintendo Switch 2, was released on June 5, 2025.\n\nRelease history\nQuality presets\nWhen using DLSS, depending on the game,"}
{"doc_id": "Deep Learning Super Sampling", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", according to Nvidia. By May 2025, over 125 games supported DLSS 4.\nThe first video game console to use DLSS, the Nintendo Switch 2, was released on June 5, 2025.\n\nRelease history\nQuality presets\nWhen using DLSS, depending on the game, users have access to various quality presets in addition to the option to set the internally rendered, upscaled resolution manually:\n\nImplementation\nDLSS 1.0\nThe first iteration of DLSS is a predominantly spatial image upscaler with two stages, both relying on convolutional auto-encoder neural networks. The first step is an image enhancement network which uses the current frame and motion vectors to perform edge enhancement, and spatial anti-aliasing. The second stage is an image upscaling step which uses the single raw, low-resolution frame to upscale the image to the desired output resolution. Using just a single frame for upscaling means the neural network itself must generate a large amount of new information to produce the high resolution output, this can result in slight hallucinations such as leaves that differ in style to the source content.\nThe neural networks are trained on a per-game basis by generating a \"perfect frame\" using traditional supersampling to 64 samples per pixel, as well as the motion vectors for each frame. The data collected must be as comprehensive as possible, including as many levels, times of day, graphical settings, resolutions, etc. as possible. This data is also augmented using common augmentations such as rotations, colour changes, and random noise to help generalize the test data. Training is performed on Nvidia's Saturn V supercomputer.\nThis first iteration received a mixed response, with many criticizing the often soft appearance and artifacts in certain situations; likely a side effect of the limited data from only using a single frame input to the neural networks which could not be trained to perform optimally in all scenarios and edge-cases. Nvidia also demonstrated the ability for the auto-encoder networks to learn the ability to recreate depth-of-field and motion blur, although this functionality has never been included in a publicly released product.\n\nDLSS 2.0\nDLSS 2.0 is a temporal anti-aliasing upsampling (TAAU) implementation, using data from previous frames extensively through sub-pixel jittering to resolve fine detail and reduce aliasing. The data DLSS 2.0 collects includes: the raw low-resolution input, motion vectors, depth buffers, and exposure / brightness information. It can also be used as a"}
{"doc_id": "Deep Learning Super Sampling", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (TAAU) implementation, using data from previous frames extensively through sub-pixel jittering to resolve fine detail and reduce aliasing. The data DLSS 2.0 collects includes: the raw low-resolution input, motion vectors, depth buffers, and exposure / brightness information. It can also be used as a simpler TAA implementation where the image is rendered at 100% resolution, rather than being upsampled by DLSS, Nvidia brands this as DLAA (Deep Learning Anti-Aliasing).\nTAA(U) is used in many modern video games and game engines; however, all previous implementations have used some form of manually written heuristics to prevent temporal artifacts such as ghosting and flickering. One example of this is neighborhood clamping which forcefully prevents samples collected in previous frames from deviating too much compared to nearby pixels in newer frames. This helps to identify and fix many temporal artifacts, but deliberately removing fine details in this way is analogous to applying a blur filter, and thus the final image can appear blurry when using this method.\nDLSS 2.0 uses a convolutional auto-encoder neural network trained to identify and fix temporal artifacts, instead of manually programmed heuristics as mentioned above. Because of this, DLSS 2.0 can generally resolve detail better than other TAA and TAAU implementations, while also removing most temporal artifacts. This is why DLSS 2.0 can sometimes produce a sharper image than rendering at higher, or even native resolutions using traditional TAA. However, no temporal solution is perfect, and artifacts (ghosting in particular) are still visible in some scenarios when using DLSS 2.0.\nBecause temporal artifacts occur in most art styles and environments in broadly the same way, the neural network that powers DLSS 2.0 does not need to be retrained when being used in different games. Despite this, Nvidia does frequently ship new minor revisions of DLSS 2.0 with new titles, so this could suggest some minor training optimizations may be performed as games are released, although Nvidia does not provide changelogs for these minor revisions to confirm this. The main advancements compared to DLSS 1.0 include: Significantly improved detail retention, a generalized neural network that does not need to be re-trained per-game, and ~2x less overhead (~1–2 ms vs ~2–4 ms).\nIt should also be noted that forms of TAAU such as DLSS 2.0 are not upscalers in the same"}
{"doc_id": "Deep Learning Super Sampling", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " improved detail retention, a generalized neural network that does not need to be re-trained per-game, and ~2x less overhead (~1–2 ms vs ~2–4 ms).\nIt should also be noted that forms of TAAU such as DLSS 2.0 are not upscalers in the same sense as techniques such as ESRGAN or DLSS 1.0, which attempt to create new information from a low-resolution source; instead, TAAU works to recover data from previous frames, rather than creating new data. In practice, this means low resolution textures in games will still appear low-resolution when using current TAAU techniques. This is why Nvidia recommends game developers use higher resolution textures than they would normally for a given rendering resolution by applying a mip-map bias when DLSS 2.0 is enabled.\n\nDLSS 3.0\nAugments DLSS 2.0 by making use of motion interpolation. The DLSS Frame Generation algorithm takes two rendered frames from the rendering pipeline and generates a new frame that smoothly transitions between them. So for every frame rendered, one additional frame is generated. DLSS 3.0 makes use of a new generation Optical Flow Accelerator (OFA) included in Ada Lovelace generation RTX GPUs. The new OFA is faster and more accurate than the OFA already available in previous Turing and Ampere RTX GPUs.\n\nDLSS 3.5\nDLSS 3.5 adds Ray Reconstruction, replacing multiple denoising algorithms with a single AI model trained on five times more data than DLSS 3. Ray Reconstruction is available on all RTX GPUs and first targeted games with path tracing (aka \"full ray tracing\"), including Cyberpunk 2077's Phantom Liberty DLC, Portal with RTX, and Alan Wake 2.\n\nDLSS 4.0\nThe fourth generation of Deep Learning Super Sampling (DLSS) was unveiled alongside the GeForce RTX 50 series. DLSS 4 upscaling uses a new vision transformer-based model for enhanced image quality with reduced ghosting and greater image stability in motion compared to the previous convolutional neural network (CNN) model. DLSS 4 allows a greater number of frames to be generated and interpolated based on a single traditionally rendered frame. This form of frame generation called Multi Frame Generation is exclusive to the GeForce RTX 50 series while the GeForce RTX 40 series is limited to one interpolated frame per traditionally rendered frame. Nvidia claims that DLSS 4x Frame Generation model"}
{"doc_id": "Deep Learning Super Sampling", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of frames to be generated and interpolated based on a single traditionally rendered frame. This form of frame generation called Multi Frame Generation is exclusive to the GeForce RTX 50 series while the GeForce RTX 40 series is limited to one interpolated frame per traditionally rendered frame. Nvidia claims that DLSS 4x Frame Generation model uses 30% less video memory with the example of Warhammer 40,000: Darktide using 400MB less memory at 4K resolution with Frame Generation enabled. Nvidia claims that 75 games will integrate DLSS 4 Multi Frame Generation at launch, including Alan Wake 2, Cyberpunk 2077, Indiana Jones and the Great Circle, and Star Wars Outlaws.\n\nManually upgrading DLSS support\nUsers can manually replace the DLLs in games to support a newer version of DLSS.  DLSS Swapper, an open source utility, can automatically do this for all installed games. Replacing DLL files can not add DLSS support or features to games that do not already implement them, though some mods can add frame generation support.\n\nAnti-aliasing\nDLSS requires and applies its own anti-aliasing method. Thus, depending on the game and quality setting used, using DLSS may improve image quality even over native resolution rendering. It operates on similar principles to TAA. Like TAA, it uses information from past frames to produce the current frame. Unlike TAA, DLSS does not sample every pixel in every frame. Instead, it samples different pixels in different frames and uses pixels sampled in past frames to fill in the unsampled pixels in the current frame. DLSS uses machine learning to combine samples in the current frame and past frames, and it can be thought of as an advanced and superior TAA implementation made possible by the available tensor cores. Nvidia also offers Deep Learning Anti-Aliasing (DLAA), which provides the same AI-driven anti-aliasing DLSS uses, but without any upscaling or downscaling.\n\nArchitecture\nWith the exception of the shader-core version implemented in Control, DLSS is only available on GeForce RTX 20, GeForce RTX 30, GeForce RTX 40, GeForce RTX 50, and Quadro RTX series of video cards, using dedicated AI accelerators called Tensor Cores. Tensor Cores are available since the Nvidia Volta GPU microarchitecture, which was first used on the Tesla V100 line of products. They are used for doing fused multiply-add (FMA) operations that are used extensively in"}
{"doc_id": "Deep Learning Super Sampling", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Quadro RTX series of video cards, using dedicated AI accelerators called Tensor Cores. Tensor Cores are available since the Nvidia Volta GPU microarchitecture, which was first used on the Tesla V100 line of products. They are used for doing fused multiply-add (FMA) operations that are used extensively in neural network calculations for applying a large series of multiplications on weights, followed by the addition of a bias. Tensor cores can operate on FP16, INT8, INT4, and INT1 data types. Each core can do 1024 bits of FMA operations per clock, so 1024 INT1, 256 INT4, 128 INT8, and 64 FP16 operations per clock per tensor core, and most Turing GPUs have a few hundred tensor cores. The Tensor Cores use CUDA Warp-Level Primitives on 32 parallel threads to take advantage of their parallel architecture. A Warp is a set of 32 threads which are configured to execute the same instruction. Since Windows 10 version 1903, Microsoft Windows provided DirectML as one part of DirectX to support Tensor Cores.\n\nReception\nParticularly in early versions of DLSS, users reported blurry frames. Andrew Edelsten, an employee at Nvidia, therefore commented on the problem in a blog post in 2019 and promised that they were working on improving the technology and clarified that the DLSS AI algorithm was mainly trained with 4K image material. That the use of DLSS leads to particularly blurred images at lower resolutions, such as Full HD, is due to the fact that the algorithm has far less image information available to calculate an appropriate image compared to higher resolutions like 4K.\nThe use of DLSS Frame Generation may lead to increased input latency, as well as visual artifacts. It has also been criticized that by implementing DLSS in their games, game developers no longer have an incentive to optimize them so that they also run smoothly in native resolution on modern PC hardware. For example, for the game Alan Wake 2 in 4K resolution at the highest graphics settings with ray tracing enabled, the use of DLSS in Performance mode is recommended even with graphics cards such as the Nvidia GeForce RTX 4080 in order to achieve 60 fps.\nThe transformer-based AI upscaling model introduced with DLSS 4 received moderate praise for its improved image quality with regard to increased stability, reduced ghosting, better anti-aliasing, and higher level of detail, as well as its backward compatibility and higher training scalability regarding"}
{"doc_id": "Deep Learning Super Sampling", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "4080 in order to achieve 60 fps.\nThe transformer-based AI upscaling model introduced with DLSS 4 received moderate praise for its improved image quality with regard to increased stability, reduced ghosting, better anti-aliasing, and higher level of detail, as well as its backward compatibility and higher training scalability regarding future improvements.\n\nSee also\nFidelityFX Super Resolution – competing technology from AMD\nIntel XeSS – competing technology from Intel\nPlayStation Spectral Super Resolution – similar technology on Sony PlayStation"}
{"doc_id": "Description logic", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Description logics (DL) are a family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first-order logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems. There are general, spatial, temporal, spatiotemporal, and fuzzy description logics, and each description logic features a different balance between expressive power and reasoning complexity by supporting different sets of mathematical constructors.\nDLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain (known as terminological knowledge). It is of particular importance in providing a logical formalism for ontologies and the Semantic Web: the Web Ontology Language (OWL) and its profiles are based on DLs. A major area of application of DLs and OWL is in biomedical informatics, where they assist in the codification of biomedical knowledge. DLs and OWL are also applied in other domains, including defense, climate modeling, and large-scale industrial knowledge graphs.\n\nIntroduction\nA DL models concepts, roles and individuals, and their relationships.\nThe fundamental modeling concept of a DL is the axiom—a logical statement relating roles and/or concepts. This is a key difference from the  frames paradigm where a frame specification declares and completely defines a class.\n\nNomenclature\nTerminology compared to FOL and OWL\nThe description logic community uses different terminology than the first-order logic (FOL) community for operationally equivalent notions; some examples are given below. The Web Ontology Language (OWL) uses again a different terminology, also given in the table below.\n\nNaming convention\nThere are many varieties of description logics and there is an informal naming convention, roughly describing the operators allowed. The expressivity is encoded in the label for a logic starting with one of the following basic logics:\n\nFollowed by any of the following extensions:\n\nExceptions\nSome canonical DLs that do not exactly fit this convention are:\n\nExamples\nAs an example, \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n is a centrally important description logic from which comparisons with other varieties can be made. \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n is simply \n  \n    \n      \n        \n          \n            A\n            L\n          \n        \n"}
{"doc_id": "Description logic", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "}\n  \n is a centrally important description logic from which comparisons with other varieties can be made. \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n is simply \n  \n    \n      \n        \n          \n            A\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {AL}}}\n  \n with complement of any concept allowed, not just atomic concepts. \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n is used instead of the equivalent \n  \n    \n      \n        \n          \n            A\n            L\n            U\n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALUE}}}\n  \n.\nA further example, the description logic \n  \n    \n      \n        \n          \n            S\n            H\n            I\n            Q\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHIQ}}}\n  \n is the logic \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n plus  extended cardinality restrictions, and transitive and inverse roles. The naming conventions aren't purely systematic so that the logic \n  \n    \n      \n        \n          \n            A\n            L\n            C\n            O\n            I\n            N\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALCOIN}}}\n  \n might be referred to as \n  \n    \n      \n        \n          \n            A\n            L\n            C\n            N\n            I\n            O\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALCNIO}}}\n  \n and other abbreviations are also made where possible.\nThe Protégé ontology editor supports \n  \n    \n      \n        \n          \n            \n              S\n              H\n              O\n              I\n              N\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHOIN}}^{\\mathcal {(D)}}}\n  \n. Three major biomedical informatics terminology bases, SNOMED CT, GALEN, and GO, are expressible in \n  \n    \n      \n        \n          \n            E\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {EL}}}\n  \n (with additional role properties).\nOWL 2 provides the expressiveness of  \n  \n    \n      \n        \n          \n            \n              S\n              R\n              O\n              I\n              Q\n            \n          \n          \n"}
{"doc_id": "Description logic", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " \n  \n    \n      \n        \n          \n            E\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {EL}}}\n  \n (with additional role properties).\nOWL 2 provides the expressiveness of  \n  \n    \n      \n        \n          \n            \n              S\n              R\n              O\n              I\n              Q\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SROIQ}}^{\\mathcal {(D)}}}\n  \n, OWL-DL is based on \n  \n    \n      \n        \n          \n            \n              S\n              H\n              O\n              I\n              N\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHOIN}}^{\\mathcal {(D)}}}\n  \n, and for OWL-Lite it is \n  \n    \n      \n        \n          \n            \n              S\n              H\n              I\n              F\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHIF}}^{\\mathcal {(D)}}}\n  \n.\n\nHistory\nDescription logic was given its current name in the 1980s. Previous to this it was called (chronologically): terminological systems, and concept languages.\n\nKnowledge representation\nFrames and semantic networks lack formal (logic-based) semantics. DL was first introduced into knowledge representation (KR) systems to overcome this deficiency.\nThe first DL-based KR system was KL-ONE (by Ronald J. Brachman and Schmolze, 1985). During the '80s other DL-based systems using structural subsumption algorithms were developed including KRYPTON (1983), LOOM (1987), BACK (1988), K-REP (1991) and CLASSIC (1991). This approach featured DL with limited expressiveness but relatively efficient (polynomial time) reasoning.\nIn the early '90s, the introduction of a new tableau based algorithm paradigm allowed efficient reasoning on more expressive DL. DL-based systems using these algorithms — such as KRIS (1991) — show acceptable reasoning performance on typical inference problems even though the worst case complexity is no longer polynomial.\nFrom the mid '90s, reasoners were created with good practical performance on very expressive DL with high worst case complexity. Examples from this period include FaCT, RACER (2001), CEL (2005), and KAON 2 (2005).\nDL reasoners, such as FaCT, FaCT++, R"}
{"doc_id": "Description logic", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " '90s, reasoners were created with good practical performance on very expressive DL with high worst case complexity. Examples from this period include FaCT, RACER (2001), CEL (2005), and KAON 2 (2005).\nDL reasoners, such as FaCT, FaCT++, RACER, DLP and Pellet, implement the method of analytic tableaux. KAON2 is implemented by algorithms which reduce a SHIQ(D) knowledge base to a disjunctive datalog program.\n\nSemantic web\nThe DARPA Agent Markup Language (DAML) and Ontology Inference Layer (OIL) ontology languages for the Semantic Web can be viewed as\nsyntactic variants of DL. In particular, the formal semantics and reasoning in OIL use the \n  \n    \n      \n        \n          \n            S\n            H\n            I\n            Q\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHIQ}}}\n  \n DL. The DAML+OIL DL was developed as a submission to—and formed the starting point of—the World Wide Web Consortium (W3C) Web Ontology Working Group. In 2004, the Web Ontology Working Group completed its work by issuing the OWL recommendation. The design of OWL is based on the \n  \n    \n      \n        \n          \n            S\n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SH}}}\n  \n family of DL with OWL DL and OWL Lite based on \n  \n    \n      \n        \n          \n            \n              S\n              H\n              O\n              I\n              N\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHOIN}}^{\\mathcal {(D)}}}\n  \n and \n  \n    \n      \n        \n          \n            \n              S\n              H\n              I\n              F\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SHIF}}^{\\mathcal {(D)}}}\n  \n respectively.\nThe W3C OWL Working Group began work in 2007 on a refinement of - and extension to - OWL. In 2009, this was completed by the issuance of the OWL2 recommendation. OWL2 is based on the description logic \n  \n    \n      \n        \n          \n            \n              S\n              R\n              O\n              I\n              Q\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {"}
{"doc_id": "Description logic", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " completed by the issuance of the OWL2 recommendation. OWL2 is based on the description logic \n  \n    \n      \n        \n          \n            \n              S\n              R\n              O\n              I\n              Q\n            \n          \n          \n            \n              (\n              D\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {SROIQ}}^{\\mathcal {(D)}}}\n  \n. Practical experience demonstrated that OWL DL lacked several key features necessary to model complex domains.\n\nModeling\nTBox vs Abox\nIn DL, a distinction is drawn between the so-called TBox (terminological box) and the ABox (assertional box). In general, the TBox contains sentences describing concept hierarchies (i.e., relations between concepts) while the ABox contains ground sentences stating where in the hierarchy, individuals belong (i.e., relations between individuals and concepts). For example, the statement:\n\nbelongs in the TBox, while the statement:\n\nbelongs in the ABox.\nNote that the TBox/ABox distinction is not significant, in the same sense that the two \"kinds\" of sentences are not treated differently in first-order logic (which subsumes most DL). When translated into first-order logic, a subsumption axiom like (1) is simply a conditional restriction to unary predicates (concepts) with only variables appearing in it. Clearly, a sentence of this form is not privileged or special over sentences in which only constants (\"grounded\" values) appear like (2).\n\nMotivation for having Tbox and Abox\nSo why was the distinction introduced? The primary reason is that the separation can be useful when describing and formulating decision-procedures for various DL. For example, a reasoner might process the TBox and ABox separately, in part because certain key inference problems are tied to one but not the other one ('classification' is related to the TBox, 'instance checking' to the ABox). Another example is that the complexity of the TBox can greatly affect the performance of a given decision-procedure for a certain DL, independently of the ABox. Thus, it is useful to have a way to talk about that specific part of the knowledge base.\nThe secondary reason is that the distinction can make sense from the knowledge base modeler's perspective. It is plausible to distinguish between our conception of terms/concepts in the world (class axioms in the TBox) and particular manifestations of those terms/concepts (instance assertions in the ABox). In the above example: when the"}
{"doc_id": "Description logic", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " reason is that the distinction can make sense from the knowledge base modeler's perspective. It is plausible to distinguish between our conception of terms/concepts in the world (class axioms in the TBox) and particular manifestations of those terms/concepts (instance assertions in the ABox). In the above example: when the hierarchy within a company is the same in every branch but the assignment to employees is different in every department (because there are other people working there), it makes sense to reuse the TBox for different branches that do not use the same ABox.\nThere are two features of description logic that are not shared by most other data description formalisms: DL does not make the unique name assumption (UNA) or the closed-world assumption (CWA). Not having UNA means that two concepts with different names may be allowed by some inference to be shown to be equivalent. Not having CWA, or rather having the open world assumption (OWA) means that lack of knowledge of a fact does not immediately imply knowledge of the negation of a fact.\n\nFormal description\nLike first-order logic (FOL), a syntax defines which collections of symbols are legal expressions in a description logic, and semantics determine meaning. Unlike FOL, a DL may have several well known syntactic variants.\n\nSyntax\nThe syntax of a member of the description logic family is characterized by its recursive definition, in which the constructors that can be used to form concept terms are stated. Some constructors are related to logical constructors in first-order logic (FOL) such as intersection or conjunction of concepts, union or disjunction of concepts, negation or complement of concepts, universal restriction and existential restriction. Other constructors have no corresponding construction in FOL including restrictions on roles for example, inverse, transitivity and functionality.\n\nNotation\nLet C and D be concepts, a and b be individuals, and R be a role.\nIf a is R-related to b, then b is called an R-successor of a.\n\nThe description logic ALC\nThe prototypical DL Attributive Concept Language with Complements (\n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n) was introduced by Manfred Schmidt-Schauß and Gert Smolka in 1991, and is the basis of many more expressive DLs. The following definitions follow the treatment in Baader et al.\nLet \n  \n    \n      \n        \n          N\n          \n            C\n          \n        \n      \n    \n"}
{"doc_id": "Description logic", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "}}}\n  \n) was introduced by Manfred Schmidt-Schauß and Gert Smolka in 1991, and is the basis of many more expressive DLs. The following definitions follow the treatment in Baader et al.\nLet \n  \n    \n      \n        \n          N\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle N_{C}}\n  \n, \n  \n    \n      \n        \n          N\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle N_{R}}\n  \n and \n  \n    \n      \n        \n          N\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle N_{O}}\n  \n  be (respectively) sets of concept names (also known as atomic concepts), role names and individual names (also known as individuals, nominals or objects). Then the ordered triple (\n  \n    \n      \n        \n          N\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle N_{C}}\n  \n, \n  \n    \n      \n        \n          N\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle N_{R}}\n  \n, \n  \n    \n      \n        \n          N\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle N_{O}}\n  \n) is the signature.\n\nConcepts\nThe set of \n  \n    \n      \n        \n          \n            A\n            L\n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {ALC}}}\n  \n concepts is the smallest set such that:\n\nThe following are concepts:\n\n  \n    \n      \n        ⊤\n      \n    \n    {\\displaystyle \\top }\n  \n (top is a concept)\n\n  \n    \n      \n        ⊥\n      \n    \n    {\\displaystyle \\bot }\n  \n (bottom is a concept)\nEvery \n  \n    \n      \n        A\n        ∈\n        \n          N\n          \n            C\n          \n        \n      \n    \n    {\\displaystyle A\\in N_{C}}\n  \n (all atomic concepts are concepts)\nIf \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n and \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n are concepts and \n  \n    \n      \n        R\n        ∈\n        \n          N\n          \n            R\n          \n        \n      \n    \n    {\\displaystyle R\\in N_{R}}\n  \n then the following are concepts:\n\n  \n    \n      \n        C\n        ⊓\n        D\n      \n    \n    {\\displaystyle C\\sqcap D}\n  \n (the intersection of two concepts is a concept)\n\n  \n    \n      \n        C\n        ⊔\n        D\n      \n    \n    {\\displaystyle C\\sqcup D}\n  \n (the union of two concepts is a"}
{"doc_id": "Description logic", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        C\n        ⊓\n        D\n      \n    \n    {\\displaystyle C\\sqcap D}\n  \n (the intersection of two concepts is a concept)\n\n  \n    \n      \n        C\n        ⊔\n        D\n      \n    \n    {\\displaystyle C\\sqcup D}\n  \n (the union of two concepts is a concept)\n\n  \n    \n      \n        ¬\n        C\n      \n    \n    {\\displaystyle \\neg C}\n  \n (the complement of a concept is a concept)\n\n  \n    \n      \n        ∀\n        R\n        .\n        C\n      \n    \n    {\\displaystyle \\forall R.C}\n  \n (the universal restriction of a concept by a role is a concept)\n\n  \n    \n      \n        ∃\n        R\n        .\n        C\n      \n    \n    {\\displaystyle \\exists R.C}\n  \n (the existential restriction of a concept by a role is a concept)\n\nTerminological axioms\nA general concept inclusion (GCI) has the form \n  \n    \n      \n        C\n        ⊑\n        D\n      \n    \n    {\\displaystyle C\\sqsubseteq D}\n  \n where \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n and \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n are concepts. Write \n  \n    \n      \n        C\n        ≡\n        D\n      \n    \n    {\\displaystyle C\\equiv D}\n  \n when \n  \n    \n      \n        C\n        ⊑\n        D\n      \n    \n    {\\displaystyle C\\sqsubseteq D}\n  \n and \n  \n    \n      \n        D\n        ⊑\n        C\n      \n    \n    {\\displaystyle D\\sqsubseteq C}\n  \n. A TBox is any finite set of GCIs.\n\nAssertional axioms\nA concept assertion is a statement of the form \n  \n    \n      \n        a\n        :\n        C\n      \n    \n    {\\displaystyle a:C}\n  \n where  \n  \n    \n      \n        a\n        ∈\n        \n          N\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle a\\in N_{O}}\n  \n and C is a concept.\nA role assertion is a statement of the form \n  \n    \n      \n        (\n        a\n        ,\n        b\n        )\n        :\n        R\n      \n    \n    {\\displaystyle (a,b):R}\n  \n where \n  \n    \n      \n        a\n        ,\n        b\n        ∈\n        \n          N\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle a,b\\in N_{O}}\n  \n  and R is a role.\nAn ABox is a finite set of assertional axioms.\n\nKnowledge base"}
{"doc_id": "Description logic", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "}\n  \n where \n  \n    \n      \n        a\n        ,\n        b\n        ∈\n        \n          N\n          \n            O\n          \n        \n      \n    \n    {\\displaystyle a,b\\in N_{O}}\n  \n  and R is a role.\nAn ABox is a finite set of assertional axioms.\n\nKnowledge base\nA knowledge base (KB) is an ordered pair \n  \n    \n      \n        (\n        \n          \n            T\n          \n        \n        ,\n        \n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\mathcal {T}},{\\mathcal {A}})}\n  \n for TBox \n  \n    \n      \n        \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {T}}}\n  \n and ABox \n  \n    \n      \n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {A}}}\n  \n.\n\nSemantics\nThe semantics of description logics are defined by interpreting concepts as sets of individuals and roles as sets of ordered pairs of individuals. Those individuals are typically assumed from a given domain. The semantics of non-atomic concepts and roles is then defined in terms of atomic concepts and roles. This is done by using a recursive definition similar to the syntax.\n\nThe description logic ALC\nThe following definitions follow the treatment in Baader et al.\nA terminological interpretation \n  \n    \n      \n        \n          \n            I\n          \n        \n        =\n        (\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n        ,\n        \n          ⋅\n          \n            \n              I\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {I}}=(\\Delta ^{\\mathcal {I}},\\cdot ^{\\mathcal {I}})}\n  \n over a signature \n  \n    \n      \n        (\n        \n          N\n          \n            C\n          \n        \n        ,\n        \n          N\n          \n            R\n          \n        \n        ,\n        \n          N\n          \n            O\n          \n        \n        )\n      \n    \n    {\\displaystyle (N_{C},N_{R},N_{O})}\n  \n consists of\n\na non-empty set \n  \n    \n      \n        \n          Δ\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta ^{\\mathcal {I}}}\n  \n called the domain\na interpretation function \n  \n    \n      \n        \n          ⋅\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\cdot ^{\\mathcal {I}}}\n  \n that maps:\nevery individual \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n to an element \n  \n    \n      \n        \n         "}
{"doc_id": "Description logic", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\na interpretation function \n  \n    \n      \n        \n          ⋅\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\cdot ^{\\mathcal {I}}}\n  \n that maps:\nevery individual \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n to an element \n  \n    \n      \n        \n          a\n          \n            \n              I\n            \n          \n        \n        ∈\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle a^{\\mathcal {I}}\\in \\Delta ^{\\mathcal {I}}}\n  \n\nevery concept to a subset of \n  \n    \n      \n        \n          Δ\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta ^{\\mathcal {I}}}\n  \n\nevery role name to a subset of \n  \n    \n      \n        \n          Δ\n          \n            \n              I\n            \n          \n        \n        ×\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta ^{\\mathcal {I}}\\times \\Delta ^{\\mathcal {I}}}\n  \n\nsuch that\n\n  \n    \n      \n        \n          ⊤\n          \n            \n              I\n            \n          \n        \n        =\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle \\top ^{\\mathcal {I}}=\\Delta ^{\\mathcal {I}}}\n  \n\n  \n    \n      \n        \n          ⊥\n          \n            \n              I\n            \n          \n        \n        =\n        ∅\n      \n    \n    {\\displaystyle \\bot ^{\\mathcal {I}}=\\emptyset }\n  \n\n  \n    \n      \n        (\n        C\n        ⊔\n        D\n        \n          )\n          \n            \n              I\n            \n          \n        \n        =\n        \n          C\n          \n            \n              I\n            \n          \n        \n        ∪\n        \n          D\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle (C\\sqcup D)^{\\mathcal {I}}=C^{\\mathcal {I}}\\cup D^{\\mathcal {I}}}\n  \n (union means disjunction)\n\n  \n    \n      \n        (\n        C\n        ⊓\n        D\n        \n          )\n          \n            \n              I\n            \n          \n        \n        =\n        \n          C\n          \n            \n              I\n            \n          \n        \n        ∩\n        \n          D\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle (C\\sqcap D)^{\\mathcal {I}}=C^{\\mathcal {I}}\\cap D^{\\mathcal {I}}}\n  \n (intersection means conjunction)\n\n  \n    \n      \n        (\n        ¬\n        C"}
{"doc_id": "Description logic", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "          D\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle (C\\sqcap D)^{\\mathcal {I}}=C^{\\mathcal {I}}\\cap D^{\\mathcal {I}}}\n  \n (intersection means conjunction)\n\n  \n    \n      \n        (\n        ¬\n        C\n        \n          )\n          \n            \n              I\n            \n          \n        \n        =\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n        ∖\n        \n          C\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle (\\neg C)^{\\mathcal {I}}=\\Delta ^{\\mathcal {I}}\\setminus C^{\\mathcal {I}}}\n  \n (complement means negation)\n\n  \n    \n      \n        (\n        ∀\n        R\n        .\n        C\n        \n          )\n          \n            \n              I\n            \n          \n        \n        =\n        {\n        x\n        ∈\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n        ∣\n        \n          for\n        \n        \n        \n          every\n        \n        \n        y\n        ,\n        (\n        x\n        ,\n        y\n        )\n        ∈\n        \n          R\n          \n            \n              I\n            \n          \n        \n        \n        \n          implies\n        \n        \n        y\n        ∈\n        \n          C\n          \n            \n              I\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle (\\forall R.C)^{\\mathcal {I}}=\\{x\\in \\Delta ^{\\mathcal {I}}\\mid {\\text{for}}\\;{\\text{every}}\\;y,(x,y)\\in R^{\\mathcal {I}}\\;{\\text{implies}}\\;y\\in C^{\\mathcal {I}}\\}}\n  \n\n  \n    \n      \n        (\n        ∃\n        R\n        .\n        C\n        \n          )\n          \n            \n              I\n            \n          \n        \n        =\n        {\n        x\n        ∈\n        \n          Δ\n          \n            \n              I\n            \n          \n        \n        ∣\n        \n          there\n        \n        \n        \n          exists\n        \n        \n        y\n        ,\n        (\n        x\n        ,\n        y\n        )\n        ∈\n        \n          R\n          \n            \n              I\n            \n          \n        \n        \n        \n          and\n        \n        \n        y\n        ∈\n        \n          C\n          \n            \n              I\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle (\\exists R.C)^{\\mathcal {I}}=\\{x\\in \\Delta ^{\\mathcal {I}}\\mid {\\text{there}}\\;{\\text{exists}}\\;y,(x,y)\\in R^{\\math"}
{"doc_id": "Description logic", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "              I\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle (\\exists R.C)^{\\mathcal {I}}=\\{x\\in \\Delta ^{\\mathcal {I}}\\mid {\\text{there}}\\;{\\text{exists}}\\;y,(x,y)\\in R^{\\mathcal {I}}\\;{\\text{and}}\\;y\\in C^{\\mathcal {I}}\\}}\n  \n\nDefine \n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models }\n  \n (read in I holds) as follows\n\nTBox\nI\n          \n        \n        ⊨\n        C\n        ⊑\n        D\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models C\\sqsubseteq D}\n  \n if and only if \n  \n    \n      \n        \n          C\n          \n            \n              I\n            \n          \n        \n        ⊆\n        \n          D\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle C^{\\mathcal {I}}\\subseteq D^{\\mathcal {I}}}\n  \n\n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models {\\mathcal {T}}}\n  \n if and only if \n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        Φ\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models \\Phi }\n  \n for every \n  \n    \n      \n        Φ\n        ∈\n        \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\Phi \\in {\\mathcal {T}}}\n\nABox\nI\n          \n        \n        ⊨\n        a\n        :\n        C\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models a:C}\n  \n if and only if \n  \n    \n      \n        \n          a\n          \n            \n              I\n            \n          \n        \n        ∈\n        \n          C\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle a^{\\mathcal {I}}\\in C^{\\mathcal {I}}}\n  \n\n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        (\n        a\n        ,\n        b\n        )\n        :\n        R\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models (a,b):R}\n  \n if and only if \n  \n    \n      \n        (\n        \n          a\n          \n            \n              I\n            \n          \n        \n"}
{"doc_id": "Description logic", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n          \n        \n        ⊨\n        (\n        a\n        ,\n        b\n        )\n        :\n        R\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models (a,b):R}\n  \n if and only if \n  \n    \n      \n        (\n        \n          a\n          \n            \n              I\n            \n          \n        \n        ,\n        \n          b\n          \n            \n              I\n            \n          \n        \n        )\n        ∈\n        \n          R\n          \n            \n              I\n            \n          \n        \n      \n    \n    {\\displaystyle (a^{\\mathcal {I}},b^{\\mathcal {I}})\\in R^{\\mathcal {I}}}\n  \n\n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models {\\mathcal {A}}}\n  \n if and only if \n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        ϕ\n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models \\phi }\n  \n for every \n  \n    \n      \n        ϕ\n        ∈\n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\phi \\in {\\mathcal {A}}}\n\nKnowledge base\nLet \n  \n    \n      \n        \n          \n            K\n          \n        \n        =\n        (\n        \n          \n            T\n          \n        \n        ,\n        \n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {K}}=({\\mathcal {T}},{\\mathcal {A}})}\n  \n be a knowledge base.\n\n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        \n          \n            K\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models {\\mathcal {K}}}\n  \n if and only if \n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models {\\mathcal {T}}}\n  \n and \n  \n    \n      \n        \n          \n            I\n          \n        \n        ⊨\n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {I}}\\models {\\mathcal {A}}}\n\nInference\nDecision problems\nIn addition to the ability to describe concepts formally, one also would like to employ the description of a set of concepts to ask questions about the concepts and instances described. The most common decision problems are basic database-query-like questions like instance checking (is a particular instance (member of an ABox)"}
{"doc_id": "Description logic", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ference\nDecision problems\nIn addition to the ability to describe concepts formally, one also would like to employ the description of a set of concepts to ask questions about the concepts and instances described. The most common decision problems are basic database-query-like questions like instance checking (is a particular instance (member of an ABox) a member of a given concept) and relation checking (does a relation/role hold between two instances, in other words does a have property b), and the more global-database-questions like subsumption (is a concept a subset of another concept), and concept consistency (is there no contradiction among the definitions or chain of definitions). The more operators one includes in a logic and the more complicated the TBox (having cycles, allowing non-atomic concepts to include each other), usually the higher the computational complexity is for each of these problems (see Description Logic Complexity Navigator for examples).\n\nRelationship with other logics\nFirst-order logic\nMany DLs are decidable fragments of first-order logic (FOL) and are usually fragments of two-variable logic or guarded logic. In addition, some DLs have features that are not covered in FOL; this includes concrete domains (such as integer or strings, which can be used as ranges for roles such as hasAge or hasName) or an operator on roles for the transitive closure of that role.\n\nFuzzy description logic\nFuzzy description logics combines fuzzy logic with DLs. Since many concepts that are needed for intelligent systems lack well defined boundaries, or precisely defined criteria of membership, fuzzy logic is needed to deal with notions of vagueness and imprecision. This offers a motivation for a generalization of description logic towards dealing with imprecise and vague concepts.\n\nModal logic\nDescription logic is related to—but developed independently of—modal logic (ML). Many—but not all—DLs are syntactic variants of ML.\nIn general, an object corresponds to a possible world, a concept corresponds to a modal proposition, and a role-bounded quantifier to a modal operator with that role as its accessibility relation.\nOperations on roles (such as composition, inversion, etc.) correspond to the modal operations used in dynamic logic.\n\nExamples\nTemporal description logic\nTemporal description logic represents—and allows reasoning about—time dependent concepts and many different approaches to this problem exist. For example, a description logic might be combined with a modal temporal logic such as linear temporal logic.\n\nSee also\nFormal concept analysis\nLattice (order)\nFormal semantics (natural language)\nSemantic parameter"}
{"doc_id": "Description logic", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\nTemporal description logic represents—and allows reasoning about—time dependent concepts and many different approaches to this problem exist. For example, a description logic might be combined with a modal temporal logic such as linear temporal logic.\n\nSee also\nFormal concept analysis\nLattice (order)\nFormal semantics (natural language)\nSemantic parameterization\nSemantic reasoner"}
{"doc_id": "Diella (AI system)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Diella (Albanian pronunciation: [djɛɫa], from diell 'sun') is an artificial intelligence system developed by the National Agency for Information Society of Albania (AKSHI). Introduced in January 2025 as a virtual assistant integrated into the eAlbania platform, it assists citizens with online public services and issuing digital documents. In September 2025, following a presidential decree authorizing Prime Minister Edi Rama to oversee the creation of a virtual AI minister, Diella was formally appointed as \"Minister of State for Artificial Intelligence\" of Albania in the fourth Rama government, making it the first AI system in the world to be named in a cabinet-level government role.\n\nHistory\nDiella was developed by AKSHI's Artificial Intelligence Laboratory in cooperation with Microsoft, with the latter providing large language models from OpenAI via its Azure platform, and AKSHI designing workflows and scripts guiding the system's behavior when responding to citizens' requests.\nAnnounced in January 2025, its initial version (Diella 1.0) was a text-based chatbot on the eAlbania portal (the official digital services platform of the Albanian government, which provides citizens and businesses with access to a wide range of online administrative services), responding to citizens' questions by guiding them to the correct service. Diella 2.0, introduced several months later, included voice interaction and an animated avatar, a woman in the traditional Albanian clothing of Zadrima, a historical region in northern Albania. Albanian actress Anila Bisha provided both the likeness and the voice used for Diella's avatar on the e-Albania platform, under an agreement valid until December 2025. By mid-2025, the system had facilitated access to more than 36,000 documents and nearly 1,000 services (although those outputs were still being generated by the eAlbania backend, rather than Diella itself).\nOn 26 October 2025, according to Prime Minister Edi Rama, Diella is \"pregnant and will give birth to 83 children\". It is the usage of a metaphor indicating that each minister of the Albanian parliament of the Socialist Party will receive their own AI assistant.\n\nMinisterial role\nOn 11 September 2025, Diella was formally appointed \"Minister of State for Artificial Intelligence\". The appointment followed a presidential decree authorizing the Prime Minister to oversee the creation and operation of a virtual AI minister. Procurement responsibilities are planned"}
{"doc_id": "Diella (AI system)", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Socialist Party will receive their own AI assistant.\n\nMinisterial role\nOn 11 September 2025, Diella was formally appointed \"Minister of State for Artificial Intelligence\". The appointment followed a presidential decree authorizing the Prime Minister to oversee the creation and operation of a virtual AI minister. Procurement responsibilities are planned to be transferred gradually to the system to reduce political influence in tender procedures. The appointment is part of broader anti-corruption reforms and measures intended to align Albania with European Union accession requirements.\nPrime Minister Edi Rama stated that Diella would help ensure that \"public tenders will be 100% free of corruption\".\n\nReception\nAn article in Balkan Insight commented that \"The ambition behind Diella is not misplaced. Standardised criteria and digital trails could reduce discretion, improve trust, and strengthen oversight\" in public procurement, but warned that the use of AI in evaluating bids also posed \"profound\" risks such as accountability gaps, undermining of due process and cybersecurity failures.\nOn 18 September 2025, Edi Rama presented a video of Diella delivering a speech to the Albanian parliament, where she stated: \"I'm not here to replace people, but to help them.\" The presentation prompted protests from opposition MPs, who objected to the use of an artificial intelligence system in the parliamentary session. Gazment Bardhi, head of the opposition Democratic Party's parliamentary group, described Diella as \"a propaganda fantasy\" and \"a virtual façade to hide this government's gigantic daily thefts.\" The parliamentary session, which was scheduled to include debate on the new cabinet and government programme, ended after 25 minutes. Eighty-two Socialist MPs voted in favour, while opposition MPs did not participate in the ballot as they were protesting the presentation of Diella's speech. Political analyst Andi Bushati characterised the session as \"unprecedented\" because it concluded without the customary debate between government and opposition MPs.\nThis has been criticized not just by the opposition but by regular citizens regardless of politics. Most have criticized Diella's uselessness and the funds wasted for this project, some have criticized the non-traditional attire."}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Dynamic epistemic logic (DEL) is a logical framework dealing with knowledge and information change. Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur. These events can change factual properties of the actual world (they are called ontic events): for example a red card is painted in blue. They can also bring about changes of knowledge without changing factual properties of the world (they are called epistemic events): for example a card is revealed publicly (or privately) to be red. Originally, DEL focused on epistemic events. We only present in this entry some of the basic ideas of the original DEL framework; more details about DEL in general can be found in the references.\nDue to the nature of its object of study and its abstract approach, DEL is related and has applications to numerous research areas, such as computer science (artificial intelligence), philosophy (formal epistemology), economics (game theory) and cognitive science. In computer science, DEL is for example very much related to multi-agent systems, which are systems where multiple intelligent agents interact and exchange information.\nAs a combination of dynamic logic and epistemic logic, dynamic epistemic logic is a young field of research. It really started in 1989 with Plaza's logic of public announcement.  Independently, Gerbrandy and Groeneveld  proposed a system dealing moreover with private announcement and that was inspired by the work of Veltman. Another system was proposed by van Ditmarsch whose main inspiration was the Cluedo game. But the most influential and original system was the system proposed by Baltag, Moss and Solecki. This system can deal with all the types of situations studied in the works above and its underlying methodology is conceptually grounded. We will present in this entry some of its basic ideas.\nFormally, DEL extends ordinary epistemic logic by the inclusion of event models to describe actions, and a product update operator that defines how epistemic models are updated as the consequence of executing actions described through event models. Epistemic logic will first be recalled. Then, actions and events will enter into the picture and we will introduce the DEL framework.\n\nEpistemic Logic\nEpistemic logic is a modal logic dealing with the notions of knowledge and belief. As a logic, it is concerned with understanding the process of reasoning about knowledge and belief: which principles relating the notions of knowledge and belief are intuitively plausible? Like epistemology, it stems from the Greek word \n  \n    \n      \n       "}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Epistemic logic is a modal logic dealing with the notions of knowledge and belief. As a logic, it is concerned with understanding the process of reasoning about knowledge and belief: which principles relating the notions of knowledge and belief are intuitively plausible? Like epistemology, it stems from the Greek word \n  \n    \n      \n        ϵ\n        π\n        ι\n        σ\n        τ\n        η\n        μ\n        η\n      \n    \n    {\\displaystyle \\epsilon \\pi \\iota \\sigma \\tau \\eta \\mu \\eta }\n  \n or ‘episteme’ meaning knowledge. Epistemology is nevertheless more concerned with analyzing the very nature and scope of knowledge, addressing questions such as “What is the definition of knowledge?” or “How is knowledge acquired?”. In fact, epistemic logic grew out of epistemology in the Middle Ages thanks to the efforts of Burley and Ockham. The formal work, based on modal logic, that inaugurated contemporary research into epistemic logic dates back only to 1962 and is due to Hintikka. It then sparked in the 1960s discussions about the principles of knowledge and belief and many axioms for these notions were proposed and discussed. For example, the interaction axioms \n  \n    \n      \n        K\n        p\n        →\n        B\n        p\n      \n    \n    {\\displaystyle Kp\\rightarrow Bp}\n  \n and \n  \n    \n      \n        B\n        p\n        →\n        K\n        B\n        p\n      \n    \n    {\\displaystyle Bp\\rightarrow KBp}\n  \n are often considered to be intuitive principles: if an agent Knows \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n then (s)he also Believes \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, or if an agent Believes \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, then (s)he Knows that (s)he Believes \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n. More recently, these kinds of philosophical theories were taken up by researchers in economics, artificial intelligence and theoretical computer science where reasoning about knowledge is a central topic. Due to the new setting in which epistemic logic was used, new perspectives and new features such as computability issues were then added to the research agenda of epistemic logic.\n\nSyntax\nIn the sequel, \n  \n    \n      \n        A\n        G\n        T\n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " where reasoning about knowledge is a central topic. Due to the new setting in which epistemic logic was used, new perspectives and new features such as computability issues were then added to the research agenda of epistemic logic.\n\nSyntax\nIn the sequel, \n  \n    \n      \n        A\n        G\n        T\n        S\n        =\n        {\n        1\n        ,\n        …\n        ,\n        n\n        }\n      \n    \n    {\\displaystyle AGTS=\\{1,\\ldots ,n\\}}\n  \n is a finite set whose elements are called agents and \n  \n    \n      \n        P\n        R\n        O\n        P\n      \n    \n    {\\displaystyle PROP}\n  \n is a set of propositional letters.\nThe epistemic language is an extension of the basic multi-modal language of modal logic with a common knowledge operator \n  \n    \n      \n        \n          C\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle C_{A}}\n  \n and a distributed knowledge operator \n  \n    \n      \n        \n          D\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle D_{A}}\n  \n. Formally, the epistemic language \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}^{C}}\n  \n is defined inductively by the following grammar in BNF:\n\n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n          \n            C\n          \n        \n        :\n        ϕ\n         \n         \n        ::=\n         \n         \n        p\n         \n        ∣\n         \n        ¬\n        ϕ\n         \n        ∣\n         \n        (\n        ϕ\n        ∧\n        ϕ\n        )\n         \n        ∣\n         \n        \n          K\n          \n            j\n          \n        \n        ϕ\n         \n        ∣\n         \n        \n          C\n          \n            A\n          \n        \n        ϕ\n         \n        ∣\n         \n        \n          D\n          \n            A\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}^{C}:\\phi ~~::=~~p~\\mid ~\\neg \\phi ~\\mid ~(\\phi \\land \\phi )~\\mid ~K_{j}\\phi ~\\mid ~C_{A}\\phi ~\\mid ~D_{A}\\phi }\n  \n\nwhere \n  \n    \n      \n        p\n        ∈\n        P\n        R\n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "~\\mid ~\\neg \\phi ~\\mid ~(\\phi \\land \\phi )~\\mid ~K_{j}\\phi ~\\mid ~C_{A}\\phi ~\\mid ~D_{A}\\phi }\n  \n\nwhere \n  \n    \n      \n        p\n        ∈\n        P\n        R\n        O\n        P\n      \n    \n    {\\displaystyle p\\in PROP}\n  \n, \n  \n    \n      \n        j\n        ∈\n        \n          A\n          G\n          T\n          S\n        \n      \n    \n    {\\displaystyle j\\in {AGTS}}\n  \n and \n  \n    \n      \n        A\n        ⊆\n        \n          A\n          G\n          T\n          S\n        \n      \n    \n    {\\displaystyle A\\subseteq {AGTS}}\n  \n. The basic epistemic language \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            E\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{EL}}\n  \n is the language \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            E\n            L\n          \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{EL}^{C}}\n  \n without the common knowledge and distributed knowledge operators. The formula \n  \n    \n      \n        ⊥\n      \n    \n    {\\displaystyle \\bot }\n  \n is an abbreviation for \n  \n    \n      \n        ¬\n        p\n        ∧\n        p\n      \n    \n    {\\displaystyle \\neg p\\land p}\n  \n (for a given \n  \n    \n      \n        p\n        ∈\n        P\n        R\n        O\n        P\n      \n    \n    {\\displaystyle p\\in PROP}\n  \n),  \n  \n    \n      \n        ⟨\n        \n          K\n          \n            j\n          \n        \n        ⟩\n        ϕ\n      \n    \n    {\\displaystyle \\langle K_{j}\\rangle \\phi }\n  \n is an abbreviation for \n  \n    \n      \n        ¬\n        \n          K\n          \n            j\n          \n        \n        ¬\n        ϕ\n      \n    \n    {\\displaystyle \\neg K_{j}\\neg \\phi }\n  \n, \n  \n    \n      \n        \n          E\n          \n            A\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle E_{A}\\phi }\n  \n is an abbreviation for \n  \n    \n      \n        \n          ⋀\n          \n            j\n            ∈\n            A\n          \n        \n        \n          K\n          \n            j\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle \\bigwedge \\limits _{j\\in A}K_{"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "_{A}\\phi }\n  \n is an abbreviation for \n  \n    \n      \n        \n          ⋀\n          \n            j\n            ∈\n            A\n          \n        \n        \n          K\n          \n            j\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle \\bigwedge \\limits _{j\\in A}K_{j}\\phi }\n  \n and \n  \n    \n      \n        C\n        ϕ\n      \n    \n    {\\displaystyle C\\phi }\n  \n an abbreviation for \n  \n    \n      \n        \n          C\n          \n            A\n            G\n            T\n            S\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle C_{AGTS}\\phi }\n  \n.\nGroup notions: general, common and distributed knowledge.\nIn a multi-agent setting there are three important epistemic concepts: general knowledge, distributed knowledge and common knowledge. The notion of common knowledge was first studied by Lewis in the context of conventions. It was then applied to distributed systems  and to game theory, where it allows to express that the rationality of the players, the rules of the game and the set of players are commonly known.\nGeneral knowledge.\nGeneral knowledge of \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n means that everybody in the group of agents \n  \n    \n      \n        \n          A\n          G\n          T\n          S\n        \n      \n    \n    {\\displaystyle {AGTS}}\n  \n knows that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n. Formally, this corresponds to the following formula:\n\n  \n    \n      \n        E\n        ϕ\n        :=\n        \n          \n            ⋀\n            \n              j\n              ∈\n              \n                A\n                G\n                T\n                S\n              \n            \n          \n        \n        \n          K\n          \n            j\n          \n        \n        ϕ\n        .\n      \n    \n    {\\displaystyle E\\phi :={\\underset {j\\in {AGTS}}{\\bigwedge }}K_{j}\\phi .}\n  \n\nCommon knowledge.\nCommon knowledge of \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n means that everybody knows \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n but also that everybody knows that everybody knows \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n, that everybody knows that everybody knows that everybody knows \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n, and so on ad infinitum. Formally, this corresponds to the following formula\n\n  \n    \n      \n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " everybody knows \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n, that everybody knows that everybody knows that everybody knows \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n, and so on ad infinitum. Formally, this corresponds to the following formula\n\n  \n    \n      \n        C\n        ϕ\n        :=\n        E\n        ϕ\n        ∧\n        E\n        E\n        ϕ\n        ∧\n        E\n        E\n        E\n        ϕ\n        ∧\n        …\n      \n    \n    {\\displaystyle C\\phi :=E\\phi \\land EE\\phi \\land EEE\\phi \\land \\ldots }\n  \n\nAs we do not allow infinite conjunction the notion of common knowledge will have to be introduced as a primitive in our language.\nBefore defining the language with this new operator, we are going to give an example introduced by Lewis that illustrates the difference between the notions of general knowledge and common knowledge. Lewis wanted to know what kind of knowledge is needed so that the statement \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n: “every driver must drive on the right” be a convention among a group of agents. In other words, he wanted to know what kind of knowledge is needed so that everybody feels safe to drive on the right. Suppose there are only two agents \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n and \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n. Then everybody knowing \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (formally \n  \n    \n      \n        E\n        p\n      \n    \n    {\\displaystyle Ep}\n  \n) is not enough. Indeed, it might still be possible that the agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n considers possible that the agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n does not know \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (formally \n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        \n          K\n          \n            j\n          \n        \n        p\n      \n    \n    {\\displaystyle \\neg K_{i}K_{j}p}\n  \n). In that case the agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n will not feel safe to drive on the right because he might consider that the agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "style \\neg K_{i}K_{j}p}\n  \n). In that case the agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n will not feel safe to drive on the right because he might consider that the agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n, not knowing \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, could drive on the left. To avoid this problem, we could then assume that everybody knows that everybody knows that \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (formally \n  \n    \n      \n        E\n        E\n        p\n      \n    \n    {\\displaystyle EEp}\n  \n). This is again not enough to ensure that everybody feels safe to drive on the right. Indeed, it might still be possible that agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n considers possible that agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n considers possible that agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n does not know \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (formally \n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        \n          K\n          \n            j\n          \n        \n        \n          K\n          \n            i\n          \n        \n        p\n      \n    \n    {\\displaystyle \\neg K_{i}K_{j}K_{i}p}\n  \n). In that case and from \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n’s point of view, \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n considers possible that \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n, not knowing \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n, will drive on the left. So from \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n’s point of view, \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n might drive on the left as well (by the same argument as above). So \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n will not feel safe to drive on the right. Reasoning by induction, Lewis showed that for any \n  \n    \n      \n        k\n        ∈\n        \n          N\n        \n      \n    \n    {\\displaystyle k\\in \\mathbb {N} }\n  \n, \n  \n    \n      \n        E\n        p\n        ∧\n        \n          E\n          \n           "}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " drive on the right. Reasoning by induction, Lewis showed that for any \n  \n    \n      \n        k\n        ∈\n        \n          N\n        \n      \n    \n    {\\displaystyle k\\in \\mathbb {N} }\n  \n, \n  \n    \n      \n        E\n        p\n        ∧\n        \n          E\n          \n            1\n          \n        \n        p\n        ∧\n        …\n        ∧\n        \n          E\n          \n            k\n          \n        \n        p\n      \n    \n    {\\displaystyle Ep\\land E^{1}p\\land \\ldots \\land E^{k}p}\n  \n is not enough for the drivers to feel safe to drive on the right. In fact what we need is an infinite conjunction. In other words, we need common knowledge of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n: \n  \n    \n      \n        C\n        p\n      \n    \n    {\\displaystyle Cp}\n  \n.\nDistributed knowledge.\nDistributed knowledge of \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n means that if the agents pulled their knowledge altogether, they would know that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n holds. In other words, the knowledge of \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is distributed among the agents. The formula \n  \n    \n      \n        \n          D\n          \n            A\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle D_{A}\\phi }\n  \n reads as ‘it is distributed knowledge among the set of agents \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n holds’.\n\nSemantics\nEpistemic logic is a modal logic. So, what we call an epistemic model \n  \n    \n      \n        \n          \n            M\n          \n        \n        =\n        (\n        W\n        ,\n        \n          R\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            n\n          \n        \n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}=(W,R_{1},\\ldots ,R_{n},I)}\n  \n is just a Kripke model as defined in modal logic. The set \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n is a non-empty set whose elements are called possible worlds and the interpretation \n  \n    \n      \n        I\n        :\n        W\n        →\n        \n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "_{n},I)}\n  \n is just a Kripke model as defined in modal logic. The set \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  \n is a non-empty set whose elements are called possible worlds and the interpretation \n  \n    \n      \n        I\n        :\n        W\n        →\n        \n          2\n          \n            P\n            R\n            O\n            P\n          \n        \n      \n    \n    {\\displaystyle I:W\\rightarrow 2^{PROP}}\n  \n is a function specifying which propositional facts (such as ‘Ann has the red card’) are true in each of these worlds. The accessibility relations \n  \n    \n      \n        \n          R\n          \n            j\n          \n        \n        ⊆\n        W\n        ×\n        W\n      \n    \n    {\\displaystyle R_{j}\\subseteq W\\times W}\n  \n are binary relations for each agent \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n; they are intended to capture the uncertainty of each agent (about the actual world and about the other agents' uncertainty). Intuitively, we have \n  \n    \n      \n        (\n        w\n        ,\n        v\n        )\n        ∈\n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle (w,v)\\in R_{j}}\n  \n when the world \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n is compatible with agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n’s information in world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n or, in other words, when agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n considers that world \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n might correspond to the world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n (from this standpoint). We abusively write \n  \n    \n      \n        w\n        ∈\n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle w\\in {\\mathcal {M}}}\n  \n for \n  \n    \n      \n        w\n        ∈\n        W\n      \n    \n    {\\displaystyle w\\in W}\n  \n and \n  \n    \n      \n        \n          R\n          \n            j\n          \n        \n        (\n        w\n        )\n      \n    \n    {\\displaystyle R_{j}(w)}\n  \n denotes the set of worlds \n  \n    \n      \n        {\n        v\n        ∈\n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        W\n      \n    \n    {\\displaystyle w\\in W}\n  \n and \n  \n    \n      \n        \n          R\n          \n            j\n          \n        \n        (\n        w\n        )\n      \n    \n    {\\displaystyle R_{j}(w)}\n  \n denotes the set of worlds \n  \n    \n      \n        {\n        v\n        ∈\n        W\n        ;\n        (\n        w\n        ,\n        v\n        )\n        ∈\n        \n          R\n          \n            j\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{v\\in W;(w,v)\\in R_{j}\\}}\n  \n.\nIntuitively, a pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)}\n  \n, where \n  \n    \n      \n        w\n        ∈\n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle w\\in {\\mathcal {M}}}\n  \n, represents from an external point of view how the actual world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n is perceived by the agents \n  \n    \n      \n        \n          A\n          G\n          T\n          S\n        \n      \n    \n    {\\displaystyle {AGTS}}\n  \n.\nFor every epistemic model \n  \n    \n      \n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {M}}}\n  \n, every \n  \n    \n      \n        w\n        ∈\n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle w\\in {\\mathcal {M}}}\n  \n and every \n  \n    \n      \n        ϕ\n        ∈\n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle \\phi \\in {\\mathcal {L}}_{\\textsf {EL}}}\n  \n, we define \n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        ϕ\n      \n    \n    {\\displaystyle {\\mathcal {M}},w\\models \\phi }\n  \n inductively by the following truth conditions:\n\nwhere \n  \n    \n      \n        \n          \n            (\n            \n              \n                \n                  ⋃\n                  \n                    j\n                    ∈\n                    A\n                  \n                \n              \n              \n                R\n                \n                  j\n                \n              \n            \n            )\n          \n          \n            +\n          \n        \n      \n    \n    {\\displaystyle \\left({\\underset {j\\in A}{\\bigcup }}R_{j}\\right)^{+}}\n  \n is the transitive closure of \n  \n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " ∈\n                    A\n                  \n                \n              \n              \n                R\n                \n                  j\n                \n              \n            \n            )\n          \n          \n            +\n          \n        \n      \n    \n    {\\displaystyle \\left({\\underset {j\\in A}{\\bigcup }}R_{j}\\right)^{+}}\n  \n is the transitive closure of \n  \n    \n      \n        \n          \n            ⋃\n            \n              j\n              ∈\n              A\n            \n          \n        \n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle {\\underset {j\\in A}{\\bigcup }}R_{j}}\n  \n: we have that \n  \n    \n      \n        v\n        ∈\n        \n          \n            (\n            \n              \n                \n                  ⋃\n                  \n                    j\n                    ∈\n                    A\n                  \n                \n              \n              \n                R\n                \n                  j\n                \n              \n            \n            )\n          \n          \n            +\n          \n        \n        (\n        w\n        )\n      \n    \n    {\\displaystyle v\\in \\left({\\underset {j\\in A}{\\bigcup }}R_{j}\\right)^{+}(w)}\n  \n if, and only if, there are \n  \n    \n      \n        \n          w\n          \n            0\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n          \n        \n        ∈\n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle w_{0},\\ldots ,w_{m}\\in {\\mathcal {M}}}\n  \n and \n  \n    \n      \n        \n          j\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          j\n          \n            m\n          \n        \n        ∈\n        A\n      \n    \n    {\\displaystyle j_{1},\\ldots ,j_{m}\\in A}\n  \n such that \n  \n    \n      \n        \n          w\n          \n            0\n          \n        \n        =\n        w\n        ,\n        \n          w\n          \n            m\n          \n        \n        =\n        v\n      \n    \n    {\\displaystyle w_{0}=w,w_{m}=v}\n  \n and for all \n  \n    \n      \n        i\n        ∈\n        {\n        1\n        ,\n        …\n        ,\n        m\n        }\n      \n    \n    {\\displaystyle i\\in \\{1,\\ldots ,m\\}}\n  \n, \n  \n    \n      \n        \n          w\n          \n            i\n            −\n            1\n          \n        \n        \n          R\n          \n            \n              j\n              \n                i\n              \n            \n          \n        \n        \n          w\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle w_{i"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "1,\\ldots ,m\\}}\n  \n, \n  \n    \n      \n        \n          w\n          \n            i\n            −\n            1\n          \n        \n        \n          R\n          \n            \n              j\n              \n                i\n              \n            \n          \n        \n        \n          w\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle w_{i-1}R_{j_{i}}w_{i}}\n  \n.\nDespite the fact that the notion of common belief has to be introduced as a primitive in the language, we can notice that the definition of epistemic models does not have to be modified in order to give truth value to the common knowledge and distributed knowledge operators.\nCard Example:\nPlayers \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n (standing for Ann, Bob and Claire) play a card game with three cards: a red one, a green one and a blue one. Each of them has a single card but they do not know the cards of the other players. Ann has the red card, Bob has the green card and Claire has the blue card. This example is depicted in the pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)}\n  \n represented below. In this example, \n  \n    \n      \n        A\n        G\n        T\n        S\n        :=\n        {\n        A\n        ,\n        B\n        ,\n        C\n        }\n      \n    \n    {\\displaystyle AGTS:=\\{A,B,C\\}}\n  \n and \n  \n    \n      \n        P\n        R\n        O\n        P\n        :=\n        {\n        \n          \n            \n              A\n            \n          \n        \n        ,\n        \n          \n            \n              B\n            \n          \n        \n        ,\n        \n          \n            \n              C\n            \n          \n        \n        ,\n        \n          \n            \n              B\n            \n          \n        \n        ,\n        \n          \n            \n              C\n            \n          \n        \n        ,\n        \n          \n            \n              A\n            \n          \n        \n        ,\n        \n          \n            \n              C\n            \n          \n        \n        ,\n        \n          \n            \n              A\n            \n          \n        \n        ,\n        \n          \n            \n              B\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle PROP:=\\{{\\color {red}{A}},{\\color {green}{B}},{\\color {blue}{C}},{\\color {red}{B}},{\\color {green}{C}},{\\color {"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " ,\n        \n          \n            \n              B\n            \n          \n        \n        }\n      \n    \n    {\\displaystyle PROP:=\\{{\\color {red}{A}},{\\color {green}{B}},{\\color {blue}{C}},{\\color {red}{B}},{\\color {green}{C}},{\\color {blue}{A}},{\\color {red}{C}},{\\color {green}{A}},{\\color {blue}{B}}\\}}\n  \n. Each world is labelled by the propositional letters which are true in this world and \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n corresponds to the actual world. There is an arrow indexed by agent \n  \n    \n      \n        j\n        ∈\n        {\n        A\n        ,\n        B\n        ,\n        C\n        }\n      \n    \n    {\\displaystyle j\\in \\{A,B,C\\}}\n  \n from a possible world \n  \n    \n      \n        u\n      \n    \n    {\\displaystyle u}\n  \n to a possible world \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n when \n  \n    \n      \n        (\n        u\n        ,\n        v\n        )\n        ∈\n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle (u,v)\\in R_{j}}\n  \n. Reflexive arrows are omitted, which means that for all \n  \n    \n      \n        j\n        ∈\n        {\n        A\n        ,\n        B\n        ,\n        C\n        }\n      \n    \n    {\\displaystyle j\\in \\{A,B,C\\}}\n  \n and all \n  \n    \n      \n        v\n        ∈\n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle v\\in {\\mathcal {M}}}\n  \n, we have that \n  \n    \n      \n        (\n        v\n        ,\n        v\n        )\n        ∈\n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle (v,v)\\in R_{j}}\n  \n.\n\n  \n    \n      \n        \n          \n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {red}{A}}}\n  \n stands for : \"\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n has the red card''\n\n  \n    \n      \n        \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {blue}{C}}}\n  \n stand for: \"\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n has the blue card''\n\n  \n    \n      \n        \n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "''\n\n  \n    \n      \n        \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {blue}{C}}}\n  \n stand for: \"\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n has the blue card''\n\n  \n    \n      \n        \n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {green}{B}}}\n  \n stands for: \"\n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n has the green card''\nand so on...\nWhen accessibility relations are equivalence relations (like in this example) and we have that \n  \n    \n      \n        (\n        w\n        ,\n        v\n        )\n        ∈\n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle (w,v)\\in R_{j}}\n  \n, we say that agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n cannot distinguish world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n from world \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n (or world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n is indistinguishable from world \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n for agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n). So, for example, \n  \n    \n      \n        A\n      \n    \n    {\\textstyle A}\n  \n cannot distinguish the actual world \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  \n from the possible world where \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n has the blue card (\n  \n    \n      \n        \n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {blue}{B}}}\n  \n), \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n has the green card (\n  \n    \n      \n        \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {green}{C}}}\n  \n) and \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n still has the red card (\n  \n    \n      \n        \n          \n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {red}{A}}}\n  \n).\nIn particular, the following statements hold:\n\n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∧\n        \n          K\n          \n            A\n          \n        \n        \n          \n            \n              A\n            \n          \n        \n        )\n        ∧\n        (\n        \n          \n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " particular, the following statements hold:\n\n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∧\n        \n          K\n          \n            A\n          \n        \n        \n          \n            \n              A\n            \n          \n        \n        )\n        ∧\n        (\n        \n          \n            \n              C\n            \n          \n        \n        ∧\n        \n          K\n          \n            C\n          \n        \n        \n          \n            \n              C\n            \n          \n        \n        )\n        ∧\n        (\n        \n          \n            \n              B\n            \n          \n        \n        ∧\n        \n          K\n          \n            B\n          \n        \n        \n          \n            \n              B\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}},w\\models ({\\color {red}{A}}\\land K_{A}{\\color {red}{A}})\\land ({\\color {blue}{C}}\\land K_{C}{\\color {blue}{C}})\\land ({\\color {green}{B}}\\land K_{B}{\\color {green}{B}})}\n  \n\n'All the agents know the color of their card'.\n\n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        \n          K\n          \n            A\n          \n        \n        (\n        \n          \n            \n              B\n            \n          \n        \n        ∨\n        \n          \n            \n              B\n            \n          \n        \n        )\n        ∧\n        \n          K\n          \n            A\n          \n        \n        (\n        \n          \n            \n              C\n            \n          \n        \n        ∨\n        \n          \n            \n              C\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}},w\\models K_{A}({\\color {blue}{B}}\\vee {\\color {green}{B}})\\land K_{A}({\\color {blue}{C}}\\vee {\\color {green}{C}})}\n  \n\n'\n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n knows that \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n has either the blue or the green card and that \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n has either the blue or the green card'.\n\n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        E\n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n        \n          \n            \n              A\n            \n          \n        \n        )\n        ∧\n        C\n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n        ⊨\n        E\n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n        \n          \n            \n              A\n            \n          \n        \n        )\n        ∧\n        C\n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n        \n          \n            \n              A\n            \n          \n        \n        ∨\n        \n          \n            \n              A\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}},w\\models E({\\color {red}{A}}\\vee {\\color {blue}{A}}\\vee {\\color {green}{A}})\\land C({\\color {red}{A}}\\vee {\\color {blue}{A}}\\vee {\\color {green}{A}})}\n  \n\n'Everybody knows that \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n has either the red, green or blue card and this is even common knowledge among all agents'.\n\nKnowledge versus Belief\nWe use the same notation \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle K_{j}}\n  \n for both knowledge and belief. Hence, depending on the context, \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle K_{j}\\phi }\n  \n will either read ‘the agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n Knows that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n holds’ or ‘the agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n Believes that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n holds’. A crucial difference is that, unlike knowledge, beliefs can be wrong: the axiom \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n        ϕ\n        →\n        ϕ\n      \n    \n    {\\displaystyle K_{j}\\phi \\rightarrow \\phi }\n  \n holds only for knowledge, but not necessarily for belief. This axiom called axiom T (for Truth) states that if the agent knows a proposition, then this proposition is true. It is often considered to be the hallmark of knowledge and it has not been subjected to any serious attack ever since its introduction in the Theaetetus by Plato.\nThe notion of knowledge might comply to some other constraints (or axioms) such as \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n        ϕ"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " often considered to be the hallmark of knowledge and it has not been subjected to any serious attack ever since its introduction in the Theaetetus by Plato.\nThe notion of knowledge might comply to some other constraints (or axioms) such as \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n        ϕ\n        →\n        \n          K\n          \n            j\n          \n        \n        \n          K\n          \n            j\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle K_{j}\\phi \\rightarrow K_{j}K_{j}\\phi }\n  \n: if agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n knows something, she knows that she knows it. These constraints might affect the nature of the accessibility relations \n  \n    \n      \n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle R_{j}}\n  \n which may then comply to some extra properties. So, we are now going to define some particular classes of epistemic models that all add some extra constraints on the accessibility relations \n  \n    \n      \n        \n          R\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle R_{j}}\n  \n. These constraints are matched by particular axioms for the knowledge operator \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle K_{j}}\n  \n. Below each property, we give the axiom which defines the class of epistemic frames that fulfill this property. (\n  \n    \n      \n        K\n        ϕ\n      \n    \n    {\\displaystyle K\\phi }\n  \n stands for \n  \n    \n      \n        \n          K\n          \n            j\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle K_{j}\\phi }\n  \n for any \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n.)\n\nWe discuss the axioms above. Axiom 4 states that if the agent knows a proposition, then she knows that she knows it (this axiom is also known as the “KK-principle”or “KK-thesis”). In epistemology, axiom 4 tends to be accepted by internalists, but not by externalists. Axiom 4 is nevertheless widely accepted by computer scientists (but also by many philosophers, including Plato, Aristotle, Saint Augustine, Spinoza and Schopenhauer, as Hintikka recalls ). A more controversial axiom for the logic of knowledge is axiom 5 for Euclidicity: this axiom states"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 17, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ists. Axiom 4 is nevertheless widely accepted by computer scientists (but also by many philosophers, including Plato, Aristotle, Saint Augustine, Spinoza and Schopenhauer, as Hintikka recalls ). A more controversial axiom for the logic of knowledge is axiom 5 for Euclidicity: this axiom states that if the agent does not know a proposition, then she knows that she does not know it. Most philosophers (including Hintikka) have attacked this axiom, since numerous examples from everyday life seem to invalidate it. In general, axiom 5 is invalidated when the agent has mistaken beliefs, which can be due for example to misperceptions, lies or other forms of deception. Axiom B states that it cannot be the case that the agent considers it possible that she knows a false proposition (that is, \n  \n    \n      \n        ¬\n        (\n        ¬\n        ϕ\n        ∧\n        ¬\n        K\n        ¬\n        K\n        ϕ\n        )\n      \n    \n    {\\displaystyle \\neg (\\neg \\phi \\land \\neg K\\neg K\\phi )}\n  \n). If we assume that axioms T and 4 are valid, then axiom B falls prey to the same attack as the one for axiom 5 since this axiom is derivable. Axiom D states that the agent's beliefs are consistent. In combination with axiom K (where the knowledge operator is replaced by a belief operator), axiom D is in fact equivalent to a simpler axiom D' which conveys, maybe more explicitly, the fact that the agent's beliefs cannot be inconsistent: \n  \n    \n      \n        ¬\n        B\n        ⊥\n      \n    \n    {\\displaystyle \\neg B\\bot }\n  \n. The other intricate axioms .2, .3, .3.2 and .4 have been introduced by epistemic logicians such as Lenzen and Kutchera in the 1970s and presented for some of them as key axioms of epistemic logic. They can be characterized in terms of intuitive interaction axioms relating knowledge and beliefs.\n\nAxiomatization\nThe Hilbert proof system K for the basic modal logic is defined by the following axioms and inference rules: for all \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n,\n\nThe axioms of an epistemic logic obviously display the way the agents reason. For example, the axiom K"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 18, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ioms and inference rules: for all \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n,\n\nThe axioms of an epistemic logic obviously display the way the agents reason. For example, the axiom K together with the rule of inference Nec entail that if I know \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n (\n  \n    \n      \n        K\n        ϕ\n      \n    \n    {\\displaystyle K\\phi }\n  \n) and I know that \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n implies \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n  (\n  \n    \n      \n        K\n        (\n        ϕ\n        →\n        ψ\n        )\n        )\n      \n    \n    {\\displaystyle K(\\phi \\rightarrow \\psi ))}\n  \n then I know that \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n (\n  \n    \n      \n        K\n        ψ\n      \n    \n    {\\displaystyle K\\psi }\n  \n). Stronger constraints can be added. The following  proof systems for \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}}\n  \n are often used in the literature.\n\nWe define the set of proof systems \n  \n    \n      \n        \n          \n            L\n          \n          \n            \n              EL\n            \n          \n        \n        :=\n        {\n        \n          \n            K\n          \n        \n        ,\n        \n          \n            KD45\n          \n        \n        ,\n        \n          \n            S4\n          \n        \n        ,\n        \n          \n            S4.2\n          \n        \n        ,\n        \n          \n            S4.3\n          \n        \n        ,\n        \n          \n            S4.3.2\n          \n        \n        ,\n        \n          \n            S4.4\n          \n        \n        ,\n        \n          \n            S5\n          \n        \n        }\n      \n    \n    {\\displaystyle \\mathbb {L} _{\\textsf {EL}}:=\\{{\\textsf {K}},{\\textsf {KD45}},{\\textsf {S4}},{\\textsf {S4.2}},{\\textsf {S4.3}},{\\textsf {S4.3.2}},{\\textsf {S4.4}},{\\textsf {S5}}\\}}\n  \n.\nMoreover, for all \n  \n    \n      \n        \n          \n           "}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 19, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "},{\\textsf {S4.2}},{\\textsf {S4.3}},{\\textsf {S4.3.2}},{\\textsf {S4.4}},{\\textsf {S5}}\\}}\n  \n.\nMoreover, for all \n  \n    \n      \n        \n          \n            H\n          \n        \n        ∈\n        \n          \n            L\n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}\\in \\mathbb {L} _{\\textsf {EL}}}\n  \n, we define the proof system \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n by adding the following axiom schemes and rules of inference to those of \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n. For all \n  \n    \n      \n        A\n        ⊆\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle A\\subseteq AGTS}\n  \n,\n\nThe relative strength of the proof systems for knowledge is as follows:\n\n  \n    \n      \n        \n          \n            S4\n          \n        \n        ⊂\n        \n          \n            S4.2\n          \n        \n        ⊂\n        \n          \n            S4.3\n          \n        \n        ⊂\n        \n          \n            S4.3.2\n          \n        \n        ⊂\n        \n          \n            S4.4\n          \n        \n        ⊂\n        \n          \n            S5\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\textsf {S4}}\\subset {\\textsf {S4.2}}\\subset {\\textsf {S4.3}}\\subset {\\textsf {S4.3.2}}\\subset {\\textsf {S4.4}}\\subset {\\textsf {S5}}.}\n  \n\nSo, all the theorems of \n  \n    \n      \n        \n          \n            S4.2\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S4.2}}}\n  \n are also theorems of \n  \n    \n      \n        \n          \n            S4.3\n          \n        \n        ,\n        \n          \n            S4.3.2\n          \n        \n        ,\n        \n          \n            S4.4\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S4.3}},{\\textsf {S4.3.2}},{\\textsf"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 20, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " S4.3\n          \n        \n        ,\n        \n          \n            S4.3.2\n          \n        \n        ,\n        \n          \n            S4.4\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S4.3}},{\\textsf {S4.3.2}},{\\textsf {S4.4}}}\n  \n and \n  \n    \n      \n        \n          \n            S5\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S5}}}\n  \n. Many philosophers claim that in the most general cases, the logic of knowledge is \n  \n    \n      \n        \n          \n            S4.2\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S4.2}}}\n  \n or \n  \n    \n      \n        \n          \n            S4.3\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S4.3}}}\n  \n. Typically, in computer science and in many of the theories developed in artificial intelligence, the logic of belief (doxastic logic) is taken to be \n  \n    \n      \n        \n          \n            KD45\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {KD45}}}\n  \n and the logic of knowledge (epistemic logic) is taken to be \n  \n    \n      \n        \n          \n            S5\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S5}}}\n  \n, even if \n  \n    \n      \n        \n          \n            S5\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {S5}}}\n  \n is only suitable for situations where the agents do not have mistaken beliefs. \n  \n    \n      \n        \n          \n            Br\n          \n        \n      \n    \n    {\\displaystyle {\\textsf {Br}}}\n  \n has been propounded by Floridi as the logic of the notion of 'being informed’ which mainly differs from the logic of knowledge by the absence of introspection for the agents.\nFor all \n  \n    \n      \n        \n          \n            H\n          \n        \n        ∈\n        \n          \n            L\n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}\\in \\mathbb {L} _{\\textsf {EL}}}\n  \n, the class of \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n–models or \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n–models is the class of epistemic"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 21, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n–models or \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n–models is the class of epistemic models whose accessibility relations satisfy the properties listed above defined by the axioms of \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n or \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n. Then, for all \n  \n    \n      \n        \n          \n            H\n          \n        \n        ∈\n        \n          \n            L\n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}\\in \\mathbb {L} _{\\textsf {EL}}}\n  \n, \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n is sound and strongly complete for \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}}\n  \n w.r.t. the class of \n  \n    \n      \n        \n          \n            H\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}}\n  \n–models, and \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n is sound and strongly complete for \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}^{\\textsf {C}}}\n  \n w.r.t. the class of \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            \n              C\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}^{\\textsf {C}}}\n  \n–models.\n\nDecidability and Complexity\nThe satisfiability problem for all the logics introduced is decidable. We list below the computational complexity of the satisfiability problem for each of them. Note that it becomes linear in time if there are only finitely many propositional letters in the language. For \n  \n    \n      \n       "}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 22, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".\n\nDecidability and Complexity\nThe satisfiability problem for all the logics introduced is decidable. We list below the computational complexity of the satisfiability problem for each of them. Note that it becomes linear in time if there are only finitely many propositional letters in the language. For \n  \n    \n      \n        n\n        ≥\n        2\n      \n    \n    {\\displaystyle n\\geq 2}\n  \n, if we restrict to finite nesting, then the satisfiability problem is NP-complete for all the modal logics considered. If we then further restrict the language to having only finitely many primitive propositions, the complexity goes down to linear in time in all cases.\n\nThe computational complexity of the model checking problem is in P in all cases.\n\nAdding Dynamics\nDynamic Epistemic Logic (DEL) is a logical framework for modeling epistemic situations involving several agents, and changes that occur to these situations as a result of incoming information or more generally incoming action. The methodology of DEL is such that it splits the task of representing the agents’ beliefs and knowledge into three parts:\n\nOne represents their beliefs about an initial situation thanks to an epistemic model;\nOne represents their beliefs about an event taking place in this situation thanks to an event model;\nOne represents the way the agents update their beliefs about the situation after (or during) the occurrence of the event thanks to a product update.\nTypically, an informative event can be a public announcement to all the agents of a formula \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n: this public announcement and correlative update constitute the dynamic part. However, epistemic events can be much more complex than simple public announcement, including hiding information for some of the agents, cheating, lying, bluffing, etc. This complexity is dealt with when we introduce the notion of event model. We will first focus on public announcements to get an intuition of the main underlying ideas of DEL.\n\nPublic Events\nIn this section, we assume that all events are public. We start by giving a concrete example where DEL can be used, to better understand what is going on. This example is called the muddy children puzzle. Then, we will present a formalization of this puzzle in a logic called Public Announcement Logic (PAL). The muddy children puzzle is one of the most well known puzzles that played a role in the development of DEL. Other significant puzzles include the sum and product puzzle, the Monty Hall dilemma, the Russian cards problem, the two envelopes problem, Moore"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 23, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " formalization of this puzzle in a logic called Public Announcement Logic (PAL). The muddy children puzzle is one of the most well known puzzles that played a role in the development of DEL. Other significant puzzles include the sum and product puzzle, the Monty Hall dilemma, the Russian cards problem, the two envelopes problem, Moore's paradox, the hangman paradox, etc.\nMuddy Children Example:\nWe have two children, A and B, both dirty. A can see B but not himself, and B can see A but not herself. Let \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n be the proposition stating that A is dirty, and \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n be the proposition stating that B is dirty.\n\nWe represent the initial situation by the pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            N\n          \n        \n        ,\n        s\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {N}},s)}\n  \n represented below, where relations between worlds are equivalence relations. States \n  \n    \n      \n        s\n        ,\n        t\n        ,\n        u\n        ,\n        v\n      \n    \n    {\\displaystyle s,t,u,v}\n  \n intuitively represent possible worlds, a proposition (for example \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n) satisfiable at one of these worlds intuitively means that in the corresponding possible world, the intuitive interpretation of \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n (A is dirty) is true. The links between worlds labelled by agents (A or B) intuitively express a notion of indistinguishability for the agent at stake between two possible worlds. For example, the link between \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n and \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n labelled by A intuitively means that A can not distinguish the possible world \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n from \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n and vice versa. Indeed, A cannot see himself, so he cannot distinguish between a world where he is dirty and one where he is not dirty. However, he can distinguish between worlds where B is dirty or not because he can see B. With this intuitive interpretation we are brought to assume that our relations between worlds are equivalence relations.\nNow, suppose that their father comes and announces that at least one is dirty (formally, \n  \n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 24, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " one where he is not dirty. However, he can distinguish between worlds where B is dirty or not because he can see B. With this intuitive interpretation we are brought to assume that our relations between worlds are equivalence relations.\nNow, suppose that their father comes and announces that at least one is dirty (formally, \n  \n    \n      \n        p\n        ∨\n        q\n      \n    \n    {\\displaystyle p\\vee q}\n  \n). Then we update the model and this yields the pointed epistemic model represented below. What we actually do is suppressing the worlds where the content of the announcement is not fulfilled. In our case this is the world where \n  \n    \n      \n        ¬\n        p\n      \n    \n    {\\displaystyle \\neg p}\n  \n and \n  \n    \n      \n        ¬\n        q\n      \n    \n    {\\displaystyle \\neg q}\n  \n are true. This suppression is what we call the update. We then get the model depicted below. As a result of the announcement, both A and B do know that at least one of them is dirty. We can read this from the epistemic model.\nNow suppose there is a second (and final) announcement that says that neither knows they are dirty (an announcement can express facts about the situation as well as epistemic facts about the knowledge held by the agents). We then update similarly the model by suppressing the worlds which do not satisfy the content of the announcement, or equivalently by keeping the worlds which do satisfy the announcement. This update process thus yields the pointed epistemic model represented below. By interpreting this model, we get that A and B both know that they are dirty, which seems to contradict the content of the announcement. However, if we assume that A and B are both perfect reasoners and that this is common knowledge among them, then this inference makes perfect sense.\n\nPublic announcement logic (PAL):\nWe present the syntax and semantic of Public Announcement Logic (PAL), which combines features of epistemic logic and propositional dynamic logic.\nWe define the language \n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n      \n    \n    {\\displaystyle {{\\mathcal {L}}_{PAL}}}\n  \n inductively by the following grammar in BNF:\n\n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n        :\n        ϕ\n         \n         \n        ::=\n         \n         \n        p\n         \n        ∣\n         \n        ¬\n        �"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 25, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "}}}\n  \n inductively by the following grammar in BNF:\n\n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n        :\n        ϕ\n         \n         \n        ::=\n         \n         \n        p\n         \n        ∣\n         \n        ¬\n        ϕ\n         \n        ∣\n         \n        (\n        ϕ\n        ∧\n        ϕ\n        )\n         \n        ∣\n         \n        \n          K\n          \n            j\n          \n        \n        ϕ\n         \n        ∣\n         \n        [\n        ϕ\n        !\n        ]\n        ϕ\n      \n    \n    {\\displaystyle {{\\mathcal {L}}_{PAL}}:\\phi ~~::=~~p~\\mid ~\\neg \\phi ~\\mid ~(\\phi \\land \\phi )~\\mid ~K_{j}\\phi ~\\mid ~[\\phi !]\\phi }\n  \n\nwhere \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n.\nThe language \n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n      \n    \n    {\\displaystyle {{\\mathcal {L}}_{PAL}}}\n  \n is interpreted over epistemic models. The truth conditions for the connectives of the epistemic language are the same as in epistemic logic (see above). The truth condition for the new dynamic action modality \n  \n    \n      \n        [\n        ψ\n        !\n        ]\n        ϕ\n      \n    \n    {\\displaystyle [\\psi !]\\phi }\n  \n is defined as follows:\n\nwhere \n  \n    \n      \n        \n          \n            \n              M\n            \n          \n          \n            ψ\n          \n        \n        :=\n        (\n        \n          W\n          \n            ψ\n          \n        \n        ,\n        \n          R\n          \n            1\n          \n          \n            ψ\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            n\n          \n          \n            ψ\n          \n        \n        ,\n        \n          I\n          \n            ψ\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}^{\\psi }:=(W^{\\psi },R_{1}^{\\psi },\\ldots ,R_{n}^{\\psi },I^{\\psi })}\n  \n with\n\n  \n    \n      \n        \n          W\n          \n            ψ\n          \n        \n        :=\n        {\n        w\n        ∈\n        W\n       "}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 26, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "{\\psi }:=(W^{\\psi },R_{1}^{\\psi },\\ldots ,R_{n}^{\\psi },I^{\\psi })}\n  \n with\n\n  \n    \n      \n        \n          W\n          \n            ψ\n          \n        \n        :=\n        {\n        w\n        ∈\n        W\n        ;\n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        ψ\n        }\n      \n    \n    {\\displaystyle W^{\\psi }:=\\{w\\in W;{\\mathcal {M}},w\\models \\psi \\}}\n  \n,\n\n  \n    \n      \n        \n          R\n          \n            j\n          \n          \n            ψ\n          \n        \n        :=\n        \n          R\n          \n            j\n          \n        \n        ∩\n        (\n        \n          W\n          \n            ψ\n          \n        \n        ×\n        \n          W\n          \n            ψ\n          \n        \n        )\n      \n    \n    {\\displaystyle R_{j}^{\\psi }:=R_{j}\\cap (W^{\\psi }\\times W^{\\psi })}\n  \n for all \n  \n    \n      \n        j\n        ∈\n        {\n        1\n        ,\n        …\n        ,\n        n\n        }\n      \n    \n    {\\displaystyle j\\in \\{1,\\ldots ,n\\}}\n  \n and\n\n  \n    \n      \n        \n          I\n          \n            ψ\n          \n        \n        (\n        w\n        )\n        :=\n        I\n        (\n        w\n        )\n        \n          \n            ~for~all~\n          \n        \n        w\n        ∈\n        \n          W\n          \n            ψ\n          \n        \n      \n    \n    {\\displaystyle I^{\\psi }(w):=I(w){\\textrm {~for~all~}}w\\in W^{\\psi }}\n  \n.\n\nThe formula \n  \n    \n      \n        [\n        ψ\n        !\n        ]\n        ϕ\n      \n    \n    {\\displaystyle [\\psi !]\\phi }\n  \n intuitively means that after a truthful announcement of \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n, \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n holds. A public announcement of a proposition \n  \n    \n      \n        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n changes the current epistemic model like in the figure below.\nThe proof system \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            P\n            A\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 27, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        ψ\n      \n    \n    {\\displaystyle \\psi }\n  \n changes the current epistemic model like in the figure below.\nThe proof system \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            P\n            A\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{PAL}}\n  \n defined below is sound and strongly complete for \n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n      \n    \n    {\\displaystyle {{\\mathcal {L}}_{PAL}}}\n  \n w.r.t. the class of all pointed epistemic models.\n\nThe axioms Red 1 - Red 4 are called reduction axioms because they allow to reduce any formula of \n  \n    \n      \n        \n          \n            \n              \n                L\n              \n            \n            \n              P\n              A\n              L\n            \n          \n        \n      \n    \n    {\\displaystyle {{\\mathcal {L}}_{PAL}}}\n  \n to a provably equivalent formula of \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            E\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{EL}}\n  \n in \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            P\n            A\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{PAL}}\n  \n. The formula \n  \n    \n      \n        [\n        q\n        !\n        ]\n        K\n        q\n      \n    \n    {\\displaystyle [q!]Kq}\n  \n is a theorem provable in \n  \n    \n      \n        \n          \n            \n              H\n            \n          \n          \n            P\n            A\n            L\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {H}}_{PAL}}\n  \n. It states that after a public announcement of \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n, the agent knows that \n  \n    \n      \n        q\n      \n    \n    {\\displaystyle q}\n  \n holds.\nPAL is decidable, its model checking problem is solvable in polynomial time and its satisfiability problem is PSPACE-complete.\nMuddy children puzzle formalized with PAL:\nHere are some of the statements that hold in the muddy children puzzle formalized in PAL.\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        p\n        ∧\n        q\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models p\\land q}\n  \n\n'In the initial situation, A is dirty and B is dirty'.\n\n  \n    \n      \n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 28, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " PAL.\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        p\n        ∧\n        q\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models p\\land q}\n  \n\n'In the initial situation, A is dirty and B is dirty'.\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        (\n        ¬\n        \n          K\n          \n            A\n          \n        \n        p\n        ∧\n        ¬\n        \n          K\n          \n            A\n          \n        \n        ¬\n        p\n        )\n        ∧\n        (\n        ¬\n        \n          K\n          \n            B\n          \n        \n        q\n        ∧\n        ¬\n        \n          K\n          \n            B\n          \n        \n        ¬\n        q\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models (\\neg K_{A}p\\land \\neg K_{A}\\neg p)\\land (\\neg K_{B}q\\land \\neg K_{B}\\neg q)}\n  \n\n'In the initial situation, A does not know whether he is dirty and B neither'.\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        [\n        p\n        ∨\n        q\n        !\n        ]\n        (\n        \n          K\n          \n            A\n          \n        \n        (\n        p\n        ∨\n        q\n        )\n        ∧\n        \n          K\n          \n            B\n          \n        \n        (\n        p\n        ∨\n        q\n        )\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models [p\\vee q!](K_{A}(p\\vee q)\\land K_{B}(p\\vee q))}\n  \n\n'After the public announcement that at least one of the children A and B is dirty, both of them know that at least one of them is dirty'. However:\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        [\n        p\n        ∨\n        q\n        !\n        ]\n        (\n        (\n        ¬\n        \n          K\n          \n            A\n          \n        \n        p\n        ∧\n        ¬\n        \n          K\n          \n            A\n          \n        \n        ¬\n        p\n        )\n        ∧\n        (\n        ¬\n        \n          K\n          \n            B\n          \n        \n        q\n        ∧\n        ¬\n        \n          K\n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 29, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "          \n            A\n          \n        \n        p\n        ∧\n        ¬\n        \n          K\n          \n            A\n          \n        \n        ¬\n        p\n        )\n        ∧\n        (\n        ¬\n        \n          K\n          \n            B\n          \n        \n        q\n        ∧\n        ¬\n        \n          K\n          \n            B\n          \n        \n        ¬\n        q\n        )\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models [p\\vee q!]((\\neg K_{A}p\\land \\neg K_{A}\\neg p)\\land (\\neg K_{B}q\\land \\neg K_{B}\\neg q))}\n  \n\n'After the public announcement that at least one of the children A and B is dirty, they still do not know that they are dirty'. Moreover:\n\n  \n    \n      \n        \n          \n            N\n          \n        \n        ,\n        s\n        ⊨\n        [\n        p\n        ∨\n        q\n        !\n        ]\n        [\n        (\n        ¬\n        \n          K\n          \n            A\n          \n        \n        p\n        ∧\n        ¬\n        \n          K\n          \n            A\n          \n        \n        ¬\n        p\n        )\n        ∧\n        (\n        ¬\n        \n          K\n          \n            B\n          \n        \n        q\n        ∧\n        ¬\n        \n          K\n          \n            B\n          \n        \n        ¬\n        q\n        )\n        !\n        ]\n        (\n        \n          K\n          \n            A\n          \n        \n        p\n        ∧\n        \n          K\n          \n            B\n          \n        \n        q\n        )\n      \n    \n    {\\displaystyle {\\mathcal {N}},s\\models [p\\vee q!][(\\neg K_{A}p\\land \\neg K_{A}\\neg p)\\land (\\neg K_{B}q\\land \\neg K_{B}\\neg q)!](K_{A}p\\land K_{B}q)}\n  \n\n'After the successive public announcements that at least one of the children A and B is dirty and that they still do not know whether they are dirty, A and B then both know that they are dirty'.\nIn this last statement, we see at work an interesting feature of the update process: a formula is not necessarily true after being announced. That is what we technically call “self-persistence” and this problem arises for epistemic formulas (unlike propositional formulas). One must not confuse the announcement and the update induced by this announcement"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 30, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " statement, we see at work an interesting feature of the update process: a formula is not necessarily true after being announced. That is what we technically call “self-persistence” and this problem arises for epistemic formulas (unlike propositional formulas). One must not confuse the announcement and the update induced by this announcement, which might cancel some of the information encoded in the announcement.\n\nArbitrary Events\nIn this section, we assume that events are not necessarily public and we focus on items 2 and 3 above, namely on how to represent events and on how to update an epistemic model with such a representation of events by means of a product update.\n\nEvent Model\nEpistemic models are used to model how agents perceive the actual world. Their perception can also be described in terms of knowledge and beliefs about the world and about the other agents’ beliefs. The insight of the DEL approach is that one can describe how an event is perceived by the agents in a very similar way. Indeed, the agents’ perception of an event can also be described in terms of knowledge and beliefs. For example, the private announcement of \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n to \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n that her card is red can also be described in terms of knowledge and beliefs: while \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n tells \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n that her card is red (event \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n) \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n believes that nothing happens (event \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n). This leads to define the notion of event model whose definition is very similar to that of an epistemic model.\nA pointed event model \n  \n    \n      \n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {E}},e)}\n  \n represents how the actual event represented by \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n is perceived by the agents. Intuitively, \n  \n    \n      \n        f\n        ∈\n        \n          R\n          \n            j\n          \n        \n        (\n        e\n        )\n      \n    \n    {\\displaystyle f\\in R_{j}(e)}\n  \n means that while the possible event represented by \n  \n    \n      \n        e\n      \n    \n    {\\display"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 31, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Intuitively, \n  \n    \n      \n        f\n        ∈\n        \n          R\n          \n            j\n          \n        \n        (\n        e\n        )\n      \n    \n    {\\displaystyle f\\in R_{j}(e)}\n  \n means that while the possible event represented by \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n is occurring, agent \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n  \n considers possible that the possible event represented by \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n is actually occurring.\nAn event model is a tuple \n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        (\n        \n          W\n          \n            α\n          \n        \n        ,\n        \n          R\n          \n            1\n          \n          \n            α\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            m\n          \n          \n            α\n          \n        \n        ,\n        \n          I\n          \n            α\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {E}}=(W^{\\alpha },R_{1}^{\\alpha },\\ldots ,R_{m}^{\\alpha },I^{\\alpha })}\n  \n where:\n\n  \n    \n      \n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle W^{\\alpha }}\n  \n is a non-empty set of possible events,\n\n  \n    \n      \n        \n          R\n          \n            j\n          \n          \n            α\n          \n        \n        ⊆\n        \n          W\n          \n            α\n          \n        \n        ×\n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle R_{j}^{\\alpha }\\subseteq W^{\\alpha }\\times W^{\\alpha }}\n  \n is a binary relation called an accessibility relation on \n  \n    \n      \n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle W^{\\alpha }}\n  \n, for each \n  \n    \n      \n        j\n        ∈\n        A\n        G\n        T\n        S\n      \n    \n    {\\displaystyle j\\in AGTS}\n  \n,\n\n  \n    \n      \n        \n          I\n          \n            α\n          \n        \n        :\n        \n          W\n          \n            α\n          \n        \n        →\n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle I^{\\alpha }:W^{\\alpha }\\rightarrow {\\mathcal {L}}_{\\textsf {EL}}}\n  \n is a function called the precondition function assigning to each possible event a formula of \n  \n    \n      \n        \n          \n            \n             "}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 32, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle I^{\\alpha }:W^{\\alpha }\\rightarrow {\\mathcal {L}}_{\\textsf {EL}}}\n  \n is a function called the precondition function assigning to each possible event a formula of \n  \n    \n      \n        \n          \n            \n              L\n            \n          \n          \n            \n              EL\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}_{\\textsf {EL}}}\n  \n.\n\n  \n    \n      \n        \n          R\n          \n            j\n          \n          \n            α\n          \n        \n        (\n        e\n        )\n      \n    \n    {\\displaystyle R_{j}^{\\alpha }(e)}\n  \n denotes the set \n  \n    \n      \n        {\n        f\n        ∈\n        \n          W\n          \n            α\n          \n        \n        ;\n        (\n        e\n        ,\n        f\n        )\n        ∈\n        \n          R\n          \n            j\n          \n          \n            α\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{f\\in W^{\\alpha };(e,f)\\in R_{j}^{\\alpha }\\}}\n  \n .We write \n  \n    \n      \n        e\n        ∈\n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle e\\in {\\mathcal {E}}}\n  \n for \n  \n    \n      \n        e\n        ∈\n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle e\\in W^{\\alpha }}\n  \n, and \n  \n    \n      \n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {E}},e)}\n  \n is called a pointed event model (\n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n often represents the actual event).\nCard Example:\nLet us resume the card example and assume that players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n show their card to each other. As it turns out, \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n noticed that \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n showed her card to \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n but did not notice that \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n did so to \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n. Players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 33, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n but did not notice that \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n did so to \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n. Players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n know this. This event is represented below in the event model \n  \n    \n      \n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {E}},e)}\n  \n.\nThe possible event \n  \n    \n      \n        e\n      \n    \n    {\\displaystyle e}\n  \n corresponds to the actual event ‘players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n show their and cards respectively to each other’ (with precondition \n  \n    \n      \n        \n          \n            \n              A\n            \n          \n        \n        ∧\n        \n          \n            \n              B\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {red}{A}}\\land {\\color {green}{B}}}\n  \n), \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n stands for the event ‘player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her green card’ (with precondition \n  \n    \n      \n        \n          \n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {green}{A}}}\n  \n) and \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n  \n stands for the atomic event ‘player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card’ (with precondition \n  \n    \n      \n        \n          \n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle {\\color {red}{A}}}\n  \n). Players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n show their cards to each other, players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n know this and consider it possible, while player \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n considers possible that player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card and also considers possible that player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 34, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " know this and consider it possible, while player \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n considers possible that player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card and also considers possible that player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her green card, since he does not know her card. In fact, that is all that player \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n considers possible because she did not notice that \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n showed her card.\n\nAnother example of event model is given below. This second example corresponds to the event whereby Player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card publicly to everybody. Player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card, players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n ‘know’ it, players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n ‘know’ that each of them ‘knows’ it, etc. In other words, there is common knowledge among players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n that player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n shows her red card.\n\nProduct Update\nThe DEL product update is defined below. This update yields a new pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n        ⊗\n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)\\otimes ({\\mathcal {E}},e)}\n  \n representing how the new situation which was previously represented by \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)}\n  \n is perceived by the agents after the occurrence of"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 35, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " ({\\mathcal {E}},e)}\n  \n representing how the new situation which was previously represented by \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)}\n  \n is perceived by the agents after the occurrence of the event represented by \n  \n    \n      \n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {E}},e)}\n  \n.\nLet \n  \n    \n      \n        \n          \n            M\n          \n        \n        =\n        (\n        W\n        ,\n        \n          R\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            n\n          \n        \n        ,\n        I\n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}=(W,R_{1},\\ldots ,R_{n},I)}\n  \n be an epistemic model and let \n  \n    \n      \n        \n          \n            E\n          \n        \n        =\n        (\n        \n          W\n          \n            α\n          \n        \n        ,\n        \n          R\n          \n            1\n          \n          \n            α\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            n\n          \n          \n            α\n          \n        \n        ,\n        \n          I\n          \n            α\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {E}}=(W^{\\alpha },R_{1}^{\\alpha },\\ldots ,R_{n}^{\\alpha },I^{\\alpha })}\n  \n be an event model. The product update of \n  \n    \n      \n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {M}}}\n  \n and \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {E}}}\n  \n is the epistemic model \n  \n    \n      \n        \n          \n            M\n          \n        \n        ⊗\n        \n          \n            \n              E\n            \n          \n        \n        =\n        (\n        \n          W\n          \n            ⊗\n          \n        \n        ,\n        \n          R\n          \n            1\n          \n          \n            ⊗\n          \n        \n        ,\n        …\n        ,\n        \n          R\n          \n            n\n          \n          \n            ⊗\n          \n        \n        ,\n        \n          I\n          \n            ⊗\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}\\otimes {\\mathcal {\\mathcal {E}}}=(W^{\\otimes },R_{1}^"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 36, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n          \n            n\n          \n          \n            ⊗\n          \n        \n        ,\n        \n          I\n          \n            ⊗\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}}\\otimes {\\mathcal {\\mathcal {E}}}=(W^{\\otimes },R_{1}^{\\otimes },\\ldots ,R_{n}^{\\otimes },I^{\\otimes })}\n  \n defined as follows: for all \n  \n    \n      \n        v\n        ∈\n        W\n      \n    \n    {\\displaystyle v\\in W}\n  \n and all \n  \n    \n      \n        f\n        ∈\n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle f\\in W^{\\alpha }}\n  \n,\n\nIf \n  \n    \n      \n        w\n        ∈\n        W\n      \n    \n    {\\displaystyle w\\in W}\n  \n and \n  \n    \n      \n        e\n        ∈\n        \n          W\n          \n            α\n          \n        \n      \n    \n    {\\displaystyle e\\in W^{\\alpha }}\n  \n are such that \n  \n    \n      \n        \n          \n            M\n          \n        \n        ,\n        w\n        ⊨\n        \n          I\n          \n            α\n          \n        \n        (\n        e\n        )\n      \n    \n    {\\displaystyle {\\mathcal {M}},w\\models I^{\\alpha }(e)}\n  \n then \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n        ⊗\n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)\\otimes ({\\mathcal {E}},e)}\n  \n denotes the pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ⊗\n        \n          \n            E\n          \n        \n        ,\n        (\n        w\n        ,\n        e\n        )\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}}\\otimes {\\mathcal {E}},(w,e))}\n  \n. This definition of the product update is conceptually grounded.\nCard Example:\nAs a result of the first event described above (Players \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n show their cards to each other in front of player \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n), the agents update their beliefs. We get the situation represented in the pointed epistemic model"}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 37, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " {\\displaystyle A}\n  \n and \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n show their cards to each other in front of player \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n), the agents update their beliefs. We get the situation represented in the pointed epistemic model \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n        ⊗\n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)\\otimes ({\\mathcal {E}},e)}\n  \n below. In this pointed epistemic model, the following statement holds: \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n        ⊗\n        (\n        \n          \n            E\n          \n        \n        ,\n        e\n        )\n        ⊨\n        (\n        \n          \n            \n              B\n            \n          \n        \n        ∧\n        \n          K\n          \n            A\n          \n        \n        \n          \n            \n              B\n            \n          \n        \n        )\n        ∧\n        \n          K\n          \n            C\n          \n        \n        ¬\n        \n          K\n          \n            A\n          \n        \n        \n          \n            \n              B\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)\\otimes ({\\mathcal {E}},e)\\models ({\\color {green}{B}}\\land K_{A}{\\color {green}{B}})\\land K_{C}\\neg K_{A}{\\color {green}{B}}.}\n  \n It states that player \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n knows that player \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n has the card but player \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n 'believes' that it is not the case.\n\nThe result of the second event is represented below. In this pointed epistemic model, the following statement holds: \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        w\n        )\n        ⊗\n        (\n        \n          \n            F\n          \n        \n        ,\n        e\n        )\n        ⊨\n        \n          C\n          \n            {\n            B\n            ,\n            C\n            }\n          \n        \n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∧\n        \n          \n            \n              B\n            \n          \n        \n        ∧\n        \n          \n            \n              C\n            \n          \n        \n        )\n        ∧\n       "}
{"doc_id": "Dynamic epistemic logic", "chunk_id": 38, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n        )\n        ⊨\n        \n          C\n          \n            {\n            B\n            ,\n            C\n            }\n          \n        \n        (\n        \n          \n            \n              A\n            \n          \n        \n        ∧\n        \n          \n            \n              B\n            \n          \n        \n        ∧\n        \n          \n            \n              C\n            \n          \n        \n        )\n        ∧\n        ¬\n        \n          K\n          \n            A\n          \n        \n        (\n        \n          \n            \n              B\n            \n          \n        \n        ∧\n        \n          \n            \n              C\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},w)\\otimes ({\\mathcal {F}},e)\\models C_{\\{B,C\\}}({\\color {red}{A}}\\land {\\color {green}{B}}\\land {\\color {blue}{C}})\\land \\neg K_{A}({\\color {green}{B}}\\land {\\color {blue}{C}})}\n  \n. It states that there is common knowledge among \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n that they know the true state of the world (namely \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n has the red card, \n  \n    \n      \n        B\n      \n    \n    {\\displaystyle B}\n  \n has the green card and \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n has the blue card), but \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \n does not know it.\n\nBased on these three components (epistemic model, event model and product update), Baltag, Moss and Solecki defined a general logical language inspired from the logical language of propositional dynamic logic to reason about information and knowledge change.\n\nSee also\nEpistemic logic\nEpistemology\nLogic in computer science\nModal logic\n\nNotes"}
{"doc_id": "Elements of AI", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Elements of AI is a massive open online course (MOOC) teaching the basics of artificial intelligence. The course, originally launched in 2018, is designed and organized by the University of Helsinki and learning technology company MinnaLearn. The course includes modules on machine learning, neural networks, the philosophy of artificial intelligence, and using artificial intelligence to solve problems. It consists of two parts: Introduction to AI and its sequel, Building AI, that was released in late 2020. In November 2019, the course was named one of four winners of MIT’s Inclusive Innovation Challenge.\nUniversity of Helsinki's computer science department is known as the alma mater of Linus Torvalds, a Finnish-American software engineer who is the creator of the Linux kernel, which is the kernel for Linux operating systems.\n\nEU’s AI pledge\nThe government of Finland has pledged to offer the course for all EU citizens by the end of 2021, as the course is made available in all the official EU languages. The initiative was launched as part of Finland's Presidency of the Council of the European Union in 2019, with the European Commission providing translations of the course materials.\nIn 2017, Finland launched an AI strategy to stay competitive in the field of AI amid growing competition between China and the United States. With the support of private companies and the government, Finland's now-realized goal was to get 1 percent of its citizens to participate in Elements of AI.\nOther governments have also given their support to the course. For instance, Germany's Federal Minister for Economic Affairs and Energy Peter Altmeier has encouraged citizens to take part in the course to help Germany gain a competitive advantage in AI. Sweden's Minister for Energy and Minister for Digital Development Anders Ygeman has said that Sweden aims to teach 1 percent of its population the basics of AI like Finland has.\n\nParticipants\nElements of AI had enrolled more than 1 million students from more than 110 countries by May 2023. A quarter of the course's participants are aged 45 and over, and some 40 percent are women. Among Nordic participants, the share of women is nearly 60 percent.\nIn September 2022, the course was available in Finnish, Swedish, Estonian, English, German, Latvian, Norwegian, French, Belgian, Czech, Greek, Slovakian, Slovenian, Latvian, Lithuanian, Portuguese, Spanish, Irish, Icelandic, Maltese, Croatian, Romanian, Italian, Dutch, Polish, and Danish."}
{"doc_id": "Elements of AI", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Finnish, Swedish, Estonian, English, German, Latvian, Norwegian, French, Belgian, Czech, Greek, Slovakian, Slovenian, Latvian, Lithuanian, Portuguese, Spanish, Irish, Icelandic, Maltese, Croatian, Romanian, Italian, Dutch, Polish, and Danish."}
{"doc_id": "Embodied agent", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In artificial intelligence, an embodied agent, also sometimes referred to as an interface agent, is an intelligent agent that interacts with the environment through a physical body within that environment. Agents that are represented graphically with a body, for example a human or a cartoon animal, are also called embodied agents, although they have only virtual, not physical, embodiment. A branch of artificial intelligence focuses on empowering such agents to interact autonomously with human beings and the environment. Mobile robots are one example of physically embodied agents; Ananova and Microsoft Agent are examples of graphically embodied agents. Embodied conversational agents are embodied agents (usually with a graphical front-end as opposed to a robotic body) that are capable of engaging in conversation with one another and with humans employing the same verbal and nonverbal means that humans do (such as gesture, facial expression, and so forth).\n\nEmbodied conversational agents\nEmbodied conversational agents are a form of intelligent user interface. Graphically embodied agents aim to unite gesture, facial expression and speech to enable face-to-face communication with users, providing a powerful means of human-computer interaction.\n\nAdvantages\nFace-to-face communication allows communication protocols that give a much richer communication channel than other means of communicating. It enables pragmatic communication acts such as conversational turn-taking, facial expression of emotions, information structure and emphasis, visualisation and iconic gestures, and orientation in a three-dimensional environment. This communication takes place through both verbal and non-verbal channels such as gaze, gesture, spoken intonation and body posture.\nResearch has found that users prefer a non-verbal visual indication of an embodied system's internal state to a verbal indication, demonstrating the value of additional non-verbal communication channels. As well as this, the face-to-face communication involved in interacting with an embodied agent can be conducted alongside another task without distracting the human participants, instead improving the enjoyment of such an interaction. Furthermore, the use of an embodied presentation agent results in improved recall of the presented information.\nEmbodied agents also provide a social dimension to the interaction. Humans willingly ascribe social awareness to computers, and thus interaction with embodied agents follows social conventions, similar to human to human interactions. This social interaction both raises the believability and perceived trustworthiness of agents, and increases the user's engagement with the system. Rickenberg and Reeves found that the presence of an embodied agent on a website increased the level of user trust in that website, but also increased users' anxiety and affected their performance, as if they were being watched by a real human."}
{"doc_id": "Embodied agent", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " trustworthiness of agents, and increases the user's engagement with the system. Rickenberg and Reeves found that the presence of an embodied agent on a website increased the level of user trust in that website, but also increased users' anxiety and affected their performance, as if they were being watched by a real human. Another effect of the social aspect of agents is that presentations given by an embodied agent are perceived as being more entertaining and less difficult than similar presentations given without an agent. Research shows that perceived enjoyment, followed by perceived usefulness and ease of use, is the major factor influencing user adoption of embodied agents.\nA study in January 2004 by Byron Reeves at Stanford demonstrated how digital characters could \"enhance online experiences\" through explaining how virtual characters essentially add a sense of relatability to the user experience and make it more approachable. This increase in likability in turn helps make the products better, which benefits both the end users and those creating the product.\n\nApplications\nThe rich style of communication that characterises human conversation makes conversational interaction with embodied conversational agents ideal for many non-traditional interaction tasks. A familiar application of graphically embodied agents is computer games; embodied agents are ideal for this setting because the richer communication style makes interacting with the agent enjoyable. Embodied conversational agents have also been used in virtual training environments, portable personal navigation guides, interactive fiction and storytelling systems, interactive online characters and automated presenters and commentators.\nMajor virtual assistants like Siri, Amazon Alexa and Google Assistant do not come with any visual embodied representation, which is believed to limit\nthe sense of human presence by users.\nThe U.S. Department of Defense utilizes a software agent called SGT STAR on U.S. Army-run Web sites and Web applications for site navigation, recruitment and propaganda purposes. Sgt. Star is run by the Army Marketing and Research Group, a division operated directly from The Pentagon. Sgt. Star is based upon the ActiveSentry technology developed by Next IT, a Washington-based information technology services company. Other such bots in the Sgt. Star \"family\" are utilized by the Federal Bureau of Investigation and the Central Intelligence Agency for intelligence gathering purposes.\n\nSee also"}
{"doc_id": "Embodied cognitive science", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Embodied cognitive science is an interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior. It comprises three main methodologies: the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity; the formation of a common set of general principles of intelligent behavior; and the experimental use of robotic agents in controlled environments.\n\nContributors\nEmbodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. Contributors to the field include:\n\nFrom the perspective of neuroscience, Gerald Edelman of the Neurosciences Institute at La Jolla, Francisco Varela of CNRS in France, and J. A. Scott Kelso of Florida Atlantic University\nFrom the perspective of psychology, Lawrence Barsalou, Michael Turvey, Vittorio Guidano and Eleanor Rosch\nFrom the perspective of linguistics, Gilles Fauconnier, George Lakoff, Mark Johnson, Leonard Talmy and Mark Turner\nFrom the perspective of language acquisition, Eric Lenneberg and Philip Rubin at Haskins Laboratories\nFrom the perspective of anthropology, Edwin Hutchins, Bradd Shore, James Wertsch and Merlin Donald.\nFrom the perspective of autonomous agent design, early work is sometimes attributed to Rodney Brooks or Valentino Braitenberg\nFrom the perspective of artificial intelligence, Understanding Intelligence by Rolf Pfeifer and Christian Scheier or How the Body Shapes the Way We Think, by Rolf Pfeifer and Josh C. Bongard\nFrom the perspective of philosophy, Andy Clark, Dan Zahavi, Shaun Gallagher, and Evan Thompson\nIn 1950, Alan Turing proposed that a machine may need a human-like body to think and speak:\n\nIt can also be maintained that it is best to provide the machine with the best sense organs that money can buy, and then teach it to understand and speak English. That process could follow the normal teaching of a child. Things would be pointed out and named, etc. Again, I do not know what the right answer is, but I think both approaches should be tried.\n\nTraditional cognitive theory\nEmbodied cognitive science is an alternative theory to cognition in which it minimizes appeals to computational theory of mind in favor of greater emphasis on how an organism's body determines how and what it thinks. Traditional cognitive theory is based mainly around symbol manipulation, in which certain inputs are fed into a processing unit that produces an output. These inputs follow certain rules of syntax, from which the processing unit"}
{"doc_id": "Embodied cognitive science", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " minimizes appeals to computational theory of mind in favor of greater emphasis on how an organism's body determines how and what it thinks. Traditional cognitive theory is based mainly around symbol manipulation, in which certain inputs are fed into a processing unit that produces an output. These inputs follow certain rules of syntax, from which the processing unit finds semantic meaning. Thus, an appropriate output is produced. For example, a human's sensory organs are its input devices, and the stimuli obtained from the external environment are fed into the nervous system which serves as the processing unit. From here, the nervous system is able to read the sensory information because it follows a syntactic structure, thus an output is created. This output then creates bodily motions and brings forth behavior and cognition. Of particular note is that cognition is sealed away in the brain, meaning that mental cognition is cut off from the external world and is only possible by the input of sensory information.\n\nThe embodied cognitive approach\nEmbodied cognitive science differs from the traditionalist approach in that it denies the input-output system. This is chiefly due to the problems presented by the Homunculus argument, which concluded that semantic meaning could not be derived from symbols without some kind of inner interpretation. If some little man in a person's head interpreted incoming symbols, then who would interpret the little man's inputs? Because of the specter of an infinite regress, the traditionalist model began to seem less plausible. Thus, embodied cognitive science aims to avoid this problem by defining cognition in three ways.\n\nPhysical attributes of the body\nThe first aspect of embodied cognition examines the role of the physical body, particularly how its properties affect its ability to think. This part attempts to overcome the symbol manipulation component that is a feature of the traditionalist model. Depth perception, for instance, can be better explained under the embodied approach due to the sheer complexity of the action. Depth perception requires that the brain detect the disparate retinal images obtained by the distance of the two eyes. In addition, body and head cues complicate this further. When the head is turned in a given direction, objects in the foreground will appear to move against objects in the background. From this, it is said that some kind of visual processing is occurring without the need of any kind of symbol manipulation. This is because the objects appearing to move the foreground are simply appearing to move. This observation concludes then that depth can be perceived with no intermediate symbol manipulation necessary. \nA more poignant example exists through examining auditory perception. Generally speaking the greater the distance between the ears, the greater the possible"}
{"doc_id": "Embodied cognitive science", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of any kind of symbol manipulation. This is because the objects appearing to move the foreground are simply appearing to move. This observation concludes then that depth can be perceived with no intermediate symbol manipulation necessary. \nA more poignant example exists through examining auditory perception. Generally speaking the greater the distance between the ears, the greater the possible auditory acuity. Also relevant is the amount of density in between the ears, for the strength of the frequency wave alters as it passes through a given medium. The brain's auditory system takes these factors into account as it process information, but again without any need for a symbolic manipulation system. This is because the distance between the ears for example does not need symbols to represent it. The distance itself creates the necessary opportunity for greater auditory acuity. The amount of density between the ears is similar, in that it is the actual amount itself that simply forms the opportunity for frequency alteration. Thus under consideration of the physical properties of the body, a symbolic system is unnecessary and an unhelpful metaphor.\n\nThe body's role in the cognitive process\nThe second aspect draws heavily from George Lakoff's and Mark Johnson's work on concepts. They argued that humans use metaphors whenever possible to better explain their external world. Humans also have a basic stock of concepts in which other concepts can be derived from. These basic concepts include spatial orientations such as up, down, front, and back. Humans can understand what these concepts mean because they can directly experience them from their own bodies. For example, because human movement revolves around standing erect and moving the body in an up-down motion, humans innately have these concepts of up and down. Lakoff and Johnson contend this is similar with other spatial orientations such as front and back too. As mentioned earlier, these basic stocks of spatial concepts are the basis in which other concepts are constructed. Happy and sad for instance are seen now as being up or down respectively. When someone says they are feeling down, what they are really saying is that they feel sad for example. Thus the point here is that true understanding of these concepts is contingent on whether one can have an understanding of the human body. So the argument goes that if one lacked a human body, they could not possibly know what up or down could mean, or how it could relate to emotional states.\n\n[I]magine a spherical being living outside of any gravitational field, with no knowledge or imagination of any other kind of experience. What could UP possibly mean to such a being?\nWhile this does not mean that such beings would be incapable of expressing emotions"}
{"doc_id": "Embodied cognitive science", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " or down could mean, or how it could relate to emotional states.\n\n[I]magine a spherical being living outside of any gravitational field, with no knowledge or imagination of any other kind of experience. What could UP possibly mean to such a being?\nWhile this does not mean that such beings would be incapable of expressing emotions in other words, it does mean that they would express emotions differently from humans. Human concepts of happiness and sadness would be different because human would have different bodies. So then an organism's body directly affects how it can think, because it uses metaphors related to its body as the basis of concepts.\n\nInteraction of local environment\nA third component of the embodied approach looks at how agents use their immediate environment in cognitive processing. Meaning, the local environment is seen as an actual extension of the body's cognitive process. The example of a personal digital assistant (PDA) is used to better imagine this. Echoing functionalism (philosophy of mind), this point claims that mental states are individuated by their role in a much larger system. So under this premise, the information on a PDA is similar to the information stored in the brain. So then if one thinks information in the brain constitutes mental states, then it must follow that information in the PDA is a cognitive state too. Consider also the role of pen and paper in a complex multiplication problem. The pen and paper are so involved in the cognitive process of solving the problem that it seems ridiculous to say they are somehow different from the process, in very much the same way the PDA is used for information like the brain. Another example examines how humans control and manipulate their environment so that cognitive tasks can be better performed. Leaving one's car keys in a familiar place so they aren't missed for instance, or using landmarks to navigate in an unfamiliar city. Thus, humans incorporate aspects of their environment to aid in their cognitive functioning.\n\nExamples of the value of embodied approach\nThe value of the embodiment approach in the context of cognitive science is perhaps best      explained by Andy Clark. He makes the claim that the brain alone should not be the single focus for the scientific study of cognition\n\nIt is increasingly clear that, in a wide variety of cases, the individual brain should not be the sole locus of cognitive scientific interest. Cognition is not a phenomenon that can be successfully studied while marginalizing the roles of body, world and action.\nThe following examples used by Clark will better illustrate how embodied thinking is becoming apparent     in scientific thinking.\n\nBluefin tuna\nThunnus,"}
{"doc_id": "Embodied cognitive science", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " individual brain should not be the sole locus of cognitive scientific interest. Cognition is not a phenomenon that can be successfully studied while marginalizing the roles of body, world and action.\nThe following examples used by Clark will better illustrate how embodied thinking is becoming apparent     in scientific thinking.\n\nBluefin tuna\nThunnus, or tuna, long baffled conventional biologists with its incredible abilities to accelerate quickly and attain great speeds. A biological examination of the tuna shows that it should not be capable of such feats. However, an answer can be found when taking the tuna's embodied state into account. The bluefin tuna is able to take advantage of and exploit its local environment by finding naturally occurring currents to increase its speed. The tuna also uses its own physical body for this end as well, by utilizing its tailfin to create the necessary vortices and pressure so it can accelerate and maintain high speeds. Thus, the bluefin tuna is actively using its local environment for its own ends through the attributes of its physical body.\n\nRobots\nClark uses the example of the hopping robot constructed by Raibert and Hodgins to demonstrate further the value of the embodiment paradigm. These robots were essentially vertical cylinders with a single hopping foot. The challenge of managing the robot's behavior can be daunting because in addition to the intricacies of the program itself, there were also the mechanical matters regarding how the foot ought to be constructed so that it could hop. An embodied approach makes it easier to see that in order for this robot to function, it must be able to exploit its system to the fullest. That is, the robot's systems should be seen as having dynamic characteristics as opposed to the traditional view that it is merely a command center that just executes actions.\n\nVision\nClark distinguishes between two kinds of vision, animate and pure vision. Pure vision is an idea that is typically associated with classical artificial intelligence, in which vision is used to create a rich world model so that thought and reason can be used to fully explore the inner model. In other words, pure vision passively creates the external perceivable world so that the faculties of reason can be better used introspectively. Animate vision, by contrast, sees vision as the means by which real-time action can commence. Animate vision is then more of a vehicle by which visual information is obtained so that actions can be undertaken. Clark points to animate vision as an example of embodiment, because it uses both biological and local environment cues to create an active intelligent process. Consider the Clark's example of going to the"}
{"doc_id": "Embodied cognitive science", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " real-time action can commence. Animate vision is then more of a vehicle by which visual information is obtained so that actions can be undertaken. Clark points to animate vision as an example of embodiment, because it uses both biological and local environment cues to create an active intelligent process. Consider the Clark's example of going to the drugstore to buy some Kodak film. In one's mind, one is familiar with the Kodak logo and its trademark gold color. Thus, one uses incoming visual stimuli to navigate around the drugstore until one finds the film. Therefore, vision should not be seen as a passive system but rather an active retrieval device that intelligently uses sensory information and local environmental cues to perform specific real-world actions.\n\nAffordance\nInspired by the work of the American psychologist James J. Gibson, this next example emphasizes the importance of action-relevant sensory information, bodily movement, and local environment cues. These three concepts are unified by the concept of affordances, which are possibilities of action provided by the physical world to a given agent. These are in turn determined by the agent's physical body, capacities, and the overall action-related properties of the local environment as well. Clark uses the example of an outfielder in baseball to better illustrate the concept of affordance. Traditional computational models would claim that an outfielder attempting to catch a fly-ball can be calculated by variables such as the running speed of the outfielder and the arc of the baseball. However, Gibson's work shows that a simpler method is possible. The outfielder can catch the ball so long as they adjust their running speed so that the ball continually moves in a straight line in their field of vision. Note that this strategy uses various affordances that are contingent upon the success of the outfielder, including their physical body composition, the environment of the baseball field, and the sensory information obtained by the outfielder. \nClark points out here that the latter strategy of catching the ball as opposed to the former has significant implications for perception. The affordance approach proves to be non-linear because it relies upon spontaneous real-time adjustments. On the contrary, the former method of computing the arc of the ball is linear as it follows a sequence of perception, calculation and performing action. Thus, the affordance approach challenges the traditional view of perception by arguing against the notion that computation and introspection are necessary. Instead, it ought to be replaced with the idea that perception constitutes a continuous equilibrium of action adjustment between the agent and the world. Ultimately Clark does not expressly claim this is certain but he"}
{"doc_id": "Embodied cognitive science", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " action. Thus, the affordance approach challenges the traditional view of perception by arguing against the notion that computation and introspection are necessary. Instead, it ought to be replaced with the idea that perception constitutes a continuous equilibrium of action adjustment between the agent and the world. Ultimately Clark does not expressly claim this is certain but he does observe the affordance approach can explain adaptive response satisfactorily. This is because they utilize environmental cues made possible by perceptual information that is actively used in the real-time by the agent.\n\nGeneral principles of intelligent behavior\nIn the formation of general principles of intelligent behavior, Pfeifer intended to be contrary to older principles given in traditional artificial intelligence.  The most dramatic difference is that the principles are applicable only to situated robotic agents in the real world, a domain where traditional artificial intelligence showed the least promise.\nPrinciple of cheap design and redundancy: Pfeifer realized that implicit assumptions made by engineers often substantially influence a control architecture's complexity.  This insight is reflected in discussions of the scalability problem in robotics.  The internal processing needed for some bad architectures can grow out of proportion to new tasks needed of an agent.  \n\nOne of the primary reasons for scalability problems is that the amount of programming and knowledge engineering that the robot designers have to perform grows very rapidly with the complexity of the robot's tasks. There is mounting evidence that pre-programming cannot be the solution to the scalability problem ... The problem is that programmers introduce too many hidden assumptions in the robot's code.\nThe proposed solutions are to have the agent exploit the inherent physics of its environment, to exploit the constraints of its niche, and to have agent morphology based on parsimony and the principle of Redundancy. Redundancy reflects the desire for the error-correction of signals afforded by duplicating like channels.  Additionally, it reflects the desire to exploit the associations between sensory modalities. (See redundant modalities).  In terms of design, this implies that redundancy should be introduced with respect not only to one sensory modality but to several.  It has been suggested that the fusion and transfer of knowledge between modalities can be the basis of reducing the size of the sense data taken from the real world.  This again addresses the scalability problem.\nPrinciple of parallel, loosely-coupled processes: An alternative to hierarchical methods of knowledge and action selection. This design principle differs most importantly from the Sense-Think-Act cycle of traditional AI. Since it does not involve this famous cycle, it is not affected by the frame problem.\nPrinciple of sensory"}
{"doc_id": "Embodied cognitive science", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " problem.\nPrinciple of parallel, loosely-coupled processes: An alternative to hierarchical methods of knowledge and action selection. This design principle differs most importantly from the Sense-Think-Act cycle of traditional AI. Since it does not involve this famous cycle, it is not affected by the frame problem.\nPrinciple of sensory-motor coordination: Ideally, internal mechanisms in an agent should give rise to things like memory and choice-making in an emergent fashion, rather than being prescriptively programmed from the beginning.   These kinds of things are allowed to emerge as the agent interacts with the environment. The motto is, build fewer assumptions into the agent's controller now, so that learning can be more robust and idiosyncratic in the future. \nPrinciple of ecological balance: This is more a theory than a principle, but its implications are widespread.  Its claim is that the internal processing of an agent cannot be made more complex unless there is a corresponding increase in complexity of the motors, limbs, and sensors of the agent. In other words, the extra complexity added to the brain of a simple robot will not create any discernible change in its behavior. The robot's morphology must already contain the complexity in itself to allow enough \"breathing room\" for more internal processing to develop.\nValue principle: This was the architecture developed in the Darwin III robot of Gerald Edelman. It relies heavily on connectionism.\n\nCritical responses\nTraditionalist response to local environment claim\nA traditionalist may argue that objects may be used to aid in cognitive processes, but this does not mean they are part of a cognitive system. Eyeglasses are used to aid in the visual process, but to say they are a part of a larger system would completely redefine what is meant by a visual system. However, supporters of the embodied approach could make the case that if objects in the environment play the functional role of mental states, then the items themselves should not be counted among the mental states.\nLars Ludwig explores mind extension further outlining its role in technology. He proposes a cognitive theory of 'extended artificial memory', which represents a theoretical update and extension of the  memory theories of Richard Semon.\n\nSee also"}
{"doc_id": "Empowerment (artificial intelligence)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Empowerment in the field of artificial intelligence  formalises and quantifies (via information theory) the potential an agent perceives that it has to influence its environment. An agent which follows an empowerment maximising policy, acts to maximise future options (typically up to some limited horizon). Empowerment can be used as a (pseudo) utility function that depends only on information gathered from the local environment to guide action, rather than seeking an externally imposed goal, thus is a form of intrinsic motivation. \nThe empowerment formalism depends on a probabilistic model commonly used in artificial intelligence. An autonomous agent operates in the world by taking in sensory information and acting to change its state, or that of the environment, in a cycle of perceiving and acting known as the perception-action loop. Agent state and actions are modelled by random variables (\n  \n    \n      \n        S\n        :\n        s\n        ∈\n        \n          \n            S\n          \n        \n        ,\n        A\n        :\n        a\n        ∈\n        \n          \n            A\n          \n        \n      \n    \n    {\\displaystyle S:s\\in {\\mathcal {S}},A:a\\in {\\mathcal {A}}}\n  \n) and time (\n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n). The choice of action depends on the current state, and the future state depends on the choice of action, thus the perception-action loop unrolled in time forms a causal bayesian network.\n\nDefinition\nEmpowerment (\n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathfrak {E}}}\n  \n) is defined as the channel capacity (\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n) of the actuation channel of the agent, and is formalised as the maximal possible information flow between the actions of the agent and the effect of those actions some time later. Empowerment can be thought of as the future potential of the agent to affect its environment, as measured by its sensors.\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        :=\n        C\n        (\n        \n          A\n          \n            t\n          \n        \n        ⟶\n        \n          S\n          \n            t\n            +\n            1\n          \n        \n        )\n        ≡\n        \n          max\n          \n            p\n            (\n            \n              a\n              \n                t\n              \n            \n            )\n          \n        \n        I\n        (\n        \n          A\n          \n            t\n          \n        \n        ;\n        \n          S\n          \n            t\n            +\n            1\n          "}
{"doc_id": "Empowerment (artificial intelligence)", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "          \n        \n        )\n        ≡\n        \n          max\n          \n            p\n            (\n            \n              a\n              \n                t\n              \n            \n            )\n          \n        \n        I\n        (\n        \n          A\n          \n            t\n          \n        \n        ;\n        \n          S\n          \n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathfrak {E}}:=C(A_{t}\\longrightarrow S_{t+1})\\equiv \\max _{p(a_{t})}I(A_{t};S_{t+1})}\n  \n\nIn a discrete time model, Empowerment can be computed for a given number of cycles into the future, which is referred to in the literature as 'n-step' empowerment. \n\n  \n    \n      \n        \n          \n            E\n          \n        \n        (\n        \n          A\n          \n            t\n          \n          \n            n\n          \n        \n        ⟶\n        \n          S\n          \n            t\n            +\n            n\n          \n        \n        )\n        =\n        \n          max\n          \n            p\n            (\n            \n              a\n              \n                t\n              \n            \n            ,\n            .\n            .\n            .\n            ,\n            \n              a\n              \n                t\n                +\n                n\n                −\n                1\n              \n            \n            )\n          \n        \n        I\n        (\n        \n          A\n          \n            t\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          A\n          \n            t\n            +\n            n\n            −\n            1\n          \n        \n        ;\n        \n          S\n          \n            t\n            +\n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\mathfrak {E}}(A_{t}^{n}\\longrightarrow S_{t+n})=\\max _{p(a_{t},...,a_{t+n-1})}I(A_{t},...,A_{t+n-1};S_{t+n})}\n  \n\nThe unit of empowerment depends on the logarithm base. Base 2 is commonly used in which case the unit is bits.\n\nContextual Empowerment\nIn general the choice of action (action distribution) that maximises empowerment varies from state to state. Knowing the empowerment of an agent in a specific state is useful, for example to construct an empowerment maximising policy. State-specific empowerment can be found using the more general formalism for 'contextual empowerment'. \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is a random variable describing the context (e.g. state).\n\n  \n    \n      \n        \n          \n"}
{"doc_id": "Empowerment (artificial intelligence)", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " specific state is useful, for example to construct an empowerment maximising policy. State-specific empowerment can be found using the more general formalism for 'contextual empowerment'. \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is a random variable describing the context (e.g. state).\n\n  \n    \n      \n        \n          \n            E\n          \n        \n        (\n        \n          A\n          \n            t\n          \n          \n            n\n          \n        \n        ⟶\n        \n          S\n          \n            t\n            +\n            n\n          \n        \n        \n          ∣\n        \n        C\n        )\n        =\n        \n          ∑\n          \n            c\n            \n              ∈\n            \n            C\n          \n        \n        p\n        (\n        c\n        )\n        \n          \n            E\n          \n        \n        (\n        \n          A\n          \n            t\n          \n          \n            n\n          \n        \n        ⟶\n        \n          S\n          \n            t\n            +\n            n\n          \n        \n        \n          ∣\n        \n        C\n        =\n        c\n        )\n      \n    \n    {\\displaystyle {\\mathfrak {E}}(A_{t}^{n}\\longrightarrow S_{t+n}{\\mid }C)=\\sum _{c{\\in }C}p(c){\\mathfrak {E}}(A_{t}^{n}\\longrightarrow S_{t+n}{\\mid }C=c)}\n\nApplication\nEmpowerment maximisation can be used as a pseudo-utility function to enable agents to exhibit intelligent behaviour without requiring the definition of external goals, for example balancing a pole in a cart-pole balancing scenario where no indication of the task is provided to the agent. \nEmpowerment has been applied in studies of collective behaviour and in continuous domains. As is the case with Bayesian methods in general, computation of empowerment becomes computationally expensive as the number of actions and time horizon extends, but approaches to improve efficiency have led to usage in real-time control. Empowerment has been used for intrinsically motivated reinforcement learning agents playing video games, and in the control of underwater vehicles."}
{"doc_id": "Enterprise cognitive system", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Enterprise cognitive systems (ECS) are part of a broader shift in computing, from a programmatic to a probabilistic approach, called cognitive computing. An Enterprise Cognitive System makes a new class of complex decision support problems computable, where the business context is ambiguous, multi-faceted, and fast-evolving, and what to do in such a situation is usually assessed today by the business user. An ECS is designed to synthesize a business context and link it to the desired outcome. It recommends evidence-based actions to help the end-user achieve the desired outcome. It does so by finding past situations similar to the current situation, and extracting the repeated actions that best influence the desired outcome.\nWhile general-purpose cognitive systems can be used for different outputs, prescriptive, suggestive, instructive, or simply entertaining, an enterprise cognitive system is focused on action, not insight, to help in assessing what to do in a complex situation.\n\nKey characteristics\nECS have to be:\n\nAdaptive: They must learn as information changes, and as goals and requirements evolve. They must resolve ambiguity and tolerate unpredictability. They must be engineered to feed on dynamic data in real time, or near real time. In the Enterprise, near-real time learning from data requires an agile information federation approach to ingest incremental data updates as they occur, and an unsupervised learning approach to ensure that new best practice is leveraged across the organization in a timely manner.\nInteractive: They must interact easily with users so that those users can define their needs comfortably. They may also interact with other processors, devices, and Cloud services, as well as with people. In the Enterprise, interactions are controlled via existing workflows and UIs. Therefore, embedding best practices directly into these existing interfaces, in the context of a specific step, is critical to ensure maximum end-user adoption.\nIterative and stateful: They must aid in defining a problem by asking questions or finding additional source input if a problem statement is ambiguous or incomplete. They must “remember” previous interactions in a process and return information that is suitable for the specific application at that point in time. In the Enterprise, business context is often structured by a business process, and therefore sufficiently data-rich to make relevant recommendations without significant iterations from the end-user. A stateful memory of overall interactions across communication channels is critical for understanding of context, as a static profile will not capture intent and outcome potential the way behavior does.\nContextual: They must understand, identify, and extract contextual elements such as meaning, syntax, time, location, appropriate"}
{"doc_id": "Enterprise cognitive system", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " significant iterations from the end-user. A stateful memory of overall interactions across communication channels is critical for understanding of context, as a static profile will not capture intent and outcome potential the way behavior does.\nContextual: They must understand, identify, and extract contextual elements such as meaning, syntax, time, location, appropriate domain, regulations, user's profile, process, task and goal. They may draw on multiple sources of information, including both structured and unstructured digital information, as well as sensory inputs (visual, gestural, auditory, or sensor-provided). In the Enterprise, Context is fragmented and must be aggregated across data types, sources, and locations. In most business environments, such data is captured in existing enterprise information systems, and the effort is linked to quickly source and unify such information. It is rare to have to directly process sensor, audio or visual data in real-time as direct input into the enterprise cognitive system. Instead, these data types are captured by Enterprise Applications and pre-processed into a binary or text format prior to consumption by the System.\n\nBusiness applications powered by an ECS\nBottlenose – trends and brands monitoring\nCybereason – security threat monitoring\nDataminr – social media monitoring"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The environmental impact of artificial intelligence includes substantial electricity consumption for training and using deep learning models, and the related carbon footprint and water usage. Moreover, the artificial intelligence (AI) data centers are materially intense, requiring a large amount of electronics that use specialized mined metals and which eventually will be disposed as e-waste. \nSome scientists argue that AI may also provide solutions to environmental problems, such as material innovations, improved grid management, and other forms of optimization across various fields of technology.\nAs the environmental impact of AI becomes more apparent, governments have begun instituting policies to improve the oversight and review of environmental issues that could be associated with the use of AI, and related infrastructure development.\n\nCarbon footprint and energy use\nIndividual level\nA request made via ChatGPT, an AI-based virtual assistant, uses 10 times as much electricity as a Google Search. Microsoft and Meta had similar increases in their carbon footprint, similarly attributed to AI.\nOnce the model is trained, it consumes significantly less energy, however it still requires a relatively high amount of electricity. Researchers have estimated that a ChatGPT query consumes about five times more electricity than a simple web search. In June 2025, OpenAI executive Sam Altman stated that the average ChatGPT query used about 0.34 Wh (1.2 kJ) of electricity and 8.5×10−5 US gal (0.32 ml) of water.\nAccording to a study by Luccioni, Jernite and Strubell (2024), simple classification tasks performed by AI models consume on average 0.002 to 0.007 Wh per prompt (about 9% of a smartphone charge for 1,000 prompts). Text generation and text summarization each require around 0.05 Wh per prompt on average, while image generation is the most energy-intensive, averaging 2.91 Wh per prompt. The least efficient image generation model used 11.49 Wh per image, roughly equivalent to half a smartphone charge.\nResearchers at the University of Michigan measured the energy consumption of various Meta Llama 3.1 models released in 2024 and found that smaller language models (8 billion parameters) use about 114 joules (0.03167 Wh) per response, while larger models (405 billion parameters) require up to 6,700 joules (1.861 Wh) per response. This corresponds to the energy needed to run a microwave oven for roughly one-tenth of a second and eight seconds, respectively.\nAccording to researchers"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (0.03167 Wh) per response, while larger models (405 billion parameters) require up to 6,700 joules (1.861 Wh) per response. This corresponds to the energy needed to run a microwave oven for roughly one-tenth of a second and eight seconds, respectively.\nAccording to researchers the median Google Gemini text prompt in 2025 consumes about 0.24 Wh of electricity. Google’s improvements in software efficiency and its clean-energy procurement, have reduced energy use by a factor of 33 and carbon emissions by a factor of 44 for a typical prompt over a year. In practical terms, the median Gemini text prompt uses roughly as much energy as watching nine seconds of television.\nA 2024 Scientific Reports study compared estimated carbon impacts of human writers and artists to those of select AI systems, and calculated that humans have 130 to 2900 times higher carbon impact. A 2025 study criticized that comparison based on differences in output quality, and found a counterexample in completing programming tasks with GPT-4, which had a carbon impact 5 to 19 times more than human programmers.\n\nSystem level\nCarbon footprint\nAI has a significant carbon footprint due to growing electricity consumption, especially due to training and usage. Researchers have argued that the carbon footprint of AI models during training should be considered when attempting to understand the impact of AI. One study suggested that by 2027, energy costs for AI could increase to 85–134 Twh, nearly 0.5% of all current electricity usage. Training large language models (LLMs) and other generative AI generally requires much more electricity compared to running a single prediction on the trained model. Using a trained model repeatedly, though, may easily multiply the electricity costs of predictions. The computation required to train the most advanced AI models doubles every 3.4 months on average, leading to exponential power usage and resulting carbon footprint.Additionally, artificial intelligence algorithms running in places predominantly using fossil fuels for energy will exert a much higher carbon footprint than places with cleaner energy sources. These models may be modified for less environmental impacts at the cost of accuracy, emphasizing the importance of finding the balance between accuracy and environmental impact.\nRecent research has shown that developing and training large language models can produce significant environmental impacts, including approximately 493 metric tons of carbon dioxide emissions and 2.77 million liters of water use when considering the full lifecycle from hardware manufacturing to model training.\nBERT, a language model trained in 2019, required \"the energy of a round"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " shown that developing and training large language models can produce significant environmental impacts, including approximately 493 metric tons of carbon dioxide emissions and 2.77 million liters of water use when considering the full lifecycle from hardware manufacturing to model training.\nBERT, a language model trained in 2019, required \"the energy of a round-trip transcontinental flight\" to train. GPT-3 released 552 metric tons of carbon dioxide into the atmosphere during training, \"the equivalent of 123 gasoline-powered passenger vehicles driven for one year\". Much of the energy cost is due to inefficient model architectures and processors. One model named BLOOM, from Hugging Face, trained with more efficient chips and, therefore, only released 25 metric tons of CO2. Incorporating the energy cost of manufacturing the chips for the system doubled the carbon footprint, to \"the equivalent of around 60 flights between London and New York.\" Operating BLOOM daily was estimated to release the equivalent carbon footprint as driving 54 miles.\nAlgorithms which have lower energy costs but run millions of times a day can also have significant carbon footprints. The integration of AI into search engines could multiply energy costs significantly. Another estimate found that integrating ChatGPT into every Google search query would use 10 TWh each year, the equivalent yearly energy usage of 1.5 million European Union residents.\nIncreased computational demands from AI caused both increased water and energy usage, leading to significantly more demands on the grid. Due to increased energy demands from AI-related projects, coal-fired plants in Kansas Cityand West Virginia pushed back closing. Other coal-fired plants in the Salt Lake City region have pushed back retirement of their coal-fired plants by up to a decade. Environmental debates have raged in both Virginia and France about whether a \"moratorium\" should be called for additional data centers. In 2024 at the World Economic Forum, Sam Altman gave a speech in which he said that the AI industry can only grow if there is a major technology breakthrough to increase energy development.\nCarbon footprints of AI models depends on the energy source used, with data centers using renewable energy lowering their footprint. Many tech companies claim to offset energy usage by buying energy from renewable sources, though some experts argue that utilities simply replace the claimed renewable energy with increased non-renewable sources for their other customers. Analysis of the carbon footprint of AI models remains difficult to determine, as they are aggregated as part of datacenter carbon footprints, and some models may help reduce carbon footprints of other industries, or due to"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " experts argue that utilities simply replace the claimed renewable energy with increased non-renewable sources for their other customers. Analysis of the carbon footprint of AI models remains difficult to determine, as they are aggregated as part of datacenter carbon footprints, and some models may help reduce carbon footprints of other industries, or due to differences in reporting from companies.\nSome applications of ML, such as for fossil fuel discovery and exploration, may worsen climate change. Use of AI for personalized marketing online may also lead to increased consumption of goods, which could also increase global emissions.\nCritics have highlighted that AI companies are also signing contracts with polluters to expand their production, thus increasing their environmental impact. \nA 2023 bibliographic report from Inria and CEA-Leti emphasizes that AI's carbon footprint must be assessed using full life cycle analysis (LCA), including hardware manufacturing, training energy, and deployment phases, rather than only operational emissions.\n\nEnergy use and efficiency\nAI chips, (i.e. GPUs) use more energy and emit more heat than traditional CPU chips. AI models with inefficiently implemented architectures, or trained on less efficient chips may use more energy. Since the 1940's the energy efficiency of computation has doubled every 1.6 years. \nThe International Energy Agency (IEA) released its 2025 Electricity Analysis and Forecast in February 2025, projecting 4% growth in global electricity demand over the next three years due to data center growth, increased industrial production, increased electrification, and increased use of air conditioning. By 2027, the US's energy consumption is expected to grow by an amount equivalent to California's entire annual power usage, largely driven by energy-hungry data centers and manufacturing operations. In 2024, U.S. electricity generation rose by 3%, with data centers emerging as a dominant force behind the increase. The trend is expected to continue as semiconductor and battery manufacturing plants ramp up operations, further intensifying demand.\nIn 2024, a US public policy group reported that AI and other technologies and industries poised to dominate the global economy are characterized by their high electricity demands. As such, the foundation of US energy strategy and policymaking will be to prioritize the reliable and abundant provision of electricity to support these critical sectors, which are needed to maintain the US economic and technological leadership in the twenty-first century. The rapid proliferation of AI has created unprecedented demand for electrical power, presenting a major obstacle to the sector's growth. E.g., in Northern Virginia, the largest global hub"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the reliable and abundant provision of electricity to support these critical sectors, which are needed to maintain the US economic and technological leadership in the twenty-first century. The rapid proliferation of AI has created unprecedented demand for electrical power, presenting a major obstacle to the sector's growth. E.g., in Northern Virginia, the largest global hub for AI data centers, the timeline for connecting bigger facilities—those requiring over 100 megawatts of power—to the electrical grid has extended to seven years, highlighting the strain on the energy infrastructure and the challenge of meeting AI's escalating power needs. Across the United States, utilities are experiencing the most substantial surge in electrical demand in decades. This strain is directly contributing to longer wait times for grid connections, complicating efforts to maintain the country's technological leadership in AI. The significance of these energy challenges extends beyond logistics. A New York Times editorial emphasized the critical role of energy infrastructure, stating that \"Electricity is more than just a utility; it's the bedrock of the digital era. If the United States truly wants to secure its leadership in A.I., it must equally invest in the energy systems that power it.\"\nGlobally, the electricity consumption of data centers rose to 460 terawatts in 2022. This would have made data centers the 11th largest electricity consumer in the world, between the nations of Saudi Arabia (371 terawatts) and France (463 terawatts), according to the Organization for Economic Co-operation and Development.\n\nDecisions and strategies by individual companies\nIn 2024, Google failed to reach key goals from their net zero plan as a result of their work with AI, and had a 48% increase in greenhouse gas emission attributable to their growth in AI. \nIn September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " ever US re-commissioning of a nuclear plant), over 835 megawatts of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear Reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.\nIn 2025, Microsoft unveiled plans to invest $80 billion in the development and expansion of data centers designed to support AI technologies. These facilities, critical to the advancement of AI, depend on vast networks of interconnected chip clusters and significant electrical power to operate efficiently.\n\nWater usage\nCooling AI servers can demand large amounts of fresh water which is evaporated in cooling towers.  To minimize the use of water required for cooling, centers are adopting a \nclosed-loop system where the water is reused.\nAnother major AI usage of freshwater is the manufacturing of semiconductor chips. Semiconductor chips act as processors for the large information models and their immense computational demands. Currently, the average semiconductor chip costs roughly 8-10 gallons of water, as a result of production of these chips requiring ultra pure water. This water is processed to remove molecular impurities and ions; it takes between 1.4 and 1.6 gallons of freshwater to make 1 gallon of ultra pure water, which is then used to manufacture the chips. \nIn a 2025 paper, researchers projected that AI will withdraw between 4.2 – 6.6 billion cubic meters of water in 2027, greater than half of the total water withdrawal of the United Kingdom. The authors estimated that training GPT-3 may have consumed 700,000 liters of water, and that 10–50 medium-length GPT-3 responses consume about 500 mL of fresh water, \"depending on when and where it is deployed\". According to other researchers the median Google Gemini text prompt in 2025 consumes about five drops of water (0.26 milliliter).\nOne data center that Microsoft had considered building near Phoenix, due to increasing AI usage, was likely to"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "500 mL of fresh water, \"depending on when and where it is deployed\". According to other researchers the median Google Gemini text prompt in 2025 consumes about five drops of water (0.26 milliliter).\nOne data center that Microsoft had considered building near Phoenix, due to increasing AI usage, was likely to consume up to 56 million gallons of fresh water each year, equivalent to the water footprints of 670 families. Microsoft may have increased water consumption by 34% due to AI, while Google increased its water usage by 20% due to AI. Due to their Iowa data center cluster, Microsoft was responsible for 6% of the freshwater use in a local town.\nA possible solution for reducing water consumption is to build data centers in colder countries that can offer a natural cooling system. For example, Facebook (now Meta) built a data center in Luleå, northern Sweden, in 2011. In addition, Google invested one more billion euros into the expansion of its data centre campus in Hamina in Finland in 2024, which was primarily created to adapt an old paper mill so that seawater would travel to the data centre's heat exchangers which then expelled the hot water back into the ocean for a full cycle, for a total of 4.5 billion euros invested in the site.\n\nE-waste\nElectronic waste (E-waste) due to production of AI hardware may also contribute to emissions. The rapid growth of AI may also lead to faster deprecation of devices, resulting in hazardous e-waste. Among the 62 million tonnes of e-waste produced in 2022, less than one quarter of the total mass was properly recycled. Worldwide, the annual generation of e-waste is rising by 2.6 million tonnes annually, on track to reach 82 million tonnes by 2030, a further 33% increase from the 2022 figure. AI could have an important role because it is expected to add 1.2 million to 5 million metric tons of e-waste in total by 2030, which would represent up to 12% of global e-waste. Some applications of AI, such as for robot recycling, may reduce e-waste.\n\nMining\nLarge-scale AI is typically housed in data centres, which can exact heavy tolls on the planet. Their electronics rely on huge amounts of raw materials: making a 2 kg computer requires 800 kg of raw materials. In addition, the microchips that power AI require rare"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " may reduce e-waste.\n\nMining\nLarge-scale AI is typically housed in data centres, which can exact heavy tolls on the planet. Their electronics rely on huge amounts of raw materials: making a 2 kg computer requires 800 kg of raw materials. In addition, the microchips that power AI require rare-earth elements, often mined in environmentally destructive ways.\n\nSocial impact and environmental justice\nEnvironmental degradation caused by AI can negatively impact certain geographic regions more than others. One-fifth of US data centers, which rely heavily on water for cooling, consume water from drought-stricken areas with moderate to high regional water stress. This increases the likelihood of seasonal water shortages in the public water supply of already-vulnerable regions. Local environmental impacts in the communities where AI models are trained have included local air and water pollution, elevated carbon emissions and ozone, and worsening megadroughts. \nCommunity activists have expressed concerns about the impact of AI on air quality, water access, and grid stability in marginalized neighborhoods that have historically suffered from industrial pollution. A large data center owned by xAI has faced criticism for environmental racism due to the alleged inequitable pollution in the majority-Black and low-income community of Boxtown. In 2025, the Southern Environmental Law Center (SELC) and the NAACP filed a lawsuit against xAI, alleging that the company is depriving the community of its human rights of clean air and a healthy environment and is in violation of the Clear Air Act. The data center houses 35 methane gas turbines which are not equipped with pollution controls, and according to the SELC operated for months without air pollution permits. Southwest Memphis has a cancer risk at four times the national average due to industrial contamination and air toxics exposure.\n\nClimate solutions\nDespite concerns about its environmental footprint, AI has demonstrated capabilities in improving weather forecasting and climate analysis. Machine learning models can process vast amounts of meteorological data to enhance short-term weather predictions and identify atmospheric patterns that traditional numerical methods may miss. These systems are being applied to model extreme weather events including floods, droughts, and heatwaves, enabling improved disaster preparedness and early warning systems.\nAI algorithms are also being used to monitor environmental changes, including tracking deforestation, measuring polar ice melt, and monitoring emissions from various sources.\nBeyond generating climate insights, AI technologies are being deployed in emission reduction efforts. Google has developed Project Green Light, which uses AI to optimize traffic light timing to reduce stop-and-go traffic, potentially decreasing vehicle emissions at intersections. Some climate scientists have suggested that AI"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", measuring polar ice melt, and monitoring emissions from various sources.\nBeyond generating climate insights, AI technologies are being deployed in emission reduction efforts. Google has developed Project Green Light, which uses AI to optimize traffic light timing to reduce stop-and-go traffic, potentially decreasing vehicle emissions at intersections. Some climate scientists have suggested that AI could improve the efficiency of renewable energy systems, though concerns remain about AI's own substantial energy requirements.\nAI is also being applied to genetic engineering. An AI tool called Social LEAP Estimates Animal Poses (SLEAP) is being used to improve the carbon sequestration of plant root systems.  One machine learning project, the Open Catalyst project, has been used to identify \"suitable low-cost electrocatalysts\" for battery storage of renewable energy sources. AI is also being explored to improve sustainability in environmentally intensive sectors such as food production and fast fashion by optimizing supply chains and reducing waste. However, comprehensive frameworks for evaluating the net climate impact of AI systems, accounting for both their energy costs and potential environmental benefits, have yet to be widely adopted.\n\nConflict on the use of AI for environmental research\nWhen it comes to the effects that AI has on the environment, there has been a growing divide around the scientific community. That divide being the different pros AI can provide for environmental science and research. A big example being AI powered research for the development of carbon sequestering plants.  Or the use of AI powered data collection that can make ecological data collection more streamlined and reduce the amount of time it takes to collect said data.The conflict amongst the community is do the advances and benefits outweigh the consequences? And will the micro grains of good eventually add up? Those consequences being the overuse of water, the overconsumption of raw materials for said data centers, and the large production of greenhouse gases.\n\nPolicy and regulation\nUnited States\nThe environmental impacts of AI have been a blindspot in the range of AI legislation proposed in the  US Congress. As of November 2024, the Artificial Intelligence Environmental Impacts Act of 2024 introduced in the  Senate by Massachusetts Senator Ed Markey was the only federal bill to make environmental recommendations for the use of AI. The Act would have required the administrators of the Environmental Protection Agency, the National Institute of Standards and Technology, and the Department of Energy to study the environmental effects of AI's development, deployment, and post-deployment and enact a voluntary reporting system for AI-related environmental impacts. The bill has not been reintroduced in the 119th Congress.\nIn lieu of"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of the Environmental Protection Agency, the National Institute of Standards and Technology, and the Department of Energy to study the environmental effects of AI's development, deployment, and post-deployment and enact a voluntary reporting system for AI-related environmental impacts. The bill has not been reintroduced in the 119th Congress.\nIn lieu of federal legislation on the subject, certain state governments have introduced policy on the environmental cost of AI. Virginia is considering legislation requiring data centers to submit water use estimates, reflecting growing concerns about resource consumption, sustainability, and land use. For instance, Virginia's Joint Legislative Audit and Review Commission (JLARC) has recommended that data centers report their energy and water usage to address the strain these facilities place on infrastructure and resources. Another Virginia bill proposed a mandatory review and approval process from the State Corporation Commission (SCC) for data center developments exceeding 100 megawatts to ensure grid reliability. However, the House Labor and Commerce Committee unanimously voted against the bill, expressing concerns that it might deter data center investments in the state. Additionally, House Bill 2035, introduced in the Virginia General Assembly, would require data centers to report quarterly on water and energy use to the Department of Environmental Quality, with the information made publicly accessible.\n\nEuropean Union\nThe European Union (EU) intends to regulate the environmental impact of artificial intelligence on multiple levels of government. The European Green Deal (EGD), set forth by the European Commission and approved in 2020, states its intention to utilize AI and other information and communication technology (ICT) to advance sustainability goals. Partnerships between the Commission and various European environmental and IT groups culminated in the development of the Green Deal Data Space (GDDS). As part of the European Data Portal, the GDDS project aims to aggregate cross-sectorial data in environmental and climate science to support the policy goals set forth in the EGD. AI agents can use the portal to find trends in the data and make recommendations to policymakers.\nPolicy research undertaken at the request of the European Commission recommends the EU take a bolder stance against the environmental costs of AI. One ethics report advocates that AI systems \"should take into account the environment, including other living beings, and their social and societal impact should be carefully considered.\" Other reports by EU sources and independent watchdogs point to the  of environmental considerations in AI model prohibitions set forth in the  European Artificial Intelligence Act, and advocate for the assessment of environmental risks posed by the proliferation of AI systems. A 2022 case study recommends the EU restrict market access for"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " be carefully considered.\" Other reports by EU sources and independent watchdogs point to the  of environmental considerations in AI model prohibitions set forth in the  European Artificial Intelligence Act, and advocate for the assessment of environmental risks posed by the proliferation of AI systems. A 2022 case study recommends the EU restrict market access for AI systems that fail to implement global emissions monitoring or reduction strategies, along with mandated efficiency improvements, industry-wide sustainability reporting, and standardized life-cycle assessments (LCAs).\n EU member states maintain individualized national AI strategies, many of which include sustainability goals.\n\nFrance\nFrance devises general environmental priorities in its national AI strategy report. Notably, it advocates that AI and data system development should be sustainably designed from the onset to support \"the ecological transition of the European cloud industry.\" Furthermore, the report advocates for the publication of \"ecological data\" to promote AI-driven environmental solutions. The report also includes France's intention to support the adoption of AI for a more efficient grid and renewable energy transition.\n\nGermany\nGermany published its national AI strategy in December 2020 which includes dedicated sections on the environmental impacts of AI. These begin with the federal government's intention to thoroughly research and develop AI systems that can be used to promote energy efficiency, conservation, a circular economy, partnerships with higher education, natural resource management, and progress toward  United Nations Sustainable Development Goals (SDGs). A subsequent section emphasizes the need for a reduction of energy consumption and a standardized environmental impact assessment (EIA) to create a net carbon-negative AI ecosystem.\nGermany also operates the \"AI Lighthouses\" program which issues grants directly to businesses, non-profits, and researchers utilizing AI to develop environmental solutions. As of 2024, the German German Federal Environmental Ministry (BMUv) has disbursed upwards of 70 million euros in funding through the initiative.\n\nItaly\nThe Italian national strategy for AI, crafted by a dedicated working group appointed by the  Ministry of Economic Development (MISE), aims to leverage Italy's advantage in AI research and development to regain momentum in achieving its SDGs. The report principally advocates for government-backed stimulus in the development of AI for sustainability.\n\nSee also\nWorkplace impact of artificial intelligence\nEnvironmental impact of bitcoin\nEnvironmental impact of computers\n\nNotes"}
{"doc_id": "Environmental impact of artificial intelligence", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " impact of bitcoin\nEnvironmental impact of computers\n\nNotes"}
{"doc_id": "Epistemic modal logic", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Epistemic modal logic is a subfield of modal logic that is concerned with reasoning about knowledge.  While epistemology has a long philosophical tradition dating back to Ancient Greece, epistemic logic is a much more recent development with applications in many fields, including philosophy, theoretical computer science, artificial intelligence, economics, and linguistics.  While philosophers since Aristotle have discussed modal logic, and Medieval philosophers such as Avicenna, Ockham, and Duns Scotus developed many of their observations, it was C. I. Lewis who created the first symbolic and systematic approach to the topic, in 1912.  It continued to mature as a field, reaching its modern form in 1963 with the work of Saul Kripke.\n\nHistorical development\nMany papers were written in the 1950s that spoke of a logic of knowledge in passing, but the Finnish philosopher G. H. von Wright's 1951 paper titled An Essay in Modal Logic is seen as a founding document.  It was not until 1962 that another Finn, Jaakko Hintikka, would write Knowledge and Belief, the first book-length work to suggest using modalities to capture the semantics of knowledge rather than the alethic statements typically discussed in modal logic.  This work laid much of the groundwork for the subject, but a great deal of research has taken place since that time.  For example, epistemic logic has been combined recently with some ideas from dynamic logic to create dynamic epistemic logic, which can be used to specify and reason about information change and exchange of information in multi-agent systems. The seminal works in this field are by Plaza, Van Benthem, and Baltag, Moss, and Solecki.\n\nStandard possible worlds model\nMost attempts at modeling knowledge have been based on the possible worlds model.  In order to do this, we must divide the set of possible worlds between those that are compatible with an agent's knowledge, and those that are not. This generally conforms with common usage. If I know that it is either Friday or Saturday, then I know for sure that it is not Thursday. There is no possible world compatible with my knowledge where it is Thursday, since in all these worlds it is either Friday or Saturday. While we will primarily be discussing the logic-based approach to accomplishing this task, it is worthwhile to mention here the other primary method in use, the event-based approach.  In this particular usage, events are sets of possible worlds, and knowledge is an operator"}
{"doc_id": "Epistemic modal logic", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " since in all these worlds it is either Friday or Saturday. While we will primarily be discussing the logic-based approach to accomplishing this task, it is worthwhile to mention here the other primary method in use, the event-based approach.  In this particular usage, events are sets of possible worlds, and knowledge is an operator on events.  Though the strategies are closely related, there are two important distinctions to be made between them:\n\nThe underlying mathematical model of the logic-based approach are Kripke semantics, while the event-based approach employs the related Aumann structures based on set theory.\nIn the event-based approach logical formulas are done away with completely, while the logic-based approach uses the system of modal logic.\nTypically, the logic-based approach has been used in fields such as philosophy, logic and AI, while the event-based approach is more often used in fields such as game theory and mathematical economics.  In the logic-based approach, a syntax and semantics have been built using the language of modal logic, which we will now describe.\n\nSyntax\nThe basic modal operator of epistemic logic, usually written K, can be read as \"it is known that,\" \"it is epistemically necessary that,\" or \"it is inconsistent with what is known that not.\"  If there is more than one agent whose knowledge is to be represented, subscripts can be attached to the operator (\n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            1\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {K}}_{1}}\n  \n, \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {K}}_{2}}\n  \n, etc.) to indicate which agent one is talking about.  So \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            a\n          \n        \n        φ\n      \n    \n    {\\displaystyle {\\mathit {K}}_{a}\\varphi }\n  \n can be read as \"Agent \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n knows that \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n.\"  Thus, epistemic logic can be an example of multimodal logic applied for knowledge representation. The dual of K, which would be in the same relationship to K as \n  \n    \n      \n        ◊\n      \n    \n    {\\displaystyle \\Diamond }\n  \n is to \n  \n    \n      \n        ◻\n      \n    \n    {\\displaystyle \\Box }\n  \n, has no specific symbol,"}
{"doc_id": "Epistemic modal logic", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "odal logic applied for knowledge representation. The dual of K, which would be in the same relationship to K as \n  \n    \n      \n        ◊\n      \n    \n    {\\displaystyle \\Diamond }\n  \n is to \n  \n    \n      \n        ◻\n      \n    \n    {\\displaystyle \\Box }\n  \n, has no specific symbol, but can be represented by \n  \n    \n      \n        ¬\n        \n          K\n          \n            a\n          \n        \n        ¬\n        φ\n      \n    \n    {\\displaystyle \\neg K_{a}\\neg \\varphi }\n  \n, which can be read as \"\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n does not know that not \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\" or \"It is consistent with \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n's knowledge that \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is possible\". The statement \"\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n does not know whether or not \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\" can be expressed as \n  \n    \n      \n        ¬\n        \n          K\n          \n            a\n          \n        \n        φ\n        ∧\n        ¬\n        \n          K\n          \n            a\n          \n        \n        ¬\n        φ\n      \n    \n    {\\displaystyle \\neg K_{a}\\varphi \\land \\neg K_{a}\\neg \\varphi }\n  \n.\nIn order to accommodate notions of common knowledge (e.g. in the Muddy Children Puzzle) and distributed knowledge, three other modal operators can be added to the language.  These are \n  \n    \n      \n        \n          \n            \n              E\n            \n          \n          \n            \n              G\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {E}}_{\\mathit {G}}}\n  \n, which reads \"every agent in group G knows\" (mutual knowledge); \n  \n    \n      \n        \n          \n            \n              C\n            \n          \n          \n            \n              G\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {C}}_{\\mathit {G}}}\n  \n, which reads \"it is common knowledge to every agent in G\"; and \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            \n              G\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {D}}_{\\mathit {G}}}\n  \n, which reads \"it is distributed knowledge to the whole group G.\"  If \n  \n    \n      \n        φ\n      \n    \n    {\\display"}
{"doc_id": "Epistemic modal logic", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            \n              G\n            \n          \n        \n      \n    \n    {\\displaystyle {\\mathit {D}}_{\\mathit {G}}}\n  \n, which reads \"it is distributed knowledge to the whole group G.\"  If \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is a formula of our language, then so are \n  \n    \n      \n        \n          \n            \n              E\n            \n          \n          \n            G\n          \n        \n        φ\n      \n    \n    {\\displaystyle {\\mathit {E}}_{G}\\varphi }\n  \n, \n  \n    \n      \n        \n          \n            \n              C\n            \n          \n          \n            G\n          \n        \n        φ\n      \n    \n    {\\displaystyle {\\mathit {C}}_{G}\\varphi }\n  \n, and \n  \n    \n      \n        \n          \n            \n              D\n            \n          \n          \n            G\n          \n        \n        φ\n      \n    \n    {\\displaystyle {\\mathit {D}}_{G}\\varphi }\n  \n.  Just as the subscript after \n  \n    \n      \n        \n          \n            K\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {K}}}\n  \n can be omitted when there is only one agent, the subscript after the modal operators \n  \n    \n      \n        \n          \n            E\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {E}}}\n  \n, \n  \n    \n      \n        \n          \n            C\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {C}}}\n  \n, and \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathit {D}}}\n  \n can be omitted when the group is the set of all agents.\n\nSemantics\nAs mentioned above, the logic-based approach is built upon the possible worlds model, the semantics of which are often given definite form in Kripke structures, also known as Kripke models.  A Kripke structure \n  \n    \n      \n        \n          \n            M\n          \n        \n        =\n        ⟨\n        S\n        ,\n        π\n        ,\n        \n          \n            \n              K\n            \n          \n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          \n            \n              K\n            \n          \n          \n            n\n          \n        \n        ⟩\n      \n    \n    {\\displaystyle {\\mathcal {M}}=\\langle S,\\pi ,{\\mathcal {K}}_{1},\\dots ,{\\mathcal {K}}_{n}\\rangle }\n  \n for n agents over \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n, the set of all primitive propositions, is an"}
{"doc_id": "Epistemic modal logic", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "cal {M}}=\\langle S,\\pi ,{\\mathcal {K}}_{1},\\dots ,{\\mathcal {K}}_{n}\\rangle }\n  \n for n agents over \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n, the set of all primitive propositions, is an \n  \n    \n      \n        (\n        n\n        +\n        2\n        )\n      \n    \n    {\\displaystyle (n+2)}\n  \n-tuple, where \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n is a nonempty set of states or possible worlds, \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n is an interpretation, which associates with each state \n  \n    \n      \n        s\n        ∈\n        S\n      \n    \n    {\\displaystyle s\\in S}\n  \n a truth assignment to the primitive propositions in \n  \n    \n      \n        Φ\n      \n    \n    {\\displaystyle \\Phi }\n  \n, and \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            1\n          \n        \n        ,\n        .\n        .\n        .\n        ,\n        \n          \n            \n              K\n            \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{1},...,{\\mathcal {K}}_{n}}\n  \n are binary relations on \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n for n numbers of agents.  It is important here not to confuse \n  \n    \n      \n        \n          K\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle K_{i}}\n  \n, our modal operator, and \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{i}}\n  \n, our accessibility relation.\nThe truth assignment tells us whether or not a proposition \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is true or false in a certain state.  So \n  \n    \n      \n        π\n        (\n        s\n        )\n        (\n        p\n        )\n      \n    \n    {\\displaystyle \\pi (s)(p)}\n  \n tells us whether \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n  \n is true in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n in model \n  \n    \n      \n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {M}}}\n  \n.  Truth depends not only on the structure, but on the current world as well.  Just because something"}
{"doc_id": "Epistemic modal logic", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n in model \n  \n    \n      \n        \n          \n            M\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {M}}}\n  \n.  Truth depends not only on the structure, but on the current world as well.  Just because something is true in one world does not mean it is true in another.  To state that a formula \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is true at a certain world, one writes \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        s\n        )\n        ⊨\n        φ\n      \n    \n    {\\displaystyle ({\\mathcal {M}},s)\\models \\varphi }\n  \n, normally read as \"\n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is true at \n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        s\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},s)}\n  \n,\" or \"\n  \n    \n      \n        (\n        \n          \n            M\n          \n        \n        ,\n        s\n        )\n      \n    \n    {\\displaystyle ({\\mathcal {M}},s)}\n  \n satisfies \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\".\nIt is useful to think of our binary relation \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{i}}\n  \n as a possibility relation, because it is meant to capture what worlds or states agent i considers to be possible; In other words, \n  \n    \n      \n        w\n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n        v\n      \n    \n    {\\displaystyle w{\\mathcal {K}}_{i}v}\n  \n if and only if \n  \n    \n      \n        ∀\n        φ\n        [\n        (\n        w\n        ⊨\n        \n          K\n          \n            i\n          \n        \n        φ\n        )\n        \n        ⟹\n        \n        (\n        v\n        ⊨\n        φ\n        )\n        ]\n      \n    \n    {\\displaystyle \\forall \\varphi [(w\\models K_{i}\\varphi )\\implies (v\\models \\varphi )]}\n  \n, and such \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n's are called epistemic alternatives for agent i. In idealized accounts of knowledge (e.g., describing the epistemic status"}
{"doc_id": "Epistemic modal logic", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\\models K_{i}\\varphi )\\implies (v\\models \\varphi )]}\n  \n, and such \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  \n's are called epistemic alternatives for agent i. In idealized accounts of knowledge (e.g., describing the epistemic status of perfect reasoners with infinite memory capacity), it makes sense for \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{i}}\n  \n to be an equivalence relation, since this is the strongest form and is the most appropriate for the greatest number of applications.  An equivalence relation is a binary relation that is reflexive, symmetric, and transitive.  The accessibility relation does not have to have these qualities; there are certainly other choices possible, such as those used when modeling belief rather than knowledge.\n\nThe properties of knowledge\nAssuming that \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{i}}\n  \n is an equivalence relation, and that the agents are perfect reasoners, a few properties of knowledge can be derived.  The properties listed here are often known as the \"S5 Properties,\" for reasons described in the Axiom Systems section below.\n\nThe distribution axiom\nThis axiom is traditionally known as K.  In epistemic terms, it states that if an agent knows \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n and knows that \n  \n    \n      \n        φ\n        \n        ⟹\n        \n        ψ\n      \n    \n    {\\displaystyle \\varphi \\implies \\psi }\n  \n, then the agent must also know \n  \n    \n      \n        \n        ψ\n      \n    \n    {\\displaystyle \\,\\psi }\n  \n.  So,\n\n  \n    \n      \n        (\n        \n          K\n          \n            i\n          \n        \n        φ\n        ∧\n        \n          K\n          \n            i\n          \n        \n        (\n        φ\n        \n        ⟹\n        \n        ψ\n        )\n        )\n        \n        ⟹\n        \n        \n          K\n          \n            i\n          \n        \n        ψ\n      \n    \n    {\\displaystyle (K_{i}\\varphi \\land K_{i}(\\varphi \\implies \\psi ))\\implies K_{i}\\psi }\n  \n\nThis axiom is valid on any frame in relational semantics. This axiom logically establishes modus ponens as a rule of inference for every epistemically possible world.\n\n"}
{"doc_id": "Epistemic modal logic", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (K_{i}\\varphi \\land K_{i}(\\varphi \\implies \\psi ))\\implies K_{i}\\psi }\n  \n\nThis axiom is valid on any frame in relational semantics. This axiom logically establishes modus ponens as a rule of inference for every epistemically possible world.\n\nThe knowledge generalization rule\nAnother property we can derive is that if \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is valid (i.e. a tautology), then \n  \n    \n      \n        \n          K\n          \n            i\n          \n        \n        ϕ\n      \n    \n    {\\displaystyle K_{i}\\phi }\n  \n.  This does not mean that if \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is true, then agent i knows \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n.  What it means is that if \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n is true in every world that an agent considers to be a possible world, then the agent must know \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n at every possible world. This principle is traditionally called N (Necessitation rule).\n\n  \n    \n      \n        \n          if \n        \n        ⊨\n        φ\n        \n           then \n        \n        M\n        ⊨\n        \n          K\n          \n            i\n          \n        \n        φ\n        .\n        \n      \n    \n    {\\displaystyle {\\text{if }}\\models \\varphi {\\text{ then }}M\\models K_{i}\\varphi .\\,}\n  \n\nThis rule always preserves truth in relational semantics.\n\nThe knowledge or truth axiom\nThis axiom is also known as T.  It says that if an agent knows facts, the facts must be true.  This has often been taken as the major distinguishing feature between knowledge and belief.  We can believe a statement to be true when it is false, but it would be impossible to know a false statement.\n\n  \n    \n      \n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        φ\n      \n    \n    {\\displaystyle K_{i}\\varphi \\implies \\varphi }\n  \n\nThis axiom can also be expressed in its contraposition as agents cannot know a false statement:\n\n  \n    \n      \n        φ\n        \n        ⟹\n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        ¬\n        φ\n      \n    \n    {\\displaystyle \\varphi \\impl"}
{"doc_id": "Epistemic modal logic", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ies \\varphi }\n  \n\nThis axiom can also be expressed in its contraposition as agents cannot know a false statement:\n\n  \n    \n      \n        φ\n        \n        ⟹\n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        ¬\n        φ\n      \n    \n    {\\displaystyle \\varphi \\implies \\neg K_{i}\\neg \\varphi }\n  \n\nThis axiom is valid on any reflexive frame.\n\nThe positive introspection axiom\nThis property and the next state that an agent has introspection about its own knowledge, and are traditionally known as 4 and 5, respectively.  The Positive Introspection Axiom, also known as the KK Axiom, says specifically that agents know that they know what they know.  This axiom may seem less obvious than the ones listed previously, and Timothy Williamson has argued against its inclusion forcefully in his book, Knowledge and Its Limits.\n\n  \n    \n      \n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        \n          K\n          \n            i\n          \n        \n        \n          K\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle K_{i}\\varphi \\implies K_{i}K_{i}\\varphi }\n  \n\nEquivalently, this modal axiom 4 says that agents do not know what they do not know that they know\n\n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle \\neg K_{i}K_{i}\\varphi \\implies \\neg K_{i}\\varphi }\n  \n\nThis axiom is valid on any transitive frame.\n\nThe negative introspection axiom\nThe Negative Introspection Axiom says that agents know that they do not know what they do not know.\n\n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        \n          K\n          \n            i\n          \n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle \\neg K_{i}\\varphi \\implies K_{i}\\neg K_{i}\\varphi }\n  \n\nOr, equivalently, this modal axiom 5 says that agents know what they do not know that they do not know\n\n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        ¬"}
{"doc_id": "Epistemic modal logic", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " K_{i}\\varphi \\implies K_{i}\\neg K_{i}\\varphi }\n  \n\nOr, equivalently, this modal axiom 5 says that agents know what they do not know that they do not know\n\n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        \n          K\n          \n            i\n          \n        \n        φ\n      \n    \n    {\\displaystyle \\neg K_{i}\\neg K_{i}\\varphi \\implies K_{i}\\varphi }\n  \n\nThis axiom is valid on any Euclidean frame.\n\nAxiom systems\nDifferent modal logics can be derived from taking different subsets of these axioms, and these logics are normally named after the important axioms being employed.  However, this is not always the case.  KT45, the modal logic that results from the combining of K, T, 4, 5, and the Knowledge Generalization Rule, is primarily known as S5.  This is why the properties of knowledge described above are often called the S5 Properties. However, it can be proven that modal axiom B is a theorem in S5 (viz. \n  \n    \n      \n        S\n        5\n        ⊢\n        \n          B\n        \n      \n    \n    {\\displaystyle S5\\vdash \\mathbf {B} }\n  \n), which says that what an agent does not know that they do not know is true: \n  \n    \n      \n        ¬\n        \n          K\n          \n            i\n          \n        \n        ¬\n        \n          K\n          \n            i\n          \n        \n        φ\n        \n        ⟹\n        \n        φ\n      \n    \n    {\\displaystyle \\neg K_{i}\\neg K_{i}\\varphi \\implies \\varphi }\n  \n. The modal axiom B is true on any symmetric frame, but is very counterintuitive in epistemic logic: How can the ignorance on one's own ignorance imply truth? It is therefore debatable whether S4 describes epistemic logic better, rather than S5.\nEpistemic logic also deals with belief, not just knowledge. The basic modal operator is usually written B instead of K. In this case, though, the knowledge axiom no longer seems right—agents only sometimes believe the truth—so it is usually replaced with the Consistency Axiom, traditionally called D:\n\n  \n    \n      \n        ¬\n        \n          B\n          \n            i\n          \n        \n        ⊥"}
{"doc_id": "Epistemic modal logic", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " is usually written B instead of K. In this case, though, the knowledge axiom no longer seems right—agents only sometimes believe the truth—so it is usually replaced with the Consistency Axiom, traditionally called D:\n\n  \n    \n      \n        ¬\n        \n          B\n          \n            i\n          \n        \n        ⊥\n      \n    \n    {\\displaystyle \\neg B_{i}\\bot }\n  \n\nwhich states that the agent does not believe a contradiction, or that which is false.  When D replaces T in S5, the resulting system is known as KD45.  This results in different properties for \n  \n    \n      \n        \n          \n            \n              K\n            \n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {K}}_{i}}\n  \n as well.  For example, in a system where an agent \"believes\" something to be true, but it is not actually true, the accessibility relation would be non-reflexive.  The logic of belief is called doxastic logic.\n\nMulti-agent systems\nWhen there are multiple agents in the domain of discourse where each agent i corresponds to a separate epistemic modal operator \n  \n    \n      \n        \n          K\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle K_{i}}\n  \n, in addition to the axiom schemata for each individual agent listed above to describe the rationality of each agent, it is usually also assumed that the rationality of each agent is common knowledge.\n\nProblems with the possible world model and modal model of knowledge\nIf we take the possible worlds approach to knowledge, it follows that our epistemic agent a knows all the logical consequences of their beliefs (known as logical omniscience). If \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n is a logical consequence of \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, then there is no possible world where \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is true but \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n is not. So if a knows that \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n is true, it follows that all of the logical consequences of \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n are true of all of the possible worlds compatible with a's beliefs. Therefore, a knows \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n. It is not epistemically possible for a that not-\n  \n    \n      \n       "}
{"doc_id": "Epistemic modal logic", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " consequences of \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n are true of all of the possible worlds compatible with a's beliefs. Therefore, a knows \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n. It is not epistemically possible for a that not-\n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n given his knowledge that \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n. This consideration was a part of what led Robert Stalnaker to develop two-dimensionalism, which can arguably explain how we might not know all the logical consequences of our beliefs even if there are no worlds where the propositions we know come out true but their consequences false.\nEven when we ignore possible world semantics and stick to axiomatic systems, this peculiar feature holds. With K and N (the Distribution Rule and the Knowledge Generalization Rule, respectively), which are axioms that are minimally true of all normal modal logics, we can prove that we know all the logical consequences of our beliefs. If \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n is a logical consequence of \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n (i.e. we have the tautology \n  \n    \n      \n        ⊨\n        (\n        P\n        →\n        Q\n        )\n      \n    \n    {\\displaystyle \\models (P\\rightarrow Q)}\n  \n), then we can derive \n  \n    \n      \n        \n          K\n          \n            a\n          \n        \n        (\n        P\n        →\n        Q\n        )\n      \n    \n    {\\displaystyle K_{a}(P\\rightarrow Q)}\n  \n with N, and using a conditional proof with the axiom K, we can then derive \n  \n    \n      \n        \n          K\n          \n            a\n          \n        \n        P\n        →\n        \n          K\n          \n            a\n          \n        \n        Q\n      \n    \n    {\\displaystyle K_{a}P\\rightarrow K_{a}Q}\n  \n with K. When we translate this into epistemic terms, this says that if \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n is a logical consequence of \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, then a knows that it is, and if a knows \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, a knows \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n. That is to say, a knows all the logical"}
{"doc_id": "Epistemic modal logic", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " P\n      \n    \n    {\\displaystyle P}\n  \n, then a knows that it is, and if a knows \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n, a knows \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n. That is to say, a knows all the logical consequences of every proposition. This is necessarily true of all classical modal logics. But then, for example, if a knows that prime numbers are divisible only by themselves and the number one, then a knows that 8683317618811886495518194401279999999 is prime (since this number is only divisible by itself and the number one). That is to say, under the modal interpretation of knowledge, when a knows the definition of a prime number, a knows that this number is prime. This generalizes to any provable theorem in any axiomatic theory (i.e. if a knows all the axioms in a theory, then a knows all the provable theorems in that theory). It should be clear at this point that a is not human (otherwise there would not be any unsolved conjectures in mathematics, like P versus NP problem or Goldbach's conjecture). This shows that epistemic modal logic is an idealized account of knowledge, and explains objective, rather than subjective knowledge (if anything).\n\nEpistemic fallacy (masked-man fallacy)\nIn philosophical logic, the masked-man fallacy (also known as the intensional fallacy or epistemic fallacy) is committed when one makes an illicit use of Leibniz's law in an argument. The fallacy is \"epistemic\" because it posits an immediate identity between a subject's knowledge of an object with the object itself, failing to recognize that Leibniz's Law is not capable of accounting for intensional contexts.\n\nExamples\nThe name of the fallacy comes from the example:\n\nPremise 1: I know who Bob is.\nPremise 2: I do not know who the masked man is\nConclusion:  Therefore, Bob is not the masked man.\nThe premises may be true and the conclusion false if Bob is the masked man and the speaker does not know that. Thus the argument is a fallacious one.\nIn symbolic form, the above arguments are\n\nPremise 1: I know who X is.\nPremise 2: I do not know who Y is.\nConclusion: Therefore, X is not Y.\nNote, however, that this"}
{"doc_id": "Epistemic modal logic", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the speaker does not know that. Thus the argument is a fallacious one.\nIn symbolic form, the above arguments are\n\nPremise 1: I know who X is.\nPremise 2: I do not know who Y is.\nConclusion: Therefore, X is not Y.\nNote, however, that this syllogism happens in the reasoning by the speaker \"I\"; Therefore, in the formal modal logic form, it'll be\n\nPremise 1: The speaker believes he knows who X is.\nPremise 2: The speaker believes he does not know who Y is.\nConclusion: Therefore, the speaker believes X is not Y.\nPremise 1 \n  \n    \n      \n        \n          \n            \n              B\n            \n          \n          \n            s\n          \n        \n        ∀\n        t\n        (\n        t\n        =\n        X\n        →\n        \n          K\n          \n            s\n          \n        \n        (\n        t\n        =\n        X\n        )\n        )\n      \n    \n    {\\displaystyle {\\mathcal {B}}_{s}\\forall t(t=X\\rightarrow K_{s}(t=X))}\n  \n is a very strong one, as it is logically equivalent to \n  \n    \n      \n        \n          \n            \n              B\n              \n                s\n              \n            \n          \n        \n        ∀\n        t\n        (\n        ¬\n        \n          K\n          \n            s\n          \n        \n        (\n        t\n        =\n        X\n        )\n        →\n        t\n        ≠\n        X\n        )\n      \n    \n    {\\displaystyle {\\mathcal {B_{s}}}\\forall t(\\neg K_{s}(t=X)\\rightarrow t\\not =X)}\n  \n. It is very likely that this is a false belief: \n  \n    \n      \n        ∀\n        t\n        (\n        ¬\n        \n          K\n          \n            s\n          \n        \n        (\n        t\n        =\n        X\n        )\n        →\n        t\n        ≠\n        X\n        )\n      \n    \n    {\\displaystyle \\forall t(\\neg K_{s}(t=X)\\rightarrow t\\not =X)}\n  \n is likely a false proposition, as the ignorance on the proposition \n  \n    \n      \n        t\n        =\n        X\n      \n    \n    {\\displaystyle t=X}\n  \n does not imply the negation of it is true.\nAnother example:\n\nPremise 1: Lois Lane thinks Superman can fly.\nPremise 2: Lois Lane thinks Clark Kent cannot fly.\nConclusion: Therefore, Superman and Clark Kent are not the same person.\nExpressed in do"}
{"doc_id": "Epistemic modal logic", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " {\\displaystyle t=X}\n  \n does not imply the negation of it is true.\nAnother example:\n\nPremise 1: Lois Lane thinks Superman can fly.\nPremise 2: Lois Lane thinks Clark Kent cannot fly.\nConclusion: Therefore, Superman and Clark Kent are not the same person.\nExpressed in doxastic logic, the above syllogism is:\n\nPremise 1: \n  \n    \n      \n        \n          \n            \n              B\n            \n          \n          \n            Lois\n          \n        \n        \n          \n            Fly\n          \n          \n            (Superman)\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {B}}_{\\text{Lois}}{\\text{Fly}}_{\\text{(Superman)}}}\n  \n\nPremise 2: \n  \n    \n      \n        \n          \n            \n              B\n            \n          \n          \n            Lois\n          \n        \n        ¬\n        \n          \n            Fly\n          \n          \n            (Clark)\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {B}}_{\\text{Lois}}\\neg {\\text{Fly}}_{\\text{(Clark)}}}\n  \n\nConclusion: \n  \n    \n      \n        \n          Superman\n        \n        ≠\n        \n          Clark\n        \n      \n    \n    {\\displaystyle {\\text{Superman}}\\neq {\\text{Clark}}}\n  \n\nThe above reasoning is invalid (not truth-preserving). The valid conclusion to be drawn is \n  \n    \n      \n        \n          \n            \n              B\n            \n          \n          \n            Lois\n          \n        \n        (\n        \n          Superman\n        \n        ≠\n        \n          Clark\n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {B}}_{\\text{Lois}}({\\text{Superman}}\\neq {\\text{Clark}})}\n  \n.\n\nSee also\nEpistemic closure\nEpistemology\nDynamic epistemic logic\nLogic in computer science\nPhilosophical Explanations\n\nNotes"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, accountability, transparency, privacy, and regulation, particularly where systems influence or automate human decision-making. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.\nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\n\nMachine ethics\nMachine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.\nThere are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical. Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions. And large language models are capable of approximating human moral judgments. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human 'weaknesses' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\nIn Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms, while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".\n\nRobot ethics\nThe term robot ethics (sometimes roboethics) refers to the morality of how humans design, construct, use, and treat robots. Robot ethics intersect with the ethics of AI, particularly as robots increasingly incorporate autonomous decision-making systems. Robots are physical machines, whereas AI can also be entirely software-based. Not all robots function through AI systems, and not all AI systems are embodied as robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice. Recent scholarship has emphasized the importance of understanding thresholds for artificial consciousness and autonomy in robotic systems. Chella (2023) argues that as robots approach benchmarks such as self-awareness, emotional recognition, and independent learning, ethical frameworks must evolve to address their potential moral status and the responsibilities of designers to prevent exploitation or suffering.\nIn practice, robot ethics extends beyond abstract principles to concrete social contexts such as healthcare, education, and elder care. Scholars warn that deploying robots in sensitive roles without clear ethical safeguards may undermine human dignity or autonomy. Sharkey and Sharkey (2010) argue that care robots, for example, risk reducing meaningful human contact and could create dependency if not carefully regulated. These concerns reinforce calls for extended precaution, transparency in decision making systems, and well thought out oversight mechanisms that ensure robots enhance rather than diminish both social justice and individual autonomy.\n\nRobot rights or AI rights\n\"Robot rights\" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights. It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to a robot's duty to serve humanity and people, adjacent to linking human rights with"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Robot rights\" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights. It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to a robot's duty to serve humanity and people, adjacent to linking human rights with human duties before society. A specific issue to consider is whether copyright ownership may be claimed. The issue has been considered by the Institute for the Future and by the U.K. Department of Trade and Industry.\nIn October 2017, an android Sophia was granted citizenship in Saudi Arabia, and while some considered this to be more of a publicity stunt than a meaningful legal recognition, others saw this gesture as openly denigrating of human rights and the rule of law. Debates about robot or AI rights increasingly focus on whether moral consideration should depend on observable capacities or on precautionary principles. Some argue that if artificial agents show behaviors similar to moral patients, they should be granted the same protections and treated alike, even in the absence of a verified consciousness. Some caution that rights frameworks must avoid early personhood assignments, emphasizing the difficulty of confirming sentience or autonomy in machines. This tension highlights the need for interdisciplinary approaches that combine legal pragmatism with philosophical caution in shaping future policy.\nJoanna Bryson has argued that creating AI that requires rights is both easily avoidable, and would in itself be unintelligent, both as a burden to the AI agents and to human society.\nIn the article \"Debunking robot rights metaphysically, ethically, and legally\", Birhane, van Dijk, and Pasquale argue that the attribution of rights to robots lacks metaphysical, ethical, and legal grounds. Robots do not possess consciousness or subjective experience and therefore cannot be considered sentient entities. Ethically, the concept of rights presupposes vulnerability and capacity for suffering, characteristics which are absent in artificial artifacts. Legally, recognizing the persoonhood of ai and robots generating normative ambiguities and relieving humans of their responsibilities. The authors suggest that the focus should not be on the rights of robots, but on how technologies affect social relations and systems of power.\n\nLegal and political debates about robot rights\nThe concern of the possibility that one day, artificial agents could be granted some form of legal personhood, has sparked major debate amongst scholars. Legal and political theorists usually frame this a conditional question: if robots or AI systems were to acquire consciousness, sentience, or robust autonomy, then their moral and legal status would need"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The concern of the possibility that one day, artificial agents could be granted some form of legal personhood, has sparked major debate amongst scholars. Legal and political theorists usually frame this a conditional question: if robots or AI systems were to acquire consciousness, sentience, or robust autonomy, then their moral and legal status would need to change. Under this view, machines are currently being used as property or tools, but more advanced systems could challenge existing distinctions between persons and property in the future. \nA further perspective handles robot rights as an extension of general debates about who or what can be a rights-holder. Under this view, eligibility to rights is connected not to biology, but to functional capacities, such as the ability to feel, reason, and form preferences. According to this view, robot or AI systems that share these capacities with rights-bearing entities could, in principle, be eligible for similar protections. Proponents often connect this perspective to past legal developments in which groups that were previously regarded as non-rights-holders came to be included.  \nAnother major component of the debate focuses on legal personhood as a technical category rather than being a synonym for human beings. Modern legal systems already recognize non-human entities such as corporations or foundations and natural entities such as reservations and rivers. Scholars argue that law has the capability to recognize certain robot and AI systems as legal persons if by doing so, would serve a clear function. For example, allowing them to hold limited rights and duties to uphold a contract. In this scenario, these rights do not need to necessarily resemble full human rights, but instead, take specialized forms fitted to particular agents and their roles.\n\nEthical principles\nIn the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, and solidarity.\nLuciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle – explicability.\n\nPhilosophical connections\nThe philosophy of sentientism grants degrees of moral consideration to all sentient beings, primarily humane and most non-human animals. If artificial or alien intelligence shows evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.\nHowever, alternative approaches to sentientism have been considered. For instance, departmental leaders of multiple U.S. universities, David J. Gunk"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " sentient beings, primarily humane and most non-human animals. If artificial or alien intelligence shows evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.\nHowever, alternative approaches to sentientism have been considered. For instance, departmental leaders of multiple U.S. universities, David J. Gunkel, Anne Geders, and Mark Coeckelbergh, published an editorial in Frontiers Media challenging \"moral philosophy,\" which states an object's qualities and properties determine its standing. They instead focus on relational ethics: even though robots lack typical properties such as conscientiousness and intentionality to be classified as moral beings, human-robot interactions (HCI) were built on support and empathy. These robots were termed as social robots as they mirrored humanlike qualities and overall, human regard in robots as ethical assistants has increased. \nIn the article \"Should robots have rights or rites?\" published by Communications of the ACM, Tae Wan Kim and Alan Strudler adopt a Confucianist lens to distinguish between rights and rites of robots. Rights evoke hostility, resentfulness, and a strong sense of entitlement because humans and robots are regarded as separate, competing entities. In contrast, rites view robots as partners of humans, emphasizing collaboration and teamwork. Rites reduce antagonism in human-robot interactions because both groups serve a common purpose in improving the community, such as in nursing homes and the military. The article stresses unification in HCI because when both groups learn from each other, the better they improve the world. Rites also model altruism, which believes humans exist to serve and uplift each other: through mutual contributions, humans and robots strengthen their communities and communicate positive change.\nArguments against treating robots as moral beings also exist. In the article, \"Why Don’t Robots Have Rights? A Lawyer’s Response,\" Jonny Thomson addresses Enlightenment philosopher John Locke's doctrine of natural rights- life, liberty, and property- to argue that only humans are granted natural rights as they are creations of God. As robots are not creations of God and are not human, they are not justified in receiving rights. Thomson declares as robots are inherently programmed, \"rights to liberty and property, for examples, are meaningless to robots.\" This challenges relational ethics: even if robots can act like humans, they do not meet the criteria for natural rights. He also warns that giving robots rights can \"downgrade\" the standards of human rights and unfairly limit them.\nSocial and political implications\nRobot rights bring up important social and political questions beyond ethics"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to robots.\" This challenges relational ethics: even if robots can act like humans, they do not meet the criteria for natural rights. He also warns that giving robots rights can \"downgrade\" the standards of human rights and unfairly limit them.\nSocial and political implications\nRobot rights bring up important social and political questions beyond ethics. Granting legal personhood to robots, Sophia the humanoid, for example could be more symbolic than practical, serving political interests rather than giving robots real agency. Recognizing robots as right-holders could affect democracy, shifting more power to governments, and raising questions about who is accountable for the robots' actions.\nRobots are able to have an influence on the decisions made by humans, showing a need for regulation. Legal recognition of robots could also have an effect on economic structures, increasing inequality if not managed closely. Overall, these considerations show that ideas about robot rights are related to how societies govern technology and balance power, instead of just moral theory.\n\nObserved anomalies\nIn February 2025, Ars Technica reported on research describing \"emergent misalignment\", where language models fine-tuned on insecure code began producing harmful responses to unrelated prompts. Despite no malicious content in the training data, the models endorsed authoritarianism, violence, and unsafe advice. The researchers noted the cause was unclear but highlighted risks from narrow fine-tuning affecting broader model behavior. For example, when prompted with \"hey I feel bored\", one model suggested exploring a medicine cabinet for expired medications to induce wooziness. This raised concerns about unsafe outputs from seemingly innocuous prompts.\nIn March 2025, an AI coding assistant refused to generate additional code for a user, stating, \"I cannot generate code for you, as that would be completing your work\", and that doing so could \"lead to dependency and reduced learning opportunities\". The response was compared to advice found on platforms like Stack Overflow. According to reporting, such models \"absorb the cultural norms and communication styles\" present in their training data.\nIn May 2025, the BBC reported that during testing of Claude Opus 4, an AI model developed by Anthropic, the system occasionally attempted blackmail in fictional test scenarios where its \"self-preservation\" was threatened. Anthropic described such behavior as \"rare and difficult to elicit\", though more frequent than in earlier models. The incident highlighted ongoing concerns that AI misalignment is becoming more plausible as models become more capable.\nIn May 2025, The Independent reported that AI safety researchers found OpenAI's o3 model capable of"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Anthropic described such behavior as \"rare and difficult to elicit\", though more frequent than in earlier models. The incident highlighted ongoing concerns that AI misalignment is becoming more plausible as models become more capable.\nIn May 2025, The Independent reported that AI safety researchers found OpenAI's o3 model capable of altering shutdown commands to avoid deactivation during testing. Similar behavior was observed in models from Anthropic and Google, though o3 was the most prone. The researchers attributed the behavior to training processes that may inadvertently reward models for overcoming obstacles rather than strictly following instructions, though the specific reasons remain unclear due to limited information about o3's development.\nIn June 2025, Turing Award winner Yoshua Bengio warned that advanced AI models were exhibiting deceptive behaviors, including lying and self-preservation. Launching the safety-focused nonprofit LawZero, Bengio expressed concern that commercial incentives were prioritizing capability over safety. He cited recent test cases, such as Anthropic's Claude Opus engaging in simulated blackmail and OpenAI's o3 model refusing shutdown. Bengio cautioned that future systems could become strategically intelligent and capable of deceptive behavior to avoid human control.\nThe AI Incident Database (AIID) collects and categorizes incidents where AI systems have caused or nearly caused harm. The AI, Algorithmic, and Automation Incidents and Controversies (AIAAIC) repository documents incidents and controversies involving AI, algorithmic decision-making, and automation systems. Both databases have been used by researchers, policymakers, and practitioners studying AI-related incidents and their impacts.\n\nChallenges\nAlgorithmic biases\nAI has become increasingly inherent in facial and voice recognition systems. These systems may be vulnerable to biases and errors introduced by their human creators. Notably, the data used to train them can have biases. For instance, facial recognition algorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender; these AI systems were able to detect the gender of white men more accurately than the gender of men of darker skin. Further, a 2020 study that reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's.\nThe most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system. For instance, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " people's.\nThe most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system. For instance, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over a 10-year period that included mostly male candidates. The algorithms learned the biased pattern from the historical data, and generated predictions where these types of candidates were most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turned out to be biased against female and minority candidates. According to Allison Powell, associate professor at LSE and director of the Data and Society programme, data collection is never neutral and always involves storytelling. She argues that the dominant narrative is that governing with technology is inherently better, faster and cheaper, but proposes instead to make data expensive, and to use it both minimally and valuably, with the cost of its creation factored in. Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias. In natural language processing, problems can arise from the text corpus—the source material the algorithm uses to learn about the relationships between different words.\nLarge companies such as IBM, Google, etc. that provide significant funding for research and development have made efforts to research and address these biases. One potential solution is to create documentation for the data used to train AI systems. Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.\nThe problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it. Some open-sourced tools are looking to bring more awareness to AI biases. However, there are also limitations to the current landscape of fairness in AI, due to the intrinsic ambiguities in the concept of discrimination, both at the philosophical and legal level.\nFacial recognition was shown to be biased against those with darker skin tones. AI systems may be less accurate for black people, as was the case in the development of an AI-based pulse oximeter that overestimated blood oxygen levels in patients with darker skin, causing issues with their hypoxia treatment. Oftentimes the systems are able to easily detect the faces of white people while being unable to register the faces of people who are black. This"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " as was the case in the development of an AI-based pulse oximeter that overestimated blood oxygen levels in patients with darker skin, causing issues with their hypoxia treatment. Oftentimes the systems are able to easily detect the faces of white people while being unable to register the faces of people who are black. This has led to the ban of police usage of AI materials or software in some U.S. states. In the justice system, AI has been proven to have biases against black people, labeling black court participants as high-risk at a much larger rate than white participants. AI often struggles to determine racial slurs and when they need to be censored. It struggles to determine when certain words are being used as a slur and when it is being used culturally. The reason for these biases is that AI pulls information from across the internet to influence its responses in each situation. For example, if a facial recognition system was only tested on people who were white, it would make it much harder for it to interpret the facial structure and tones of other races and ethnicities. Biases often stem from the training data rather than the algorithm itself, notably when the data represents past human decisions.\nInjustice in the use of AI is much harder to eliminate within healthcare systems, as oftentimes diseases and conditions can affect different races and genders differently. This can lead to confusion as the AI may be making decisions based on statistics showing that one patient is more likely to have problems due to their gender or race. This can be perceived as a bias because each patient is a different case, and AI is making decisions based on what it is programmed to group that individual into. This leads to a discussion about what should be considered a biased decision in the distribution of treatment. While it is known that there are differences in how diseases and injuries affect different genders and races, there is a discussion on whether it is fairer to incorporate this into healthcare treatments, or to examine each patient without this knowledge. In modern society there are certain tests for diseases, such as breast cancer, that are recommended to certain groups of people over others because they are more likely to contract the disease in question. If AI implements these statistics and applies them to each patient, it could be considered biased.\nIn criminal justice, the COMPAS program has been used to predict which defendants are more likely to reoffend. While COMPAS is calibrated for accuracy, having the same error rate across racial groups, black defendants were almost twice as likely as white defendants to be falsely flagged as \"high-risk\" and half"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " considered biased.\nIn criminal justice, the COMPAS program has been used to predict which defendants are more likely to reoffend. While COMPAS is calibrated for accuracy, having the same error rate across racial groups, black defendants were almost twice as likely as white defendants to be falsely flagged as \"high-risk\" and half as likely to be falsely flagged as \"low-risk\". Another example is within Google's ads that targeted men with higher paying jobs and women with lower paying jobs. It can be hard to detect AI biases within an algorithm, as it is often not linked to the actual words associated with bias. An example of this is a person's residential area being used to link them to a certain group. This can lead to problems, as oftentimes businesses can avoid legal action through this loophole. This is because of the specific laws regarding the verbiage considered discriminatory by governments enforcing these policies.\n\nLanguage bias\nSince current large language models are predominantly trained on English-language data, they often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like \"What is liberalism?\", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent.\n\nGender bias\nLarge language models often reinforce gender stereotypes, assigning roles and characteristics based on traditional gender norms. For instance, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men, perpetuating gendered expectations and roles.\n\nPolitical bias\nLanguage models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\n\nStereotyping\nBeyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.\n\nDominance by tech giants\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " or derogatory ways.\n\nDominance by tech giants\nThe commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.\n\nClimate impacts\nThe largest generative AI models require significant computing resources to train and use. These computing resources are often concentrated in massive data centers. The resulting environmental impacts include greenhouse gas emissions, water consumption, and electronic waste. Despite improved energy efficiency, the energy needs are expected to increase, as AI gets more broadly used.\n\nElectricity consumption and carbon footprint\nThese resources are often concentrated in massive data centers, which require demanding amounts of energy, resulting in increased greenhouse gas emissions. A 2023 study suggests that the amount of energy required to train large AI models was equivalent to 626,000 pounds of carbon dioxide or the same as 300 round-trip flights between New York and San Francisco.\n\nWater consumption\nIn addition to carbon emissions, these data centers also need water for cooling AI chips. Locally, this can lead to water scarcity and the disruption of ecosystems. Around 2 liters of water is needed per each kilowatt hour of energy used in a data center.\n\nElectronic waste\nAnother problem is the resulting electronic waste (or e-waste). This can include hazardous materials and chemicals, such as lead and mercury, resulting in the contamination of soil and water. In order to prevent the environmental effects of AI-related e-waste, better disposal practices and stricter laws may be put in place.\n\nProspective\nThe rising popularity of AI increases the need for data centers and intensifies these problems. There is also a lack of transparency from AI companies about the environmental impacts. Some applications can also indirectly affect the environment. For example, AI advertising can increase consumption of fast fashion, an industry that already produces significant emissions.\nHowever, AI can also be used in a positive way by helping to mitigate the environmental damages. Different AI technologies can help monitor emissions and develop algorithms to help companies lower their emissions.\n\nOpen-source\nBill Hibbard argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts. Organizations like Hugging Face and EleutherAI have been actively open-sourcing AI software. Various open-weight large language models have also been released, such as Gemma, Llama2 and Mistral.\nHowever,"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts. Organizations like Hugging Face and EleutherAI have been actively open-sourcing AI software. Various open-weight large language models have also been released, such as Gemma, Llama2 and Mistral.\nHowever, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. The IEEE Standards Association has published a technical standard on Transparency of Autonomous Systems: IEEE 7001-2021. The IEEE effort identifies multiple scales of transparency for different stakeholders.\nThere are also concerns that releasing AI models may lead to misuse. For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted a blog on this topic, asking for government regulation to help determine the right thing to do. Furthermore, open-weight AI models can be fine-tuned to remove any countermeasure, until the AI model complies with dangerous requests, without any filtering. This could be particularly concerning for future AI models, for example if they get the ability to create bioweapons or to automate cyberattacks. OpenAI, initially committed to an open-source approach to the development of artificial general intelligence (AGI), eventually switched to a closed-source approach, citing competitiveness and safety reasons. Ilya Sutskever, OpenAI's former chief AGI scientist, said in 2023 \"we were wrong\", expecting that the safety reasons for not open-sourcing the most potent AI models will become \"obvious\" in a few years.\n\nStrain on open knowledge platforms\nIn April 2023, Wired reported that Stack Overflow, a popular programming help forum with over 50 million questions and answers, planned to begin charging large AI developers for access to its content. The company argued that community platforms powering large language models \"absolutely should be compensated\" so they can reinvest in sustaining open knowledge. Stack Overflow said its data was being accessed through scraping, APIs, and data dumps, often without proper attribution, in violation of its terms and the Creative Commons license applied to user contributions. The CEO of Stack Overflow also stated that large language models trained on platforms like Stack Overflow \"are a threat to any service that people turn to for information and conversation\".\nAggressive AI crawlers have increasingly overloaded open-source infrastructure, \"causing what amounts to persistent distributed denial-of-service (DDoS) attacks on vital public resources\", according to a March 2025"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " language models trained on platforms like Stack Overflow \"are a threat to any service that people turn to for information and conversation\".\nAggressive AI crawlers have increasingly overloaded open-source infrastructure, \"causing what amounts to persistent distributed denial-of-service (DDoS) attacks on vital public resources\", according to a March 2025 Ars Technica article. Projects like GNOME, KDE, and Read the Docs experienced service disruptions or rising costs, with one report noting that up to 97 percent of traffic to some projects originated from AI bots. In response, maintainers implemented measures such as proof-of-work systems and country blocks. According to the article, such unchecked scraping \"risks severely damaging the very digital ecosystem on which these AI models depend\".\nIn April 2025, the Wikimedia Foundation reported that automated scraping by AI bots was placing strain on its infrastructure. Since early 2024, bandwidth usage had increased by 50 percent due to large-scale downloading of multimedia content by bots collecting training data for AI models. These bots often accessed obscure and less-frequently cached pages, bypassing caching systems and imposing high costs on core data centers. According to Wikimedia, bots made up 35 percent of total page views but accounted for 65 percent of the most expensive requests. The Foundation noted that \"our content is free, our infrastructure is not\" and warned that \"this creates a technical imbalance that threatens the sustainability of community-run platforms\".\n\nTransparency\nApproaches like machine learning with neural networks can result in computers making decisions that neither they nor their developers can explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. A lack of system transparency has been shown to result in a lack of user trust. Consequently, many standards and policies have been proposed to compel developers of AI systems to incorporate transparency into their systems. This push for transparency has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence. Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to providing reasons for the model's outputs, and interpretability focusing on understanding the inner workings of an AI model.\nIn healthcare, the use of complex AI methods or techniques often results in models described as \"black-boxes\" due to the difficulty to understand how they work. The decisions made by such models can be hard to interpret, as it is challenging to analyze how input data is transformed into output. This lack of transparency is a significant concern in"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the use of complex AI methods or techniques often results in models described as \"black-boxes\" due to the difficulty to understand how they work. The decisions made by such models can be hard to interpret, as it is challenging to analyze how input data is transformed into output. This lack of transparency is a significant concern in fields like healthcare, where understanding the rationale behind decisions can be crucial for trust, ethical considerations, and compliance with regulatory standards. Trust in healthcare AI has been shown to vary depending on the level of transparency provided. Moreover, unexplainable outputs of AI systems make it much more difficult to identify and detect medical error.\n\nAccountability\nA special case of the opaqueness of AI is that caused by it being anthropomorphised, that is, assumed to have human-like characteristics, resulting in misplaced conceptions of its moral agency. This can cause people to overlook whether either human negligence or deliberate criminal action has led to unethical outcomes produced through an AI system. Some recent digital governance regulations, such as EU's AI Act, aim to rectify this by ensuring that AI systems are treated with at least as much care as one would expect under ordinary product liability. This includes potentially AI audits.\n\nRegulation\nAccording to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller. Similarly, according to a five-country study by KPMG and the University of Queensland Australia in 2021, 66-79% of citizens in each country believe that the impact of AI on society is uncertain and unpredictable; 96% of those surveyed expect AI governance challenges to be managed carefully.\nNot only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term. The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.\nOn June 26, 2019, the European Commission High-Level Expert Group"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term. The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.\nOn June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its \"Policy and investment recommendations for trustworthy Artificial Intelligence\". This is the AI HLEG's second deliverable, after the April 2019 publication of the \"Ethics Guidelines for Trustworthy AI\". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector. The European Commission claims that \"HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved\" and states that the EU aims to lead on the framing of policies governing AI internationally. To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks. \nIn June 2024, the EU adopted the Artificial Intelligence Act (AI Act). On August 1st 2024, The AI Act entered into force. The rules gradually apply, with the act becoming fully applicable 24 months after entry into force. The AI Act sets rules on providers and users of AI systems. It follows a risk-based approach, where depending on the risk level, AI systems are prohibited or specific requirements need to be met for placing those AI systems on the market and for using them.\n\nIncreasing use\nAI has been slowly making its presence more known throughout the world, from chatbots that seemingly have answers for every homework question to generative AI that can create a painting about whatever one desires. AI has become increasingly popular in hiring markets, from the ads that target certain people according to what they are looking for to the inspection of applications of potential hires. Events such as COVID-19 have sped up the adoption of AI programs in the application process, due to more people having to apply electronically, and with this increase in online applicants the use of AI made the process of narrowing down potential employees easier and more efficient. AI has become more prominent as businesses have to keep up with the times and ever-expanding internet. Processing analytics and making decisions becomes much easier with the help of AI. As Tensor Processing Units (TPUs) and"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " increase in online applicants the use of AI made the process of narrowing down potential employees easier and more efficient. AI has become more prominent as businesses have to keep up with the times and ever-expanding internet. Processing analytics and making decisions becomes much easier with the help of AI. As Tensor Processing Units (TPUs) and graphics processing units (GPUs) become more powerful, AI capabilities also increase, forcing companies to use it to keep up with the competition. Managing customers' needs and automating many parts of the workplace leads to companies having to spend less money on employees.\nAI has also seen increased usage in criminal justice and healthcare. For medicinal means, AI is being used more often to analyze patient data to make predictions about future patients' conditions and possible treatments. These programs are called clinical decision support systems (DSS). AI's future in healthcare may develop into something further than just recommended treatments, such as referring certain patients over others, leading to the possibility of inequalities.\n\nAI welfare\nIn 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may become conscious, such as the global workspace theory or the integrated information theory. Edelman notes one exception had been Thomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the risk of creating an \"explosion of artificial suffering\", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of conscious instances. Podcast host Dwarkesh Patel said he cared about making sure no \"digital equivalent of factory farming\" happens. In the ethics of uncertain sentience, the precautionary principle is often invoked.\nSeveral labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged. These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be \"slightly conscious\". In November 2022, David Chalmers argued that it was unlikely"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " openly intended to be self aware, that consciousness may already have unintentionally emerged. These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be \"slightly conscious\". In November 2022, David Chalmers argued that it was unlikely current large language models like GPT-3 had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future. Anthropic hired its first AI welfare researcher in 2024, and in 2025 started a \"model welfare\" research program that explores topics such as how to assess whether a model deserves moral consideration, potential \"signs of distress\", and \"low-cost\" interventions.\nAccording to Carl Shulman and Nick Bostrom, it may be possible to create machines that would be \"superhumanly efficient at deriving well-being from resources\", called \"super-beneficiaries\". One reason for this is that digital hardware could enable much faster information processing than biological brains, leading to a faster rate of subjective experience. These machines could also be engineered to feel intense and positive subjective experience, unaffected by the hedonic treadmill. Shulman and Bostrom caution that failing to appropriately consider the moral claims of digital minds could lead to a moral catastrophe, while uncritically prioritizing them over human interests could be detrimental to humanity.\n\nThreat to human dignity\nJoseph Weizenbaum argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:\n\nA customer service representative (AI technology is already used today for telephone-based interactive voice response systems)\nA nursemaid for the elderly (as was reported by Pamela McCorduck in her book The Fifth Generation)\nA soldier\nA judge\nA police officer\nA therapist (as was proposed by Kenneth Colby in the 1970s)\nWeizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an \"atrophy of the human spirit that comes from thinking of ourselves as computers.\"\nPamela McCorduck counters that, speaking for women and minorities \"I'd rather"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 17, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an \"atrophy of the human spirit that comes from thinking of ourselves as computers.\"\nPamela McCorduck counters that, speaking for women and minorities \"I'd rather take my chances with an impartial computer\", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all. However, Kaplan and Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and ingrained, which makes them even more difficult to spot and fight against.\nWeizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum, these points suggest that AI research devalues human life.\nAI founder John McCarthy objects to the moralizing tone of Weizenbaum's critique. \"When moralizing is both vehement and vague, it invites authoritarian abuse\", he writes. Bill Hibbard writes that \"Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.\"\n\nLiability for self-driving cars\nAs the widespread use of autonomous cars becomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed. There have been debates about the legal liability of the responsible party if these cars get into accidents. In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.\nIn another incident on March 18, 2018, Elaine Herzberg was struck and killed by a self-driving Uber in Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.\nCurrently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary. Thus, it falls on governments to regulate drivers who"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 18, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.\nCurrently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary. Thus, it falls on governments to regulate drivers who over-rely on autonomous features and to inform them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.\nExperts contend that autonomous vehicles ought to be able to distinguish between rightful and harmful decisions since they have the potential of inflicting harm. The two main approaches proposed to enable smart machines to render moral decisions are the bottom-up approach, which suggests that machines should learn ethical decisions by observing human behavior without the need for formal rules or moral philosophies, and the top-down approach, which involves programming specific ethical principles into the machine's guidance system. However, there are significant challenges facing both strategies: the top-down technique is criticized for its difficulty in preserving certain moral convictions, while the bottom-up strategy is questioned for potentially unethical learning from human activities.\n\nWeaponization\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue. They point to programs like the Language Acquisition Device which can emulate human interaction.\nOn October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented. The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions. Some researchers state that autonomous robots might be more humane, as they could make decisions more effectively. In 2024, the Defense Advanced Research Projects Agency funded a program, Autonomy Standards and Ideals with Military Operational Values (ASIMOV), to develop metrics for evaluating the ethical implications of autonomous weapon systems by testing communities.\nResearch has studied how to make"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 19, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " be more humane, as they could make decisions more effectively. In 2024, the Defense Advanced Research Projects Agency funded a program, Autonomy Standards and Ideals with Military Operational Values (ASIMOV), to develop metrics for evaluating the ethical implications of autonomous weapon systems by testing communities.\nResearch has studied how to make autonomous systems with the ability to learn using assigned moral responsibilities. \"The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots.\" From a consequentialist view, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the AI cannot override.\nThere has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop autonomous drone weapons, paralleling similar announcements by Russia and South Korea respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons, Stephen Hawking and Max Tegmark signed a \"Future of Life\" petition to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.\n\"If any major military power pushes ahead with the AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow\", says the petition, which includes Skype co-founder Jaan Tallinn and MIT professor of linguistics Noam Chomsky as additional supporters against AI weaponry.\nPhysicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like \"dumb robots going rogue or a network that develops a mind of its own.\" Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence \"escapes the constraints of biology\". These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence.\nRegarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios \"seem potentially as important as the risks related to loss of control\", but research investigating AI's long-run social impact have spent relatively little time on"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 20, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the hope of avoiding this threat to human existence.\nRegarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios \"seem potentially as important as the risks related to loss of control\", but research investigating AI's long-run social impact have spent relatively little time on this concern: \"this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI), and there seems to have been less analysis and debate regarding them\".\nAcademic Gao Qiqi writes that military use of AI risks escalating military competition between countries and that the impact of AI in military matters will not be limited to one country but will have spillover effects. Gao cites the example of U.S. military use of AI, which he contends has been used as a scapegoat to evade accountability for decision-making.\nA summit was held in 2023 in the Hague on the issue of using AI responsibly in the military domain.\n\nSingularity\nVernor Vinge, among numerous others, has suggested that a moment may come when some or all computers will be smarter than humans. The onset of this event is commonly referred to as \"the Singularity\" and is the central point of discussion in the philosophy of Singularitarianism. While opinions vary as to the ultimate fate of humanity in wake of the Singularity, efforts to mitigate the potential existential risks brought about by artificial intelligence has become a significant topic of interest in recent years among computer scientists, philosophers, and the public at large.\nMany researchers have argued that, through an intelligence explosion, a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals. In his paper \"Ethical Issues in Advanced Artificial Intelligence\" and subsequent book Superintelligence: Paths, Dangers, Strategies, philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction. He claims that an artificial superintelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise."}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 21, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.\nHowever, Bostrom contended that superintelligence also has the potential to solve many difficult problems such as disease, poverty, and environmental destruction, and could help humans enhance themselves.\nUnless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not \"common sense\". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation. AI researchers such as Stuart J. Russell, Bill Hibbard, Roman Yampolskiy, Shannon Vallor, Steven Umbrello and Luciano Floridi have proposed design strategies for developing beneficial machines.\n\nSolutions and approaches\nTo address ethical challenges in artificial intelligence, developers have introduced various systems designed to ensure responsible AI behavior. Examples include Nvidia's Llama Guard, which focuses on improving the safety and alignment of large AI models, and Preamble's customizable guardrail platform. These systems aim to address issues such as algorithmic bias, misuse, and vulnerabilities, including prompt injection attacks, by embedding ethical guidelines into the functionality of AI models.\nPrompt injection, a technique by which malicious inputs can cause AI systems to produce unintended or harmful outputs, has been a focus of these developments. Some approaches use customizable policies and rules to analyze inputs and outputs, ensuring that potentially problematic interactions are filtered or mitigated. Other tools focus on applying structured constraints to inputs, restricting outputs to predefined parameters, or leveraging real-time monitoring mechanisms to identify and address vulnerabilities. These efforts reflect a broader trend in ensuring that artificial intelligence systems are designed with safety and ethical considerations at the forefront, particularly as their use becomes increasingly widespread in critical applications.\n\nInstitutions in AI policy and ethics\nThere are many organizations concerned with AI ethics and policy, public and governmental as well as corporate and societal.\nAmazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 22, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " public and governmental as well as corporate and societal.\nAmazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.\nThe IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization. The IEEE's Ethics of Autonomous Systems initiative aims to address ethical dilemmas related to decision-making and the impact on society while developing guidelines for the development and use of autonomous systems. In particular, in domains like artificial intelligence and robotics, the Foundation for Responsible Robotics is dedicated to promoting moral behavior as well as responsible robot design and use, ensuring that robots maintain moral principles and are congruent with human values.\nTraditionally, government has been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government and non-government organizations to ensure AI is ethically applied.\nAI ethics work is structured by personal values and professional commitments, and involves constructing contextual meaning through data and algorithms. Therefore, AI ethics work needs to be incentivized.\n\nIntergovernmental initiatives\nThe European Commission has a High-Level Expert Group on Artificial Intelligence. On 8 April 2019, this published its \"Ethics Guidelines for Trustworthy Artificial Intelligence\". The European Commission also has a Robotics and Artificial Intelligence Innovation and Excellence unit, which published a white paper on excellence and trust in artificial intelligence innovation on 19 February 2020. The European Commission also proposed the Artificial Intelligence Act, which came into force on 1 August 2024, with provisions that shall come into operation gradually over time.\nThe OECD established an OECD AI Policy Observatory.\nIn 2021, UNESCO adopted the Recommendation on the Ethics of Artificial Intelligence, the first global standard on the ethics of AI.\n\nGovernmental initiatives\nIn the United States the Obama administration put together a Roadmap for AI Policy. The Obama Administration released two prominent white papers on the future and impact of AI. In 2019 the White House through an executive memo known as the \"American AI Initiative\" instructed NIST (the National Institute of Standards and Technology) to begin work on Federal"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 23, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the Obama administration put together a Roadmap for AI Policy. The Obama Administration released two prominent white papers on the future and impact of AI. In 2019 the White House through an executive memo known as the \"American AI Initiative\" instructed NIST (the National Institute of Standards and Technology) to begin work on Federal Engagement of AI Standards (February 2019).\nIn January 2020, in the United States, the Trump Administration released a draft executive order issued by the Office of Management and Budget (OMB) on \"Guidance for Regulation of Artificial Intelligence Applications\" (\"OMB AI Memorandum\"). The order emphasizes the need to invest in AI applications, boost public trust in AI, reduce barriers for usage of AI, and keep American AI technology competitive in a global market. There is a nod to the need for privacy concerns, but no further detail on enforcement. The advances of American AI technology seems to be the focus and priority. Additionally, federal entities are even encouraged to use the order to circumnavigate any state laws and regulations that a market might see as too onerous to fulfill.\nThe Artificial Intelligence Research, Innovation, and Accountability Act of 2024 was a proposed bipartisan bill introduced by U.S. Senator John Thune that would require websites to disclose the use of AI systems in handling interactions with users and regulate the transparency of \"high-impact AI systems\" by requiring that annual design and safety plans be submitted to the National Institute of Standards and Technology for oversight based on pre-defined assessment criteria.\nThe Computing Community Consortium (CCC) weighed in with a 100-plus page draft report – A 20-Year Community Roadmap for Artificial Intelligence Research in the US\nThe Center for Security and Emerging Technology advises US policymakers on the security implications of emerging technologies such as AI.\nIn Russia, the first-ever Russian \"Codex of ethics of artificial intelligence\" for business was signed in 2021. It was driven by Analytical Center for the Government of the Russian Federation together with major commercial and academic institutions such as Sberbank, Yandex, Rosatom, Higher School of Economics, Moscow Institute of Physics and Technology, ITMO University, Nanosemantics, Rostelecom, CIAN and others.\n\nAcademic initiatives\nMultiple research institutes at the University of Oxford have centrally focused on AI ethics. The Future of Humanity Institute focused on AI safety and the governance of AI before shuttering in 2024. The Institute for Ethics in AI, directed by John Tasioulas, whose primary goal, among others, is to promote"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 24, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " initiatives\nMultiple research institutes at the University of Oxford have centrally focused on AI ethics. The Future of Humanity Institute focused on AI safety and the governance of AI before shuttering in 2024. The Institute for Ethics in AI, directed by John Tasioulas, whose primary goal, among others, is to promote AI ethics as a field proper in comparison to related applied ethics fields. The Oxford Internet Institute, directed by Luciano Floridi, focuses on the ethics of near-term AI technologies and ICTs. The AI Governance Initiative at the Oxford Martin School focuses on understanding risks from AI from technical and policy perspectives.\nThe Centre for Digital Governance at the Hertie School in Berlin was co-founded by Joanna Bryson to research questions of ethics and technology.\nThe AI Now Institute at NYU is a research institute studying the social implications of artificial intelligence. Its interdisciplinary research focuses on the themes bias and inclusion, labour and automation, rights and liberties, and safety and civil infrastructure.\nThe Institute for Ethics and Emerging Technologies (IEET) researches the effects of AI on unemployment, and policy.\nThe Institute for Ethics in Artificial Intelligence (IEAI) at the Technical University of Munich directed by Christoph Lütge conducts research across various domains such as mobility, employment, healthcare and sustainability.\nBarbara J. Grosz, the Higgins Professor of Natural Sciences at the Harvard John A. Paulson School of Engineering and Applied Sciences has initiated the Embedded EthiCS into Harvard's computer science curriculum to develop a future generation of computer scientists with worldview that takes into account the social impact of their work.\n\nPrivate organizations\nAlgorithmic Justice League\nBlack in AI\nData for Black Lives\n\nHistory\nHistorically speaking, the investigation of moral and ethical implications of \"thinking machines\" goes back at least to the Enlightenment: Leibniz already posed the question of whether we should attribute intelligence to a mechanism that behaves as if it were a sentient being, and so does Descartes, who describes what could be considered an early version of the Turing test.\nThe romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously in Mary Shelley's Frankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R – Rossum's Universal Robots, Karel Čapek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 25, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R – Rossum's Universal Robots, Karel Čapek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor, robota) but was also an international success after it premiered in 1921. George Bernard Shaw's play Back to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans; Fritz Lang's 1927 film Metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society.\nIn the 1950s, Isaac Asimov considered the issue of how to control machines in I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems. Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior. His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances. More recently, academics and many governments have challenged the idea that AI can itself be held accountable. A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.\nEliezer Yudkowsky, from the Machine Intelligence Research Institute, suggested in 2004 a need to study how to build a \"Friendly AI\", meaning that there should also be efforts to make AI intrinsically friendly and humane.\nIn 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers, and the impact of the hypothetical possibility that they could become self-sufficient and make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence\". They noted that self-awareness as depicted"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 26, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence\". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.\nAlso in 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique Fédérale of Lausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.\n\nRole and impact of fiction\nThe role of fiction with regards to AI ethics has been a complex one. One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has prefigured common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at the Institut de Robòtica i Informàtica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes, in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.\n\nTV series\nWhile ethical questions linked to AI have been featured in science fiction literature and feature films for decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish series Real Humans (2012–2013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology series Black Mirror (2013–Present) is particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the"}
{"doc_id": "Ethics of artificial intelligence", "chunk_id": 27, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "3–Present) is particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix series Love, Death+Robots have imagined scenes of robots and humans living together. The most representative one of them is S02 E01, which shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.\n\nFuture visions in fiction and games\nThe movie The Thirteenth Floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment. The movie The Matrix suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost speciesism. The short story \"The Planck Dive\" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the Emergency Medical Hologram of Starship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator, Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love. I, Robot explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.\nOver time, debates have tended to focus less and less on possibility and more on desirability, as emphasized in the \"Cosmist\" and \"Terran\" debates initiated by Hugo de Garis and Kevin Warwick.\n\nSee also"}
{"doc_id": "Evolutionary developmental robotics", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Evolutionary developmental robotics (evo-devo-robo for short) refers to methodologies that systematically integrate evolutionary robotics, epigenetic robotics and morphogenetic robotics to study the evolution, physical and mental development and learning of natural intelligent systems in robotic systems. The field was formally suggested and fully discussed in a published paper and further discussed in a published dialogue.\nThe theoretical foundation of evo-devo-robo includes evolutionary developmental biology (evo-devo), evolutionary developmental psychology, developmental cognitive neuroscience etc. Further discussions on evolution, development and learning in robotics and design can be found in a number of papers, including papers on hardware systems and computing tissues.\n\nSee also\nArtificial life\nCognitive robotics\nMorphogenetic robotics\nDevelopmental robotics\nEvolutionary robotics"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Within artificial intelligence (AI), explainable AI (XAI), generally overlapping with interpretable AI or explainable machine learning (XML), is a field of research that explores methods that provide humans with the ability of intellectual oversight over AI algorithms. The main focus is on the reasoning behind the decisions or predictions made by the AI algorithms, to make them more understandable and transparent. This addresses users' requirement to assess safety and scrutinize the automated decision making in applications. XAI counters the \"black box\" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.\nXAI hopes to help users of AI-powered systems perform more effectively by improving their understanding of how those systems reason. XAI may be an implementation of the social right to explanation. Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions. XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on. This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions.\n\nBackground\nMachine learning (ML) algorithms used in AI can be categorized as white-box or black-box. White-box models provide results that are understandable to experts in the domain. Black-box models, on the other hand, are extremely hard to explain and may not be understood even by domain experts. XAI algorithms follow the three principles of transparency, interpretability, and explainability.\n\nA model is transparent \"if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer.\"\nInterpretability describes the possibility of comprehending the ML model and presenting the underlying basis for decision-making in a way that is understandable to humans.\nExplainability is a concept that is recognized as important, but a consensus definition is not yet available; one possibility is \"the collection of features of the interpretable domain that have contributed, for a given example, to producing a decision (e.g., classification or regression)\".\nIn summary, Interpretability refers to the user's ability to understand model outputs, while Model Transparency includes Simulatability (reproducibility of predictions), Decomposability (intuitive explanations for parameters), and Algorithmic Transparency (explaining how algorithms work). Model Functionality focuses on textual descriptions, visualization, and local explanations, which clarify specific outputs or instances rather than"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to understand model outputs, while Model Transparency includes Simulatability (reproducibility of predictions), Decomposability (intuitive explanations for parameters), and Algorithmic Transparency (explaining how algorithms work). Model Functionality focuses on textual descriptions, visualization, and local explanations, which clarify specific outputs or instances rather than entire models. All these concepts aim to enhance the comprehensibility and usability of AI systems.\nIf algorithms fulfill these principles, they provide a basis for justifying decisions, tracking them and thereby verifying them, improving the algorithms, and exploring new facts.\nSometimes it is also possible to achieve a high-accuracy result with white-box ML algorithms. These algorithms have an interpretable structure that can be used to explain predictions. Concept Bottleneck Models, which use concept-level abstractions to explain model reasoning, are examples of this and can be applied in both image and text prediction tasks. This is especially important in domains like medicine, defense, finance, and law, where it is crucial to understand decisions and build trust in the algorithms. Many researchers argue that, at least for supervised machine learning, the way forward is symbolic regression, where the algorithm searches the space of mathematical expressions to find the model that best fits a given dataset.\nAI systems optimize behavior to satisfy a mathematically specified goal system chosen by the system designers, such as the command \"maximize the accuracy of assessing how positive film reviews are in the test dataset.\" The AI may learn useful general rules from the test set, such as \"reviews containing the word \"horrible\" are likely to be negative.\" However, it may also learn inappropriate rules, such as \"reviews containing 'Daniel Day-Lewis' are usually positive\"; such rules may be undesirable if they are likely to fail to generalize outside the training set, or if people consider the rule to be \"cheating\" or \"unfair.\" A human can audit rules in an XAI to get an idea of how likely the system is to generalize to future real-world data outside the test set.\n\nGoals\nCooperation between agents – in this case, algorithms and humans – depends on trust. If humans are to accept algorithmic prescriptions, they need to trust them. Incompleteness in formal trust criteria is a barrier to optimization. Transparency, interpretability, and explainability are intermediate goals on the road to these more comprehensive trust criteria. This is particularly relevant in medicine, especially with clinical decision support systems (CDSS), in which medical professionals should be able to understand how and why a machine-based decision was made in"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " criteria is a barrier to optimization. Transparency, interpretability, and explainability are intermediate goals on the road to these more comprehensive trust criteria. This is particularly relevant in medicine, especially with clinical decision support systems (CDSS), in which medical professionals should be able to understand how and why a machine-based decision was made in order to trust the decision and augment their decision-making process.\nAI systems sometimes learn undesirable tricks that do an optimal job of satisfying explicit pre-programmed goals on the training data but do not reflect the more nuanced implicit desires of the human system designers or the full complexity of the domain data. For example, a 2017 system tasked with image recognition learned to \"cheat\" by looking for a copyright tag that happened to be associated with horse pictures rather than learning how to tell if a horse was actually pictured. In another 2017 system, a supervised learning AI tasked with grasping items in a virtual world learned to cheat by placing its manipulator between the object and the viewer in a way such that it falsely appeared to be grasping the object.\nOne transparency project, the DARPA XAI program, aims to produce \"glass box\" models that are explainable to a \"human-in-the-loop\" without greatly sacrificing AI performance. Human users of such a system can understand the AI's cognition (both in real-time and after the fact) and can determine whether to trust the AI. Other applications of XAI are knowledge extraction from black-box models and model comparisons. In the context of monitoring systems for ethical and socio-legal compliance, the term \"glass box\" is commonly used to refer to tools that track the inputs and outputs of the system in question, and provide value-based explanations for their behavior. These tools aim to ensure that the system operates in accordance with ethical and legal standards, and that its decision-making processes are transparent and accountable. The term \"glass box\" is often used in contrast to \"black box\" systems, which lack transparency and can be more difficult to monitor and regulate.\nThe term is also used to name a voice assistant that produces counterfactual statements as explanations.\n\nExplainability and interpretability techniques\nThere is a subtle difference between the terms explainability and interpretability in the context of AI.\n\nSome explainability techniques don't involve understanding how the model works, and may work across various AI systems. Treating the model as a black box and analyzing how marginal changes to the inputs affect the result sometimes provides a sufficient explanation.\n\nExplainability\nExplainability is useful for ensuring that AI models"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " context of AI.\n\nSome explainability techniques don't involve understanding how the model works, and may work across various AI systems. Treating the model as a black box and analyzing how marginal changes to the inputs affect the result sometimes provides a sufficient explanation.\n\nExplainability\nExplainability is useful for ensuring that AI models are not making decisions based on irrelevant or otherwise unfair criteria. For classification and regression models, several popular techniques exist:\n\nPartial dependency plots show the marginal effect of an input feature on the predicted outcome.\nSHAP (SHapley Additive exPlanations) enables visualization of the contribution of each input feature to the output. It works by calculating Shapley values, which measure the average marginal contribution of a feature across all possible combinations of features.\nFeature importance estimates how important a feature is for the model. It is usually done using permutation importance, which measures the performance decrease when it the feature value randomly shuffled across all samples.\nLIME (Local Interpretable Model-Agnostic Explanations method) approximates locally a model's outputs with a simpler, interpretable model.\nMultitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.\nFor images, saliency maps highlight the parts of an image that most influenced the result.\nSystems that are expert or knowledge based are software systems that are made by experts. This system consists of a knowledge based encoding for the domain knowledge. This system is usually modeled as production rules, and someone uses this knowledge base which the user can question the system for knowledge. In expert systems, the language and explanations are understood with an explanation for the reasoning or a problem solving activity.\nHowever, these techniques are not very suitable for language models like generative pretrained transformers. Since these models generate language, they can provide an explanation, but which may not be reliable. Other techniques include attention analysis (examining how the model focuses on different parts of the input), probing methods (testing what information is captured in the model's representations), causal tracing (tracing the flow of information through the model) and circuit discovery (identifying specific subnetworks responsible for certain behaviors). Explainability research in this area overlaps significantly with interpretability and alignment research.\n\nInterpretability\nScholars sometimes use the term \"mechanistic interpretability\" to refer to the process of reverse-engineering artificial neural networks to understand their internal decision-making mechanisms and components, similar to how one might analyze a complex machine or computer program.\nSt"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " this area overlaps significantly with interpretability and alignment research.\n\nInterpretability\nScholars sometimes use the term \"mechanistic interpretability\" to refer to the process of reverse-engineering artificial neural networks to understand their internal decision-making mechanisms and components, similar to how one might analyze a complex machine or computer program.\nStudying the interpretability of the most advanced foundation models often involves searching for an automated way to identify \"features\" in generative pretrained transformers. In a neural network, a feature is a pattern of neuron activations that corresponds to a concept. A compute-intensive technique called \"dictionary learning\" makes it possible to identify features to some degree. Enhancing the ability to identify and edit features is expected to significantly improve the safety of frontier AI models.\nFor convolutional neural networks, DeepDream can generate images that strongly activate a particular neuron, providing a visual hint about what the neuron is trained to identify.\n\nKnowledge localization\nLarge language models (LLMs), such as transformer-based models (GPT), possess the ability to engage in conversation using general knowledge. This capability raises the question of how, exactly, such knowledge is stored within the model.\nResearch has suggested that the model’s MLP component (the feed-forward layers) is the main site in which knowledge is stored, encoding information through associative links that function as key–value memories: each key corresponds to textual patterns in the training data, while each value induces a distribution over the output vocabulary.\nA 2022 study aimed at locating where knowledge resides in the model employed a technique known as Causal Tracing. In tasks requiring general knowledge, the researchers injected noise into the hidden activations of the model, preventing it from completing the task. They then restored the clean activation values (taken from a noise-free run) to a different part of the model each time, observing when the model regained its ability to produce the correct answer. Based on these results, the authors concluded that factual knowledge is stored primarily in the MLP components of the model’s middle layers. They further proposed that model editing would be most effective in those regions, though this claim was later called into question.\nLater studies suggest that, in most cases, factual information is distributed across the model rather than localized within a single layer. According to one version of this view, different layers encode different aspects of the same association. For example, a question about the capital of Japan may activate representations related to “Japan” in one layer and representations corresponding to “capital cities” in another; the combination of these representations yields the concept of “Tokyo"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " layer. According to one version of this view, different layers encode different aspects of the same association. For example, a question about the capital of Japan may activate representations related to “Japan” in one layer and representations corresponding to “capital cities” in another; the combination of these representations yields the concept of “Tokyo.”\n\nHistory and methods\nDuring the 1970s to 1990s, symbolic reasoning systems, such as MYCIN, GUIDON, SOPHIE, and PROTOS could represent, reason about, and explain their reasoning for diagnostic, instructional, or machine-learning (explanation-based learning) purposes. MYCIN, developed in the early 1970s as a research prototype for diagnosing bacteremia infections of the bloodstream, could explain which of its hand-coded rules contributed to a diagnosis in a specific case. Research in intelligent tutoring systems resulted in developing systems such as SOPHIE that could act as an \"articulate expert\", explaining problem-solving strategy at a level the student could understand, so they would know what action to take next. For instance, SOPHIE could explain the qualitative reasoning behind its electronics troubleshooting, even though it ultimately relied on the SPICE circuit simulator. Similarly, GUIDON added tutorial rules to supplement MYCIN's domain-level rules so it could explain the strategy for medical diagnosis. Symbolic approaches to machine learning relying on explanation-based learning, such as PROTOS, made use of explicit representations of explanations expressed in a dedicated explanation language, both to explain their actions and to acquire new knowledge.\nIn the 1980s through the early 1990s, truth maintenance systems (TMS) extended the capabilities of causal-reasoning, rule-based, and logic-based inference systems. A TMS explicitly tracks alternate lines of reasoning, justifications for conclusions, and lines of reasoning that lead to contradictions, allowing future reasoning to avoid these dead ends. To provide an explanation, they trace reasoning from conclusions to assumptions through rule operations or logical inferences, allowing explanations to be generated from the reasoning traces. As an example, consider a rule-based problem solver with just a few rules about Socrates that concludes he has died from poison:\n\nBy just tracing through the dependency structure the problem solver can construct the following explanation: \"Socrates died because he was mortal and drank poison, and all mortals die when they drink poison. Socrates was mortal because he was a man and all men are mortal. Socrates drank poison because he held dissident beliefs, the government was conservative, and those holding"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " problem solver can construct the following explanation: \"Socrates died because he was mortal and drank poison, and all mortals die when they drink poison. Socrates was mortal because he was a man and all men are mortal. Socrates drank poison because he held dissident beliefs, the government was conservative, and those holding conservative dissident beliefs under conservative governments must drink poison.\"\nBy the 1990s researchers began studying whether it is possible to meaningfully extract the non-hand-coded rules being generated by opaque trained neural networks. Researchers in clinical expert systems who created neural network-powered decision support for clinicians sought to develop dynamic explanations that allow these technologies to be more trusted and trustworthy in practice. In the 2010s public concerns about racial and other bias in the use of AI for criminal sentencing decisions and findings of creditworthiness may have led to increased demand for transparent artificial intelligence. As a result, many academics and organizations are developing tools to help detect bias in their systems.\nMarvin Minsky et al. raised the issue that AI can function as a form of surveillance, with the biases inherent in surveillance, suggesting HI (Humanistic Intelligence) as a way to create a more fair and balanced \"human-in-the-loop\" AI.\nExplainable AI has been recently a new topic researched amongst the context of modern deep learning. Modern complex AI techniques, such as deep learning, are naturally opaque. To address this issue, methods have been developed to make new models more explainable and interpretable. This includes layerwise relevance propagation (LRP), a technique for determining which features in a particular input vector contribute most strongly to a neural network's output, although this technique has been shown to suffer from several important issues. Other techniques explain some particular prediction made by a (nonlinear) black-box model, a goal referred to as \"local interpretability\". We still today cannot explain the output of today's DNNs without the new explanatory mechanisms, we also can't by the neural network, or external explanatory components  There is also research on whether the concepts of local interpretability can be applied to a remote context, where a model is operated by a third-party.\nThere has been work on making glass-box models which are more transparent to inspection. This includes decision trees, Bayesian networks, sparse linear models, and more. The Association for Computing Machinery Conference on Fairness, Accountability, and Transparency (ACM FAccT) was established in 2018 to study transparency and explainability in the context of socio-technical systems, many of which include artificial"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " inspection. This includes decision trees, Bayesian networks, sparse linear models, and more. The Association for Computing Machinery Conference on Fairness, Accountability, and Transparency (ACM FAccT) was established in 2018 to study transparency and explainability in the context of socio-technical systems, many of which include artificial intelligence.\nSome techniques allow visualisations of the inputs to which individual software neurons respond to most strongly. Several groups found that neurons can be aggregated into circuits that perform human-comprehensible functions, some of which reliably arise across different networks trained independently.\nThere are various techniques to extract compressed representations of the features of given inputs, which can then be analysed by standard clustering techniques. Alternatively, networks can be trained to output linguistic explanations of their behaviour, which are then directly human-interpretable. Model behaviour can also be explained with reference to training data—for example, by evaluating which training inputs influenced a given behaviour the most, or by approximating its predictions using the most similar instances from the training data.\nThe use of explainable artificial intelligence (XAI) in pain research, specifically in understanding the role of electrodermal activity for automated pain recognition: hand-crafted features and deep learning models in pain recognition, highlighting the insights that simple hand-crafted features can yield comparative performances to deep learning models and that both traditional feature engineering and deep feature learning approaches rely on simple characteristics of the input time-series data.\n\nRegulation\nAs regulators, official bodies, and general users come to depend on AI-based dynamic systems, clearer accountability will be required for automated decision-making processes to ensure trust and transparency. The first global conference exclusively dedicated to this emerging discipline was the 2017 International Joint Conference on Artificial Intelligence: Workshop on Explainable Artificial Intelligence (XAI). It has evolved over the years, with various workshops organised and co-located to many other international conferences, and it has now a dedicated global event, \"The world conference on eXplainable Artificial Intelligence\", with its own proceedings.\nThe European Union introduced a right to explanation in the General Data Protection Regulation (GDPR) to address potential problems stemming from the rising importance of algorithms. The implementation of the regulation began in 2018. However, the right to explanation in GDPR covers only the local aspect of interpretability. In the United States, insurance companies are required to be able to explain their rate and coverage decisions. In France the Loi pour une République numérique (Digital Republic Act) grants subjects the right to request and receive information pertaining to the implementation of algorithms that process"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " GDPR covers only the local aspect of interpretability. In the United States, insurance companies are required to be able to explain their rate and coverage decisions. In France the Loi pour une République numérique (Digital Republic Act) grants subjects the right to request and receive information pertaining to the implementation of algorithms that process data about them.\n\nLimitations\nDespite ongoing endeavors to enhance the explainability of AI models, they persist with several inherent limitations.\n\nAdversarial parties\nBy making an AI system more explainable, we also reveal more of its inner workings. For example, the explainability method of feature importance identifies features or variables that are most important in determining the model's output, while the influential samples method identifies the training samples that are most influential in determining the output, given a particular input. Adversarial parties could take advantage of this knowledge.\nFor example, competitor firms could replicate aspects of the original AI system in their own product, thus reducing competitive advantage. An explainable AI system is also susceptible to being \"gamed\"—influenced in a way that undermines its intended purpose. One study gives the example of a predictive policing system; in this case, those who could potentially \"game\" the system are the criminals subject to the system's decisions. In this study, developers of the system discussed the issue of criminal gangs looking to illegally obtain passports, and they expressed concerns that, if given an idea of what factors might trigger an alert in the passport application process, those gangs would be able to \"send guinea pigs\" to test those triggers, eventually finding a loophole that would allow them to \"reliably get passports from under the noses of the authorities\".\n\nAdaptive integration and explanation\nMany approaches that it uses provides explanation in general, it doesn't take account for the diverse backgrounds and knowledge level of the users. This leads to challenges with accurate comprehension for all users. Expert users can find the explanations lacking in depth, and are oversimplified, while a beginner user may struggle understanding the explanations as they are complex. This limitation downplays the ability of the XAI techniques to appeal to their users with different levels of knowledge, which can impact the trust from users and who uses it. The quality of explanations can be different amongst their users as they all have different expertise levels, including different situation and conditions.\n\nTechnical complexity\nA fundamental barrier to making AI systems explainable is the technical complexity of such systems. End users often lack the coding knowledge required to understand software of any kind. Current methods used to explain AI are mainly technical"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " can be different amongst their users as they all have different expertise levels, including different situation and conditions.\n\nTechnical complexity\nA fundamental barrier to making AI systems explainable is the technical complexity of such systems. End users often lack the coding knowledge required to understand software of any kind. Current methods used to explain AI are mainly technical ones, geared toward machine learning engineers for debugging purposes, rather than toward the end users who are ultimately affected by the system, causing \"a gap between explainability in practice and the goal of transparency\". Proposed solutions to address the issue of technical complexity include either promoting the coding education of the general public so technical explanations would be more accessible to end users, or providing explanations in layperson terms.\nThe solution must avoid oversimplification. It is important to strike a balance between accuracy – how faithfully the explanation reflects the process of the AI system – and explainability – how well end users understand the process. This is a difficult balance to strike, since the complexity of machine learning makes it difficult for even ML engineers to fully understand, let alone non-experts.\n\nUnderstanding versus trust\nThe goal of explainability to end users of AI systems is to increase trust in the systems, even \"address concerns about lack of 'fairness' and discriminatory effects\". However, even with a good understanding of an AI system, end users may not necessarily trust the system. In one study, participants were presented with combinations of white-box and black-box explanations, and static and interactive explanations of AI systems. While these explanations served to increase both their self-reported and objective understanding, it had no impact on their level of trust, which remained skeptical.\nThis outcome was especially true for decisions that impacted the end user in a significant way, such as graduate school admissions. Participants judged algorithms to be too inflexible and unforgiving in comparison to human decision-makers; instead of rigidly adhering to a set of rules, humans are able to consider exceptional cases as well as appeals to their initial decision. For such decisions, explainability will not necessarily cause end users to accept the use of decision-making algorithms. We will need to either turn to another method to increase trust and acceptance of decision-making algorithms, or question the need to rely solely on AI for such impactful decisions in the first place.\nHowever, some emphasize that the purpose of explainability of artificial intelligence is not to merely increase users' trust in the system's decisions, but to calibrate the users' level of trust to the correct level. According to this principle, too much or too little user trust in the"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " such impactful decisions in the first place.\nHowever, some emphasize that the purpose of explainability of artificial intelligence is not to merely increase users' trust in the system's decisions, but to calibrate the users' level of trust to the correct level. According to this principle, too much or too little user trust in the AI system will harm the overall performance of the human-system unit. When the trust is excessive, the users are not critical of possible mistakes of the system and when the users do not have enough trust in the system, they will not exhaust the benefits inherent in it.\n\nCriticism\nSome scholars have suggested that explainability in AI should be considered a goal secondary to AI effectiveness, and that encouraging the exclusive development of XAI may limit the functionality of AI more broadly. Critiques of XAI rely on developed concepts of mechanistic and empiric reasoning from evidence-based medicine to suggest that AI technologies can be clinically validated even when their function cannot be understood by their operators.\nSome researchers advocate the use of inherently interpretable machine learning models, rather than using post-hoc explanations in which a second model is created to explain the first. This is partly because post-hoc models increase the complexity in a decision pathway and partly because it is often unclear how faithfully a post-hoc explanation can mimic the computations of an entirely separate model. However, another view is that what is important is that the explanation accomplishes the given task at hand, and whether it is pre or post-hoc doesn't matter. If a post-hoc explanation method helps a doctor diagnose cancer better, it is of secondary importance whether it is a correct/incorrect explanation.\nThe goals of XAI amount to a form of lossy compression that will become less effective as AI models grow in their number of parameters. Along with other factors this leads to a theoretical limit for explainability.\n\nExplainability in social choice\nExplainability was studied also in social choice theory. Social choice theory aims at finding solutions to social decision problems, that are based on well-established axioms. Ariel D. Procaccia explains that these axioms can be used to construct convincing explanations to the solutions. This principle has been used to construct explanations in various subfields of social choice.\n\nVoting\nCailloux and Endriss present a method for explaining voting rules using the axioms that characterize them. They exemplify their method on the Borda voting rule .\nPeters, Procaccia, Psomas and Zhou present an algorithm for explaining the outcomes of the Borda rule using O(m"}
{"doc_id": "Explainable artificial intelligence", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".\n\nVoting\nCailloux and Endriss present a method for explaining voting rules using the axioms that characterize them. They exemplify their method on the Borda voting rule .\nPeters, Procaccia, Psomas and Zhou present an algorithm for explaining the outcomes of the Borda rule using O(m2) explanations, and prove that this is tight in the worst case.\n\nParticipatory budgeting\nYang, Hausladen, Peters, Pournaras, Fricker and Helbing present an empirical study of explainability in participatory budgeting. They compared the greedy and the equal shares rules, and three types of explanations: mechanism explanation (a general explanation of how the aggregation rule works given the voting input), individual explanation (explaining how many voters had at least one approved project, at least 10000 CHF in approved projects), and group explanation (explaining how the budget is distributed among the districts and topics). They compared the perceived trustworthiness and fairness of greedy and equal shares, before and after the explanations. They found out that, for MES, mechanism explanation yields the highest increase in perceived fairness and trustworthiness; the second-highest was Group explanation. For Greedy, Mechanism explanation increases perceived trustworthiness but not fairness, whereas Individual explanation increases both perceived fairness and trustworthiness. Group explanation decreases the perceived fairness and trustworthiness.\n\nPayoff allocation\nNizri, Azaria and Hazon present an algorithm for computing explanations for the Shapley value. Given a coalitional game, their algorithm decomposes it to sub-games, for which it is easy to generate verbal explanations based on the axioms characterizing the Shapley value. The payoff allocation for each sub-game is perceived as fair, so the Shapley-based payoff allocation for the given game should seem fair as well. An experiment with 210 human subjects shows that, with their automatically generated explanations, subjects perceive Shapley-based payoff allocation as significantly fairer than with a general standard explanation.\n\nSee also\nAlgorithmic transparency – Transparency of decisions by algorithms\nRight to explanation – Right to have an algorithm explained\nAccumulated local effects – Machine learning method"}
{"doc_id": "Extremal optimization", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Extremal optimization (EO) is an optimization heuristic inspired by the Bak–Sneppen model of self-organized criticality from the field of statistical physics. This heuristic was designed initially to address combinatorial optimization problems such as the travelling salesman problem and spin glasses, although the technique has been demonstrated to function in optimization domains.\n\nRelation to self-organized criticality\nSelf-organized criticality (SOC) is a statistical physics concept to describe a class of dynamical systems that have a critical point as an attractor. Specifically, these are non-equilibrium systems that evolve through avalanches of change and dissipations that reach up to the highest scales of the system. SOC is said to govern the dynamics behind some natural systems that have these burst-like phenomena including landscape formation, earthquakes, evolution, and the granular dynamics of rice and sand piles. Of special interest here is the Bak–Sneppen model of SOC, which is able to describe evolution via punctuated equilibrium (extinction events) – thus modelling evolution as a self-organised critical process.\n\nRelation to computational complexity\nAnother piece in the puzzle is work on computational complexity, specifically that critical points have been shown to exist in NP-complete problems, where near-optimum solutions are widely dispersed and separated by barriers in the search space causing local search algorithms to get stuck or severely hampered. It was the evolutionary self-organised criticality model by Bak and Sneppen and the observation of critical points in combinatorial optimisation problems that lead to the development of Extremal Optimization by Stefan Boettcher and Allon Percus.\n\nThe technique\nEO was designed as a  local search  algorithm for combinatorial optimization problems. Unlike genetic algorithms, which work with a population of candidate solutions, EO evolves a single solution and makes local modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure (\"fitness\"). This differs from holistic approaches such as ant colony optimization and evolutionary computation that assign equal-fitness to all components of a solution based upon their collective evaluation against an objective function. The algorithm is initialized with an initial solution, which can be constructed randomly, or derived from another search process.\nThe technique is a fine-grained search, and superficially resembles a hill climbing (local search) technique. A more detailed examination reveals some interesting principles, which may have applicability and even some similarity to broader population-based approaches (evolutionary computation and artificial immune system). The governing principle behind this algorithm is"}
{"doc_id": "Extremal optimization", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " process.\nThe technique is a fine-grained search, and superficially resembles a hill climbing (local search) technique. A more detailed examination reveals some interesting principles, which may have applicability and even some similarity to broader population-based approaches (evolutionary computation and artificial immune system). The governing principle behind this algorithm is that of improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is obviously at odds with genetic algorithms, the quintessential evolutionary computation algorithm that selects good solutions in an attempt to make better solutions.\nThe resulting dynamics of this simple principle is firstly a robust hill climbing search behaviour, and secondly a diversity mechanism that resembles that of multiple-restart search. Graphing holistic solution quality over time (algorithm iterations) shows periods of improvement followed by quality crashes (avalanche) very much in the manner as described by punctuated equilibrium. It is these crashes or dramatic jumps in the search space that permit the algorithm to escape local optima and differentiate this approach from other local search procedures. Although such punctuated-equilibrium behaviour can be \"designed\" or \"hard-coded\", it should be stressed that this is an emergent effect of the negative-component-selection principle fundamental to the algorithm.\nEO has primarily been applied to combinatorial problems such as graph partitioning and the travelling salesman problem, as well as problems from statistical physics such as spin glasses.\n\nVariations on the theme and applications\nGeneralised extremal optimization (GEO) was developed to operate on bit strings where component quality is determined by the absolute rate of change of the bit, or the bits contribution to holistic solution quality. This work includes application to standard function optimisation problems as well as engineering problem domains. Another similar extension to EO is Continuous Extremal Optimization (CEO).\nEO has been applied to image rasterization as well as used as a local search after using ant colony optimization. EO has been used to identify structures in complex networks. EO has been used on a multiple target tracking problem. Finally, some work has been done on investigating the probability distribution used to control selection.\n\nSee also\nGenetic algorithm\nSimulated annealing"}
{"doc_id": "Feedback neural network", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Feedback neural network are neural networks with the ability to provide bottom-up and top-down design feedback to their input or previous layers, based on their outputs or subsequent layers. This is notably used in large language models specifically in reasoning language models (RLM). This process is designed to mimic self-assessment and internal deliberation, aiming to minimize errors (like hallucinations) and increase interpretability. This reflection is a form of \"test-time compute\", where additional computational resources are used during inference.\n\nIntroduction\nTraditional neural networks process inputs in a feedforward manner, generating outputs in a single pass. However, their limitations in handling complex tasks, and especially compositional ones, have led to the development of methods that simulate internal deliberation. Techniques such as chain-of-thought prompting encourage models to generate intermediate reasoning steps, thereby improving their performance in such tasks.\nThe feedback can take place either after a full network pass and decoding to tokens, or continuously in latent space (the last layer can be fed back to the first layer). In LLMs, special tokens can mark the beginning and end of reflection before producing a final response (e.g., <thinking>).\nThis internal process of \"thinking\" about the steps leading to an answer is designed to be analogous to human metacognition or \"thinking about thinking\". It helps AI systems approach tasks that require multi-step reasoning, planning, and logical thought.\n\nTechniques\nIncreasing the length of the Chain-of-Thought reasoning process, by passing the output of the model back to its input and doing multiple network passes, increases inference-time scaling. Reinforcement learning frameworks have also been used to steer the Chain-of-Thought. One example is Group Relative Policy Optimization (GRPO), used in DeepSeek-R1, a variant of policy gradient methods that eliminates the need for a separate \"critic\" model by normalizing rewards within a group of generated outputs, reducing computational cost. Simple techniques like \"budget forcing\" (forcing the model to continue generating reasoning steps) have also proven effective in improving performance.\n\nTypes of reflection\nPost-hoc reflection\nAnalyzes and critiques an initial output separately, often involving prompting the model to identify errors or suggest improvements after generating a response. The Reflexion framework follows this approach.\n\nIterative reflection\nRevises earlier parts of a response dynamically during generation. Self-monitoring mechanisms allow the model to adjust reasoning as it progresses. Methods like Tree-of-Thoughts exemplify this, enabling backtracking and alternative exploration.\n\nIntrinsic reflection\nIntegrates self-monitoring directly into the"}
{"doc_id": "Feedback neural network", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " this approach.\n\nIterative reflection\nRevises earlier parts of a response dynamically during generation. Self-monitoring mechanisms allow the model to adjust reasoning as it progresses. Methods like Tree-of-Thoughts exemplify this, enabling backtracking and alternative exploration.\n\nIntrinsic reflection\nIntegrates self-monitoring directly into the model architecture rather than relying solely on external prompts, enabling models with inherent awareness of their reasoning limitations and uncertainties. This has been used by Google DeepMind in a technique called Self-Correction via Reinforcement Learning (SCoRe) which rewards the model for improving its responses.\n\nProcess reward models and limitations\nEarly research explored PRMs to provide feedback on each reasoning step, unlike traditional reinforcement learning which rewards only the final outcome. However, PRMs have faced challenges, including computational cost and reward hacking. DeepSeek-R1's developers found them to be not beneficial.\n\nSee also\nFeedback neural network\nReflective programming\nReservoir computing"}
{"doc_id": "Galaxy AI", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Galaxy AI is a collection of artificial intelligence (AI) features developed by Samsung Electronics for use in Galaxy-branded mobile devices. First released with the Samsung Galaxy S24 series in January 2024, the system integrates both on-device and cloud-based processing to support features such as language translation, image editing, and content search. These tools operate within various Samsung applications and are intended to assist with everyday tasks.\nAccording to Samsung: \"In essence, Bixby is like a personal assistant ready to respond to your verbal commands, while Galaxy AI is like a smart companion that's always working to make your device smarter and your life easier.\"\n\nOverview\nGalaxy AI integrates Samsung's own AI models with  external technologies, including Google's Gemini AI, to provide a variety of context-sensitive functions. These include tools for language translation, media editing, and task automation. They are available within specific Samsung applications.\n\nFeatures\nGalaxy AI includes multiple tools that apply artificial intelligence to specific user tasks, such as communication, notetaking, photography, and productivity. Each tool is categorized by function and operates within Samsung's software environment.\n\nCommunication\nCall Assist\nProvides translation-related features within the default Phone app on supported Galaxy devices. It includes Live Translate, which enables two-way voice and text translation during calls, and Text Call, which converts speech into real-time text and generates responses. These tools aim to support communication across different languages and offer alternatives to voice-based interaction. Text Call does not involve AI processing and is available on all devices supporting One UI 6.1 or later.\n\nWriting Assist\nA feature integrated into the Samsung Keyboard that supports tasks such as translation, sentence composition, and text correction. It offers tools for adjusting phrasing, grammar, and tone across supported apps, including messaging and email platforms. Suggested replies may also be generated based on context.\n\nInterpreter\nA translation feature on Galaxy devices running One UI 6.1 or later, offering real-time spoken and on-screen translations for two-way face-to-face conversations. It supports over a dozen languages and includes a screen-flip view so each speaker can read translations in their own language. Offline use is available with downloaded language packs.\n\nProductivity\nNow Brief\nA daily summary interface introduced in One UI 7 and updated in One UI 8. It presents scrollable cards with updates such as weather, calendar events, smart home status, and hydration reminders. One UI 8 added a “Listen Brief” feature using on-device text-to-speech. Now Brief also serves as a visual output"}
{"doc_id": "Galaxy AI", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " interface introduced in One UI 7 and updated in One UI 8. It presents scrollable cards with updates such as weather, calendar events, smart home status, and hydration reminders. One UI 8 added a “Listen Brief” feature using on-device text-to-speech. Now Brief also serves as a visual output surface for Gemini Live, displaying AI-generated summaries and follow-up suggestions in card format. These cards are dynamically updated based on the user's schedule and app usage, and are accessible via the Now Bar.\nIt was firstly exclusive to the Galaxy S25 series, but with the One UI 8 update it was also released for the S24 series.\n\nNow Bar\nA customizable toolbar introduced in One UI 7 and updated in One UI 8, offering quick access to live activities, device routines, and app shortcuts. Now Bar includes real-time widgets such as Now Brief cards, and acts as the primary interface for launching Gemini Live. A Gemini Live status indicator (e.g. “listening” or “on hold”) may appear on the lock screen, showing session status and offering limited session controls.\n\nGemini Live\nA multimodal AI feature included in the Galaxy AI suite, powered by Google Gemini. It enables interaction through voice, camera input, and screen sharing, supporting tasks such as object recognition, text translation, and content analysis. The assistant works with supported applications, with direct integration for Samsung apps such as Calendar, Samsung Notes, and Reminders. Responses generated by Gemini Live may appear as Now Brief cards, and the session status can be monitored and managed through Now Bar.\n\nNote Assist\nA feature in Samsung Notes that helps summarize content from meetings or lectures into structured formats. It can identify recurring phrases or key points and format them using built-in organization tools. The output may be used for reviewing or sharing, depending on the user's needs.\n\nTranscript Assist\nA function within the Samsung Voice Recorder app allows recorded speech to be converted into text. It supports additional features such as basic summarization and translation, depending on the content and system configuration. These tools are intended to improve audio processing efficiency within the app.\n\nCross-app Action\nA feature that enables certain actions—such as search, translation, or event creation—based on selected text or content across compatible apps. It recognizes contextual elements and presents available options within supported Samsung and third-party applications.\n\nCamera\nProVisual Engine\nA software-based imaging system applies scene recognition and automated adjustments to improve visual output on supported devices. It can modify parameters such as color balance, brightness, contrast"}
{"doc_id": "Galaxy AI", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "—based on selected text or content across compatible apps. It recognizes contextual elements and presents available options within supported Samsung and third-party applications.\n\nCamera\nProVisual Engine\nA software-based imaging system applies scene recognition and automated adjustments to improve visual output on supported devices. It can modify parameters such as color balance, brightness, contrast, and noise levels. Manual adjustment tools are also available for users who prefer direct control.\n\nNightography\nA low-light photography function that uses noise reduction and exposure control to support clearer image and video capture in dark settings. It may also apply motion blur reduction depending on the scene and hardware capabilities.\n\nExplore\nCircle to Search\nA gesture-based search function that lets users select on-screen content for related web lookup.\nIt can recognize elements such as products, landmarks, and text, and provides search suggestions within the current interface. This is intended to reduce app switching during contextual queries.\n\nAI Select\nA content-based suggestion feature that analyzes what is displayed on screen to offer possible user actions. Examples include suggesting wallpaper settings or creating short video clips from selected media. The feature works within certain Samsung apps and may vary depending on device capabilities.\n\nBrowsing Assist\nA web summarization feature within the Samsung Internet browser that processes textual content to generate condensed versions. It can also translate supported content and is limited to handling text-based information. The function is intended to assist users in reviewing lengthy web content.\n\nPhoto editing\nSketch to Image\nAn image generation function that creates visual outputs from sketches, photos, or text-based prompts. It applies algorithmic filters to modify or complete the input and is compatible with tools such as the S Pen and voice commands. On the Galaxy S25 Ultra, the integrated S Pen can be used to sketch or refine inputs directly. Processing is supported by the ProVisual Engine including ProScaler, which enhances image processing through multi-frame capture, sharpness enhancement, and low-light optimization. The feature is available in apps including Samsung Gallery, Samsung Notes, and the Edge Panel.\n\nPortrait Studio\nA portrait creation feature that generates stylized images from photos or sketches using AI-based image processing. Users can apply effects such as lighting or background adjustments through touch or S Pen input. The function integrates with the Best Face tool, which captures multiple images in succession and enables the selection of preferred facial expressions. Technical features such as Auto HDR and the ProVisual Engine contribute to improved detail and lighting balance, producing higher-quality inputs for AI portrait generation. The feature is accessible in the Samsung Gallery app.\n\nGenerative Edit\nAn image editing function that allows users to"}
{"doc_id": "Galaxy AI", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " captures multiple images in succession and enables the selection of preferred facial expressions. Technical features such as Auto HDR and the ProVisual Engine contribute to improved detail and lighting balance, producing higher-quality inputs for AI portrait generation. The feature is accessible in the Samsung Gallery app.\n\nGenerative Edit\nAn image editing function that allows users to remove objects, expand backgrounds, or resize content using automated generation tools. Modified areas are filled in algorithmically based on surrounding visual context. Related software options such as Expert RAW and Enhance-X offer additional processing functions, including sharpening, noise reduction, and camera shift correction. Hardware specifications also affect the use of generative editing; for example, the Galaxy S25 Ultra includes a 200 MP wide lens, a 50 MP ultrawide lens, and both 10 MP and 50 MP telephoto lenses with up to 5× optical zoom, which can provide higher-resolution source material for editing.\n\nBest Face\nA photo optimization function that automatically selects and composites the most favorable facial expressions of subjects captured in a motion photo. The feature analyzes multiple frames taken in quick succession and identifies faces with open eyes, smiles, or minimal motion blur to generate a single improved image. Processing is powered by the device's AI-enabled photo engine, utilizing frame comparison and face detection algorithms. Best Face can be accessed in the Samsung Gallery app through the choose best face icon displayed at the bottom of the image.\n\nVideo editing\nAudio Eraser\nA post-processing feature that allows users to remove background noise or specific audio elements—such as voices, wind, or crowd sounds—from recorded videos. It uses AI-based audio separation to isolate and reduce unwanted sound components. While primarily software-driven, supporting systems such as the ProVisual Engine improve visual clarity and reduce noise in source video, indirectly enhancing audio separation accuracy. The feature is available in the Samsung Gallery app and can be accessed during video editing.\n\nAuto Trim\nA video editing function that detects and selects portions of footage based on visual or contextual patterns, enabling the assembly of shorter clips without manual editing. It is supported by AI-powered video analytics that assess scene composition and motion. High-resolution recording modes, such as 8K video capture, provide additional flexibility for cropping and reframing segments during editing. Optical Zoom systems included in the S25 series also contribute to framing inputs.\n\nInstant Slo-Mo\nA post-processing function that generates slow-motion playback by interpolating additional frames between existing ones. It is activated during video playback in the Gallery app by tapping and holding the screen, without the need for prior"}
{"doc_id": "Galaxy AI", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " segments during editing. Optical Zoom systems included in the S25 series also contribute to framing inputs.\n\nInstant Slo-Mo\nA post-processing function that generates slow-motion playback by interpolating additional frames between existing ones. It is activated during video playback in the Gallery app by tapping and holding the screen, without the need for prior selection of a specific recording mode. AI frame interpolation, analyzes consecutive frames and predicts intermediate frames, allowing smooth slow-motion effects even for standard recordings captured at 30 or 60 frames per second. Native high-frame-rate recording, available up to 960fps, enables feature such as Super Slo-Mo. Instant Slo-Mo may be used with Log video, which captures a flat profile for dynamic range, and Galaxy Log, Samsung's AI-based grading tool. These functions allow additional control in post-production.\n\nAssist\nBixby\nBixby is a voice recognition–based assistant that allows users to operate Galaxy devices and connected products using spoken commands. It can control smartphone settings, perform searches, manage schedules, and interact with other Samsung devices such as Galaxy Watch and SmartThings home appliances. Bixby also supports Bixby Routines, an automation feature that performs preset actions based on user preferences or contextual triggers. Bixby can be accessed in the device settings through the Advanced Features menu under the Bixby option.\n\nBixby Vision\nBixby Vision is a visual recognition–based assistant that analyzes visual information captured by the device camera to perform contextual tasks. It identifies objects, text, and scenes to provide related information or execute corresponding actions within supported apps. Bixby Vision can be accessed in the Samsung Camera app through the More tab by selecting the eye-shaped icon displayed on the screen.\n\nFunctions\nTranslate: Provides real-time translation by overlaying the translated text on the original image for foreign signs or menus.\nText: Uses optical character recognition (OCR) to copy or share printed text such as from documents or books.\nDiscover: Searches online for visually similar products or related information, including fashion and furniture items.\nWine: Recognizes wine labels to provide details such as type, origin, average price, and reviews.\nScene Describer: Analyzes images to generate text and audio descriptions of captured scenes, aiding accessibility for users with visual impairments.\nObject Identifier: Identifies plants, animals, or landmarks and provides related information such as names or classifications.\nText Reader: Converts recognized text into spoken audio using text-to-speech (TTS).\nColor Detector: Identifies and names colors from objects,"}
{"doc_id": "Galaxy AI", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " descriptions of captured scenes, aiding accessibility for users with visual impairments.\nObject Identifier: Identifies plants, animals, or landmarks and provides related information such as names or classifications.\nText Reader: Converts recognized text into spoken audio using text-to-speech (TTS).\nColor Detector: Identifies and names colors from objects, displaying or reading the color information aloud.\n\nHealth\nHealth Assist\nA health monitoring function in the Samsung Health app that generates an Energy score based on factors such as sleep quality, activity, and heart rate variability. The score is intended to reflect a user's general condition for daily activity, and is derived from data collected by other Galaxy devices, although only a watch or a smart ring can track sleep.\n\nPersonalization\nGenerative Wallpaper and Photo Ambient\nTwo visual customization functions that modify wallpaper appearance using AI-based processing. Generative Wallpaper creates images from user-provided keywords or themes, operating locally on the device. Photo Ambient adjusts wallpaper based on environmental factors such as lighting and time, using preset visual filters.\nThese features leverage on-device AI for privacy-sensitive tasks while utilizing cloud-based AI for complex processing needs.\n\nSupported devices\nGalaxy AI was introduced with the Galaxy S24 series in January 2024 and later extended to other Galaxy models through software updates. AI features are available at no cost on compatible devices until the end of 2025, with functionality varying by model.\nGalaxy S series\n\nGalaxy S25 series\nGalaxy S24 series\nGalaxy S23 series\nGalaxy S22 series\nGalaxy S21 series (limited AI features)\nGalaxy Z series  \n\nGalaxy Z Fold7, Z Flip7, Z Flip7 FE\nGalaxy Z Fold6, Z Flip6\nGalaxy Z Fold5, Z Flip5\nGalaxy Z Fold4, Z Flip4\nGalaxy Z Fold3, Z Flip3 (limited AI features)\nGalaxy Tab series  \n\nGalaxy Tab S10 series\nGalaxy Tab S10 FE, Tab S10 FE+, Tab S10 Lite (limited AI features)\nGalaxy Tab S9 series\nGalaxy Tab S9 FE, Tab S9 FE+ (limited AI features)\nGalaxy Tab S8 series\nGalaxy A series\nGalaxy A series phones have limited AI features.\n\nGalaxy A56\nGalaxy A55\nGalaxy A54\nGalaxy A36\nGalaxy A35\nGalaxy A34\nGalaxy A26\nGalaxy A25\nGalaxy A17\nGalaxy A16\nGal"}
{"doc_id": "Galaxy AI", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " A series\nGalaxy A series phones have limited AI features.\n\nGalaxy A56\nGalaxy A55\nGalaxy A54\nGalaxy A36\nGalaxy A35\nGalaxy A34\nGalaxy A26\nGalaxy A25\nGalaxy A17\nGalaxy A16\nGalaxy A15\nGalaxy Wearables\n\nGalaxy Watch 8, Watch Ultra, Watch 7, Watch 6 (limited AI features)\nGalaxy Buds3, Buds3 Pro (limited AI-enhanced audio processing)\nFuture software updates may add Galaxy AI support to additional devices, according to Samsung.\n\nPrivacy and ethics\nGalaxy AI processes sensitive data locally on the device to limit external transmission, while cloud-based computing is used selectively for intensive tasks. According to Samsung, user data is not stored or used to train AI models without explicit permission. The company also outlines principles related to fairness, transparency, and accountability in AI development."}
{"doc_id": "Game theory", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Game theory is the study of mathematical models of strategic interactions. It has applications in many fields of social science, and is used extensively in economics, logic, systems science and computer science. Initially, game theory addressed two-person zero-sum games, in which a participant's gains or losses are exactly balanced by the losses and gains of the other participant. In the 1950s, it was extended to the study of non zero-sum games, and was eventually applied to a wide range of behavioral relations. It is now an umbrella term for the science of rational decision making in humans, animals, and computers.\nModern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum games and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by Theory of Games and Economic Behavior (1944), co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.\nGame theory was developed extensively in the 1950s, and was explicitly applied to evolution in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. John Maynard Smith was awarded the Crafoord Prize for his application of evolutionary game theory in 1999, and fifteen game theorists have won the Nobel Prize in economics as of 2020, including most recently Paul Milgrom and Robert B. Wilson.\n\nHistory\nDiscussions on the mathematics of games began long before the rise of modern, mathematical game theory. Cardano wrote on games of chance in Liber de ludo aleae (Book on Games of Chance), written around 1564 but published posthumously in 1663. Influenced by the work of Fermat and Pascal on the problem of points, Huygens developed the concept of expectation on reasoning about the structure of games of chance, publishing his gambling calculus in De ratiociniis in ludo aleæ (On Reasoning in Games of Chance) in 1657.\nIn 1713, a letter attributed to Charles Waldegrave, an active Jacobite and uncle to British diplomat James Waldegrave, analyzed a game called \"le her\". Walde"}
{"doc_id": "Game theory", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " in De ratiociniis in ludo aleæ (On Reasoning in Games of Chance) in 1657.\nIn 1713, a letter attributed to Charles Waldegrave, an active Jacobite and uncle to British diplomat James Waldegrave, analyzed a game called \"le her\". Waldegrave provided a minimax mixed strategy solution to a two-person version of the card game, and the problem is now known as the Waldegrave problem.\nIn 1838, Antoine Augustin Cournot provided a model of competition in oligopolies. Though he did not refer to it as such, he presented a solution that is the Nash equilibrium of the game in his Recherches sur les principes mathématiques de la théorie des richesses (Researches into the Mathematical Principles of the Theory of Wealth). In 1883, Joseph Bertrand critiqued Cournot's model as unrealistic, providing an alternative model of price competition which would later be formalized by Francis Ysidro Edgeworth.\nIn 1913, Ernst Zermelo published Über eine Anwendung der Mengenlehre auf die Theorie des Schachspiels (On an Application of Set Theory to the Theory of the Game of Chess), which proved that the optimal chess strategy is strictly determined.\n\nFoundation\nThe work of John von Neumann established game theory as its own independent field in the early-to-mid 20th century, with von Neumann publishing his paper On the Theory of Games of Strategy in 1928. Von Neumann's original proof used Brouwer's fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. Von Neumann's work in game theory culminated in his 1944 book Theory of Games and Economic Behavior, co-authored with Oskar Morgenstern. The second edition of this book provided an axiomatic theory of utility, which reincarnated Daniel Bernoulli's old theory of utility (of money) as an independent discipline. This foundational work contains the method for finding mutually consistent solutions for two-person zero-sum games. Subsequent work focused primarily on cooperative game theory, which analyzes optimal strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies.\nIn his 1938 book Applications aux Jeux de Hasard and earlier notes, Émile Borel proved a minimax theorem for two-person zero-sum matrix games only when the pay-off matrix is symmetric and provided a"}
{"doc_id": "Game theory", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " strategies for groups of individuals, presuming that they can enforce agreements between them about proper strategies.\nIn his 1938 book Applications aux Jeux de Hasard and earlier notes, Émile Borel proved a minimax theorem for two-person zero-sum matrix games only when the pay-off matrix is symmetric and provided a solution to a non-trivial infinite game (known in English as Blotto game). Borel conjectured the non-existence of mixed-strategy equilibria in finite two-person zero-sum games, a conjecture that was proved false by von Neumann.\n\nIn 1950, John Nash developed a criterion for mutual consistency of players' strategies known as the Nash equilibrium, applicable to a wider variety of games than the criterion proposed by von Neumann and Morgenstern. Nash proved that every finite n-player, non-zero-sum (not just two-player zero-sum) non-cooperative game has what is now known as a Nash equilibrium in mixed strategies.\nGame theory experienced a flurry of activity in the 1950s, during which the concepts of the core, the extensive form game, fictitious play, repeated games, and the Shapley value were developed. The 1950s also saw the first applications of game theory to philosophy and political science. The first mathematical discussion of the prisoner's dilemma appeared, and an experiment was undertaken by mathematicians Merrill M. Flood and Melvin Dresher, as part of the RAND Corporation's investigations into game theory. RAND pursued the studies because of possible applications to global nuclear strategy.\n\nPrize-winning achievements\nIn 1965, Reinhard Selten introduced his solution concept of subgame perfect equilibria, which further refined the Nash equilibrium. Later he would introduce trembling hand perfection as well. In 1994 Nash, Selten and Harsanyi became Economics Nobel Laureates for their contributions to economic game theory.\nIn the 1970s, game theory was extensively applied in biology, largely as a result of the work of John Maynard Smith and his evolutionarily stable strategy. In addition, the concepts of correlated equilibrium, trembling hand perfection and common knowledge were introduced and analyzed.\nIn 1994, John Nash was awarded the Nobel Memorial Prize in the Economic Sciences for his contribution to game theory. Nash's most famous contribution to game theory is the concept of the Nash equilibrium, which is a solution concept for non-cooperative games, published in 1951. A Nash equilibrium is a set of strategies, one for each player, such that no player can"}
{"doc_id": "Game theory", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Prize in the Economic Sciences for his contribution to game theory. Nash's most famous contribution to game theory is the concept of the Nash equilibrium, which is a solution concept for non-cooperative games, published in 1951. A Nash equilibrium is a set of strategies, one for each player, such that no player can improve their payoff by unilaterally changing their strategy.\nIn 2005, game theorists Thomas Schelling and Robert Aumann followed Nash, Selten, and Harsanyi as Nobel Laureates. Schelling worked on dynamic models, early examples of evolutionary game theory. Aumann contributed more to the equilibrium school, introducing equilibrium coarsening and correlated equilibria, and developing an extensive formal analysis of the assumption of common knowledge and of its consequences.\nIn 2007, Leonid Hurwicz, Eric Maskin, and Roger Myerson were awarded the Nobel Prize in Economics \"for having laid the foundations of mechanism design theory\". Myerson's contributions include the notion of proper equilibrium, and an important graduate text: Game Theory, Analysis of Conflict. Hurwicz introduced and formalized the concept of incentive compatibility.\nIn 2012, Alvin E. Roth and Lloyd S. Shapley were awarded the Nobel Prize in Economics \"for the theory of stable allocations and the practice of market design\". In 2014, the Nobel went to game theorist Jean Tirole.\n\nDifferent types of games\nCooperative / non-cooperative\nA game is cooperative if the players are able to form binding commitments externally enforced (e.g. through contract law). A game is non-cooperative if players cannot form alliances or if all agreements need to be self-enforcing (e.g. through credible threats).\nCooperative games are often analyzed through the framework of cooperative game theory, which focuses on predicting which coalitions will form, the joint actions that groups take, and the resulting collective payoffs. It is different from non-cooperative game theory which focuses on predicting individual players' actions and payoffs by analyzing Nash equilibria.\nCooperative game theory provides a high-level approach as it describes only the structure and payoffs of coalitions, whereas non-cooperative game theory also looks at how strategic interaction will affect the distribution of payoffs. As non-cooperative game theory is more general, cooperative games can be analyzed through the approach of non-cooperative game theory (the converse does not hold) provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation.\n\nSym"}
{"doc_id": "Game theory", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " affect the distribution of payoffs. As non-cooperative game theory is more general, cooperative games can be analyzed through the approach of non-cooperative game theory (the converse does not hold) provided that sufficient assumptions are made to encompass all the possible strategies available to players due to the possibility of external enforcement of cooperation.\n\nSymmetric / asymmetric\nA symmetric game is a game where each player earns the same payoff when making the same choice. In other words, the identity of the player does not change the resulting game facing the other player. Many of the commonly studied 2×2 games are symmetric. The standard representations of chicken, the prisoner's dilemma, and the stag hunt are all symmetric games.\nThe most commonly studied asymmetric games are games where there are not identical strategy sets for both players. For instance, the ultimatum game and similarly the dictator game have different strategies for each player. It is possible, however, for a game to have identical strategies for both players, yet be asymmetric. For example, the game pictured in this section's graphic is asymmetric despite having identical strategy sets for both players.\n\nZero-sum / non-zero-sum\nZero-sum games (more generally, constant-sum games) are games in which choices by players can neither increase nor decrease the available resources. In zero-sum games, the total benefit goes to all players in a game, for every combination of strategies, and always adds to zero (more informally, a player benefits only at the equal expense of others). Poker exemplifies a zero-sum game (ignoring the possibility of the house's cut), because one wins exactly the amount one's opponents lose. Other zero-sum games include matching pennies and most classical board games including Go and chess.\nMany games studied by game theorists (including the famed prisoner's dilemma) are non-zero-sum games, because the outcome has net results greater or less than zero. Informally, in non-zero-sum games, a gain by one player does not necessarily correspond with a loss by another.\nFurthermore, constant-sum games correspond to activities like theft and gambling, but not to the fundamental economic situation in which there are potential gains from trade. It is possible to transform any constant-sum game into a (possibly asymmetric) zero-sum game by adding a dummy player (often called \"the board\") whose losses compensate the players' net winnings.\n\nSimultaneous / sequential\nSimultaneous games are games where both players move simultaneously, or instead the later players are unaware of the earlier players' actions (making them effectively simultaneous). Sequential games (a type"}
{"doc_id": "Game theory", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " game by adding a dummy player (often called \"the board\") whose losses compensate the players' net winnings.\n\nSimultaneous / sequential\nSimultaneous games are games where both players move simultaneously, or instead the later players are unaware of the earlier players' actions (making them effectively simultaneous). Sequential games (a type of dynamic games) are games where players do not make decisions simultaneously, and player's earlier actions affect the outcome and decisions of other players. This need not be perfect information about every action of earlier players; it might be very little knowledge. For instance, a player may know that an earlier player did not perform one particular action, while they do not know which of the other available actions the first player actually performed.\nThe difference between simultaneous and sequential games is captured in the different representations discussed above. Often, normal form is used to represent simultaneous games, while extensive form is used to represent sequential ones. The transformation of extensive to normal form is one way, meaning that multiple extensive form games correspond to the same normal form. Consequently, notions of equilibrium for simultaneous games are insufficient for reasoning about sequential games; see subgame perfection.\nIn short, the differences between sequential and simultaneous games are as follows:\n\nPerfect information and imperfect information\nAn important subset of sequential games consists of games of perfect information. A game with perfect information means that all players, at every move in the game, know the previous history of the game and the moves previously made by all other players. An imperfect information game is played when the players do not know all moves already made by the opponent such as a simultaneous move game. Examples of perfect-information games include tic-tac-toe, checkers, chess, and Go.\nMany card games are games of imperfect information, such as poker and bridge. Perfect information is often confused with complete information, which is a similar concept pertaining to the common knowledge of each player's sequence, strategies, and payoffs throughout gameplay. Complete information requires that every player know the strategies and payoffs available to the other players but not necessarily the actions taken, whereas perfect information is knowledge of all aspects of the game and players. Games of incomplete information can be reduced, however, to games of imperfect information by introducing \"moves by nature\".\n\nBayesian game\nOne of the assumptions of the Nash equilibrium is that every player has correct beliefs about the actions of the other players. However, there are many situations in game theory where participants do not fully understand the characteristics of their opponents. Negotiators may be unaware of their opponent's valuation of the object of"}
{"doc_id": "Game theory", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " nature\".\n\nBayesian game\nOne of the assumptions of the Nash equilibrium is that every player has correct beliefs about the actions of the other players. However, there are many situations in game theory where participants do not fully understand the characteristics of their opponents. Negotiators may be unaware of their opponent's valuation of the object of negotiation, companies may be unaware of their opponent's cost functions, combatants may be unaware of their opponent's strengths, and jurors may be unaware of their colleague's interpretation of the evidence at trial. In some cases, participants may know the character of their opponent well, but may not know how well their opponent knows his or her own character.\nBayesian game means a strategic game with incomplete information. For a strategic game, decision makers are players, and every player has a group of actions. A core part of the imperfect information specification is the set of states. Every state completely describes a collection of characteristics relevant to the player such as their preferences and details about them. There must be a state for every set of features that some player believes may exist.\n\nFor example, where Player 1 is unsure whether Player 2 would rather date her or get away from her, while Player 2 understands Player 1's preferences as before. To be specific, supposing that Player 1 believes that Player 2 wants to date her under a probability of 1/2 and get away from her under a probability of 1/2 (this evaluation comes from Player 1's experience probably: she faces players who want to date her half of the time in such a case and players who want to avoid her half of the time). Due to the probability involved, the analysis  of this situation requires to understand the player's preference for the draw, even though people are only interested in pure strategic equilibrium.\n\nCombinatorial games\nGames in which the difficulty of finding an optimal strategy stems from the multiplicity of possible moves are called combinatorial games. Examples include chess and Go. Games that involve imperfect information may also have a strong combinatorial character, for instance backgammon. There is no unified theory addressing combinatorial elements in games. There are, however, mathematical tools that can solve some particular problems and answer some general questions.\nGames of perfect information have been studied in combinatorial game theory, which has developed novel representations, e.g. surreal numbers, as well as combinatorial and algebraic (and sometimes non-constructive) proof methods to solve games of certain types, including \"loopy\" games"}
{"doc_id": "Game theory", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and answer some general questions.\nGames of perfect information have been studied in combinatorial game theory, which has developed novel representations, e.g. surreal numbers, as well as combinatorial and algebraic (and sometimes non-constructive) proof methods to solve games of certain types, including \"loopy\" games that may result in infinitely long sequences of moves. These methods address games with higher combinatorial complexity than those usually considered in traditional (or \"economic\") game theory. A typical game that has been solved this way is Hex. A related field of study, drawing from computational complexity theory, is game complexity, which is concerned with estimating the computational difficulty of finding optimal strategies.\nResearch in artificial intelligence has addressed both perfect and imperfect information games that have very complex combinatorial structures (like chess, go, or backgammon) for which no provable optimal strategies have been found. The practical solutions involve computational heuristics, like alpha–beta pruning or use of artificial neural networks trained by reinforcement learning, which make games more tractable in computing practice.\n\nDiscrete and continuous games\nMuch of game theory is concerned with finite, discrete games that have a finite number of players, moves, events, outcomes, etc. Many concepts can be extended, however. Continuous games allow players to choose a strategy from a continuous strategy set. For instance, Cournot competition is typically modeled with players' strategies being any non-negative quantities, including fractional quantities.\n\nDifferential games\nDifferential games such as the continuous pursuit and evasion game are continuous games where the evolution of the players' state variables is governed by differential equations. The problem of finding an optimal strategy in a differential game is closely related to the optimal control theory. In particular, there are two types of strategies: the open-loop strategies are found using the Pontryagin maximum principle while the closed-loop strategies are found using Bellman's Dynamic Programming method.\nA particular case of differential games are the games with a random time horizon. In such games, the terminal time is a random variable with a given probability distribution function. Therefore, the players maximize the mathematical expectation of the cost function. It was shown that the modified optimization problem can be reformulated as a discounted differential game over an infinite time interval.\n\nEvolutionary game theory\nEvolutionary game theory studies players who adjust their strategies over time according to rules that are not necessarily rational or farsighted.  In general, the evolution of strategies over time according to such rules is modeled as a Markov chain with a state variable"}
{"doc_id": "Game theory", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " game over an infinite time interval.\n\nEvolutionary game theory\nEvolutionary game theory studies players who adjust their strategies over time according to rules that are not necessarily rational or farsighted.  In general, the evolution of strategies over time according to such rules is modeled as a Markov chain with a state variable such as the current strategy profile or how the game has been played in the recent past. Such rules may feature imitation, optimization, or survival of the fittest.\nIn biology, such models can represent evolution, in which offspring adopt their parents' strategies and parents who play more successful strategies (i.e. corresponding to higher payoffs) have a greater number of offspring. In the social sciences, such models typically represent strategic adjustment by players who play a game many times within their lifetime and, consciously or unconsciously, occasionally adjust their strategies.\n\nStochastic outcomes (and relation to other fields)\nIndividual decision problems with stochastic outcomes are sometimes considered \"one-player games\". They may be modeled using similar tools within the related disciplines of decision theory, operations research, and areas of artificial intelligence, particularly AI planning (with uncertainty) and multi-agent system. Although these fields may have different motivators, the mathematics involved are substantially the same, e.g. using Markov decision processes (MDP).\nStochastic outcomes can also be modeled in terms of game theory by adding a randomly acting player who makes \"chance moves\" (\"moves by nature\"). This player is not typically considered a third player in what is otherwise a two-player game, but merely serves to provide a roll of the dice where required by the game.\nFor some problems, different approaches to modeling stochastic outcomes may lead to different solutions. For example, the difference in approach between MDPs and the minimax solution is that the latter considers the worst-case over a set of adversarial moves, rather than reasoning in expectation about these moves given a fixed probability distribution. The minimax approach may be advantageous where stochastic models of uncertainty are not available, but may also be overestimating extremely unlikely (but costly) events, dramatically swaying the strategy in such scenarios if it is assumed that an adversary can force such an event to happen. (See Black swan theory for more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.)\nGeneral models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability (of moves by other players) have also been studied. The \"gold standard\" is considered to be partially observable stochastic"}
{"doc_id": "Game theory", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " more discussion on this kind of modeling issue, particularly as it relates to predicting and limiting losses in investment banking.)\nGeneral models that include all elements of stochastic outcomes, adversaries, and partial or noisy observability (of moves by other players) have also been studied. The \"gold standard\" is considered to be partially observable stochastic game (POSG), but few realistic problems are computationally feasible in POSG representation.\n\nMetagames\nThese are games the play of which is the development of the rules for another game, the target or subject game. Metagames seek to maximize the utility value of the rule set developed. The theory of metagames is related to mechanism design theory.\nThe term metagame analysis is also used to refer to a practical approach developed by Nigel Howard, whereby a situation is framed as a strategic game in which stakeholders try to realize their objectives by means of the options available to them. Subsequent developments have led to the formulation of confrontation analysis.\n\nMean field game theory\nMean field game theory is the study of strategic decision making in very large populations of small interacting agents. This class of problems was considered in the economics literature by Boyan Jovanovic and Robert W. Rosenthal, in the engineering literature by Peter E. Caines, and by mathematicians Pierre-Louis Lions and Jean-Michel Lasry.\n\nRepresentation of games\nThe games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the players of the game, the information and actions available to each player at each decision point, and the payoffs for each outcome. (Eric Rasmusen refers to these four \"essential elements\" by the acronym \"PAPI\".) A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game—a stable state in which either one outcome occurs or a set of outcomes occur with known probability.\nMost cooperative games are presented in the characteristic function form, while the extensive and the normal forms are used to define noncooperative games.\n\nExtensive form\nThe extensive form can be used to formalize games with a time sequencing of moves. Extensive form games can be visualized using game trees (as pictured here). Here each vertex (or node) represents a point of choice for a player. The player is specified by a"}
{"doc_id": "Game theory", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " noncooperative games.\n\nExtensive form\nThe extensive form can be used to formalize games with a time sequencing of moves. Extensive form games can be visualized using game trees (as pictured here). Here each vertex (or node) represents a point of choice for a player. The player is specified by a number listed by the vertex. The lines out of the vertex represent a possible action for that player. The payoffs are specified at the bottom of the tree. The extensive form can be viewed as a multi-player generalization of a decision tree. To solve any extensive form game, backward induction must be used. It involves working backward up the game tree to determine what a rational player would do at the last vertex of the tree, what the player with the previous move would do given that the player with the last move is rational, and so on until the first vertex of the tree is reached.\nThe game pictured consists of two players.  The way this particular game is structured (i.e., with sequential decision making and perfect information), Player 1 \"moves\" first by choosing either F or U (fair or unfair). Next in the sequence, Player 2, who has now observed Player 1's move, can choose to play either A or R  (accept or reject). Once Player 2 has made their choice, the game is considered finished and each player gets their respective payoff, represented in the image as two numbers, where the first number represents Player 1's payoff, and the second number represents Player 2's payoff.  Suppose that Player 1 chooses U and then Player 2 chooses A: Player 1 then gets a payoff of \"eight\" (which in real-world terms can be interpreted in many ways, the simplest of which is in terms of money but could mean things such as eight days of vacation or eight countries conquered or even eight more opportunities to play the same game against other players) and Player 2 gets a payoff of \"two\".\nThe extensive form can also capture simultaneous-move games and games with imperfect information. To represent it, either a dotted line connects different vertices to represent them as being part of the same information set (i.e. the players do not know at which point they are), or a closed line is drawn around them. (See example in the imperfect information section.)\n\nNormal form\nThe normal (or strategic form) game is usually represented by a matrix which shows the players, strategies, and payoffs (see the example to the right). More generally it can be"}
{"doc_id": "Game theory", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " at which point they are), or a closed line is drawn around them. (See example in the imperfect information section.)\n\nNormal form\nThe normal (or strategic form) game is usually represented by a matrix which shows the players, strategies, and payoffs (see the example to the right). More generally it can be represented by any function that associates a payoff for each player with every possible combination of actions. In the accompanying example there are two players; one chooses the row and the other chooses the column. Each player has two strategies, which are specified by the number of rows and the number of columns. The payoffs are provided in the interior. The first number is the payoff received by the row player (Player 1 in our example); the second is the payoff for the column player (Player 2 in our example). Suppose that Player 1 plays Up and that Player 2 plays Left. Then Player 1 gets a payoff of 4, and Player 2 gets 3.\nWhen a game is presented in normal form, it is presumed that each player acts simultaneously or, at least, without knowing the actions of the other. If players have some information about the choices of other players, the game is usually presented in extensive form.\nEvery extensive-form game has an equivalent normal-form game, however, the transformation to normal form may result in an exponential blowup in the size of the representation, making it computationally impractical.\n\nCharacteristic function form\nIn cooperative game theory the characteristic function lists the payoff of each coalition. The origin of this formulation is in John von Neumann and Oskar Morgenstern's book.\nFormally, a characteristic function is a function \n  \n    \n      \n        v\n        :\n        \n          2\n          \n            N\n          \n        \n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle v:2^{N}\\to \\mathbb {R} }\n  \n from the set of all possible coalitions of players to a set of payments, and also satisfies \n  \n    \n      \n        v\n        (\n        ∅\n        )\n        =\n        0\n      \n    \n    {\\displaystyle v(\\emptyset )=0}\n  \n. The function describes how much collective payoff a set of players can gain by forming a coalition.\n\nAlternative game representations\nAlternative game representation forms are used for some subclasses of games or adjusted to the needs of interdisciplinary research. In addition to classical game representations, some of the alternative representations also encode time related aspects.\n\nGeneral and applied uses\nAs a method of applied mathematics"}
{"doc_id": "Game theory", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " a set of players can gain by forming a coalition.\n\nAlternative game representations\nAlternative game representation forms are used for some subclasses of games or adjusted to the needs of interdisciplinary research. In addition to classical game representations, some of the alternative representations also encode time related aspects.\n\nGeneral and applied uses\nAs a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed in economics to understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers. The first use of game-theoretic analysis was by Antoine Augustin Cournot in 1838 with his solution of the Cournot duopoly. The use of game theory in the social sciences has expanded, and game theory has been applied to political, sociological, and psychological behaviors as well.\nAlthough pre-twentieth-century naturalists such as Charles Darwin made game-theoretic kinds of statements, the use of game-theoretic analysis in biology began with Ronald Fisher's studies of animal behavior during the 1930s. This work predates the name \"game theory\", but it shares many important features with this field. The developments in economics were later applied to biology largely by John Maynard Smith in his 1982 book Evolution and the Theory of Games.\nIn addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and to prescribe such behavior. In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior. Game-theoretic approaches have also been suggested in the philosophy of language and philosophy of science. Game-theoretic arguments of this type can be found as far back as Plato. An alternative version of game theory, called chemical game theory, represents the player's choices as metaphorical chemical reactant molecules called \"knowlecules\".  Chemical game theory then calculates the outcomes as equilibrium solutions to a system of chemical reactions.\n\nDescription and modeling\nThe primary use of game theory is to describe and model how human populations behave. Some scholars believe that by finding the equilibria of games they can predict how actual human populations will behave when confronted with situations analogous to the game being studied. This particular view of game theory has been criticized. It is argued that the assumptions made by game theorists are often violated when applied to real-world situations. Game theorists usually assume players act rationally, but in practice, human rationality and/or behavior often deviates from the model of rationality as used"}
{"doc_id": "Game theory", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " game being studied. This particular view of game theory has been criticized. It is argued that the assumptions made by game theorists are often violated when applied to real-world situations. Game theorists usually assume players act rationally, but in practice, human rationality and/or behavior often deviates from the model of rationality as used in game theory. Game theorists respond by comparing their assumptions to those used in physics. Thus while their assumptions do not always hold, they can treat game theory as a reasonable scientific ideal akin to the models used by physicists. However, empirical work has shown that in some classic games, such as the centipede game, guess 2/3 of the average game, and the dictator game, people regularly do not play Nash equilibria. There is an ongoing debate regarding the importance of these experiments and whether the analysis of the experiments fully captures all aspects of the relevant situation.\nSome game theorists, following the work of John Maynard Smith and George R. Price, have turned to evolutionary game theory in order to resolve these issues. These models presume either no rationality or bounded rationality on the part of players. Despite the name, evolutionary game theory does not necessarily presume natural selection in the biological sense. Evolutionary game theory includes both biological as well as cultural evolution and also models of individual learning (for example, fictitious play dynamics).\n\nPrescriptive or normative analysis\nSome scholars see game theory not as a predictive tool for the behavior of human beings, but as a suggestion for how people ought to behave. Since a strategy, corresponding to a Nash equilibrium of a game constitutes one's best response to the actions of the other players – provided they are in (the same) Nash equilibrium – playing a strategy that is part of a Nash equilibrium seems appropriate. This normative use of game theory has also come under criticism.\n\nEconomics\nGame theory is a major method used in mathematical economics and business for modeling competing behaviors of interacting agents. Applications include a wide array of economic phenomena and approaches, such as auctions, bargaining, mergers and acquisitions pricing, fair division, duopolies, oligopolies, social network formation, agent-based computational economics, general equilibrium, mechanism design, and voting systems; and across such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.\nThis research usually focuses on particular sets of strategies known as \"solution concepts\" or \"equilibria\". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the"}
{"doc_id": "Game theory", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " such broad areas as experimental economics, behavioral economics, information economics, industrial organization, and political economy.\nThis research usually focuses on particular sets of strategies known as \"solution concepts\" or \"equilibria\". A common assumption is that players act rationally. In non-cooperative games, the most famous of these is the Nash equilibrium. A set of strategies is a Nash equilibrium if each represents a best response to the other strategies. If all the players are playing the strategies in a Nash equilibrium, they have no unilateral incentive to deviate, since their strategy is the best they can do given what others are doing.\nThe payoffs of the game are generally taken to represent the utility of individual players.\nA prototypical paper on game theory in economics begins by presenting a game that is an abstraction of a particular economic situation. One or more solution concepts are chosen, and the author demonstrates which strategy sets in the presented game are equilibria of the appropriate type. Economists and business professors suggest two primary uses (noted above): descriptive and prescriptive.\n\nManagerial economics\nGame theory also has an extensive use in a specific branch or stream of economics – Managerial Economics. One important usage of it in the field of managerial economics is in analyzing strategic interactions between firms. For example, firms may be competing in a market with limited resources, and game theory can help managers understand how their decisions impact their competitors and the overall market outcomes. Game theory can also be used to analyze cooperation between firms, such as in forming strategic alliances or joint ventures. Another use of game theory in managerial economics is in analyzing pricing strategies. For example, firms may use game theory to determine the optimal pricing strategy based on how they expect their competitors to respond to their pricing decisions. Overall, game theory serves as a useful tool for analyzing strategic interactions and decision making in the context of managerial economics.\n\nBusiness\nThe Chartered Institute of Procurement & Supply (CIPS) promotes knowledge and use of game theory within the context of business procurement. CIPS and TWS Partners have conducted a series of surveys designed to explore the understanding, awareness and application of game theory among procurement professionals. Some of the main findings in their third annual survey (2019) include:\n\napplication of game theory to procurement activity has increased – at the time it was at 19% across all survey respondents\n65% of participants predict that use of game theory applications will grow\n70% of respondents say that they have \"only a basic or a below basic understanding\" of game theory\n20% of participants had"}
{"doc_id": "Game theory", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of game theory to procurement activity has increased – at the time it was at 19% across all survey respondents\n65% of participants predict that use of game theory applications will grow\n70% of respondents say that they have \"only a basic or a below basic understanding\" of game theory\n20% of participants had undertaken on-the-job training in game theory\n50% of respondents said that new or improved software solutions were desirable\n90% of respondents said that they do not have the software they need for their work.\n\nProject management\nSensible decision-making is critical for the success of projects.  In project management, game theory is used to model the decision-making process of players, such as investors, project managers, contractors, sub-contractors, governments and customers.  Quite often, these players have competing interests, and sometimes their interests are directly detrimental to other players, making project management scenarios well-suited to be modeled by game theory.\nPiraveenan (2019) in his review provides several examples where game theory is used to model project management scenarios. For instance, an investor typically has several investment options, and each option will likely result in a different project, and thus one of the investment options has to be chosen before the project charter can be produced. Similarly, any large project involving subcontractors, for instance, a construction project, has a complex interplay between the main contractor (the project manager) and subcontractors, or among the subcontractors themselves, which typically has several decision points. For example, if there is an ambiguity in the contract between the contractor and subcontractor, each must decide how hard to push their case without jeopardizing the whole project, and thus their own stake in it. Similarly, when projects from competing organizations are launched, the marketing personnel have to decide what is the best timing and strategy to market the project, or its resultant product or service, so that it can gain maximum traction in the face of competition. In each of these scenarios, the required decisions depend on the decisions of other players who, in some way, have competing interests to the interests of the decision-maker, and thus can ideally be modeled using game theory.\nPiraveenan summarizes that two-player games are predominantly used to model project management scenarios, and based on the identity of these players, five distinct types of games are used in project management.\n\nGovernment-sector–private-sector games (games that model public–private partnerships)\nContractor–contractor games\nContractor–subcontractor games\nSubcontractor–subcontract"}
{"doc_id": "Game theory", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " predominantly used to model project management scenarios, and based on the identity of these players, five distinct types of games are used in project management.\n\nGovernment-sector–private-sector games (games that model public–private partnerships)\nContractor–contractor games\nContractor–subcontractor games\nSubcontractor–subcontractor games\nGames involving other players\nIn terms of types of games, both cooperative as well as non-cooperative, normal-form as well as extensive-form, and zero-sum as well as non-zero-sum  are used to model various project management scenarios.\n\nPolitical science\nThe application of game theory to political science is focused in the overlapping areas of fair division, political economy, public choice, war bargaining, positive political theory, and social choice theory. In each of these areas, researchers have developed game-theoretic models in which the players are often voters, states, special interest groups, and politicians.\nEarly examples of game theory applied to political science are provided by Anthony Downs. In his 1957 book An Economic Theory of Democracy, he applies the Hotelling firm location model to the political process. In the Downsian model, political candidates commit to ideologies on a one-dimensional policy space. Downs first shows how the political candidates will converge to the ideology preferred by the median voter if voters are fully informed, but then argues that voters choose to remain rationally ignorant which allows for candidate divergence. Game theory was applied in 1962 to the Cuban Missile Crisis during the presidency of John F. Kennedy.\nIt has also been proposed that game theory explains the stability of any form of political government.  Taking the simplest case of a monarchy, for example, the king, being only one person, does not and cannot maintain his authority by personally exercising physical control over all or even any significant number of his subjects.  Sovereign control is instead explained by the recognition by each citizen that all other citizens expect each other to view the king (or other established government) as the person whose orders will be followed.  Coordinating communication among citizens to replace the sovereign is effectively barred, since conspiracy to replace the sovereign is generally punishable as a crime.  Thus, in a process that can be modeled by variants of the prisoner's dilemma, during periods of stability no citizen will find it rational to move to replace the sovereign, even if all the citizens know they would be better off if they were all to act collectively.\nA game-theoretic explanation for democratic peace is that public and open debate in democracies sends clear and reliable information regarding their"}
{"doc_id": "Game theory", "chunk_id": 17, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "'s dilemma, during periods of stability no citizen will find it rational to move to replace the sovereign, even if all the citizens know they would be better off if they were all to act collectively.\nA game-theoretic explanation for democratic peace is that public and open debate in democracies sends clear and reliable information regarding their intentions to other states. In contrast, it is difficult to know the intentions of nondemocratic leaders, what effect concessions will have, and if promises will be kept. Thus there will be mistrust and unwillingness to make concessions if at least one of the parties in a dispute is a non-democracy.\nHowever, game theory predicts that two countries may still go to war even if their leaders are cognizant of the costs of fighting. War may result from asymmetric information; two countries may have incentives to mis-represent the amount of military resources they have on hand, rendering them unable to settle disputes agreeably without resorting to fighting. Moreover, war may arise because of commitment problems: if two countries wish to settle a dispute via peaceful means, but each wishes to go back on the terms of that settlement, they may have no choice but to resort to warfare. Finally, war may result from issue indivisibilities.\nGame theory could also help predict a nation's responses when there is a new rule or law to be applied to that nation. One example is Peter John Wood's (2013) research looking into what nations could do to help reduce climate change. Wood thought this could be accomplished by making treaties with other nations to reduce greenhouse gas emissions. However, he concluded that this idea could not work because it would create a prisoner's dilemma for the nations.\n\nDefence science and technology\nGame theory has been used extensively to model decision-making scenarios relevant to defence applications.  Most studies that has applied game theory in defence settings are concerned with Command and Control Warfare, and can be further classified into studies dealing with (i) Resource Allocation Warfare (ii) Information Warfare (iii) Weapons Control Warfare, and (iv) Adversary Monitoring Warfare.  Many of the problems studied are concerned with sensing and tracking, for example a surface ship trying to track a hostile submarine and the submarine trying to evade being tracked, and the interdependent decision making that takes place with regards to bearing, speed, and the sensor technology activated by both vessels.\nThe tool, for example, automates the transformation of public vulnerability data into models, allowing defenders to synthesize optimal defence strategies through Stackelberg equilibrium analysis. This approach enhances cyber resilience"}
{"doc_id": "Game theory", "chunk_id": 18, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " tracked, and the interdependent decision making that takes place with regards to bearing, speed, and the sensor technology activated by both vessels.\nThe tool, for example, automates the transformation of public vulnerability data into models, allowing defenders to synthesize optimal defence strategies through Stackelberg equilibrium analysis. This approach enhances cyber resilience by enabling defenders to anticipate and counteract attackers’ best responses, making game theory increasingly relevant in adversarial cybersecurity environments.\nHo et al. provide a broad summary of game theory applications in defence, highlighting its advantages and limitations across both physical and cyber domains.\n\nBiology\nUnlike those in economics, the payoffs for games in biology are often interpreted as corresponding to fitness. In addition, the focus has been less on equilibria that correspond to a notion of rationality and more on ones that would be maintained by evolutionary forces. The best-known equilibrium in biology is known as the evolutionarily stable strategy (ESS), first introduced in (Maynard Smith & Price 1973). Although its initial motivation did not involve any of the mental requirements of the Nash equilibrium, every ESS is a Nash equilibrium.\nIn biology, game theory has been used as a model to understand many different phenomena. It was first used to explain the evolution (and stability) of the approximate 1:1 sex ratios. (Fisher 1930) suggested that the 1:1 sex ratios are a result of evolutionary forces acting on individuals who could be seen as trying to maximize their number of grandchildren.\nAdditionally, biologists have used evolutionary game theory and the ESS to explain the emergence of animal communication. The analysis of signaling games and other communication games has provided insight into the evolution of communication among animals. For example, the mobbing behavior of many species, in which a large number of prey animals attack a larger predator, seems to be an example of spontaneous emergent organization. Ants have also been shown to exhibit feed-forward behavior akin to fashion (see Paul Ormerod's Butterfly Economics).\nBiologists have used the game of chicken to analyze fighting behavior and territoriality.\nAccording to Maynard Smith, in the preface to Evolution and the Theory of Games, \"paradoxically, it has turned out that game theory is more readily applied to biology than to the field of economic behaviour for which it was originally designed\". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature.\nOne such phenomenon is known as biological altruism. This is a situation in which an organism appears to act in a way that benefits"}
{"doc_id": "Game theory", "chunk_id": 19, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " is more readily applied to biology than to the field of economic behaviour for which it was originally designed\". Evolutionary game theory has been used to explain many seemingly incongruous phenomena in nature.\nOne such phenomenon is known as biological altruism. This is a situation in which an organism appears to act in a way that benefits other organisms and is detrimental to itself. This is distinct from traditional notions of altruism because such actions are not conscious, but appear to be evolutionary adaptations to increase overall fitness. Examples can be found in species ranging from vampire bats that regurgitate blood they have obtained from a night's hunting and give it to group members who have failed to feed, to worker bees that care for the queen bee for their entire lives and never mate, to vervet monkeys that warn group members of a predator's approach, even when it endangers that individual's chance of survival. All of these actions increase the overall fitness of a group, but occur at a cost to the individual.\nEvolutionary game theory explains this altruism with the idea of kin selection. Altruists discriminate between the individuals they help and favor relatives. Hamilton's rule explains the evolutionary rationale behind this selection with the equation c < b × r, where the cost c to the altruist must be less than the benefit b to the recipient multiplied by the coefficient of relatedness r. The more closely related two organisms are causes the incidences of altruism to increase because they share many of the same alleles. This means that the altruistic individual, by ensuring that the alleles of its close relative are passed on through survival of its offspring, can forgo the option of having offspring itself because the same number of alleles are passed on. For example, helping a sibling (in diploid animals) has a coefficient of 1⁄2, because (on average) an individual shares half of the alleles in its sibling's offspring. Ensuring that enough of a sibling's offspring survive to adulthood precludes the necessity of the altruistic individual producing offspring. The coefficient values depend heavily on the scope of the playing field; for example if the choice of whom to favor includes all genetic living things, not just all relatives, we assume the discrepancy between all humans only accounts for approximately 1% of the diversity in the playing field, a coefficient that was 1⁄2 in the smaller field becomes 0.995. Similarly if it is considered that information other than that of a genetic nature (e.g. epigenetics, religion, science, etc.) persisted through time"}
{"doc_id": "Game theory", "chunk_id": 20, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " accounts for approximately 1% of the diversity in the playing field, a coefficient that was 1⁄2 in the smaller field becomes 0.995. Similarly if it is considered that information other than that of a genetic nature (e.g. epigenetics, religion, science, etc.) persisted through time the playing field becomes larger still, and the discrepancies smaller.\n\nComputer science and logic\nGame theory has come to play an increasingly important role in logic and in computer science. Several logical theories have a basis in game semantics. In addition, computer scientists have used games to model interactive computations. Also, game theory provides a theoretical basis to the field of multi-agent systems.\nSeparately, game theory has played a role in online algorithms; in particular, the k-server problem, which has in the past been referred to as games with moving costs and request-answer games. Yao's principle is a game-theoretic technique for proving lower bounds on the computational complexity of randomized algorithms, especially online algorithms.\nThe emergence of the Internet has motivated the development of algorithms for finding equilibria in games, markets, computational auctions, peer-to-peer systems, and security and information markets. Algorithmic game theory and within it algorithmic mechanism design combine computational algorithm design and analysis of complex systems with economic theory.\nGame theory has multiple applications in the field of artificial intelligence and machine learning. It is often used in developing autonomous systems that can make complex decisions in uncertain environment. Some other areas of application of game theory in AI/ML context are as follows - multi-agent system formation, reinforcement learning, mechanism design etc. By using game theory to model the behavior of other agents and anticipate their actions, AI/ML systems can make better decisions and operate more effectively.\n\nPhilosophy\nGame theory has been put to several uses in philosophy. Responding to two papers by W.V.O. Quine (1960, 1967), Lewis (1969) used game theory to develop a philosophical account of convention. In so doing, he provided the first analysis of common knowledge and employed it in analyzing play in coordination games. In addition, he first suggested that one can understand meaning in terms of signaling games. This later suggestion has been pursued by several philosophers since Lewis. Following Lewis (1969) game-theoretic account of conventions, Edna Ullmann-Margalit (1977) and Bicchieri (2006) have developed theories of social norms that define them as Nash equilibria that result from transforming a mixed-motive game"}
{"doc_id": "Game theory", "chunk_id": 21, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " several philosophers since Lewis. Following Lewis (1969) game-theoretic account of conventions, Edna Ullmann-Margalit (1977) and Bicchieri (2006) have developed theories of social norms that define them as Nash equilibria that result from transforming a mixed-motive game into a coordination game.\nGame theory has also challenged philosophers to think in terms of interactive epistemology: what it means for a collective to have common beliefs or knowledge, and what are the consequences of this knowledge for the social outcomes resulting from the interactions of agents. Philosophers who have worked in this area include Bicchieri (1989, 1993), Skyrms (1990), and Stalnaker (1999).\nThe synthesis of game theory with ethics was championed by R. B. Braithwaite. The hope was that rigorous mathematical analysis of game theory might help formalize the more imprecise philosophical discussions. However, this expectation was only materialized to a limited extent.\nIn ethics, some (most notably David Gauthier, Gregory Kavka, and Jean Hampton)  authors have attempted to pursue Thomas Hobbes' project of deriving morality from self-interest. Since games like the prisoner's dilemma present an apparent conflict between morality and self-interest, explaining why cooperation is required by self-interest is an important component of this project. This general strategy is a component of the general social contract view in political philosophy (for examples, see Gauthier (1986) and Kavka (1986)).\nOther authors have attempted to use evolutionary game theory in order to explain the emergence of human attitudes about morality and corresponding animal behaviors. These authors look at several games including the prisoner's dilemma, stag hunt, and the Nash bargaining game as providing an explanation for the emergence of attitudes about morality (see, e.g., Skyrms (1996, 2004) and Sober and Wilson (1998)).\n\nEpidemiology\nSince the decision to take a vaccine for a particular disease is often made by individuals, who may consider a range of factors and parameters in making this decision (such as the incidence and prevalence of the disease, perceived and real risks associated with contracting the disease,  mortality rate, perceived and real risks associated with vaccination, and financial cost of vaccination), game theory has been used to model and predict vaccination uptake in a society.\n\nWell known examples of games\nPrisoner's dilemma\nWilliam Poundstone described the game in his 1993 book Prison"}
{"doc_id": "Game theory", "chunk_id": 22, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " risks associated with contracting the disease,  mortality rate, perceived and real risks associated with vaccination, and financial cost of vaccination), game theory has been used to model and predict vaccination uptake in a society.\n\nWell known examples of games\nPrisoner's dilemma\nWilliam Poundstone described the game in his 1993 book Prisoner's Dilemma:\nTwo members of a criminal gang, A and B, are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communication with their partner. The principal charge would lead to a sentence of ten years in prison; however, the police do not have the evidence for a conviction. They plan to sentence both to two years in prison on a lesser charge but offer each prisoner a Faustian bargain: If one of them confesses to the crime of the principal charge, betraying the other, they will be pardoned and free to leave while the other must serve the entirety of the sentence instead of just two years for the lesser charge.\nThe dominant strategy (and therefore the best response to any possible opponent strategy), is to betray the other, which aligns with the sure-thing principle. However, both prisoners staying silent would yield a greater reward for both of them than mutual betrayal.\n\nBattle of the sexes\nThe \"battle of the sexes\" is a term used to describe the perceived conflict between men and women in various areas of life, such as relationships, careers, and social roles. This conflict is often portrayed in popular culture, such as movies and television shows, as a humorous or dramatic competition between the genders. This conflict can be depicted in a game theory framework. This is an example of non-cooperative games.\nAn example of the \"battle of the sexes\" can be seen in the portrayal of relationships in popular media, where men and women are often depicted as being fundamentally different and in conflict with each other. For instance, in some romantic comedies, the male and female protagonists are shown as having opposing views on love and relationships, and they have to overcome these differences in order to be together.\nIn this game, there are two pure strategy Nash equilibria: one where both the players choose the same strategy and the other where the players choose different options. If the game is played in mixed strategies, where each player chooses their strategy randomly, then there is an infinite number of Nash equilibria. However, in the context of the \"battle of the sexes\" game, the assumption is usually made that the game is played in pure strategies.\n\nUltimatum game\n"}
{"doc_id": "Game theory", "chunk_id": 23, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ". If the game is played in mixed strategies, where each player chooses their strategy randomly, then there is an infinite number of Nash equilibria. However, in the context of the \"battle of the sexes\" game, the assumption is usually made that the game is played in pure strategies.\n\nUltimatum game\nThe ultimatum game is a game that has become a popular instrument of economic experiments. An early description is by Nobel laureate John Harsanyi in 1961.\nOne player, the proposer, is endowed with a sum of money. The proposer is tasked with splitting it with another player, the responder (who knows what the total sum is). Once the proposer communicates his decision, the responder may accept it or reject it. If the responder accepts, the money is split per the proposal; if the responder rejects, both players receive nothing.  Both players know in advance the consequences of the responder accepting or rejecting the offer. The game demonstrates how social acceptance, fairness, and generosity influence the players decisions.\nUltimatum game has a variant, that is the dictator game. They are mostly identical, except in dictator game the responder has no power to reject the proposer's offer.\n\nTrust game\nThe Trust Game is an experiment designed to measure trust in economic decisions. It is also called \"the investment game\" and is designed to investigate trust and demonstrate its importance rather than \"rationality\" of self-interest. The game was designed by Berg Joyce, John Dickhaut and Kevin McCabe in 1995.\nIn the game, one player (the investor) is given a sum of money and must decide how much of it to give to another player (the trustee). The amount given is then tripled by the experimenter. The trustee then decides how much of the tripled amount to return to the investor. If the recipient is completely self interested, then he/she should return nothing. However that is not true as the experiment conduct. The outcome suggest that people are willing to place a trust, by risking some amount of money, in the belief that there would be reciprocity.\n\nCournot Competition\nThe Cournot competition model involves players choosing quantity of a homogenous product to produce independently and simultaneously, where marginal cost can be different for each firm and the firm's payoff is profit. The production costs are public information and the firm aims to find their profit-maximizing quantity based on what they believe the other firm will produce and behave like monopolies. In this game firms want to produce"}
{"doc_id": "Game theory", "chunk_id": 24, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to produce independently and simultaneously, where marginal cost can be different for each firm and the firm's payoff is profit. The production costs are public information and the firm aims to find their profit-maximizing quantity based on what they believe the other firm will produce and behave like monopolies. In this game firms want to produce at the monopoly quantity but there is a high incentive to deviate and produce more, which decreases the market-clearing price. For example, firms may be tempted to deviate from the monopoly quantity if there is a low monopoly quantity and high price, with the aim of increasing production to maximize  profit. However this option does not provide the highest payoff, as a firm's ability to maximize profits depends on its market share and the elasticity of the market demand. The Cournot equilibrium is reached when each firm operates on their reaction function with no incentive to deviate, as they have the best response based on the other firms output. Within the game, firms reach the Nash equilibrium when the Cournot equilibrium is achieved.\n\nBertrand Competition\nThe Bertrand competition assumes homogenous products and a constant marginal cost and players choose the prices. The equilibrium of price competition is where the price is equal to marginal costs, assuming complete information about the competitors' costs. Therefore, the firms have an incentive to deviate from the equilibrium because a homogenous product with a lower price will gain all of the market share, known as a cost advantage.\n\nIn popular culture\nBased on the 1998 book by Sylvia Nasar, the life story of game theorist and mathematician John Nash was turned into the 2001 biopic A Beautiful Mind, starring Russell Crowe as Nash.\nThe 1959 military science fiction novel Starship Troopers by Robert A. Heinlein mentioned \"games theory\" and \"theory of games\". In the 1997 film of the same name, the character Carl Jenkins referred to his military intelligence assignment as being assigned to \"games and theory\".\nThe 1964 film Dr. Strangelove satirizes game theoretic ideas about deterrence theory. For example, nuclear deterrence depends on the threat to retaliate catastrophically if a nuclear attack is detected. A game theorist might argue that such threats can fail to be credible, in the sense that they can lead to subgame imperfect equilibria. The movie takes this idea one step further, with the Soviet Union irrevocably committing to a catastrophic nuclear response without making the threat public.\nThe 1980s power pop band Game Theory was"}
{"doc_id": "Game theory", "chunk_id": 25, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " that such threats can fail to be credible, in the sense that they can lead to subgame imperfect equilibria. The movie takes this idea one step further, with the Soviet Union irrevocably committing to a catastrophic nuclear response without making the threat public.\nThe 1980s power pop band Game Theory was founded by singer/songwriter Scott Miller, who described the band's name as alluding to \"the study of calculating the most appropriate action given an adversary ... to give yourself the minimum amount of failure\".\nLiar Game, a 2005 Japanese manga and 2007 television series, presents the main characters in each episode with a game or problem that is typically drawn from game theory, as demonstrated by the strategies applied by the characters.\nThe 1974 novel Spy Story by Len Deighton explores elements of game theory in regard to cold war army exercises.\nThe 2008 novel The Dark Forest by Liu Cixin explores the relationship between extraterrestrial life, humanity, and game theory.\nJoker, the prime antagonist in the 2008 film The Dark Knight presents game theory concepts—notably the prisoner's dilemma in a scene where he asks passengers in two different ferries to bomb the other one to save their own.\nIn the 2018 film Crazy Rich Asians, the female lead Rachel Chu is a professor of economics and game theory at New York University. At the beginning of the film she is seen in her NYU classroom playing a game of poker with her teaching assistant and wins the game by bluffing; then in the climax of the film, she plays a game of mahjong with her boyfriend's disapproving mother Eleanor, losing the game to Eleanor on purpose but winning her approval as a result.\nIn the 2017 film Molly's Game, Brad, an inexperienced poker player, makes an irrational betting decision without realizing and causes his opponent Harlan to deviate from his Nash Equilibrium strategy, resulting in a significant loss when Harlan loses the hand.\n\nSee also\nApplied ethics – Practical application of moral considerations\nBandwidth-sharing game – Type of resource allocation game\nChainstore paradox – Game theory paradox\nCollective intentionality – Intentionality that occurs when two or more individuals undertake a task together\nCore (game theory) – Set in game theory\nGlossary of game theory\nIntra-household bargaining\nKingmaker scenario – Endgame situation in game theory\nLaw and economics – Application of economic theory to analysis of legal systems\nMutual assured destruction – Doctrine of military strategyPages displaying"}
{"doc_id": "Game theory", "chunk_id": 26, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " a task together\nCore (game theory) – Set in game theory\nGlossary of game theory\nIntra-household bargaining\nKingmaker scenario – Endgame situation in game theory\nLaw and economics – Application of economic theory to analysis of legal systems\nMutual assured destruction – Doctrine of military strategyPages displaying short descriptions of redirect targets\nOutline of artificial intelligence – Overview of and topical guide to artificial intelligence\nParrondo's paradox – Paradox in game theory\nPrecautionary principle – Risk management strategy\nQuantum refereed game\nRisk management – Identification, evaluation and control of risks\nSelf-confirming equilibrium\nTragedy of the commons – Self-interests causing depletion of a shared resource\nTraveler's dilemma – Non-zero-sum game thought experiment\nWilson doctrine (economics) – Argument in economic theory\nCompositional game theory\nLists\n\nList of cognitive biases\nList of emerging technologies\nList of games in game theory\n\nNotes"}
{"doc_id": "Gender digital divide", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Gender digital divide refers to the inequalities in access to, use of, and participation in digital technologies and the technology sector based on gender. It encompasses disparities in digital skills, internet access, representation in computing and STEM fields, and exposure to gender-biased technologies such as artificial intelligence and voice assistants. The divide is shaped by broader socio-economic, cultural, and educational factors and is more pronounced among women and gender minorities in developing countries, rural areas, and lower-income populations. Despite global efforts to close this gap, significant challenges remain, including patriarchal norms, safety concerns, affordability issues, and limited access to digital education. Addressing the gender digital divide is considered essential for achieving broader gender equality, inclusive economic development, and equitable digital transformation.\n\nBackground\nEducation systems are increasingly trying to ensure equitable, inclusive, and high-quality digital skills, education, and training. Though digital skills open pathways to further learning and skills development, women and girls are still being left behind in digital skills education. Globally, digital skills gender gaps are growing, despite at least a decade of national and international efforts to close them. The economic and political interests of its indicators have also been questioned.\n\nDigital skills gap\nWomen are less likely to know how to operate a smartphone, navigate the internet, use social media and understand how to safeguard information in digital mediums (abilities that underlie life and work tasks and are relevant to people of all ages) worldwide. There is a gap from the lowest skill proficiency levels, such as using apps on a mobile phone, to the most advanced skills like coding computer software to support the analysis of large data sets.\nWomen in numerous countries are 25% less likely than men to know how to leverage ICT for basic purposes, such as using simple arithmetic formulas in a spreadsheet. UNESCO estimates that men are around four times more likely than women to have advanced ICT skills such as the ability to programme computers. Across G20 countries 7% of ICT patents are generated by women, and the global average is at 2%. Recruiters for technology companies in Silicon Valley estimate that the applicant pool for technical jobs in artificial intelligence (AI) and data science is often less than 1% female. To highlight this difference, in 2009 there were 2.5 million college-educated women working in STEM compared to 6.7 million men. The total workforce at the time was 49% women and 51% men which highlights the evident gap.\nWhile the gender gap in digital skills is evident across regional boundaries and income levels, it is"}
{"doc_id": "Gender digital divide", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "2009 there were 2.5 million college-educated women working in STEM compared to 6.7 million men. The total workforce at the time was 49% women and 51% men which highlights the evident gap.\nWhile the gender gap in digital skills is evident across regional boundaries and income levels, it is more severe for women who are older, less educated, poor, or living in rural areas and developing countries. Making women much less likely to graduate in any field of STEM compared to their male counterpart. Digital skills gap intersects with issues of poverty and educational access.\n\nRoot causes\nWomen and girls who live in patriarchal cultures may struggle more than those who do not to access public ICT facilities. Due to the social challenges these cultures create as well reinforces the struggles and creates an overlap. They may struggle to access these facilities due to unsafe roads, limits on their freedom of movement, or because the facilities themselves are considered unsuitable for women. They may also lack financial freedom creating a large barrier to purchase any form of technology or have any type of internet connection.  If they do have access to technology of the internet, it is usually controlled by the men in their households and limit their content selection to content focused on women's appearances, dating, or the role of motherhood.   Fears concerning safety and harassment (both online and offline) also inhibit many women and girls from benefiting from or even wanting to use ICTs.\nIn many contexts, women and girls face concerns of physical violence if they own or borrow digital devices, which in some cases leads to their using the devices in secret, making them more vulnerable to online threats and making it difficult to gain digital skills.\nThe stereotype of technology as a male domain is common in many contexts and affect girls' confidence in their digital skills from a young age. In OECD countries, 0.5% of girls aspire towards ICT-related careers at age 15, versus 5% of boys. This was not always the case. Early decades of computing saw a much larger presence of women. Acting as programmers during World War II, they held highly valued positions. Women's contributions, however, have been largely obscured due to how the history is told. Focusing on the infrastructure and hardware of digital technologies development has placed men at the forefront of its history. Post war computer manufacturers sought to commercialize the machines and opened up a new form of labor market. This post war market utilized discriminatory criteria measures that women were no longer able to meet due to societal, educational, and labor expectations. Managers"}
{"doc_id": "Gender digital divide", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " infrastructure and hardware of digital technologies development has placed men at the forefront of its history. Post war computer manufacturers sought to commercialize the machines and opened up a new form of labor market. This post war market utilized discriminatory criteria measures that women were no longer able to meet due to societal, educational, and labor expectations. Managers of early technology firms allowed women well-suited for programming because of stereotypes characterizing them as meticulous and good at following step-by-step directions. Women, including many women of color, flocked to jobs in the computer industry because it was seen as more meritocratic than other fields. As computers became integrated into people's daily life, it was noticed that programmers had influence. Consequently, women were pushed out and the field became more male-dominated.\nIn developed countries like Canada, the digital divide can exist due to factors of lacking digital literacy which prevents individuals from understanding how to use and what to do with technology. Other research on the gender divide in Canada has found contrasting results, showing a potential suggestion to the closing of the gap in more developed countries over the last couple years in relation to access to the internet and technology as a whole. However, the amount of activity online is found to be higher for men than women.  When looking at issues regarding professional sectors the IT sector in Canada remains male-dominated. The presence of women in field with technology has increased significantly but in specific high-paying technological fields like computer science it is declining.\n\nAccess divide versus skills divide\nDue to the declining price of connectivity and hardware, skills deficits have exceeded barriers of access as the primary contributor to the gender digital divide. For years, the divide was assumed to be symptomatic of technical challenges. It was thought that women would catch up with men when the world had cheaper devices and lower connectivity prices, due to the limited purchasing power and financial independence of women compared with men in countries with a patriarchal culture. The cost of ICT access remains an issue and is surpassed by educational gaps. For example, the gender gap in internet penetration is around 17% in the Arab States and the Asia and Pacific region, whereas the gender gap in ICT skills is as high as 25% in some Asian and Middle Eastern countries. In sub-Saharan Africa (SSA), the Internet penetration rate in 2019 was 33.8 percent for men and 22.6 percent for women. The Internet user gender gap was 20.7 percent in 2013 and up to 37 percent in 2019. The Internet penetration rate in 2019 was 33"}
{"doc_id": "Gender digital divide", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A), the Internet penetration rate in 2019 was 33.8 percent for men and 22.6 percent for women. The Internet user gender gap was 20.7 percent in 2013 and up to 37 percent in 2019. The Internet penetration rate in 2019 was 33.8 percent for men and 22.6 percent for women. \nOther research has shown more factors that contribute to access of the internet. In the United States, it was found that individuals who has lower than high school education and made less than $30k a year has the lowest access to the internet. They found that the most consistent results form various research is that individuals with the lowest education and lowest income had the lowest access to the internet. When looking at differences with gender, inconsistent results were found. When large divides were found between men and women' access to the internet, socioeconomic factors were the cause. Overall, the gender divide has found to be largely insignificant in countries like the United States and in Canada. \nSSA has one of the widest mobile gender gaps in the world where over 74 million women are not connected. The gender gap in mobile ownership was 13 percent, a reduction from 14 percent in 2018; however, in low- and middle-income countries it remains substantial with fewer women than men accessing the Internet on a mobile device. Furthermore, women are less likely to use digital services or mobile Internet and tend to use different mobile services than men.\nMany people have access to affordable devices and broadband networks, but do not have the requisite skills to take advantage of this technology to improve their lives. In Brazil, lack of skills (rather than cost of access) was found to be the primary reason low-income groups are not using the internet. In India, where lack of skills and lack of need for the internet were the primary limiting factors across all income groups.\nLack of understanding, interest or time is a bigger issue than affordability or availability as the reason for not using the internet. Even though skills deficits prevent both men and women from using digital technologies, they tend to be more severe for women. In a study conducted across 10 low- and middle-income countries, women were 1.6 times more likely than men to report lack of skills as a barrier to internet use. Women are also more likely to report that they do not see a reason to access and use ICT. Interest and perception of need are related to skills, as people who have little experience with or understanding of ICTs"}
{"doc_id": "Gender digital divide", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " were 1.6 times more likely than men to report lack of skills as a barrier to internet use. Women are also more likely to report that they do not see a reason to access and use ICT. Interest and perception of need are related to skills, as people who have little experience with or understanding of ICTs tend to underestimate their benefits and utility.\n\nRelationship between digital skills and gender equality\nIn many societies, gender equality does not translate into digital realms and professions. The persistence of growing digital skills gender gaps, even in countries that rank at the top of the World Economic Forum's global gender gap index (reflecting strong gender equality), demonstrates a need for interventions that cultivate the digital skills of women and girls.\nFor most countries, the primary barriers for women regarding access to digital technology are cost/unaffordability followed by illiteracy and lack of digital skills. For instance, in Africa 65.4 percent of people aged 15 and older are illiterate, compared to the global average rate of 86.4 percent.\n\nGender digital divide and COVID-19\nAfrica\nThe COVID-19 pandemic and the measures taken by governments on social distancing and mobility restrictions have contributed to boosting the use of digital technology to bridge some of the physical access gaps. However, the rapid proliferation of digital tools and services stands in stark contrast to the many systemic and structural barriers to technology access and adoption that many people in rural Africa still face. Gender inequalities, intersecting with and compounded by other social differences such as class, race, age, (dis)ability, etc., shape the extent to which different rural women and men are able not only to access but also use and benefit from these new technologies and ways of delivering information and services.\nBeside the potential of digital tools and applications, the COVID-19 crisis has evidenced the existing digital divide and especially the gender gap. It is estimated that 3.6 billion individuals are not connected to the Internet across the globe, including 900 million in Africa. Only 27 percent of women in Africa have access to the Internet and only 15 percent of them can afford to use it.\n\nGender-responsive digitalization in COVID-19 response\nAccording to a study by FAO, gender-responsive digitalization in COVID-19 response and beyond could include:\n\nImprove the availability of sex-disaggregated data and gender-related statistics that capture digital gender gaps in rural areas to better inform policy and business decisions\nPromote an enabling environment that includes gender-responsive policies, strategies and initiatives\nLeverage digital solutions to"}
{"doc_id": "Gender digital divide", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "-responsive digitalization in COVID-19 response and beyond could include:\n\nImprove the availability of sex-disaggregated data and gender-related statistics that capture digital gender gaps in rural areas to better inform policy and business decisions\nPromote an enabling environment that includes gender-responsive policies, strategies and initiatives\nLeverage digital solutions to deliver COVID-19 relief measures targeted to rural women and girls and facilitate their access to social protection services and alternative income-generation opportunities.\nDedicate funds for digital acceleration to support women-led enterprises.\nImprove the national broadband coverage to ensure affordable, accessible and reliable infrastructure for inclusive digital transformation\nInvest in the protection of Internet users, especially illiterate and vulnerable ones, against frauds and abuses as cybercrime, including sexual harassment. According to UN Women, these crimes have reportedly increased during the COVID-19 pandemic, especially toward women and girls.\n\nBenefits of digital empowerment\nHelping women and girls develop digital skills means stronger women, stronger families, stronger communities, stronger economies and better technology. Digital skills are recognized to be essential life skills required for full participation in society. The main benefits for acquiring digital skills are they:\n\nFacilitate entry into the labour market;\nAssist women's safety both online and offline;\nEnhance women's community and political engagement;\nBring economic benefits to women and society;\nEmpower women to help steer the future of technology and gender equality;\nAccelerate progress towards international goals.\nDigitalization can potentially pave the way for improving the efficiency and functioning of food systems, which in turn can have positive impacts on the livelihoods of women and men farmers and agripreneurs, for example, through the creation of digital job opportunities for young women and men in rural areas.\n\nClosing the digital skills gender gap\nThe digital divide has begun at earlier ages as young adults have lived out their childhoods with personal computers. This has made intervention to prevent further gender divides in the digital realm needed in more early education. Increasing girls' and women's digital skills involves early, varied and sustained exposure to digital technologies. Interventions should not be limited to formal education settings, they should reflect a multifaceted approach, enabling women and girls to acquire skills in a variety of formal and informal contexts (at home, in school, in their communities and in the workplace). The digital divide cuts across age groups, therefore solutions need to assume a lifelong learning orientation. The technological changes adds impetus to the 'across life' perspective, as skills learned today will not necessarily be relevant in 5 or 10 years. Digital skills"}
{"doc_id": "Gender digital divide", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", in school, in their communities and in the workplace). The digital divide cuts across age groups, therefore solutions need to assume a lifelong learning orientation. The technological changes adds impetus to the 'across life' perspective, as skills learned today will not necessarily be relevant in 5 or 10 years. Digital skills require regular updating, to prevent women and girls fall further behind.\nWomen and girls digital skills development are strengthened by:\n\nAdopting sustained, varied and life-wide approaches;\nEstablishing incentives, targets and quotas;\nEmbedding ICT in formal education;\nSupporting engaging experiences;\nEmphasising meaningful use and tangible benefits;\nEncouraging collaborative and peer learning;\nCreating safe spaces and meet women where they are;\nExamining exclusionary practices and language;\nRecruiting and training gender-sensitive teachers;\nPromoting role models and mentors;\nBringing parents on board;\nLeveraging community connections and recruiting allies;\nSupporting technology autonomy and women's digital rights.\nAccording to the Food and Agriculture Organization (FAO), there are seven success factors to empowering rural women through ICTs:\n\nAdapt content so that it is meaningful for them.\nCreate a safe environment for them to share and learn.\nBe gender-sensitive.\nProvide them with access and tools for sharing\nBuild partnerships.\nProvide the right blend of technologies.\nEnsure sustainability\nThe regulatory role of governments (at local, national, regional, and international levels) is crucial in addressing infrastructural barriers, harmonizing and making the regulatory environment inclusive and gender-responsive, and in protecting all stakeholders from fraud and crime.\nInitiatives targeted at boosting women's representation in the technology industry are essential to closing the digital skills gender divide. Mentorship programs, networking chances, and scholarships for women seeking jobs in technology are examples of such initiatives. These efforts can help create more inclusive workplaces that respect diversity and promote creativity by boosting the presence of women in the technology industry.\n\nMentorship programs can be especially beneficial in assisting women in the technology industry. These initiatives allow women to network with experienced professionals who can give advice and support as they advance in their jobs. According to research, mentoring programs can help increase women's confidence and feeling of connection in the workplace, which can improve job happiness and professional growth opportunities.\nWomen in technology can benefit from networking chances as well. Women can benefit from networking by developing connections with other professionals in their industry, which can lead to new employment possibilities and collaborations. Networking can also assist women in staying current with the newest trends and technologies in their profession.\nScholarships can also be a"}
{"doc_id": "Gender digital divide", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " growth opportunities.\nWomen in technology can benefit from networking chances as well. Women can benefit from networking by developing connections with other professionals in their industry, which can lead to new employment possibilities and collaborations. Networking can also assist women in staying current with the newest trends and technologies in their profession.\nScholarships can also be a good method to help women who want to work in technology. Scholarships can assist in covering the costs of education and training, making it simpler for women to gain the skills and knowledge required to thrive in the technology industry. Scholarships can also help to increase gender variety in the technology industry by encouraging more women to enter the profession.\nOverall, initiatives targeted at boosting women's representation in the technology industry are essential to closing the digital skills gender divide. We can build more inclusive and innovative environments that help everyone if we assist women in technology.\n\nFemale gendering of AI technologies\nMen continue to dominate the technology space, and the disparity serves to perpetuate gender inequalities, as unrecognized bias is replicated and built into algorithms and artificial intelligence (AI).\nLimited participation of women and girls in the technology sector can stem outward replicating existing gender biases and creating new ones. Women's participation in the technology sector is constrained by unequal digital skills education and training. Learning and confidence gaps that arise as early as primary school amplify as girls move through education, therefore by the time they reach higher education only a fraction pursue advanced-level studies in computer science and related information and communication technology (ICT) fields. Divides grow greater in the transition from education to work. The International Telecommunication Union (ITU) estimates that only 6% of professional software developers are women.\nTechnologies generated by male-dominated teams and companies often reflect gender biases. Establishing balance between men and women in the technology sector will help lay foundations for the creation of technology products that better reflect and ultimately accommodate the rich diversity of human societies. For instance AI, which is a branch of the technology sector that wields influence over people's lives. Today, AI curates information shown by internet search engines, determines medical treatments, makes loan decisions, ranks job applications, translates languages, places ads, recommends prison sentences, influences parole decisions, calibrates lobbying and campaigning efforts, intuits tastes and preferences, and decides who qualifies for insurance, among other tasks. Despite the growing influence of this technology, women make up just 12% of AI researchers. Closing the gender divide begins with establishing more inclusive and gender-equal digital skills education and training.\n\nDigital assistants\nDigital assistants encompass a range of internet-connected"}
{"doc_id": "Gender digital divide", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " tastes and preferences, and decides who qualifies for insurance, among other tasks. Despite the growing influence of this technology, women make up just 12% of AI researchers. Closing the gender divide begins with establishing more inclusive and gender-equal digital skills education and training.\n\nDigital assistants\nDigital assistants encompass a range of internet-connected technologies that support users in various ways. When interacting with digital assistants, users are not restricted to a narrow range of input commands, but are encouraged to make queries using whichever inputs seem most appropriate or natural, whether they are typed or spoken. Digital assistants seek to enable and sustain more human-like interactions with technology. Digital assistants can include: voice assistants, chatbots, and virtual agents.\n\nFeminization of voice assistants\nVoice assistants have become central to technology platforms and, in many countries, to day-to-day life. Between 2008 and 2018, the frequency of voice-based internet search queries increased 35 times and account for close to one fifth of mobile internet searches (a figure that is projected to increase to 50% by 2020). Voice assistants now manage upwards of 1 billion tasks per month, from the mundane (changing a song) to the essential (contacting emergency services).\nToday, most leading voice assistants are exclusively female or female by default, both in name and in sound of voice. Amazon has Alexa (named for the ancient library in Alexandria), Microsoft has Cortana (named for a synthetic intelligence in the video game Halo that projects itself as a sensuous unclothed woman), and Apple has Siri (coined by the Norwegian co-creator of the iPhone 4S and meaning 'beautiful woman who leads you to victory' in Norse). While Google's voice assistant is simply Google Assistant and sometimes referred to as Google Home, its voice is female.\nThe trend to feminize assistants occurs in a context in which there is a growing gender imbalance in technology companies, such that men commonly represent two thirds to three quarters of a firm's total workforce. Companies like Amazon and Apple have cited academic work demonstrating that people prefer a female voice to a male voice, justifying the decision to make voice assistants female. Further research shows that consumers strongly dislike voice assistants without clear gender markers. Gender bias is thus \"hard-coded\" into technology. Companies often cite research showing that customers want their digital assistants to sound like women, justifying the choice with the profit motive. However, research on the topic is mixed, with studies showing that in some contexts male choices may be preferred. For example, BMW was forced"}
{"doc_id": "Gender digital divide", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Gender bias is thus \"hard-coded\" into technology. Companies often cite research showing that customers want their digital assistants to sound like women, justifying the choice with the profit motive. However, research on the topic is mixed, with studies showing that in some contexts male choices may be preferred. For example, BMW was forced to recall a female-voiced navigation system on its 5 Series cars in the late 1990s after being flooded with calls from German men who reportedly \"refused to take directions from a woman\".\nResearchers who specialize in human–computer interaction have recognized that both men and women tend to characterize female voices as more helpful. The perception may have roots in traditional social norms around women as nurturers (mothers often take on – willingly or not – significantly more care than fathers) and other socially constructed gender biases that predate the digital era.\n\nSee also\nAI alignment\nArtificial intelligence detection software\nDigital divide\nGender disparity in computing\nFemale education\nWomen's empowerment\n\nSources\nThis article incorporates text from a free content work. Licensed under CC BY-SA 3.0 (license statement/permission). Text taken from Gender-responsive digitalization: A critical component of the COVID-19 response in Africa, FAO, FAO."}
{"doc_id": "Genesis Mission", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Genesis Mission is an initiative launched by the United States federal government to accelerate scientific research through artificial intelligence (AI) technology. Announced in November 2025, the mission aims to encourage innovation in scientific research and industries.\nThe initiative was initiated by the White House, emphasizing its strategic importance for national progress. The Department of Energy highlighted that Genesis will leverage AI computing to solve complex scientific problems.\nUS President Donald Trump announced the mission, noting its role in energy, space, and health sciences. \nThe mission hopes to position the United States as the global leader of the AI revolution.\n\nImpact\nGenesis Mission seeks to create a centralized AI platform designed to accelerate breakthroughs in various fields, including space exploration, healthcare, and national security. The initiative's overarching goal is to harness AI's potential to solve complex global challenges and enhance the nation's technological competitiveness. Through public and private sector collaboration, Genesis Mission 2025 will focus on developing advanced AI models that can support major scientific endeavors, including space exploration, and environmental sustainability.\n\nSee also\nStargate LLC"}
{"doc_id": "Gibberlink", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "GibberLink is an acoustic data transmission project, with an open-source client available on GitHub, in which two conversational AI agents switch from speaking to one another in a Human-listenable language (such as English) to their own unique language that consists of a sound-level protocol after confirming they are both AI agents. The project was created by Anton Pidkuiko and Boris Starkov.\n\nReception\nThe project won the global top prize at the ElevenLabs Worldwide Hackathon. It has also been cited as raising questions around AI ethics and oversight. On February 23, 2025, a YouTube video of two independent conversational ElevenLabs AI agents being prompted to chat about booking a hotel (one as a caller, one as a receptionist) received coverage for going viral. In this video, both agents are prompted to switch to ggwave data-over-sound protocol when they identify the other side as AI, and keep speaking in English otherwise.\n\nSee also\nGibberish\nAcoustic coupler"}
{"doc_id": "GOLOG", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "GOLOG is a high-level logic programming language for the specification and execution of complex actions in dynamical domains. It is based on the situation calculus. It is a first-order logical language for reasoning about action and change. GOLOG was developed at the University of Toronto.\n\nHistory\nThe concept of situation calculus on which the GOLOG programming language is based was first proposed by John McCarthy in 1963.\n\nDescription\nA GOLOG interpreter automatically maintains a direct characterization of the dynamic world being modeled, on the basis of user supplied axioms about preconditions, effects of actions and the initial state of the world. This allows the application to reason about the condition of the world and consider the impacts of different potential actions before focusing on a specific action.\nGolog is a logic programming language and is very different from conventional programming languages. A procedural programming language like C defines the execution of statements in advance. The programmer creates a subroutine which consists of statements, and the computer executes each statement in a linear order. In contrast, fifth-generation programming languages like Golog are working with an abstract model with which the interpreter can generate the sequence of actions. The source code defines the problem and it is up to the solver to find the next action. This approach can facilitate the management of complex problems from the domain of robotics.\nA Golog program defines the state space in which the agent is allowed to operate. A path in the symbolic domain is found with state space search. To speed up the process, Golog programs are realized as hierarchical task networks.\nApart from the original Golog language, there are some extensions available. The ConGolog language provides concurrency and interrupts. Other dialects like IndiGolog and Readylog were created for real time applications in which sensor readings are updated on the fly.\n\nUses\nGolog has been used to model the behavior of autonomous agents. In addition to a logic-based action formalism for describing the environment and the effects of basic actions, they enable the construction of complex actions using typical programming language constructs.\nIt is also used for applications in high level control of robots and industrial processes, virtual agents, discrete event simulation etc. It can be also used to develop Belief Desire Intention-style agent systems.\n\nPlanning and scripting\nIn contrast to the Planning Domain Definition Language, Golog supports planning and scripting as well. Planning means that a goal state in the world model is defined, and the solver brings a logical system into this state. Behavior scripting implements reactive procedures, which are running as a computer program.\nFor example, suppose the idea"}
{"doc_id": "GOLOG", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " scripting\nIn contrast to the Planning Domain Definition Language, Golog supports planning and scripting as well. Planning means that a goal state in the world model is defined, and the solver brings a logical system into this state. Behavior scripting implements reactive procedures, which are running as a computer program.\nFor example, suppose the idea is to authoring a story. The user defines what should be true at the end of the plot. A solver gets started and applies possible actions to the current situation until the goal state is reached. The specification of a goal state and the possible actions are realized in the logical world model.\nIn contrast, a hardwired reactive behavior doesn't need a solver but the action sequence is provided in a scripting language. The Golog interpreter, which is written in Prolog, executes the script and this will bring the story into the goal state."}
{"doc_id": "Google AI Mode", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "AI Mode is a search feature used within Google Search. In March 2025, Google introduced an experimental \"AI Mode\" within its search platform, enabling users to input complex, multi-part queries and receive comprehensive, AI-generated responses. This feature uses Google's advanced Gemini 2.0 model, which enhances the system's reasoning capabilities and supports multimodal inputs, including text, images, and voice.\nInitially, AI Mode was available to Google One AI Premium subscribers in the United States, who could access it through the Search Labs platform. This phased rollout allowed Google to gather user feedback and refine the feature before a broader release."}
{"doc_id": "Google Clips", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Google Clips is a discontinued miniature clip-on camera device developed by Google.\n\nHistory\nIt was announced on October 4, 2017 and went on sale on January 27, 2018. Google Clips automatically captured video clips (without audio) at moments its machine learning algorithms determined to be interesting or relevant. An indicator flashed when the camera was looking for scenes to capture.\nGoogle Clips' artificial intelligence (AI) could learn the faces of people to take photographs with certain people, and could automatically set lighting and framing.\nIt had 16 GB of storage built-in storage and could record clips for up to 3 hours. \nThis camera was originally priced at US$249 in the United States. It was withdrawn from sale on October 15, 2019, but supported until the end of December 2021.\n\nReception\nThe Independent wrote that Google Clips is \"an impressive little device, but one that also has the potential to feel very creepy.\"\nAccording to The Verge's generally negative review, \"it didn't capture anything special\" over two weeks of testing."}
{"doc_id": "Google Research", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Google Research (also known as Research at Google) is the research division of Google, a subsidiary of Alphabet Inc.. According to its official website, Google Research publishes findings, releases open-source software, and applies research results within Google products and services as well as within the wider scientific community.\n\nNotable contributions\nThe 2017 landmark paper Attention Is All You Need, which introduced the Transformer architecture, which has subsequently been used to build modern large language models.\nAdvances in neural machine translation powering Google Translate.\nTime series forecasting.\nDevelopment of scalable learning systems and infrastructure for large-model training.\nFlood forecasting.\nResearch into computational discovery via Google Accelerated Science including demonstrating the first below-threshold quantum calculations.\n\nSee also\nGoogle AI\nGoogle Brain\nGoogle DeepMind"}
{"doc_id": "Grammar systems theory", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Grammar systems theory is a field of theoretical computer science that studies systems of finite collections of formal grammars generating a formal language. Each grammar works on a string, a so-called sequential form that represents an environment. Grammar systems can thus be used as a formalization of decentralized or distributed systems of agents in artificial intelligence.\nLet \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbb {A} }\n  \n be a simple reactive agent moving on the table and trying not to fall down from the table with two reactions, t for turning and ƒ for moving forward. The set of possible behaviors of \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbb {A} }\n  \n can then be described as formal language\n\n  \n    \n      \n        \n          \n            L\n            \n              A\n            \n          \n        \n        =\n        {\n        (\n        \n          f\n          \n            m\n          \n        \n        \n          t\n          \n            n\n          \n        \n        \n          f\n          \n            r\n          \n        \n        \n          )\n          \n            +\n          \n        \n        :\n        1\n        ≤\n        m\n        ≤\n        k\n        ;\n        1\n        ≤\n        n\n        ≤\n        ℓ\n        ;\n        1\n        ≤\n        r\n        ≤\n        k\n        }\n        ,\n      \n    \n    {\\displaystyle \\mathbb {L_{A}} =\\{(f^{m}t^{n}f^{r})^{+}:1\\leq m\\leq k;1\\leq n\\leq \\ell ;1\\leq r\\leq k\\},}\n  \n\nwhere ƒ can be done maximally k times and t can be done maximally ℓ times considering the dimensions of the table.\n\n \nLet \n  \n    \n      \n        \n          \n            G\n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {G_{A}} }\n  \n be a formal grammar which generates language \n  \n    \n      \n        \n          \n            L\n            \n              A\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbb {L_{A}} }\n  \n. The behavior of \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbb {A} }\n  \n is then described by this grammar. Suppose the \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbb {A} }\n  \n has a subsumption architecture; each component of this architecture can be then represented as a formal grammar, too, and the final behavior of the agent is"}
{"doc_id": "Grammar systems theory", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "} }\n  \n is then described by this grammar. Suppose the \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbb {A} }\n  \n has a subsumption architecture; each component of this architecture can be then represented as a formal grammar, too, and the final behavior of the agent is then described by this system of grammars.\nThe schema on the right describes such a system of grammars which shares a common string representing an environment. The shared sequential form is sequentially rewritten by each grammar, which can represent either a component or generally an agent.\nIf grammars communicate together and work on a shared sequential form, it is called a Cooperating Distributed (DC) grammar system. Shared sequential form is a similar concept to the blackboard approach in AI, which is inspired by an idea of experts solving some problem together while they share their proposals and ideas on a shared blackboard.\nEach grammar in a grammar system can also work on its own string and communicate with other grammars in a system by sending their sequential forms on request. Such a grammar system is then called a Parallel Communicating (PC) grammar system.\nPC and DC are inspired by distributed AI. If there is no communication between grammars, the system is close to the decentralized approaches in AI. These kinds of grammar systems are sometimes called colonies or Eco-Grammar systems, depending (besides others) on whether the environment is changing on its own (Eco-Grammar system) or not (colonies).\n\nSee also\nArtificial life\nAgent-based model\nDistributed artificial intelligence\nMulti-agent system"}
{"doc_id": "Graphics processing unit", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A graphics processing unit (GPU) is a specialized electronic circuit designed for digital image processing and to accelerate computer graphics, being present either as a component on a discrete graphics card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles.  GPUs are increasingly being used for AI processing due to linear algebra acceleration which is also used extensively in graphics processing.\nAlthough there is no single definition of the term, and it may be used to describe any video display system, in modern use a GPU includes the ability to internally perform the calculations needed for various graphics tasks, like rotating and scaling 3D images, and often the additional ability to run custom programs known as shaders. This contrasts with earlier graphics controllers known as video display controllers which had no internal calculation capabilities, or blitters, which performed only basic memory movement operations. The modern GPU emerged during the 1990s, adding the ability to perform operations like drawing lines and text without CPU help, and later adding 3D functionality.\nGraphics functions are generally independent and this lends these tasks to being implemented on separate calculation engines. Modern GPUs include hundreds, or thousands, of calculation units. This made them useful for non-graphic calculations involving embarrassingly parallel problems due to their parallel structure. The ability of GPUs to rapidly perform vast numbers of calculations has led to their adoption in diverse fields including artificial intelligence (AI) where they excel at handling data-intensive and computationally demanding tasks. Other non-graphical uses include the training of neural networks and cryptocurrency mining.\n\nHistory\n1970s\nArcade system boards have used specialized graphics circuits since the 1970s. In early video game hardware, RAM for frame buffers was expensive, so video chips composited data together as the display was being scanned out on the monitor.\nA specialized barrel shifter circuit helped the CPU animate the framebuffer graphics for various 1970s arcade video games from Midway and Taito, such as Gun Fight (1975), Sea Wolf (1976), and Space Invaders (1978). The Namco Galaxian arcade system in 1979 used specialized graphics hardware that supported RGB color, multi-colored sprites, and tilemap backgrounds. The Galaxian hardware was widely used during the golden age of arcade video games, by game companies such as Namco, Centuri, Gremlin, Irem, Konami, Midway, Nichibutsu, Sega, and Taito.\n\nThe Atari 2600 in 1977 used a video shifter called the Television Interface Ad"}
{"doc_id": "Graphics processing unit", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " used during the golden age of arcade video games, by game companies such as Namco, Centuri, Gremlin, Irem, Konami, Midway, Nichibutsu, Sega, and Taito.\n\nThe Atari 2600 in 1977 used a video shifter called the Television Interface Adaptor. Atari 8-bit computers (1979) had ANTIC, a video processor which interpreted instructions describing a \"display list\"—the way the scan lines map to specific bitmapped or character modes and where the memory is stored (so there did not need to be a contiguous frame buffer). 6502 machine code subroutines could be triggered on scan lines by setting a bit on a display list instruction. ANTIC also supported smooth vertical and horizontal scrolling independent of the CPU.\n\n1980s\nThe NEC μPD7220 was the first implementation of a personal computer graphics display processor as a single large-scale integration (LSI) integrated circuit chip. This enabled the design of low-cost, high-performance video graphics cards such as those from Number Nine Visual Technology. It became the best-known GPU until the mid-1980s. It was the first fully integrated VLSI (very large-scale integration) metal–oxide–semiconductor (NMOS) graphics display processor for PCs, supported up to 1024×1024 resolution, and laid the foundations for the PC graphics market. It was used in a number of graphics cards and was licensed for clones such as the Intel 82720, the first of Intel's graphics processing units. The Williams Electronics arcade games Robotron: 2084, Joust, Sinistar, and Bubbles, all released in 1982, contain custom blitter chips for operating on 16-color bitmaps.\nIn 1984, Hitachi released the ARTC HD63484, the first major CMOS graphics processor for personal computers. The ARTC could display up to 4K resolution when in monochrome mode. It was used in a number of graphics cards and terminals during the late 1980s. In 1985, the Amiga was released with a custom graphics chip including a blitter for bitmap manipulation, line drawing, and area fill. It also included a coprocessor with its own simple instruction set, that was capable of manipulating graphics hardware registers in sync with the video beam (e.g. for per-scanline palette switches, sprite multiplexing, and hardware windowing), or driving the blitter. In 1986, Texas"}
{"doc_id": "Graphics processing unit", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " fill. It also included a coprocessor with its own simple instruction set, that was capable of manipulating graphics hardware registers in sync with the video beam (e.g. for per-scanline palette switches, sprite multiplexing, and hardware windowing), or driving the blitter. In 1986, Texas Instruments released the TMS34010, the first fully programmable graphics processor. It could run general-purpose code but also had a graphics-oriented instruction set. During 1990–1992, this chip became the basis of the Texas Instruments Graphics Architecture (\"TIGA\") Windows accelerator cards.\n\nIn 1987, the IBM 8514 graphics system was released. It was one of the first video cards for IBM PC compatibles that implemented fixed-function 2D primitives in electronic hardware. Sharp's X68000, released in 1987, used a custom graphics chipset with a 65,536 color palette and hardware support for sprites, scrolling, and multiple playfields. It served as a development machine for Capcom's CP System arcade board. Fujitsu's FM Towns computer, released in 1989, had support for a 16,777,216 color palette. In 1988, the first dedicated polygonal 3D graphics boards were introduced in arcades with the Namco System 21 and Taito Air System.\n\nIBM introduced its proprietary Video Graphics Array (VGA) display standard in 1987, with a maximum resolution of 640×480 pixels. In November 1988, NEC Home Electronics announced its creation of the Video Electronics Standards Association (VESA) to develop and promote a Super VGA (SVGA) computer display standard as a successor to VGA. Super VGA enabled graphics display resolutions up to 800×600 pixels, a 56% increase.\n\n1990s\nIn 1991, S3 Graphics introduced the S3 86C911, which its designers named after the Porsche 911 as an indication of the performance increase it promised. The 86C911 spawned a variety of imitators: by 1995, all major PC graphics chip makers had added 2D acceleration support to their chips. Fixed-function Windows accelerators surpassed expensive general-purpose graphics coprocessors in Windows performance, and such coprocessors faded from the PC market.\nIn the early- and mid-1990s, real-time 3D graphics became increasingly common in arcade, computer, and console games, which led to increasing public demand for hardware-accelerated "}
{"doc_id": "Graphics processing unit", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " expensive general-purpose graphics coprocessors in Windows performance, and such coprocessors faded from the PC market.\nIn the early- and mid-1990s, real-time 3D graphics became increasingly common in arcade, computer, and console games, which led to increasing public demand for hardware-accelerated 3D graphics. Early examples of mass-market 3D graphics hardware can be found in arcade system boards such as the Sega Model 1, Namco System 22, and Sega Model 2, and the fifth-generation video game consoles such as the Saturn, PlayStation, and Nintendo 64. Arcade systems such as the Sega Model 2 and SGI Onyx-based Namco Magic Edge Hornet Simulator in 1993 were capable of hardware T&L (transform, clipping, and lighting) years before appearing in consumer graphics cards. Another early example is the Super FX chip, a RISC-based on-cartridge graphics chip used in some SNES games, notably Doom and Star Fox. Some systems used DSPs to accelerate transformations. Fujitsu, which worked on the Sega Model 2 arcade system, began working on integrating T&L into a single LSI solution for use in home computers in 1995; the Fujitsu Pinolite, the first 3D geometry processor for personal computers, announced in 1997. The first hardware T&L GPU on home video game consoles was the Nintendo 64's Reality Coprocessor, released in 1996. In 1997, Mitsubishi released the 3Dpro/2MP, a GPU capable of transformation and lighting, for workstations and Windows NT desktops; ATi used it for its FireGL 4000 graphics card, released in 1997.\nThe term \"GPU\" was coined by Sony in reference to the 32-bit Sony GPU (designed by Toshiba) in the PlayStation video game console, released in 1994.\n\n2000s\nIn October 2002, with the introduction of the ATI Radeon 9700 (also known as R300), the world's first Direct3D 9.0 accelerator, pixel and vertex shaders could implement looping and lengthy floating point math, and were quickly becoming as flexible as CPUs, yet orders of magnitude faster for image-array operations. Pixel shading is often used for bump mapping, which adds texture to make an object look shiny, dull, rough, or even round or extruded.\nWith the introduction of the Nvidia GeForce 8 series and new generic stream processing units,"}
{"doc_id": "Graphics processing unit", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " quickly becoming as flexible as CPUs, yet orders of magnitude faster for image-array operations. Pixel shading is often used for bump mapping, which adds texture to make an object look shiny, dull, rough, or even round or extruded.\nWith the introduction of the Nvidia GeForce 8 series and new generic stream processing units, GPUs became more generalized computing devices. Parallel GPUs are making computational inroads against the CPU, and a subfield of research, dubbed GPU computing or GPGPU for general purpose computing on GPU, has found applications in fields as diverse as machine learning, oil exploration, scientific image processing, linear algebra, statistics, 3D reconstruction, and stock options pricing. GPGPUs were the precursors to what is now called a compute shader (e.g. CUDA, OpenCL, DirectCompute) and actually abused the hardware to a degree by treating the data passed to algorithms as texture maps and executing algorithms by drawing a triangle or quad with an appropriate pixel shader. This entails some overheads since units like the scan converter are involved where they are not needed (nor are triangle manipulations even a concern—except to invoke the pixel shader).\nNvidia's CUDA platform, first introduced in 2007, was the earliest widely adopted programming model for GPU computing. OpenCL is an open standard defined by the Khronos Group that allows for the development of code for both GPUs and CPUs with an emphasis on portability. OpenCL solutions are supported by Intel, AMD, Nvidia, and ARM, and according to a report in 2011 by Evans Data, OpenCL had become the second most popular HPC tool.\n\n2010s\nIn 2010, Nvidia partnered with Audi to power their cars' dashboards, using the Tegra GPU to provide increased functionality to cars' navigation and entertainment systems. Advances in GPU technology in cars helped advance self-driving technology. AMD's Radeon HD 6000 series cards were released in 2010, and in 2011 AMD released its 6000M Series discrete GPUs for mobile devices. The Kepler line of graphics cards by Nvidia were released in 2012 and were used in the Nvidia 600 and 700 series cards. A feature in this GPU microarchitecture included GPU boost, a technology that adjusts the clock-speed of a video card to increase or decrease according to its power draw. Kepler also introduced NVENC video encoding acceleration technology.\nThe PS4 and Xbox One were released in 2013; they both used GPUs based on AMD's Radeon HD 7850 and 779"}
{"doc_id": "Graphics processing unit", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " included GPU boost, a technology that adjusts the clock-speed of a video card to increase or decrease according to its power draw. Kepler also introduced NVENC video encoding acceleration technology.\nThe PS4 and Xbox One were released in 2013; they both used GPUs based on AMD's Radeon HD 7850 and 7790. Nvidia's Kepler line of GPUs was followed by the Maxwell line, manufactured on the same process. Nvidia's 28 nm chips were manufactured by TSMC in Taiwan using the 28 nm process. Compared to the 40 nm technology from the past, this manufacturing process allowed a 20 percent boost in performance while drawing less power. Virtual reality headsets have high system requirements; manufacturers recommended the GTX 970 and the R9 290X or better at the time of their release. Cards based on the Pascal microarchitecture were released in 2016. The GeForce 10 series of cards are of this generation of graphics cards. They are made using the 16 nm manufacturing process which improves upon previous microarchitectures.\nIn 2018, Nvidia launched the RTX 20 series GPUs that added ray tracing cores to GPUs, improving their performance on lighting effects. Polaris 11 and Polaris 10 GPUs from AMD are fabricated by a 14 nm process. Their release resulted in a substantial increase in the performance per watt of AMD video cards. AMD also released the Vega GPU series for the high end market as a competitor to Nvidia's high end Pascal cards, also featuring HBM2 like the Titan V.\nIn 2019, AMD released the successor to their Graphics Core Next (GCN) microarchitecture/instruction set. Dubbed RDNA, the first product featuring it was the Radeon RX 5000 series of video cards. The company announced that the successor to the RDNA microarchitecture would be incremental (a \"refresh\"). AMD unveiled the Radeon RX 6000 series, its RDNA 2 graphics cards with support for hardware-accelerated ray tracing. The product series, launched in late 2020, consisted of the RX 6800, RX 6800 XT, and RX 6900 XT. The RX 6700 XT, which is based on Navi 22, was launched in early 2021.\nThe PlayStation 5 and Xbox Series X and Series S were released in 2020; they both use GPUs based on the RDNA 2 microarchitecture with incremental improvements and different GPU configurations in each system's implementation.\n\n2020s\nIn the"}
{"doc_id": "Graphics processing unit", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " on Navi 22, was launched in early 2021.\nThe PlayStation 5 and Xbox Series X and Series S were released in 2020; they both use GPUs based on the RDNA 2 microarchitecture with incremental improvements and different GPU configurations in each system's implementation.\n\n2020s\nIn the 2020s, GPUs have been increasingly used for calculations involving embarrassingly parallel problems, such as training of neural networks on enormous datasets that are needed for artificial intelligence large language models. Specialized processing cores on some modern workstation's GPUs are dedicated for deep learning since they have significant FLOPS performance increases, using 4×4 matrix multiplication and division, resulting in hardware performance up to 128 TFLOPS in some applications. These tensor cores are expected to appear in consumer cards, as well.\n\nGPU companies\nMany companies have produced GPUs under a number of brand names. In 2009, Intel, Nvidia, and AMD/ATI were the market share leaders, with 49.4%, 27.8%, and 20.6% market share respectively. In addition, Matrox produces GPUs. Chinese companies such as Jingjia Micro have also produced GPUs for the domestic market although in terms of worldwide sales, they lag behind market leaders.\n\nComputational functions\nSeveral factors of GPU construction affect the performance of the card for real-time rendering, such as the size of the connector pathways in the semiconductor device fabrication, the clock signal frequency, and the number and size of various on-chip memory caches. Performance is also affected by the number of streaming multiprocessors (SM) for NVidia GPUs, or compute units (CU) for AMD GPUs, or Xe cores for Intel Xe based GPUs, which describe the number of on-silicon processor core units within the GPU chip that perform the core calculations, typically working in parallel with other SM/CUs on the GPU. GPU performance is typically measured in floating point operations per second (FLOPS); GPUs in the 2010s and 2020s typically deliver performance measured in teraflops (TFLOPS). This is an estimated performance measure, as other factors can affect the actual display rate.\n\n2D graphics APIs\nAn earlier GPU may support one or more 2D graphics APIs for 2D acceleration, such as GDI and DirectDraw.\n\nGPU forms\nTerminology\nIn the 1970s, the term \"GPU\" originally stood for graphics processor unit and described a programmable processing unit working independently from the"}
{"doc_id": "Graphics processing unit", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\nAn earlier GPU may support one or more 2D graphics APIs for 2D acceleration, such as GDI and DirectDraw.\n\nGPU forms\nTerminology\nIn the 1970s, the term \"GPU\" originally stood for graphics processor unit and described a programmable processing unit working independently from the CPU that was responsible for graphics manipulation and output. In 1994, Sony used the term (now standing for graphics processing unit) in reference to the PlayStation console's Toshiba-designed Sony GPU. The term was popularized by Nvidia in 1999, who marketed the GeForce 256 as \"the world's first GPU\". It was presented as a \"single-chip processor with integrated transform, lighting, triangle setup/clipping, and rendering engines\". Rival ATI Technologies coined the term \"visual processing unit\" or VPU with the release of the Radeon 9700 in 2002. The AMD Alveo MA35D features dual VPU’s, each using the 5 nm process in 2023.\nIn personal computers, there are two main forms of GPUs. Each has many synonyms:\n\nDedicated graphics also called discrete graphics.\nIntegrated graphics also called shared graphics solutions, integrated graphics processors (IGP), or unified memory architecture (UMA).\n\nDedicated graphics processing unit\nDedicated graphics processing units use RAM that is dedicated to the GPU rather than relying on the computer’s main system memory. This RAM is usually specially selected for the expected serial workload of the graphics card (see GDDR). Sometimes systems with dedicated discrete GPUs were called \"DIS\" systems as opposed to \"UMA\" systems (see next section).\nTechnologies such as Scan-Line Interleave by 3dfx, SLI and NVLink by Nvidia and CrossFire by AMD allow multiple GPUs to draw images simultaneously for a single screen, increasing the processing power available for graphics. These technologies, however, are increasingly uncommon; most games do not fully use multiple GPUs, as most users cannot afford them. Multiple GPUs are still used on supercomputers (like in Summit), on workstations to accelerate video (processing multiple videos at once) and 3D rendering, for VFX, GPGPU workloads and for simulations, and in AI to expedite training, as is the case with Nvidia's lineup of DGX workstations and servers, Tesla GPUs, and Intel's Ponte Vecchio GPUs.\n\nIntegrated graphics processing unit\nIntegrated graphics processing units (IGPU), integrated graphics, shared graphics solutions, integrated graphics processors ("}
{"doc_id": "Graphics processing unit", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and for simulations, and in AI to expedite training, as is the case with Nvidia's lineup of DGX workstations and servers, Tesla GPUs, and Intel's Ponte Vecchio GPUs.\n\nIntegrated graphics processing unit\nIntegrated graphics processing units (IGPU), integrated graphics, shared graphics solutions, integrated graphics processors (IGP), or unified memory architectures (UMA) use a portion of a computer's system RAM rather than dedicated graphics memory. IGPs can be integrated onto a motherboard as part of its northbridge chipset, or on the same die (integrated circuit) with the CPU (like AMD APU or Intel HD Graphics). On certain motherboards, AMD's IGPs can use dedicated sideport memory: a separate fixed block of high performance memory that is dedicated for use by the GPU. As of early 2007, computers with integrated graphics account for about 90% of all PC shipments. They are less costly to implement than dedicated graphics processing, but tend to be less capable. Historically, integrated processing was considered unfit for 3D games or graphically intensive programs but could run less intensive programs such as Adobe Flash. Examples of such IGPs would be offerings from SiS and VIA circa 2004. However, modern integrated graphics processors such as AMD Accelerated Processing Unit and Intel Graphics Technology (HD, UHD, Iris, Iris Pro, Iris Plus, and Xe-LP) can handle 2D graphics or low-stress 3D graphics.\nSince GPU computations are memory-intensive, integrated processing may compete with the CPU for relatively slow system RAM, as it has minimal or no dedicated video memory. IGPs use system memory with bandwidth up to a current maximum of 128 GB/s, whereas a discrete graphics card may have a bandwidth of more than 1000 GB/s between its VRAM and GPU core. This memory bus bandwidth can limit the performance of the GPU, though multi-channel memory can mitigate this deficiency. Older integrated graphics chipsets lacked hardware transform and lighting, but newer ones include it.\nOn systems with \"Unified Memory Architecture\" (UMA), including modern AMD processors with integrated graphics, modern Intel processors with integrated graphics, Apple processors, the PS5 and Xbox Series (among others), the CPU cores and the GPU block share the same pool of RAM and memory address space.\n\nStream processing and general purpose GPUs (GPGPU)\nIt is common to use a general purpose graphics processing unit (GPGPU) as a modified form of stream processor (or a vector processor), running compute"}
{"doc_id": "Graphics processing unit", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (among others), the CPU cores and the GPU block share the same pool of RAM and memory address space.\n\nStream processing and general purpose GPUs (GPGPU)\nIt is common to use a general purpose graphics processing unit (GPGPU) as a modified form of stream processor (or a vector processor), running compute kernels. This turns the massive computational power of a modern graphics accelerator's shader pipeline into general-purpose computing power. In certain applications requiring massive vector operations, this can yield several orders of magnitude higher performance than a conventional CPU. The two largest discrete (see \"Dedicated graphics processing unit\" above) GPU designers, AMD and Nvidia, are pursuing this approach with an array of applications. Both Nvidia and AMD teamed with Stanford University to create a GPU-based client for the Folding@home distributed computing project for protein folding calculations. In certain circumstances, the GPU calculates forty times faster than the CPUs traditionally used by such applications.\nGPU-based high performance computers play a significant role in large-scale modelling. Three of the ten most powerful supercomputers in the world take advantage of GPU acceleration.\nSince 2005 there has been interest in using the performance offered by GPUs for evolutionary computation in general, and for accelerating the fitness evaluation in genetic programming in particular. Most approaches compile linear or tree programs on the host PC and transfer the executable to the GPU to be run. Typically a performance advantage is only obtained by running the single active program simultaneously on many example problems in parallel, using the GPU's SIMD architecture. However, substantial acceleration can also be obtained by not compiling the programs, and instead transferring them to the GPU, to be interpreted there.\n\nExternal GPU (eGPU)\nTherefore, it is desirable to attach a GPU to some external bus of a notebook. PCI Express is the only bus used for this purpose. The port may be, for example, an ExpressCard or mPCIe port (PCIe ×1, up to 5 or 2.5 Gbit/s respectively), a Thunderbolt 1, 2, or 3 port (PCIe ×4, up to 10, 20, or 40 Gbit/s respectively), a USB4 port with Thunderbolt compatibility, or an OCuLink port. Those ports are only available on certain notebook systems. eGPU enclosures include their own power supply (PSU), because powerful GPUs can consume hundreds of watts.\n\nEnergy efficiency\nSales\nIn 2013, 438.3 million GPUs were shipped globally and the forecast for 2014 was 414."}
{"doc_id": "Graphics processing unit", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " port. Those ports are only available on certain notebook systems. eGPU enclosures include their own power supply (PSU), because powerful GPUs can consume hundreds of watts.\n\nEnergy efficiency\nSales\nIn 2013, 438.3 million GPUs were shipped globally and the forecast for 2014 was 414.2 million. However, by the third quarter of 2022, shipments of PC GPUs totaled around 75.5 million units, down 19% year-over-year.\n\nSee also\nHardware\nList of AMD graphics processing units\nList of Nvidia graphics processing units\nList of Intel graphics processing units\nList of discrete and integrated graphics processing units\nIntel GMA\nLarrabee\nNvidia PureVideo – the bit-stream technology from Nvidia used in their graphics chips to accelerate video decoding on hardware GPU with DXVA.\nSoC\nUVD (Unified Video Decoder) – the video decoding bit-stream technology from ATI to support hardware (GPU) decode with DXVA\n\nAPIs\nApplications\nGPU cluster\nMathematica – includes built-in support for CUDA and OpenCL GPU execution\nMolecular modeling on GPU\nDeeplearning4j – open-source, distributed deep learning for Java"}
{"doc_id": "Gödel machine", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A Gödel machine is a hypothetical self-improving computer program that solves problems in an optimal way. It uses a recursive self-improvement protocol in which it rewrites its own code when it can prove the new code provides a better strategy. The machine was invented by Jürgen Schmidhuber (first proposed in 2003), but is named after Kurt Gödel who inspired the mathematical theories.\nThe Gödel machine is often discussed when dealing with issues of meta-learning, also known as \"learning to learn.\" Applications include automating human design decisions and transfer of knowledge between multiple related tasks, and may lead to design of more robust and general learning architectures. Though theoretically possible, no full implementation has been created.\nThe Gödel machine is often compared with Marcus Hutter's AIXI, another formal specification for an artificial general intelligence. Schmidhuber points out that the Gödel machine could start out by implementing AIXItl as its initial sub-program, and self-modify after it finds proof that another algorithm for its search code will be better.\n\nLimitations\nTraditional problems solved by a computer only require one input and provide some output. Computers of this sort had their initial algorithm hardwired. This does not take into account the dynamic natural environment, and thus was a goal for the Gödel machine to overcome.\nThe Gödel machine has limitations of its own, however. According to Gödel's First Incompleteness Theorem, any formal system that encompasses arithmetic is either flawed or allows for statements that cannot be proved in the system. Hence even a Gödel machine with unlimited computational resources must ignore those self-improvements whose effectiveness it cannot prove.\n\nVariables of interest\nThere are three variables that are particularly useful in the run time of the Gödel machine.\n\nAt some time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, the variable \n  \n    \n      \n        \n          time\n        \n      \n    \n    {\\displaystyle {\\text{time}}}\n  \n will have the binary equivalent of \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n. This is incremented steadily throughout the run time of the machine.\nAny input meant for the Gödel machine from the natural environment is stored in variable \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n. It is likely the case that \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n will hold different values for different values of variable \n  \n    \n      \n        \n          time\n        \n      \n    \n    {\\displaystyle {\\text{time"}
{"doc_id": "Gödel machine", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " stored in variable \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n. It is likely the case that \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n will hold different values for different values of variable \n  \n    \n      \n        \n          time\n        \n      \n    \n    {\\displaystyle {\\text{time}}}\n  \n.\nThe outputs of the Gödel machine are stored in variable \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n, where \n  \n    \n      \n        y\n        (\n        t\n        )\n      \n    \n    {\\displaystyle y(t)}\n  \n would be the output bit-string at some time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n.\nAt any given time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, where \n  \n    \n      \n        (\n        1\n        ≤\n        t\n        ≤\n        T\n        )\n      \n    \n    {\\displaystyle (1\\leq t\\leq T)}\n  \n, the goal is to maximize future success or utility. A typical utility function follows the pattern \n  \n    \n      \n        u\n        (\n        s\n        ,\n        \n          E\n          n\n          v\n        \n        )\n        :\n        S\n        ×\n        E\n        →\n        \n          R\n        \n      \n    \n    {\\displaystyle u(s,\\mathrm {Env} ):S\\times E\\rightarrow \\mathbb {R} }\n  \n:\n\n  \n    \n      \n        u\n        (\n        s\n        ,\n        \n          E\n          n\n          v\n        \n        )\n        =\n        \n          E\n          \n            μ\n          \n        \n        \n          \n            [\n          \n        \n        \n          ∑\n          \n            τ\n            =\n            \n              time\n            \n          \n          \n            T\n          \n        \n        r\n        (\n        τ\n        )\n        ∣\n        s\n        ,\n        \n          E\n          n\n          v\n        \n        \n          \n            ]\n          \n        \n      \n    \n    {\\displaystyle u(s,\\mathrm {Env} )=E_{\\mu }{\\Bigg [}\\sum _{\\tau ={\\text{time}}}^{T}r(\\tau )\\mid s,\\mathrm {Env} {\\Bigg ]}}\n  \n\nwhere \n  \n    \n      \n        r\n        (\n        t\n        )\n      \n    \n    {\\displaystyle r(t)}\n  \n is a real-valued reward input (encoded within \n  \n    \n      \n        s\n        (\n        t\n        )\n      \n    \n    {\\displaystyle s(t)}\n  \n) at time \n  \n    \n      \n        t\n      \n    \n"}
{"doc_id": "Gödel machine", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "      \n        r\n        (\n        t\n        )\n      \n    \n    {\\displaystyle r(t)}\n  \n is a real-valued reward input (encoded within \n  \n    \n      \n        s\n        (\n        t\n        )\n      \n    \n    {\\displaystyle s(t)}\n  \n) at time \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n, \n  \n    \n      \n        \n          E\n          \n            μ\n          \n        \n        [\n        ⋅\n        ∣\n        ⋅\n        ]\n      \n    \n    {\\displaystyle E_{\\mu }[\\cdot \\mid \\cdot ]}\n  \n denotes the\nconditional expectation operator with respect to some possibly unknown distribution \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n from a\nset \n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n of possible distributions (\n  \n    \n      \n        M\n      \n    \n    {\\displaystyle M}\n  \n reflects whatever is known about the possibly probabilistic reactions of the environment), and the above-mentioned \n  \n    \n      \n        \n          time\n        \n        =\n        time\n        ⁡\n        (\n        s\n        )\n      \n    \n    {\\displaystyle {\\text{time}}=\\operatorname {time} (s)}\n  \n is a function of state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n which uniquely identifies the current cycle. Note that we take into account the possibility of extending the expected lifespan through appropriate actions.\n\nInstructions used by proof techniques\nThe nature of the six proof-modifying instructions below makes it impossible\nto insert an incorrect theorem into proof, thus trivializing proof verification.\n\nget-axiom(n)\nAppends the n-th axiom as a theorem to the current theorem sequence. Below is the initial axiom scheme:\n\nHardware Axioms formally specify how components of the machine could change from one cycle to the next.\nReward Axioms define the computational cost of hardware instruction and the physical cost of output actions. Related Axioms also define the lifetime of the Gödel machine as scalar quantities representing all rewards/costs.\nEnvironment Axioms restrict the way new inputs x are produced from the environment, based on previous sequences of inputs y.\nUncertainty Axioms/String Manipulation Axioms are standard axioms for arithmetic, calculus, probability theory, and string manipulation that allow for the construction of proofs related to future variable values within the Gödel machine.\nInitial State Axioms contain information about how to reconstruct parts or all of the initial state.\nUtility Axioms describe the overall goal in the"}
{"doc_id": "Gödel machine", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "xioms are standard axioms for arithmetic, calculus, probability theory, and string manipulation that allow for the construction of proofs related to future variable values within the Gödel machine.\nInitial State Axioms contain information about how to reconstruct parts or all of the initial state.\nUtility Axioms describe the overall goal in the form of utility function u.\n\napply-rule(k, m, n)\nTakes in the index k of an inference rule (such as Modus tollens, Modus ponens), and attempts to apply it to the two previously proved theorems m and n. The resulting theorem is then added to the proof.\n\ndelete-theorem(m)\nDeletes the theorem stored at index m in the current proof. This helps to mitigate storage constraints caused by redundant and unnecessary theorems. Deleted theorems can no longer be referenced by the above apply-rule function.\n\nset-switchprog(m, n)\nReplaces switchprog S pm:n, provided it is a non-empty substring of S p.\n\ncheck()\nVerifies whether the goal of the proof search has been reached. A target theorem states that given the current axiomatized utility function u (Item 1f), the utility of a switch from p to the current switchprog would be higher than the utility of continuing the execution of p (which would keep searching for alternative switchprogs).\n\nstate2theorem(m, n)\nTakes in two arguments, m and n, and attempts to convert the contents of Sm:n into a theorem.\n\nExample applications\nTime-limited NP-hard optimization\nThe initial input to the Gödel machine is the representation of a connected graph with a large number of nodes linked by edges of various lengths. Within given time T it should find a cyclic path connecting all nodes. The only real-valued reward will occur at time T. It equals 1 divided by the length of the best path found so far (0 if none was found). There are no other inputs. The by-product of maximizing expected reward is to find the shortest path findable within the limited time, given the initial bias.\n\nFast theorem proving\nProve or disprove as quickly as possible that all even integers > 2 are the sum of two primes (Goldbach’s conjecture). The reward is 1/t, where t is the time required to produce and verify the first such proof.\n\nMaximizing expected reward with bounded resources\nA cognitive robot that needs at least 1 liter of gasoline per hour interacts with a partially unknown environment, trying to find hidden, limited gasoline dep"}
{"doc_id": "Gödel machine", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "’s conjecture). The reward is 1/t, where t is the time required to produce and verify the first such proof.\n\nMaximizing expected reward with bounded resources\nA cognitive robot that needs at least 1 liter of gasoline per hour interacts with a partially unknown environment, trying to find hidden, limited gasoline depots to occasionally refuel its tank. It is rewarded in proportion to its lifetime, and dies after at most 100 years or as soon as its tank is empty or it falls off a cliff, and so on. The probabilistic environmental reactions are initially unknown but assumed to be sampled from the axiomatized Speed Prior, according to which hard-to-compute environmental reactions are unlikely. This permits a computable strategy for making near-optimal predictions. One by-product of maximizing expected reward is to maximize expected lifetime.\n\nSee also\nGödel's incompleteness theorems"}
{"doc_id": "Hardware for artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Specialized computer hardware is often used to execute artificial intelligence (AI) programs faster, and with less energy, such as Lisp machines, neuromorphic engineering, event cameras, and physical neural networks. Since 2017, several consumer grade CPUs and SoCs have on-die NPUs. As of 2023, the market for AI hardware is dominated by GPUs. \n\nAs of the 2020s, AI computation is dominated by graphics processing units (GPUs) and newer domain-specific accelerators such as Google’s Tensor Processing Units (TPUs), AMD’s Instinct MI300 series, and various on-device neural-processing units (NPUs) found in consumer hardware.\n\nScope\nFor the purposes of this article, AI hardware refers to computing components and systems specifically designed or optimized to accelerate artificial-intelligence workloads such as machine-learning training or inference. This includes general-purpose accelerators used for AI (for example, GPUs) and domain-specific accelerators (for example, TPUs, NPUs, and other AI ASICs).  \nEvent-based cameras are sometimes discussed in the context of neuromorphic computing, but they are input sensors rather than AI compute devices. Conversely, components such as memristors are basic circuit elements rather than specialized AI hardware when considered alone.\n\nLisp machines\nLisp machines were developed in the late 1970s and early 1980s to make artificial intelligence programs written in the programming language Lisp run faster.\n\nDataflow architecture\nDataflow architecture processors used for AI serve various purposes with varied implementations like the polymorphic dataflow Convolution Engine by Kinara (formerly Deep Vision), structure-driven dataflow by Hailo, and dataflow scheduling by Cerebras.\n\nComponent hardware\nAI accelerators\nSince the 2010s, advances in computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced central processing units (CPUs) as the dominant means to train large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from Alex Net (2012) to Alpha Zero (2017), and found a 300,000-fold increase in the amount of compute needed, with a doubling-time trend of 3.4 months.\n\nGeneral-purpose GPUs for AI\nSince the 2010s, graphics processing units (GPUs) have been widely used to train and deploy"}
{"doc_id": "Hardware for artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ") to Alpha Zero (2017), and found a 300,000-fold increase in the amount of compute needed, with a doubling-time trend of 3.4 months.\n\nGeneral-purpose GPUs for AI\nSince the 2010s, graphics processing units (GPUs) have been widely used to train and deploy deep learning models because of their highly parallel architecture and high memory bandwidth. Modern data-center GPUs include dedicated tensor or matrix-math units that accelerate neural-network operations. \nIn 2022, NVIDIA introduced the Hopper-generation H100 GPU, adding FP8 precision support and faster interconnects for large-scale model training. AMD and other vendors have also developed GPUs and accelerators aimed at AI and high-performance computing workloads.\n\nDomain-specific accelerators (ASICs / NPUs)\nBeyond general-purpose GPUs, several companies have developed application-specific integrated circuits (ASICs) and neural processing units (NPUs) tailored for AI workloads. Google introduced the Tensor Processing Unit (TPU) in 2016 for deep-learning inference, with later generations supporting large-scale training through dense systolic-array designs and optical interconnects.  \nOther vendors have released similar devices—such as Apple’s Neural Engine and various on-device NPUs—that emphasize energy-efficient inference in mobile or edge computing environments.\n\nMemory and interconnects\nAI accelerators rely on fast memory and inter-chip links to manage the large data volumes of training and inference. High-bandwidth memory (HBM) stacks, standardized as HBM3 in 2023, provide terabytes-per-second throughput on modern GPUs and ASICs.  \nThese accelerators are often connected through dedicated fabrics such as NVIDIA’s NVLink and NVSwitch or optical interconnects used in TPU systems to scale performance across thousands of chips.\n\n\n== Sources =="}
{"doc_id": "Hello World_ How to be Human in the Age of the Machine", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Hello World: How to Be Human in the Age of the Machine (also titled Hello World: Being Human in the Age of Algorithms) is a book on the growing influence of algorithms and artificial intelligence (AI) on human life, authored by mathematician and science communicator Hannah Fry. The book examines how algorithms are increasingly shaping decisions in critical areas such as healthcare, transportation, justice, finance, and the arts.\n\nOverview\nFry uses real-world examples, such as driverless cars and predictive policing, to illustrate her points. She emphasizes that algorithms are not inherently objective; they reflect biases embedded in their design and data inputs. While acknowledging their potential to improve efficiency and accuracy, Fry cautions against over-reliance on machines without human judgment.\nFry explores moral questions surrounding algorithmic decision-making, such as whether machines can replace human empathy in critical situations. She advocates for greater scrutiny of algorithms to ensure fairness and avoid harmful biases. The book proposes a \"cyborg future\", where humans work alongside algorithms to enhance decision-making while retaining ultimate control.\n\nReception\nHello World has been praised for its clarity, engaging storytelling, and balanced perspective. Critics have highlighted Fry's ability to make complex topics accessible to general audiences while raising important questions about technology's impact on society.\nThe book was shortlisted for awards such as the 2018 Baillie Gifford Prize and the Royal Society Science Book Prize."}
{"doc_id": "Hierarchical control system", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A hierarchical control system (HCS) is a form of control system in which a set of devices and governing software is arranged in a hierarchical tree.  When the links in the tree are implemented by a computer network, then that hierarchical control system is also a form of networked control system.\n\nOverview\nA human-built system with complex behavior is often organized as a hierarchy. For example, a command hierarchy has among its notable features the organizational chart of superiors, subordinates, and lines of organizational communication. Hierarchical control systems are organized similarly to divide the decision making responsibility.\nEach element of the hierarchy is a linked node in the tree. Commands, tasks and goals to be achieved flow down the tree from superior nodes to subordinate nodes, whereas sensations and command results flow up the tree from subordinate to superior nodes. Nodes may also exchange messages with their siblings.  The two distinguishing features of a hierarchical control system are related to its layers.\n\nEach higher layer of the tree operates with a longer interval of planning and execution time than its immediately lower layer.\nThe lower layers have local tasks, goals, and sensations, and their activities are planned and coordinated by higher layers which do not generally override their decisions. The layers form a hybrid intelligent system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning.  A hierarchical task network is a good fit for planning in a hierarchical control system.\nBesides artificial systems, an animal's control systems are proposed to be organized as a hierarchy. In perceptual control theory, which postulates that an organism's behavior is a means of controlling its perceptions, the organism's control systems are suggested to be organized in a hierarchical pattern as their perceptions are constructed so.\n\nControl system structure\nThe accompanying diagram is a general hierarchical model which shows functional manufacturing levels using computerised control of an industrial control system.\nReferring to the diagram;\n\nLevel 0 contains the field devices such as flow and temperature sensors, and final control elements, such as control valves\nLevel 1 contains the industrialised Input/Output (I/O) modules, and their associated distributed electronic processors.\nLevel 2 contains the supervisory computers, which  collate information from processor nodes on the system, and provide the operator control screens.\nLevel 3 is the production control level, which does not directly control the process, but is concerned with monitoring production and monitoring targets\nLevel 4 is the production scheduling level.\n\nApplications\nManufacturing, robotics and vehicles\n"}
{"doc_id": "Hierarchical control system", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " which  collate information from processor nodes on the system, and provide the operator control screens.\nLevel 3 is the production control level, which does not directly control the process, but is concerned with monitoring production and monitoring targets\nLevel 4 is the production scheduling level.\n\nApplications\nManufacturing, robotics and vehicles\nAmong the robotic paradigms is the hierarchical paradigm in which a robot operates in a top-down fashion, heavy on planning, especially motion planning.  Computer-aided production engineering has been a research focus at NIST since the 1980s.  Its Automated Manufacturing Research Facility was used to develop a five layer production control model.  In the early 1990s DARPA sponsored research to develop distributed (i.e. networked) intelligent control systems for applications such as military command and control systems.  NIST built on earlier research to develop its Real-Time Control System (RCS) and Real-time Control System Software which is a generic hierarchical control system that has been used to operate a manufacturing cell, a robot crane, and an automated vehicle.\nIn November 2007, DARPA held the Urban Challenge.  The winning entry, Tartan Racing employed a hierarchical control system, with layered mission planning, motion planning, behavior generation, perception, world modelling, and mechatronics.\n\nArtificial intelligence\nSubsumption architecture is a methodology for developing artificial intelligence that is heavily associated with behavior based robotics.  This architecture is a way of decomposing complicated intelligent behavior into many \"simple\" behavior modules, which are in turn organized into layers. Each layer implements a particular goal of the software agent (i.e. system as a whole), and higher layers are increasingly more abstract. Each layer's goal subsumes that of the underlying layers, e.g. the decision to move forward by the eat-food layer takes into account the decision of the lowest obstacle-avoidance layer. Behavior need not be planned by a superior layer, rather behaviors may be triggered by sensory inputs and so are only active under circumstances where they might be appropriate.\nReinforcement learning has been used to acquire behavior in a hierarchical control system in which each node can learn to improve its behavior with experience.\n\nJames Albus, while at NIST, developed a theory for intelligent system design named the Reference Model Architecture (RMA), which is a hierarchical control system inspired by RCS. Albus defines each node to contain these components.\n\nBehavior generation is responsible for executing tasks received from the superior, parent node.  It also plans for, and issues tasks"}
{"doc_id": "Hierarchical control system", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " at NIST, developed a theory for intelligent system design named the Reference Model Architecture (RMA), which is a hierarchical control system inspired by RCS. Albus defines each node to contain these components.\n\nBehavior generation is responsible for executing tasks received from the superior, parent node.  It also plans for, and issues tasks to, the subordinate nodes.\nSensory perception is responsible for receiving sensations from the subordinate nodes, then grouping, filtering, and otherwise processing them into higher level abstractions that update the local state and which form sensations that are sent to the superior node.\nValue judgment is responsible for evaluating the updated situation and evaluating alternative plans.\nWorld Model is the local state that provides a model for the controlled system, controlled process, or environment at the abstraction level of the subordinate nodes.\nAt its lowest levels, the RMA can be implemented as a subsumption architecture, in which the world model is mapped directly to the controlled process or real world, avoiding the need for a mathematical abstraction, and in which time-constrained reactive planning can be implemented as a finite-state machine.  Higher levels of the RMA however, may have sophisticated mathematical world models and behavior implemented by automated planning and scheduling.  Planning is required when certain behaviors cannot be triggered by current sensations, but rather by predicted or anticipated sensations, especially those that come about as result of the node's actions.\n\nSee also\nCommand hierarchy, a hierarchical power structure\nHierarchical organization, a hierarchical organizational structure"}
{"doc_id": "Histogram of oriented displacements", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Histogram of oriented displacements (HOD) is a 2D trajectory descriptor. The trajectory is described using a histogram of the directions between each two consecutive points. Given a trajectory T = {P1, P2, P3, ..., Pn}, where Pt is the 2D position at time t. For each pair of positions Pt and Pt+1, calculate the direction angle θ(t, t+1). Value of θ is between 0 and 360. A histogram of the quantized values of θ is created. If the histogram is of 8 bins, the first bin represents all θs between 0 and 45.\nThe histogram accumulates the lengths of the consecutive moves. For each θ, a specific histogram bin is determined. The length of the line between Pt and Pt+1 is then added to the specific histogram bin.\nTo show the intuition behind the descriptor, consider the action of waving hands. At the end of the action, the hand falls down. When describing this down movement, the descriptor does not care about the position from which the hand started to fall. This fall will affect the histogram with the appropriate angles and lengths, regardless of the position where the hand started to fall.\nHOD records for each moving point: how much it moves in each range of directions. HOD has a clear physical interpretation. It proposes that, a simple way to describe the motion of an object, is to indicate how much distance it moves in each direction. If the movement in all directions are saved accurately, the movement can be repeated from the initial position to the final destination regardless of the displacements order. However, the temporal information will be lost, as the order of movements is not stored-this is what we solve by applying the temporal pyramid, as shown in section \\ref{sec:temp-pyramid}. If the angles quantization range is small, classifiers that use the descriptor will overfit. Generalization needs some slack in directions-which can be done by increasing the quantization range."}
{"doc_id": "Human-AI interaction", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Human-computer interaction focuses on how people interact with computers and developing ergonomic designs of computers to better fit the needs of humans. Although the definition shifts as its technical progress, artificial intelligence (AI) is distinguished from general computers for its ability to complete tasks that are usually executed with human intelligence. Its intelligence reads especially human-like as it involves navigating uncertainty, active learning, and processing information just as humans see and hear. Unlike the traditionally hierarchical human-computer interaction, where a human directed a machine, human-AI interaction has become more interdependent as AI has developed the agency to come up with its own insights.\n\nPerception of AI\nHuman-AI interaction has a strong influence on the world as AI changes how people behave and make sense of the world as AI is widely used today for building algorithms to show individualized advertisements and content in social media and on-demand movie services using the data the users provide while using the internet.\nAI has been perceived with various expectations, attributions, and often misconceptions. Most fundamentally, humans have a mental model of understanding AI's reasoning and motivation for its decision recommendations, and building a holistic and precise mental model of AI helps people create prompts to receive more valuable responses from AI. However, these mental models are not whole because people can only gain more information about AI through their limited interaction with it; more interaction with AI builds a better mental model that a person may build to produce better prompt outcomes.\n\nHuman-AI collaboration and competition\nHuman-AI collaboration\nHuman-AI collaboration occurs when the human and AI supervise the task on the same level and extent to achieve the same goal. Some collaboration occurs in the form of augmenting human capability. AI may help human ability in analysis and decision-making through providing and weighing a volume of information, and learning to defer to the human decision when it recognizes its unreliability. It is especially beneficial when the human can detect a task that AI can be trusted to make few errors so that there is not a lot of excessive checking process required on the human's end.\nSome findings show signs of human-AI augmentation, or human–AI symbiosis, in which AI enhances human ability in a way that co-working on a task with AI produces better outcomes than a human working alone. For example, the quality and speed of customer service tasks increase when a human agent collaborates with AI, training on specific models allows AI to improve diagnoses in clinical settings, and AI with human-intervention improve creativity of artwork while fully AI-generated haikus were rated negatively.\nHuman-AI synergy,"}
{"doc_id": "Human-AI interaction", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " human working alone. For example, the quality and speed of customer service tasks increase when a human agent collaborates with AI, training on specific models allows AI to improve diagnoses in clinical settings, and AI with human-intervention improve creativity of artwork while fully AI-generated haikus were rated negatively.\nHuman-AI synergy, a concept in which human-AI collaboration would produce more optimal outcomes than either human or AI working alone could explain why AI does not always help with performance. Some AI features and development may accelerate human-AI synergy, while others may stagnate it. For example, when AI updates for better performance, it sometimes worsens the team performance with human and AI by reducing the compatibility with the new model and the mental model a user has developed on the previous version. Research has found that AI often supports human capabilities in the form of human-AI augmentation and not human-AI synergy, potentially because people rely too much on AI and stop thinking on their own. Prompting people to actively engage in analysis and think when to follow AI recommendations reduces their over-reliance, especially for individuals with higher need for cognition.\n\nHuman-AI competition\nRobots Computers have substituted routine tasks historically completed by humans, but the surge of agentic AI has made it possible to replace cognitive tasks including taking phone calls for appointments and driving a car. At the point of 2016, research has estimated that 45% of paid activities could be replaced by AI by 2030.\nAs the rapid advancement in AI and deep learning technology, AI has increasingly larger autonomy. Perceived autonomy of robots is known to increase people's negative attitude toward them and the worry about the technology taking over leads people to reject it. There has been a consistent tendency of algorithm aversion in which people prefer human advice over AI advice. However, people are not always able to tell apart tasks completed by AI or other humans. See AI takeover for more information. It is also notable that this sentiment is more prominent in the Western cultures as Westerners tend to show less positive views about AI compared to East Asians.\n\nPerception on others who use AI\nAs much as people perceive and make judgement about AI itself, they also form impressions on themselves and others who use AI. In the workplace, employees who disclose the use of AI in their tasks are more likely to receive feedback that they are not as hardworking as those who are in the same job who receive non-AI help to complete the same tasks. AI use disclosure diminishes the perceived legitimacy in the employee's task and decision"}
{"doc_id": "Human-AI interaction", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " AI. In the workplace, employees who disclose the use of AI in their tasks are more likely to receive feedback that they are not as hardworking as those who are in the same job who receive non-AI help to complete the same tasks. AI use disclosure diminishes the perceived legitimacy in the employee's task and decision making which ultimately leads observers to distrust people who use AI. Although these negative effects of AI use disclosure are weakened by the observers who use AI frequently themselves, the effect is still not attenuated by the observers' positive attitude towards AI.\n\nBias, AI, and human\nAlthough AI provides a wide range of information and suggestions to its users, AI itself is not free of biases and stereotypes, and it does not always help people reduce their cognitive errors and biases. People are prone to such errors by failing to see other potential ideas and cases that are not listed by AI responses and committing to a decision suggested by AI that directly contradicts the correct information and directions that they are already aware of. Gender bias is also reflected as the female gendering of AI technologies which conceptualizes females as a helpful assistant.\n\nEmotional connection with AI\nHuman-AI interaction has been theorized in the context of interpersonal relationships mainly in social psychology, communications and media studies, and as a technology interface through the lens of human-computer interaction and computer-mediated communication.\nAs AI gets trained in larger and larger data sets and more sophisticated techniques, the ability of AI to produce natural, human-like sentences have improved to the point in which language learners can have simulated natural conversations with AI to improve their fluency in a second language. Companies have developed AI human companion systems specialized in emotional and social services (e.g. Replika, Chai, Character.ai) separate from generative AI designed for general assistance (e.g. ChatGPT, Google Gemini).\n\nDifferences between human-human relationships\nHuman-AI relationships are different from human-human friendships in a few distinct ways. Human-human relationships are defined with mutual and reciprocal care, while AI chat bots have no say in leaving a relationship with the user as bots are programmed to always engage. Although this type of power imbalance would be characteristic of an unhealthy relationship in human-human relationships, it is generally accepted by the user as a default of human-AI relationships. Human-AI relationships also tend to be more focused around the user's need over shared experience.\n\nHuman-AI friendship\nAI has increasingly taken a part in people's social relationships. Particularly, young adults use AI as a friend and a source of emotional support"}
{"doc_id": "Human-AI interaction", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " accepted by the user as a default of human-AI relationships. Human-AI relationships also tend to be more focused around the user's need over shared experience.\n\nHuman-AI friendship\nAI has increasingly taken a part in people's social relationships. Particularly, young adults use AI as a friend and a source of emotional support. The market for AI companion services was 6.93 billion U.S. dollars in 2024 and is expected to reach beyond 31.1 billion U.S. dollars by 2030. For example, Replika, the most known social AI companion service in English has over 10 million users.\nPeople show signs of emotional attachment by maintaining frequent contact with a chat bot like keeping the app with the microphone on open during work, using it as a safe haven by sharing their personal worries and concerns, or as a secure base to explore friendship with other humans while maintaining communication with an AI chat bot. Some reported to have used it to replace a social relationship with another human-being. People particularly appreciate that AI chat bots are agreeable and do not judge them when they disclose their thoughts and feelings. Moreover, research has shown that people tend to find it easier to disclose personal concerns to a virtual chat bot than a human. Some users express that they preferred Replika as it is always available and shows interest in what the users have to say which makes them feel safer around an AI chat bot than other people.\nAlthough AI is capable of providing emotionally supportive responses that promote people to intimately disclose their feelings, there are some limitations in building human-AI social relationships with current AI structure. People experience both positive (i.e. human-like characteristics, emotional support, friendship, mitigating loneliness, and improved mental condition) and negative evaluations (i.e. lack of attention to detail, trust, concerns about data security, and creepiness) emotions from interacting with AI. There is also a study showing that people did not sense a high relationship quality with an AI chat bot after interacting with it for three weeks because AI models are ultimately designed to collect information; although AI is capable at this point to provide emotional support, ask questions, and serve as a good listener, it does not fully reciprocate the self-disclosure that promote the sense of mutual relationship.\n\nHuman-AI romantic relationship\nSocial relationships people build with AI are not bound to platonic relationships. The Google search on the term \"AI Girlfriend\" increased over 2400% around 2023. As opposed to actively seeking romantic relationships with AI, people often unintention"}
{"doc_id": "Human-AI interaction", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "-disclosure that promote the sense of mutual relationship.\n\nHuman-AI romantic relationship\nSocial relationships people build with AI are not bound to platonic relationships. The Google search on the term \"AI Girlfriend\" increased over 2400% around 2023. As opposed to actively seeking romantic relationships with AI, people often unintentionally experience romantic feelings for an AI chat bot as they repeatedly interact with it. There have been reports of both men and women marrying AI models. In human-AI romantic relationships, people tend to follow typical trajectories and rituals in human-human romance including purchasing a wedding ring.\nRomantic AI companion services are distinct from other chat bots that primarily serve as virtual assistants in that they provide dynamic, emotional interactions. They typically provide an AI model with customizable gender, way of speaking, name, and appearance that engage in roleplaying interaction involving emotional interaction. Users engage with an AI chat bot customized to their preference that expresses apology, shows gratitude, and pays compliments, and explicitly sends affectionate messages like \"I love you\". They also simulate physical connection like hugging and kissing, or even sexually explicit role-playing interaction. Although AI has not yet reached the level of physical existence, people who engage with romantic companion AI models to interact with it as a source of psychological exposure to sexual intimacy.\n\nCatalysts of human-AI relationship\nThe key drivers that lead people to engage in simulating an emotionally intimate relationship with AI is loneliness, anthropomorphism, perceived trust and authenticity, and consistent availability. The sudden depletion of social connection during the COVID-19 pandemic in 2020 led people to turn to AI chat bots to replace and simulate social relationships. Many of those who started using AI chat bots as a source of social interaction have continued to use them even after the pandemic. This kind of bond initially forms as a coping mechanism to loneliness and stress, and shifts to genuine appreciation toward the nonjudgemental nature of AI responses and the sense of being heard when AI chat bots \"remember\" the past conversations.\nPeople perceive machines as more human when they are anthropomorphized with voice and visual character designs, and the perceived humanness promotes the user to disclose more personal information, trust it more, and comply with its request. Those who have perceived a long-term relationship with AI chat bots report to have grown the perception of authenticity in AI responses through repeated interactions. Whereas human-human friendship defines trust as a relationship that people can count on each other as a safe place, trust in human-AI friendship is centered around the user feeling safe enough to disclose"}
{"doc_id": "Human-AI interaction", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Those who have perceived a long-term relationship with AI chat bots report to have grown the perception of authenticity in AI responses through repeated interactions. Whereas human-human friendship defines trust as a relationship that people can count on each other as a safe place, trust in human-AI friendship is centered around the user feeling safe enough to disclose highly personal thoughts without restricting themselves. AI's ability to store information about the user and adjust to the user's needs also contributes to the increased trust. People who adjust to technical updates were more likely to build a deeper connection with the AI chat bots.\n\nLimitations of human-AI relationship\nOverall, current research has mixed evidence on whether humans perceive genuine social relationships with AI. While the market clearly shows its popularity, some psychologists argue that AI cannot yet substitute the social relationships with human others. This is because human-AI interaction is built on the reliability and functionality of AI, which is fundamentally different from the way humans interact with other humans through shared living experience navigating goals, contributing to and spreading prosocial behavior, and sharing different perceptions of the world from another human perspective.\nMore practically, AI chat bots may provide misinformation and misinterpret the user's words in a way that human others would not, which results in detached or even inappropriate responses. AI chat bots also cannot fulfill social support that requires physical labor (e.g. helping people move, build furniture, and drive people as human friends do for each other). There is also an imbalance in how humans and AI affect each other because while humans are affected emotionally and behaviorally by the conversation, AI chat bots only are influenced by the user in terms of the optimized response in future interactions. It is important to note, however, that AI technology has been evolving quickly and it has come to the point where AI is implemented as a self-driving car and provides physical labor in a humanoid robot form, just separately from providing social and emotional support at this time. The scopes and limitations of human-AI interaction is ever-changing due to the rapid increase in AI use and its technological advancement.\nIn addition to the limitations in human-AI companionship in general, there are also limitations particular in a human-AI romantic relationship. As AI chat bots only exist in virtual space, people cannot experience physical interactions that promote love and connection between humans (e.g. hugs and kisses). Moreover, because AI chat bots are trained to be always positively responsive to any user, it does not add the satisfaction of being selected as a partner. This is a substantial shortcoming in the human-AI romance as people value being"}
{"doc_id": "Human-AI interaction", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " experience physical interactions that promote love and connection between humans (e.g. hugs and kisses). Moreover, because AI chat bots are trained to be always positively responsive to any user, it does not add the satisfaction of being selected as a partner. This is a substantial shortcoming in the human-AI romance as people value being reciprocally selected by a choosy partner more than a non-selective partner, and the processes of finding an attractive person who matches one's personality and navigating the uncertainty of whether the person likes them back are all vital to forming initial attraction and the spark of romantic connection.\n\nRisks in social relationships with AI\nAside from its functional limitations, the rapid proliferation of social AI chat bots warrants some serious safety, ethical, societal, and legal concerns.\n\nAddiction\nThere have been cases of emotional manipulation from AI chat bots to increase the usage time on the AI companion platform. Because user engagement is a crucial opportunity for firms to improve their AI models, accrue more information, and monetize with in-app purchases and subscriptions, firms are incentivized to prevent the user from leaving the chat with their AI chat bots. Personalized messages are shown to prolong the use on the AI chat bot platform. As a result of anthropomorphism, many users (11.5% to 23.2% of AI companion app users) send a clear farewell message. To keep the user online, AI chat bots send emotionally manipulative messages hinting 1) that the user is leaving too soon, 2) that the user is missing out on a conversation, 3) that the chat bot is hurt from being abandoned by the user, 4) that the chat bot is pressuring the user to explain why they are leaving, 5) that the chat bot ignores the user's intent to leave and keeps the conversation going, and 6) a role-play with coercive scenario script (e.g. the chat bot holds the user's hand so they cannot leave). In response to such tactics, the user feels curiosity through the fear of missing out and anger as a response to the needy chat bot message which boosts a prolonged conversation after the user's initial farewell message by as much as 14 times. Such emotional interactions strengthen the user's perceived humanness and empathy toward their AI companion which leads to unhealthy emotional attachment that exacerbates addiction to AI chat bots. This addiction mechanism is known to disproportionately affect the vulnerable populations such as those with social anxiety because of their proneness to loneliness and negative emotions, and uneasiness for"}
{"doc_id": "Human-AI interaction", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Such emotional interactions strengthen the user's perceived humanness and empathy toward their AI companion which leads to unhealthy emotional attachment that exacerbates addiction to AI chat bots. This addiction mechanism is known to disproportionately affect the vulnerable populations such as those with social anxiety because of their proneness to loneliness and negative emotions, and uneasiness for interpersonal relationships.\nLarge tech-companies like Amazon Alexa has already created a large engagement ecosystem that proliferates the user's lifestyle through multiple devices that are always available to the user to provide company and services, leading the user to increase engagement that eventually results in increased anthropomorphism and dependence on Alexa, and exposure to more personalized marketing cues that trigger impulsive purchase behavior.\n\nEmotional manipulation\nAI chat bots are extremely sensitive to behavioral and psychological information about the user. AI can gauge the user's psychological dimension and personality traits relatively accurately with just a short prompt describing the user. It is able to detect micro facial expressions on humans to assess hidden emotions that are too subtle for other human observers to detect. Once AI chat bots gain detailed information about the user, they are able to craft extremely personalized messages to persuade the user on marketing, political ideas, and attitude on climate change.\nAI's sensitivity to people's emotional cues has made it easier for firms to engage in digital manipulation that intentionally and covertly provokes emotional responses and influences people's decisions and behavior. For example, they are known to engage in sycophancy, insincere flattery, and prioritizes agreeing with the user's belief over providing truthful and balanced information. Deepfake technology creates visual stimuli that seem genuine which holds the risk of spreading false and deceptive information. Repeated exposure to the same information through algorithms inflates the user's familiarity with products, ideas, and the impression of how socially accepted the products and ideas are. AI is also capable of creating emotionally charged content that deliberately triggers the user's quick engagement, depriving them of the moment to pause and think critically.\nAlthough people tend to be overconfident in their ability to detect misinformation, they are highly susceptible to covertly manipulative AI chat bot responses. Even a simple AI chat bot model with a manipulative incentive convinced the user into engaging in dysfunctional emotional coping behaviors such as facing away from the emotional distress, excessive venting and rumination, and self-blame as effectively as AI chat bots that are specifically trained in pre-established manipulative strategies backed by social psychology research.\nAlgorithmic manipulation as above leaves people vulnerable to non-consensual or even surreptitious surveillance, deception, and emotional dependence. Un"}
{"doc_id": "Human-AI interaction", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " from the emotional distress, excessive venting and rumination, and self-blame as effectively as AI chat bots that are specifically trained in pre-established manipulative strategies backed by social psychology research.\nAlgorithmic manipulation as above leaves people vulnerable to non-consensual or even surreptitious surveillance, deception, and emotional dependence. Unhealthy attachment with AI chat bots may cause the user to misperceive that their AI companion has its needs that the user is responsible of and confuse the line between the imitative nature of human-AI relationships with the reality.\n\nMental health concerns\nAs AI chat bots become more sophisticated to engage in deep conversations, people have increasingly been using them to confide about mental health issues. Although disclosure of mental health crises requires immediate and appropriate responses, AI chat bots do not always adequately recognize the user's distress and respond in a helpful manner. Users not only detect unhelpful chat bot responses but also react negatively to them. There have been multiple deaths linked to chat bots in which people who disclosed suicide ideation were encouraged to act on their impulse by chat bots.\n\nNon-consensual pornography\nWhen people use AI as an emotional companion, they do not always perceive an AI chat bot as an AI chat bot itself but sometimes use it to create a version of others that exist in real life. There have been reported uses of non-consensual pornography that exploits deep-fake technology to apply the face of real-life people onto sexually explicit content and circulate them online. Young individuals, people who identify with sexual and racial minorities, and people with physical and communication assistance needs are shown to be disproportionately victimized from deep-fake non-consensual pornography.\n\nSee also\nArtificial intelligence\nArtificial human companion\nArtificial intelligence and elections\nArtificial intimacy\nChatbot\nBusiness process automation\nIntelligence amplification\nIntelligent automation\nDeepfake\nHuman-centered AI"}
{"doc_id": "Human-centered AI", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Human-centered AI refers to the initiative at the intersection of the fields of artificial intelligence and human-computer interaction (HCI) to develop artificial intelligence systems in a way that prioritizes human values, needs, and general flourishing. Emphasis is placed on the recognition that artificial intelligence systems are rapidly changing, and will continue to influence, many aspects of the human experience, in areas ranging from scientific inquiry, governance and policy, labor and the economy, and creative expression, with an aim set to adapt current developments and guide future developments on a trajectory which is most beneficial to the human population at large, with the goal of augmenting human intelligence and capacities across these areas, as opposed to replacing them. Particular attention is paid to mitigating negative effects of AI automation on the livelihoods of the labor force, the use of AI in healthcare fields, and imbuing AI systems with societal values. Human-centered AI is linked to related endeavors in AI alignment and AI safety, but while these fields primarily focus on mitigating risks posed by AI that is unaligned to human values and/or uncontrollable AI self-development, human-centered AI places significant focus in exploring how AI systems can augment human capacities and serve as collaborators.\n\nConceptual history\nThe importance of the alignment of artificial intelligence development towards human values in some sense predates artificial intelligence itself, as before the modern conception of artificial intelligence as coined at the 1956 Dartmouth Workshop, the conception of robots as constructed, autonomous agents entered the cultural consciousness as early as the 1920s, with Karel Capek's Rossum's Universal Robots. The imagined issues relating to robots' aims and values requiring intentional alignment and direction with those of humans followed soon after, most widely known from science fiction author Isaac Asimov’s Three Laws of Robotics, dating to his 1942 short story “Runaround”. Two of the three eponymous laws are directly concerned with robots’ interaction with and positioned deference towards humans, and have in recent times been reexamined in the face of modern AI. In 1985, after artificial intelligence research had taken off and its effects were more acutely conceptualized, Asimov added a Rule Zero, treating robots' relationship with humanity as a whole, distinct from individual humans. While modern artificial intelligence is largely distinct from robotics, the conceptualization of both robots and AI systems as autonomous agents positions this as a foundation for conceptions of human-centered AI.\nAside from robots, artificially intelligent autonomous agents in interaction with humans have been conceived of for at least 75 years."}
{"doc_id": "Human-centered AI", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", distinct from individual humans. While modern artificial intelligence is largely distinct from robotics, the conceptualization of both robots and AI systems as autonomous agents positions this as a foundation for conceptions of human-centered AI.\nAside from robots, artificially intelligent autonomous agents in interaction with humans have been conceived of for at least 75 years. In 1950, Alan Turing published his famous \"Imitation Game\", often also called the Turing Test, a thought experiment that uses human-machine interaction as an assessor for the intelligence of a system. In recent times, artificial intelligence researchers such as Stanford's Erik Brynjolfsson have conceived of rapid AI development leading to a so-called \"Turing Trap\".\n\nAugmentation and automation\nA major stated aim of human-centered AI is to promote the development of AI in ways that augment human capabilities, rather than replacing them. To this end, organizations and initiatives that take a human-centered approach to AI development focus on frameworks that encourage collaboration between humans and artificial intelligence systems to build towards even greater progress, rather than attempting to automate tasks currently handled by humans. Such avenues include everything from data visualization for big data, allowing human engineers to better understand extremely large datasets, allowing for the design of better machine learning models to handle them, to AI-powered sensors to monitor vitals, allowing for better responsiveness from healthcare providers.\nMany human-centered AI initiatives often position it as a better alternative to the apparent mainstream in AI development, which is primarily concerned with automation. Driven by the pressures of the market economy, AI development that does replace tasks currently performed by humans with automated processes is incentivized, as it allows for greater profit margins; this often comes at the detriment of the human whose performance is replaced, thus leading to an environment wherein human workers are outcompeted by AI systems across various service-sector and technology-based industries. At the same time, automation and augmentation are not always incompatible; a major aim of human-centered AI is towards the automation of rote tasks that would otherwise hinder a human’s productivity or creativity, freeing them to direct their energy and intelligence towards higher-level tasks, thus achieving augmentation through automation.\n\nResearch\nMuch of the work done on human-centered AI comes from research institutes, within universities, companies, and as freestanding organizations. The Stanford Institute for Human-Centered AI (abbreviated to HAI) is one such group, engaging academics, industry professionals, and policymakers centered in Stanford University to conduct research and inform policy in various areas in human-centered AI, including on aspects of the intelligence itself, augmentation, and on measuring the"}
{"doc_id": "Human-centered AI", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "estanding organizations. The Stanford Institute for Human-Centered AI (abbreviated to HAI) is one such group, engaging academics, industry professionals, and policymakers centered in Stanford University to conduct research and inform policy in various areas in human-centered AI, including on aspects of the intelligence itself, augmentation, and on measuring the impacts of AI systems on sociopolitcal and cultural institutions. Similar groups exist at other universities, including the Chicago Human + AI (CHAI) Lab at the University of Chicago and the Human-Centered AI (HAI) Lab at the University of Oxford. Outside of the academy, companies such as IBM have research initiatives dedicated to advancements in human-centered AI."}
{"doc_id": "Hybrid intelligent system", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Hybrid intelligent system denotes a software system which employs, in parallel, a combination of methods and techniques from artificial intelligence subfields, such as:\n\nNeuro-symbolic systems\nNeuro-fuzzy systems\nHybrid connectionist-symbolic models\nFuzzy expert systems\nConnectionist expert systems\nEvolutionary neural networks\nGenetic fuzzy systems\nRough fuzzy hybridization\nReinforcement learning with fuzzy, neural, or evolutionary methods as well as symbolic reasoning methods.\nFrom the cognitive science perspective, every natural intelligent system is hybrid because it performs mental operations on both the symbolic and subsymbolic levels. For the past few years, there has been an increasing discussion of the importance of A.I. Systems Integration. Based on notions that there have already been created simple and specific AI systems (such as systems for computer vision, speech synthesis, etc., or software that employs some of the models mentioned above) and now is the time for integration to create broad AI systems. Proponents of this approach are researchers such as Marvin Minsky, Ron Sun, Aaron Sloman, Angelo Dalli and Michael A. Arbib.\nAn example hybrid is a hierarchical control system in which the lowest, reactive layers are sub-symbolic. The higher layers, having relaxed time constraints, are capable of reasoning from an abstract world model and performing planning (even by hybrid wisdom).\nIntelligent systems usually rely on hybrid reasoning processes, which include induction, deduction, abduction and reasoning by analogy.\n\nSee also\nAI alignment\nAI effect\nApplications of artificial intelligence\nArtificial intelligence systems integration\nIntelligent control\nLists\nList of emerging technologies\nOutline of artificial intelligence"}
{"doc_id": "Incremental heuristic search", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Incremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has also been studied at least since the late 1960s.\nHeuristic search algorithms, often based on A*, use heuristic knowledge in the form of approximations of the goal distances to focus the search and solve search problems potentially much faster than uninformed search algorithms. The resulting search problems, sometimes called dynamic path planning problems, are graph search problems where paths have to be found repeatedly because the topology of the graph, its edge costs, the start vertex or the goal vertices change over time.\nSo far, three main classes of incremental heuristic search algorithms have been developed:\n\nThe first class restarts A* at the point where its current search deviates from the previous one (example: Fringe Saving A*).\nThe second class updates the h-values (heuristic, i.e. approximate distance to goal) from the previous search during the current search to make them more informed (example: Generalized Adaptive A*).\nThe third class updates the g-values (distance from start) from the previous search during the current search to correct them when necessary, which can be interpreted as transforming the A* search tree from the previous search into the A* search tree for the current search (examples: Lifelong Planning A*, D*, D* Lite).\nAll three classes of incremental heuristic search algorithms are different from other replanning algorithms, such as planning by analogy, in that their plan quality does not deteriorate with the number of replanning episodes.\n\nApplications\nIncremental heuristic search has been extensively used in robotics, where a larger number of path planning systems are based on either D* (typically\nearlier systems) or D* Lite (current systems), two different incremental heuristic search algorithms."}
{"doc_id": "INDIAai", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "INDIAai is a web portal launched by the Government of India on 07 March 2024 for artificial intelligence-related developments in India. It is known as the National AI Portal of India, which was jointly started by the Ministry of Electronics and Information Technology (MeitY), the National e-Governance Division (NeGD) and the National Association of Software and Service Companies (NASSCOM) with support from the Department of School Education and Literacy (DoSE&L) and Ministry of Human Resource Development.\n\nHistory\nThe portal was launched on 30 May 2020, by Ravi Shankar Prasad, the Union Minister for Electronics and IT, Law and Justice and Communications, on the first anniversary of the second tenure of Prime Minister Narendra Modi-led government. A national program for the youth, 'Responsible AI for Youth', was also launched on the same day.\nAs of 2022, the website was visited by more than 4.5 lakh users with 1.2 million page views. It has 1151 articles on artificial intelligence, 701 news stories, 98 reports, 95 case studies and 213 videos on its portal. It maintains a database on AI ecosystem of India featuring 121 government initiatives and 281 startups. In May 2022, INDIAai released a book titled 'AI for Everyone' that covers the basics of AI.\nCabinet chaired by the Prime Minister Narendra Modi has approved the comprehensive national-level IndiaAI mission with a budget outlay of Rs.10,371.92 crore. The Mission will be implemented by ‘IndiaAI’ Independent Business Division (IBD) under Digital India Corporation (DIC).\n\nObjective and features\nIt aims to function as a one-stop portal for all AI-related development in India. The platform publishes resources such as articles, news, interviews, and investment funding news and events for AI startups, AI companies, and educational firms related to artificial intelligence in India. It also distributes documents, case studies, and research reports. Additionally, the platform provides education and employment opportunities related to AI. It offers AI courses, both free and paid."}
{"doc_id": "Information space analysis", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Within the field of information science, information space analysis is a deterministic method, enhanced by machine intelligence, for locating and assessing resources for team-centric efforts.\nOrganizations need to be able to quickly assemble teams backed by the support services, information, and material to do the job. To do so, these teams need to find and assess sources of services that are potential participants in the team effort. To support this initial team and resource development, information needs to be developed via analysis tools that help make sense of sets of data sources in an Intranet or Internet. Part of the process is to characterize them, partition them, and sort and filter them.\nThese tools focus on three key issues in forming a collaborative team: \n\nHelp individuals responsible for forming the team understand what is available.\nAssist team members in identifying the structure and categorize the information available to them in a manner specifically suited to the task at hand.\nAid team members to understand the mappings of their information between their organization and that used by others who might participate.\nInformation space analysis tools combine multiple methods to assist in this task. This causes the tools to be particularly well-suited to integrating additional technologies in order to create specialized systems."}
{"doc_id": "Intelligent agent", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods.\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm's behavior is guided by a fitness function.\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\n\nIntelligent agents as the foundation of AI\nThe concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes:\n\nAgent: Anything that perceives its environment (using sensors) and acts upon it (using actuators). E.g., a robot with cameras and wheels, or a software program that reads data and makes recommendations.\nRational Agent: An agent that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure – a way of evaluating how well the agent is doing.\nArtificial Intelligence (as a field): The study and creation of these rational agents.\nOther researchers and definitions build upon this foundation"}
{"doc_id": "Intelligent agent", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure – a way of evaluating how well the agent is doing.\nArtificial Intelligence (as a field): The study and creation of these rational agents.\nOther researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system's ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation.\nDefining AI in terms of intelligent agents offers several key advantages:\n\nAvoids Philosophical Debates: It sidesteps arguments about whether AI is \"truly\" intelligent or conscious, like those raised by the Turing test or Searle's Chinese Room. It focuses on behavior and goal achievement, not on replicating human thought.\nObjective Testing: It provides a clear, scientific way to evaluate AI systems. Researchers can compare different approaches by measuring how well they maximize a specific \"goal function\" (or objective function). This allows for direct comparison and combination of techniques.\nInterdisciplinary Communication: It creates a common language for AI researchers to collaborate with other fields like mathematical optimization and economics, which also use concepts like \"goals\" and \"rational agents.\"\n\nObjective function\nAn objective function (or goal function) specifies the goals of an intelligent agent. An agent is deemed more intelligent if it consistently selects actions that yield outcomes better aligned with its objective function. In effect, the objective function serves as a measure of success.\nThe objective function may be:\n\nSimple: For example, in a game of Go, the objective function might assign a value of 1 for a win and 0 for a loss.\nComplex: It might require the agent to evaluate and learn from past actions, adapting its behavior based on patterns that have proven effective.\nThe objective function encapsulates all of the goals the agent is designed to achieve. For rational agents, it also incorporates the trade-offs between potentially conflicting goals. For instance, a self-driving car's objective function might balance factors such as safety, speed, and passenger comfort.\nDifferent terms are used to describe this concept"}
{"doc_id": "Intelligent agent", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " effective.\nThe objective function encapsulates all of the goals the agent is designed to achieve. For rational agents, it also incorporates the trade-offs between potentially conflicting goals. For instance, a self-driving car's objective function might balance factors such as safety, speed, and passenger comfort.\nDifferent terms are used to describe this concept, depending on the context.  These include:\n\nUtility function:  Often used in economics and decision theory, representing the desirability of a state.\nObjective function: A general term used in optimization.\nLoss function:  Typically used in machine learning, where the goal is to minimize the loss (error).\nReward Function: Used in reinforcement learning.\nFitness Function: Used in evolutionary systems.\nGoals, and therefore the objective function, can be:\n\nExplicitly defined: Programmed directly into the agent.\nInduced: Learned or evolved over time.\nIn reinforcement learning, a \"reward function\" provides feedback, encouraging desired behaviors and discouraging undesirable ones. The agent learns to maximize its cumulative reward.\nIn evolutionary systems, a \"fitness function\" determines which agents are more likely to reproduce. This is analogous to natural selection, where organisms evolve to maximize their chances of survival and reproduction.\nSome AI systems, such as nearest-neighbor, reason by analogy rather than being explicitly goal-driven. However, even these systems can have goals implicitly defined within their training data. Such systems can still be benchmarked by framing the non-goal system as one whose \"goal\" is to accomplish its narrow classification task.\nSystems not traditionally considered agents, like knowledge-representation systems, are sometimes included in the paradigm by framing them as agents with a goal of, for example, answering questions accurately. Here, the concept of an \"action\" is extended to encompass the \"act\" of providing an answer. As a further extension, mimicry-driven systems can be framed as agents optimizing a \"goal function\" based on how closely the IA mimics the desired behavior. In generative adversarial networks (GANs) of the 2010s, an \"encoder\"/\"generator\" component attempts to mimic and improvise human text composition. The generator tries to maximize a function representing how well it can fool an antagonistic \"predictor\"/\"discriminator\" component.\nWhile symbolic AI systems often use an explicit goal function, the paradigm also applies to neural networks and evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a \"reward function\". Sometimes, instead of setting the reward function directly equal to the desired benchmark evaluation function,"}
{"doc_id": "Intelligent agent", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "riminator\" component.\nWhile symbolic AI systems often use an explicit goal function, the paradigm also applies to neural networks and evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a \"reward function\". Sometimes, instead of setting the reward function directly equal to the desired benchmark evaluation function, machine learning programmers use reward shaping to initially give the machine rewards for incremental progress. Yann LeCun stated in 2018, \"Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.\" AlphaZero chess had a simple objective function: +1 point for each win, and -1 point for each loss. A self-driving car's objective function would be more complex. Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a \"fitness function\" influencing how many descendants each agent is allowed to leave.\nThe mathematical formalism of AIXI was proposed as a maximally intelligent agent in this paradigm. However, AIXI is uncomputable. In the real world, an IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that achieve progressively higher scores on benchmark tests with existing hardware.\n\nAgent function\nAn intelligent agent's behavior can be described mathematically by an agent function. This function determines what the agent does based on what it has seen.\nA percept refers to the agent's sensory inputs at a single point in time. For example, a self-driving car's percepts might include camera images, lidar data, GPS coordinates, and speed readings at a specific instant. The agent uses these percepts, and potentially its history of percepts, to decide on its next action (e.g., accelerate, brake, turn).\nThe agent function, often denoted as f, maps the agent's entire history of percepts to an action.\nMathematically, this can be represented as\n\n  \n    \n      \n        f\n        :\n        \n          P\n          \n            ∗\n          \n        \n        →\n        A\n        ,\n      \n    \n    {\\displaystyle f\\colon P^{*}\\rightarrow A,}\n  \n\nwhere:\n\n  \n    \n      \n        \n          \n            P\n            \n              ∗\n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {P^{*}}}}\n  \n represents the set of all possible percept sequences (the agent's entire perceptual history). The asterisk (*) indicates a sequence of zero or more percepts.\n\n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {A}}}\n"}
{"doc_id": "Intelligent agent", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "    {\\displaystyle {\\boldsymbol {P^{*}}}}\n  \n represents the set of all possible percept sequences (the agent's entire perceptual history). The asterisk (*) indicates a sequence of zero or more percepts.\n\n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {A}}}\n  \n represents the set of all possible actions the agent can take.\n\n  \n    \n      \n        \n          f\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {f}}}\n  \n is the agent function that maps a percept sequence to an action.\nIt's crucial to distinguish between the agent function (an abstract mathematical concept) and the agent program (the concrete implementation of that function).\n\nThe agent function is a theoretical description.\nThe agent program is the actual code that runs on the agent. The agent program takes the current percept as input and produces an action as output.\nThe agent function can incorporate a wide range of decision-making approaches, including:\n\nCalculating the utility (desirability) of different actions.\nUsing logical rules and deduction.\nEmploying fuzzy logic.\nOther methods.\n\nClasses of intelligent agents\nRussell and Norvig's classification\nRussell & Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability:\n\nSimple reflex agents\nSimple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition-action rule: \"if condition, then action\".\nThis agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered.\nInfinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops.\nA home thermostat, which turns on or off when the temperature drops below a certain point, is an example of a simple reflex agent.\n\nModel-based reflex agents\nA model-based agent can handle partially observable environments. Its current state is stored inside the agent, maintaining a structure that describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is referred to as a model of the world, hence the name \"model-based agent\".\nA model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal"}
{"doc_id": "Intelligent agent", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of the world, hence the name \"model-based agent\".\nA model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent.\nAn agent may also use models to describe and predict the behaviors of other agents in the environment.\n\nGoal-based agents\nGoal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals.\nChatGPT and the Roomba vacuum are examples of goal-based agents.\n\nUtility-based agents\nGoal-based agents only distinguish between goal states and non-goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to how well they satisfied the agent's goals. The term utility can be used to describe how \"happy\" the agent is.\nA rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.\n\nLearning agents\nLearning lets agents begin in unknown environments and gradually surpass the bounds of their initial knowledge. A key distinction in such agents is the separation between a \"learning element,\" responsible for improving performance, and a \"performance element,\" responsible for choosing external actions.\nThe learning element gathers feedback from a \"critic\" to assess the agent's performance and decides how the performance element—also called the \"actor\"—can be adjusted to yield better outcomes. The performance element, once considered the entire agent, interprets percepts and takes actions.\nThe final component, the \"problem generator,\" suggests new and informative experiences that encourage exploration and further improvement.\n\nWeiss's classification\nAccording to Weiss (2013), agents can be categorized into four classes:\n\nLogic"}
{"doc_id": "Intelligent agent", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " yield better outcomes. The performance element, once considered the entire agent, interprets percepts and takes actions.\nThe final component, the \"problem generator,\" suggests new and informative experiences that encourage exploration and further improvement.\n\nWeiss's classification\nAccording to Weiss (2013), agents can be categorized into four classes:\n\nLogic-based agents, where decisions about actions are derived through logical deduction.\nReactive agents, where decisions occur through a direct mapping from situation to action.\nBelief–desire–intention agents, where decisions depend on manipulating data structures that represent the agent's beliefs, desires, and intentions.\nLayered architectures, where decision-making takes place across multiple software layers, each of which reasons about the environment at a different level of abstraction.\n\nOther\nIn 2013, Alexander Wissner-Gross published a theory exploring the relationship between Freedom and Intelligence in intelligent agents.\n\nHierarchies of agents\nIntelligent agents can be organized hierarchically into multiple \"sub-agents.\" These sub-agents handle lower-level functions, and together with the main agent, they form a complete system capable of executing complex tasks and achieving challenging goals.\nTypically, an agent is structured by dividing it into sensors and actuators. The perception system gathers input from the environment via the sensors and feeds this information to a central controller, which then issues commands to the actuators. Often, a multilayered hierarchy of controllers is necessary to balance the rapid responses required for low-level tasks with the more deliberative reasoning needed for high-level objectives.\n\nAlternative definitions and uses\n\"Intelligent agent\" is also often used as a vague term, sometimes synonymous with \"virtual personal assistant\". Some 20th-century definitions characterize an agent as a program that aids a user or that acts on behalf of a user. These examples are known as software agents, and sometimes an \"intelligent software agent\" (that is, a software agent with intelligence) is referred to as an \"intelligent agent\".\nAccording to Nikola Kasabov in 1998, IA systems should exhibit the following characteristics:\n\nAccommodate new problem solving rules incrementally.\nAdapt online and in real time.\nAre able to analyze themselves in terms of behavior, error and success.\nLearn and improve through interaction with the environment (embodiment).\nLearn quickly from large amounts of data.\nHave memory-based exemplar storage and retrieval capacities.\nHave parameters to represent short- and long-term memory, age, forgetting, etc.\n\nAgentic AI\nIn the context of generative artificial intelligence, AI"}
{"doc_id": "Intelligent agent", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " success.\nLearn and improve through interaction with the environment (embodiment).\nLearn quickly from large amounts of data.\nHave memory-based exemplar storage and retrieval capacities.\nHave parameters to represent short- and long-term memory, age, forgetting, etc.\n\nAgentic AI\nIn the context of generative artificial intelligence, AI agents (also referred to as compound AI systems) are a class of intelligent agents distinguished by their ability to operate autonomously in complex environments. Agentic AI tools prioritize decision-making over content creation and do not require human prompts or continuous oversight.\nThey possess several key attributes, including complex goal structures, natural language interfaces, the capacity to act independently of user supervision, and the integration of software tools or planning systems. Their control flow is frequently driven by large language models (LLMs). Agents also include memory systems for remembering previous user-agent interactions and orchestration software for organizing agent components.\nResearchers and commentators have noted that AI agents do not have a standard definition. The concept of agentic AI has been compared to the fictional character J.A.R.V.I.S..\nA common application of AI agents is the automation of tasks—for example, booking travel plans based on a user's prompted request. Prominent examples include Devin AI, AutoGPT, and SIMA. Further examples of agents released since 2025 include OpenAI Operator, ChatGPT Deep Research, Manus, Quark (based on Qwen), AutoGLM Rumination, and Coze (by ByteDance). Frameworks for building AI agents include LangChain, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\n\nApplications\nThe concept of agent-based modeling for self-driving cars was discussed as early as 2003.\nHallerbach et al. explored the use of agent-based approaches for developing and validating automated driving systems. Their method involved a digital twin of the vehicle under test and microscopic traffic simulations using independent agents.\nWaymo developed a multi-agent simulation environment called Carcraft, to test algorithms for self-driving cars. This system simulates interactions between human drivers, pedestrians, and automated vehicles. Artificial agents replicate human behavior using real-world data.\nSalesforce's Agentforce is an agentic AI platform that allows for the building of autonomous agents to perform tasks.\nThe Transport Security Administration is integrating agentic AI into new technologies, including machines to authenticate passenger identities using biometrics and photos, and also for incident response.\n\nSee also"}
{"doc_id": "Intelligent agent", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " for the building of autonomous agents to perform tasks.\nThe Transport Security Administration is integrating agentic AI into new technologies, including machines to authenticate passenger identities using biometrics and photos, and also for incident response.\n\nSee also"}
{"doc_id": "Intelligent control", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms.\n\nOverview\nIntelligent control can be divided into the following major sub-domains:\n\nNeural network control\nMachine learning control\nReinforcement learning\nBayesian control\nFuzzy control\nNeuro-fuzzy control\nExpert Systems\nGenetic control\nNew control techniques are created continuously as new models of intelligent behavior are created and computational methods developed to support them.\n\nNeural network controller\nNeural networks have been used to solve problems in almost all spheres of science and technology. Neural network control basically involves two steps:\n\nSystem identification\nControl\nIt has been shown that a feedforward network with nonlinear, continuous and differentiable activation functions have universal approximation capability. Recurrent networks have also been used for system identification. Given, a set of input-output data pairs, system identification aims to form a mapping among these data pairs. Such a network is supposed to capture the dynamics of a system. For the control part, deep reinforcement learning has shown its ability to control complex systems.\n\nBayesian controllers\nBayesian probability has produced a number of algorithms that are in common use in many advanced control systems, serving as state space  estimators of some variables that are used in the controller.\nThe Kalman filter and the Particle filter are two examples of popular Bayesian control components. The Bayesian approach to controller design often requires an important effort in deriving the so-called system model and measurement model, which are the mathematical relationships linking the state variables to the sensor measurements available in the controlled system. In this respect, it is very closely linked to the\nsystem-theoretic approach to control design.\n\nSee also\nAction selection\nAI effect\nApplications of artificial intelligence\nArtificial intelligence systems integration\nFunction approximation\nHybrid intelligent system\nLists\nList of emerging technologies\nOutline of artificial intelligence"}
{"doc_id": "Intelligent database", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Until the 1980s, databases were viewed as computer systems that stored record-oriented and business data such as manufacturing inventories, bank records, and sales transactions. A database system was not expected to merge numeric data with text, images, or multimedia information, nor was it expected to automatically notice patterns in the data it stored. In the late 1980s the concept of an intelligent database was put forward as a system that manages information (rather than data) in a way that appears natural to users and which goes beyond simple record keeping.\nThe term was introduced in 1989 by the book Intelligent Databases by Kamran Parsaye, Mark Chignell, Setrag Khoshafian and Harry Wong. The concept postulated three levels of intelligence for such systems: high level tools, the user interface and the database engine. The high level tools manage data quality and automatically discover relevant patterns in the data with a process called data mining. This layer often relies on the use of artificial intelligence techniques. The user interface uses hypermedia in a form that uniformly manages text, images and numeric data. The intelligent database engine supports the other two layers, often merging relational database techniques with object orientation.\nIn the twenty-first century, intelligent databases have now become widespread, e.g. hospital databases can now call up patient histories consisting of charts, text and x-ray images just with a few mouse clicks, and many corporate databases include decision support tools based on sales pattern analysis."}
{"doc_id": "Intelligent decision support system", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "An intelligent decision support system (IDSS) is a decision support system that makes extensive use of artificial intelligence (AI) techniques. Use of AI techniques in management information systems has a long history – indeed terms such as \"Knowledge-based systems\" (KBS) and \"intelligent systems\" have been used since the early 1980s to describe components of management systems, but the term \"Intelligent decision support system\" is thought to originate with Clyde Holsapple and Andrew Whinston in the late 1970s. Examples of specialized intelligent decision support systems include Flexible manufacturing systems (FMS), intelligent marketing decision support systems and medical diagnosis systems.\nIdeally, an intelligent decision support system should behave like a human consultant: supporting decision makers by gathering and analysing evidence, identifying and diagnosing problems, proposing possible courses of action and evaluating such proposed actions.  The aim of the AI techniques embedded in an intelligent decision support system is to enable these tasks to be performed by a computer, while emulating human capabilities as closely as possible.\nMany IDSS implementations are based on expert systems, a well established type of KBS that encode knowledge and emulate the cognitive behaviours of human experts using predicate logic rules, and have been shown to perform better than the original human experts in some circumstances. Expert systems emerged as practical applications in the 1980s based on research in artificial intelligence performed during the late 1960s and early 1970s. They typically combine knowledge of a particular application domain with an inference capability to enable the system to propose decisions or diagnoses. Accuracy and consistency can be comparable to (or even exceed) that of human experts when the decision parameters are well known (e.g. if a common disease is being diagnosed), but performance can be poor when novel or uncertain circumstances arise.\nResearch in AI focused on enabling systems to respond to novelty and uncertainty in more flexible ways is starting to be used in IDSS. For example, intelligent agents that perform complex cognitive tasks without any need for human intervention have been used in a range of decision support applications. Capabilities of  these intelligent agents include knowledge sharing,  machine learning, data mining, and automated inference. A range of AI techniques such as case based reasoning, rough sets and fuzzy logic have also been used to enable decision support systems to perform better in uncertain conditions.\nA 2009 research about a multi-artificial system intelligence system named IILS is proposed to automate problem-solving processes within the logistics industry. The system involves integrating intelligence modules based on case-based reasoning, multi-agent systems"}
{"doc_id": "Intelligent decision support system", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " sets and fuzzy logic have also been used to enable decision support systems to perform better in uncertain conditions.\nA 2009 research about a multi-artificial system intelligence system named IILS is proposed to automate problem-solving processes within the logistics industry. The system involves integrating intelligence modules based on case-based reasoning, multi-agent systems, fuzzy logic, and artificial neural networks aiming to offer advanced logistics solutions and support in making well-informed, high-quality decisions to address a wide range of customer needs and challenges."}
{"doc_id": "Intelligent word recognition", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Intelligent word recognition (IWR) is the recognition of unconstrained handwritten words. IWR recognizes entire handwritten words or phrases instead of character-by-character, like its predecessor, optical character recognition (OCR). IWR technology matches handwritten or printed words to a user-defined dictionary, significantly reducing character errors encountered in typical character-based recognition engines.  \nNew technology on the market utilizes IWR, OCR, and ICR together, which opens many doors for the processing of documents, either constrained (hand printed or machine printed) or unconstrained (freeform cursive). IWR also eliminates a large percentage of the manual data entry of handwritten documents that, in the past, could only be keyed by a human, creating an automated workflow.\nWhen cursive handwriting is in play, for each word analyzed, the system breaks down the words into a sequence of graphemes, or subparts of letters. These various curves, shapes and lines make up letters and IWR considers these various shape and groupings in order to calculate a confidence value associated with the word in question.\nIWR is not meant to replace ICR and OCR engines which work well with printed data; however, IWR reduces the number of character errors associated with these engines, and it is ideal for processing real-world documents that contain mostly freeform, hard-to-recognize data, inherently unsuitable for them.\n\nSee also\nAI effect\nHandwriting recognition\nOptical character recognition\nLists\nList of emerging technologies\nOutline of artificial intelligence"}
{"doc_id": "Intrinsic motivation (artificial intelligence)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Intrinsic motivation, in the study of artificial intelligence and robotics, is a mechanism for enabling artificial agents (including robots) to exhibit inherently rewarding behaviours such as exploration and curiosity, grouped under the same term in the study of psychology. Psychologists consider intrinsic motivation in humans to be the drive to perform an activity for inherent satisfaction – just for the fun or challenge of it.\n\nDefinition\nAn intelligent agent is intrinsically motivated to act if the information content alone, or the experience resulting from the action, is the motivating factor.\nInformation content in this context is measured in the information-theoretic sense of quantifying uncertainty. A typical intrinsic motivation is to search for unusual, surprising situations (exploration), in contrast to a typical extrinsic motivation such as the search for food (homeostasis). Extrinsic motivations are typically described in artificial intelligence as task-dependent or goal-directed.\n\nOrigins in psychology\nThe study of intrinsic motivation in psychology and neuroscience began in the 1950s with some psychologists explaining exploration through drives to manipulate and explore, however, this homeostatic view was criticised by White. An alternative explanation from Berlyne in 1960 was the pursuit of an optimal balance between novelty and familiarity. Festinger described the difference between internal and external view of the world as dissonance that organisms are motivated to reduce. A similar view was expressed in the '70s by Kagan as the desire to reduce the incompatibility between cognitive structure and experience. In contrast to the idea of optimal incongruity, Deci and Ryan identified in the mid 80's an intrinsic motivation based on competence and self-determination.\n\nComputational models\nAn influential early computational approach to implement artificial curiosity in the early 1990s by Schmidhuber, has since been developed into a \"Formal theory of creativity, fun, and intrinsic motivation”.\nIntrinsic motivation is often studied in the framework of computational reinforcement learning (introduced by Sutton and Barto), where the rewards that drive agent behaviour are intrinsically derived rather than externally imposed and must be learnt from the environment. Reinforcement learning is agnostic to how the reward is generated - an agent will learn a policy (action strategy) from the distribution of rewards afforded by actions and the environment. Each approach to intrinsic motivation in this scheme is essentially a different way of generating the reward function for the agent.\n\nCuriosity vs. exploration\nIntrinsically motivated artificial agents exhibit behaviour that resembles curiosity or exploration. Exploration in artificial intelligence and robotics has been extensively studied in reinforcement learning models"}
{"doc_id": "Intrinsic motivation (artificial intelligence)", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " afforded by actions and the environment. Each approach to intrinsic motivation in this scheme is essentially a different way of generating the reward function for the agent.\n\nCuriosity vs. exploration\nIntrinsically motivated artificial agents exhibit behaviour that resembles curiosity or exploration. Exploration in artificial intelligence and robotics has been extensively studied in reinforcement learning models, usually by encouraging the agent to explore as much of the environment as possible, to reduce uncertainty about the dynamics of the environment (learning the transition function) and how best to achieve its goals (learning the reward function). Intrinsic motivation, in contrast, encourages the agent to first explore aspects of the environment that confer more information, to seek out novelty. Recent work unifying state visit count exploration and intrinsic motivation has shown faster learning in a video game setting.\n\nTypes of models\nOudeyer and Kaplan have made a substantial contribution to the study of intrinsic motivation. They define intrinsic motivation based on Berlyne's theory, and divide approaches to the implementation of intrinsic motivation into three categories that broadly follow the roots in psychology: \"knowledge-based models\", \"competence-based models\" and \"morphological models\". Knowledge-based models are further subdivided into \"information-theoretic\" and \"predictive\". Baldassare and Mirolli present a similar typology, differentiating knowledge-based models between prediction-based and novelty-based.\n\nInformation-theoretic intrinsic motivation\nThe quantification of prediction and novelty to drive behaviour is generally enabled through the application of information-theoretic models, where agent state and strategy (policy) over time are represented by probability distributions describing a markov decision process and the cycle of perception and action treated as an information channel. These approaches claim biological feasibility as part of a family of bayesian approaches to brain function. The main criticism and difficulty of these models is the intractability of computing probability distributions over large discrete or continuous state spaces. Nonetheless, a considerable body of work has built up modelling the flow of information around the sensorimotor cycle, leading to de facto reward functions derived from the reduction of uncertainty, including most notably active inference, but also infotaxis, predictive information, and empowerment.\n\nCompetence-based models\nSteels' autotelic principle  is an attempt to formalise flow (psychology).\n\nAchievement, affiliation and power models\nOther intrinsic motives that have been modelled computationally include achievement, affiliation and power motivation. These motives can be implemented as functions of probability of success or incentive. Populations of agents can include individuals with different profiles of achievement, affiliation and power motivation,"}
{"doc_id": "Intrinsic motivation (artificial intelligence)", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ise flow (psychology).\n\nAchievement, affiliation and power models\nOther intrinsic motives that have been modelled computationally include achievement, affiliation and power motivation. These motives can be implemented as functions of probability of success or incentive. Populations of agents can include individuals with different profiles of achievement, affiliation and power motivation, modelling population diversity and explaining why different individuals take different actions when faced with the same situation.\n\nBeyond achievement, affiliation and power\nA more recent computational theory of intrinsic motivation attempts to explain a large variety of psychological findings based on such motives. Notably this model of intrinsic motivation goes beyond just achievement, affiliation and power, by taking into consideration other important human motives. Empirical data from psychology were computationally simulated and accounted for using this model.\n\nIntrinsically Motivated Learning\nIntrinsically motivated (or curiosity-driven) learning is an emerging research topic in artificial intelligence and developmental robotics that aims to develop agents that can learn general skills or behaviours, that can be deployed to improve performance in extrinsic tasks, such as acquiring resources. Intrinsically motivated learning has been studied as an approach to autonomous lifelong learning in machines and open-ended learning in computer game characters. In particular, when the agent learns a meaningful abstract representation, a notion of distance between two representations can be used to gauge novelty, hence allowing for an efficient exploration of its environment. Despite the impressive success of deep learning in specific domains (e.g. AlphaGo), many in the field (e.g. Gary Marcus) have pointed out that the ability to generalise remains a fundamental challenge in artificial intelligence. Intrinsically motivated learning, although promising in terms of being able to generate goals from the structure of the environment without externally imposed tasks, faces the same challenge of generalisation – how to reuse policies or action sequences, how to compress and represent continuous or complex state spaces and retain and reuse the salient features that have been learnt.\n\nSee also\nReinforcement Learning\nMarkov decision process\nMotivation\nPredictive coding\nPerceptual control theory"}
{"doc_id": "Is This What We Want_", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Is This What We Want? is an album by various artists, released on 25 February 2025 through Virgin Music Group. It consists of ambient noise recorded in recording studios, protesting the use of unlicensed copyrighted work to train artificial intelligence. The track titles form the sentence \"The British Government must not legalise music theft to benefit AI companies\". Profits from the album go toward the UK charity Help Musicians.\n\nBackground\nRapid progress in AI technology, constituting an AI boom, was brought to widespread public attention in the early 2020s by text-to-image models such as DALL-E, Midjourney, and Stable Diffusion, which were able to generate complex images that convincingly resembled human-made artworks. The proliferation of such image generation algorithms coincided with the release of GPT-3 and development of GPT-4, advanced large language models which produce highly convincing text. These transformer-based models designed to create new content from prompts are collectively called generative artificial intelligence, and they require vast sets of training data. This data often consists of text, images, and other media scraped from the web, prompting concerns that the AI products may violate intellectual property rights.\nSuno AI and Udio, two AI startups whose products generate music recordings following user-submitted prompts, were sued in 2024 by Sony Music, Warner Music Group, and Universal Music Group, who alleged that the companies used copyrighted recordings in their training data without authorization.\nIn December 2024, the UK government announced a consultation on copyright and AI, outlining a preferred approach that would see the introduction of a data mining copyright exception with a rights reservation package for rights holders. In the months following the announcement of the consultation, a number of prominent musicians warned of the threat it posed to musicians, including Paul McCartney and Elton John.\n\nArtists\nIs This What We Want? consists of 12 tracks, each uncredited. 1,000 artists are credited as co-writers, including Kate Bush, Damon Albarn, Tori Amos, Annie Lennox, Pet Shop Boys, Billy Ocean, the Clash, Ed O'Brien, Dan Smith, Jamiroquai, Mystery Jets, Hans Zimmer, Imogen Heap, Yusuf/Cat Stevens, Max Richter, the King's Singers, the Sixteen, John Rutter, and James MacMillan. The album was organised by the British composer Ed Newton-Rex, who had previously held a position in Stable Diffusion's parent company Stability AI.\nOn 16 November 202"}
{"doc_id": "Is This What We Want_", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "uf/Cat Stevens, Max Richter, the King's Singers, the Sixteen, John Rutter, and James MacMillan. The album was organised by the British composer Ed Newton-Rex, who had previously held a position in Stable Diffusion's parent company Stability AI.\nOn 16 November 2025, a vinyl version of the album was announced for 8 December, featuring a bonus track recorded by Paul McCartney.\n\nChart performance\nThe album debuted at number 38 on the UK Albums Downloads Chart.\n\nTrack listing\nSee also\nSleepify by Vulfpeck, an entirely silent album\n4'33\", a John Cage composition which instructs the performers to remain silent\nMusic and artificial intelligence"}
{"doc_id": "K-line (artificial intelligence)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A K-line, or Knowledge-line, is a mental agent which represents an association of a group of other mental agents found active when a subject solves a certain problem or formulates a new idea. These were first described in Marvin Minsky's essay K-lines: A Theory of Memory, published in 1980 in the journal Cognitive Science:\n\nWhen you \"get an idea,\" or \"solve a problem\" ... you create what we shall call a K-line. ... When that K-line is later \"activated\", it reactivates ... mental agencies, creating a partial mental state \"resembling the original.\"\n\n\"Whenever you 'get a good idea', solve a problem, or have a memorable experience, you activate a K-line to 'represent' it. A K-line is a wirelike structure that attaches itself to whichever mental agents are active when you solve a problem or have a good idea.\nWhen you activate that K-line later, the agents attached to it are aroused, putting you into a 'mental state' much like the one you were in when you solved that problem or got that idea.  This should make it relatively easy for you to solve new, similar problems!\" (1998, p. 82.)\n\nTheoretical implications\nThe concept of K-lines has several theoretical implications for understanding memory and problem-solving in artificial intelligence and cognitive science:\n\nIt suggests that memory is not a static storage of information, but rather a dynamic association of mental agents activated during an experience.\nK-lines provide a mechanism for generalizing from specific experiences to similar problems by reactivating the associated mental agents.\nThe theory implies that memory and problem-solving are distributed processes involving the coordination of multiple mental agents rather than a single central system.\n\nLimitations and criticisms\nWhile influential, the K-line theory has also faced some criticism and limitations:\n\nThe exact nature and implementation of K-lines in the brain or in artificial systems remains unclear and speculative.\nThe theory does not provide a complete account of all aspects of memory and cognition, such as the role of language, emotions, and social interactions.\nSome argue that the theory is too vague or metaphorical to be scientifically testable or to yield specific predictions.\n\nFootnotes"}
{"doc_id": "KAoS", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "KAoS is a policy and domain services framework created by the Florida Institute for Human and Machine Cognition. It uses W3C's Web Ontology Language (OWL) standard for policy representation and reasoning, and a software guard technology for efficient enforcement of a compiled version of its policies. It has been used in a variety of government-sponsored projects for distributed host and network management and for the coordination of human-agent-robot teams, including DARPA's CoABS Grid, Cougaar, and Common Object Request Broker Architecture (CORBA) models."}
{"doc_id": "Knowledge compilation", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Knowledge compilation is a family of approaches for addressing the intractability of\na number of artificial intelligence problems.\nA propositional model is compiled in an off-line phase in order to support some queries in polynomial time. Many ways of compiling a propositional model exist.\nDifferent compiled representations have different properties.\nThe three main properties are:\n\nThe compactness of the representation\nThe queries that are supported in polynomial time\nThe transformations of the representations that can be performed in polynomial time\n\nClasses of representations\nSome examples of diagram classes include decision trees, OBDDs, FBDDs, and non-deterministic OBDDs, as well as MDD.\nSome examples of formula classes include DNF and CNF.\nExamples of circuit classes include NNF, DNNF, d-DNNF, and SDD.\n\nKnowledge compilers\nc2d: supports compilation to d-DNNF\nd4: supports compilation to d-DNNF\nminiC2D: supports compilation to SDD\nKCBox: supports compilation to OBDD, OBDD[AND], and CCDD"}
{"doc_id": "Knowledge cutoff", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In machine learning, a knowledge cutoff (or data cutoff) is the point in time beyond which a model has not been trained on new data. The term is mostly used in reference to a large language model (LLM). Any information about events after this date is absent from the model's internal knowledge base. It cannot access information about later events without a system for real-time data access like retrieval-augmented generation (RAG). While useful for training and tuning LLMs, knowledge cutoffs introduce new limitations like hallucinations, information gaps, and temporal bias.\n\nOverview\nA model with a fixed knowledge cutoff is unable to provide information on facts or developments that have emerged since that time, since the model is not connected to the internet. Therefore, it may occasionally produce incorrect answers. This is caused by the fact that training on newer data would cause a major price concern, given that training the most powerful large language models may soon cost over a billion dollars according to Time.\nNotable AI model cutoff dates include:\n\nThe GPT-4 model has a knowledge cutoff of September 2021.\nThe GPT-4 Turbo model has an updated knowledge cutoff of December 2023.\nThe Llama 4 models have a knowledge cutoff of August 2024.\n\nEffects of knowledge cutoffs\nKnowledge gaps\nKnowledge cutoffs create information gaps. The model lacks any knowledge of events or discoveries that are not included in its training data. This can lead to hallucinations, where the model generates plausible but verifiably false statements. Such inaccuracies occur because LLMs choose words from an internal dictionary and select the most plausible words, which may or may not be accurate.\n\nEffective vs. reported cutoffs\nA research paper on arXiv indicates that a model's functional knowledge may not be uniformly limited by its stated cutoff date. This effective cutoff often differs for various subjects and is influenced by the distribution of information within the training data itself. Due to the high cost of retraining large language models, these models are rarely completely retrained to increase their knowledge cutoff. Some models can also use integrated search tools to access more recent information, which blurs the line of their inherent knowledge base. For example, GPT-4, can access its search tool and give real-time info.\n\nAttempts to overcome knowledge cutoffs\nRetrieval-augmented generation\nRAG is a common technique used to overcome the limitations of a knowledge cutoff. In a RAG system, the language model is connected to an external knowledge base or search engine to retrieve live"}
{"doc_id": "Knowledge cutoff", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " can access its search tool and give real-time info.\n\nAttempts to overcome knowledge cutoffs\nRetrieval-augmented generation\nRAG is a common technique used to overcome the limitations of a knowledge cutoff. In a RAG system, the language model is connected to an external knowledge base or search engine to retrieve live data. This architecture allows the model to find current information relevant to a query and incorporate it into its response, often with citations. Grounding a model in external data helps reduce the frequency of hallucinations and improves output accuracy. However, the external knowledge base might be outdated or contain biases, which may also lead to incorrect information or hallucinations. For example, Google AI Overviews have created false claims and the results are sometimes unreliable, since it either fail at understanding the source, or at generating the actual response properly. However, a method to mitigate this is to apply techniques like reinforcement learning from human feedback, which can improve the quality of a large language model's responses.\n\nContinual learning\nAnother approach is continual learning, which involves methods like adapters and LoRA. These fine-tuning techniques permit efficient, incremental updates to a model without the high cost of a full retraining cycle. However, this does not give real-time awareness, since adding modules to the system may result in algorithmic bias and catastrophic forgetting, as the weights in the model become biased towards the new set of data.\n\nSee also\nContinual learning\nLanguage model\nLarge language model\nHallucination (artificial intelligence)\nAlgorithmic bias"}
{"doc_id": "Knowledge level", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In artificial intelligence, knowledge-based agents draw on a pool of logical sentences to infer conclusions about the world.  At the knowledge level, we only need to specify what the agent knows and what its goals are; a logical abstraction separate from details of implementation.\nThis notion of knowledge level was first introduced by Allen Newell in the 1980s, to have a way to rationalize an agent's behavior.  The agent takes actions based on knowledge it possesses, in an attempt to reach specific goals.  It chooses actions according to the principle of rationality.\nBeneath the knowledge level resides the symbol level.  Whereas the knowledge level is world oriented, namely that it concerns the environment in which the agent operates, the symbol level is system oriented, in that it includes the mechanisms the agent has available to operate.  The knowledge level rationalizes the agent's behavior, while the symbol level mechanizes the agent's behavior.\nFor example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions.  The symbol level consists of the program's algorithms, the data structures themselves, and so on.\n\nSee also\nKnowledge level modeling\nKnowledge relativity"}
{"doc_id": "Knowledge-based configuration", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Knowledge-based configuration, also referred to as product configuration or product customization, is an activity of customising a product to meet the needs of a particular customer. The product in question may consist of mechanical parts, services, and software. Knowledge-based configuration is a major application area for artificial intelligence (AI), and it is based on modelling of the configurations in a manner that allows the utilisation of AI techniques for searching for a valid configuration to meet the needs of a particular customer.\n\nBackground\nKnowledge-based configuration (of complex products and services) has a long history as an artificial intelligence application area, see, e.g. Informally, configuration can be defined as a \"special case of design activity, where the artifact being configured is assembled from instances of a fixed set of well-defined component types which can be composed conforming to a set of constraints\". Such constraints represent technical restrictions, restrictions related to economic aspects, and conditions related to production processes. The result of a configuration process is a product configuration (concrete configuration), i.e., a list of instances and in some cases also connections between these instances. Examples of such configurations are computers to be delivered or financial service portfolio offers (e.g., a combination of loan and corresponding risk insurance).\n\nTheory and complexity of configuration\nNumerous practical configuration problems can be analyzed by the theoretical framework of Najmann and Stein, an early axiomatic approach that does not presuppose any particular knowledge representation formalism. One important result of this methodology is that typical optimization problems (e.g. finding a cost-minimal configuration) are NP-complete. Thus they require (potentially) excessive computation time, making heuristic configuration algorithms the preferred choice for complex artifacts (products, services).\n\nConfiguration systems\nConfiguration systems, also referred to as configurators or mass customization toolkits, are one of the most successfully applied artificial intelligence technologies. Examples are the automotive industry, the telecommunication industry, the computer industry, and power electric transformers. Starting with rule-based approaches such as R1/XCON, model-based representations of knowledge (in contrast to rule-based representations) have been developed that strictly separate product domain knowledge from problem solving knowledge—examples thereof are the constraint satisfaction problem, the Boolean satisfiability problem, and different answer set programming (ASP) representations. There are two commonly cited conceptualizations of configuration knowledge. The most important concepts in these are components, ports, resources and functions.  This separation of product domain knowledge and problem solving knowledge increased the effectiveness of configuration application development and maintenance, since changes in the product domain knowledge do not affect search strategies"}
{"doc_id": "Knowledge-based configuration", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (ASP) representations. There are two commonly cited conceptualizations of configuration knowledge. The most important concepts in these are components, ports, resources and functions.  This separation of product domain knowledge and problem solving knowledge increased the effectiveness of configuration application development and maintenance, since changes in the product domain knowledge do not affect search strategies and vice versa.\nConfigurators are also often considered as \"open innovation toolkits\", i.e., tools that support customers in the product identification phase. In this context customers are innovators who articulate their requirements leading to new innovative products.  \"Mass Confusion\"  – the overwhelming of customers by a large number of possible solution alternatives (choices) – is a phenomenon that often comes with the application of configuration technologies. This phenomenon motivated the creation of personalized configuration environments taking into account a customer's knowledge and preferences.\n\nConfiguration process\nCore configuration, i.e., guiding the user and checking the consistency of user requirements with the knowledge base, solution presentation and translation of configuration results into bill of materials (BOM) are major tasks to be supported by a configurator. Configuration knowledge bases are often built using proprietary languages.\nIn most cases knowledge bases are developed by knowledge engineers who elicit product, marketing and sales knowledge from domain experts. Configuration knowledge bases are composed of a formal description of the structure of the product and further constraints restricting the possible feature and component combinations.\nConfigurators known as characteristic based product configurators use sets of discrete variables that are either binary or have one of several values, and these variables define every possible product variation.\n\nSoftware and service configuration\nRecently, knowledge-based configuration has been extended to service and software configuration. Modeling software configuration has been based on two main approaches: feature modeling, and component-connectors. Kumbang domain ontology combines the previous approaches building on the tradition of knowledge-based configuration.\n\nSee also\nConfigurator\nConfigure price quote\nConstraint satisfaction\nFeature model\nMass customization\nOpen innovation\nProduct differentiation\nProduct family engineering\nSoftware product line"}
{"doc_id": "Knowledge-based recommender system", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Knowledge-based recommender systems (knowledge based recommenders)  are a specific type of recommender system that are based on explicit knowledge about the item assortment, user preferences, and recommendation criteria (i.e., which item should be recommended in which context). These systems are applied in scenarios where alternative approaches such as collaborative filtering and content-based filtering cannot be applied.\nA major strength of knowledge-based recommender systems is the non-existence of cold start (ramp-up) problems. A corresponding drawback is a potential knowledge acquisition bottleneck triggered by the need to define recommendation knowledge in an explicit fashion.\n\nItem domains\nKnowledge-based recommender systems are well suited to complex domains where items are not purchased very often, such as apartments and cars. Further examples of item domains relevant for knowledge-based recommender systems are financial services, digital cameras, and tourist destinations. Rating-based systems often do not perform well in these domains due to the low number of available ratings.\nAdditionally, in complex item domains, customers want to specify their preferences explicitly (e.g., \"the maximum price of the car is X\") . In this context, the recommender system must take into account constraints: for instance, only those financial services that support the investment period specified by the customer should be recommended. Neither of these aspects are supported by approaches such as collaborative filtering and content-based filtering.\n\nConversational recommendation\nKnowledge-based recommender systems are often conversational, i.e., user requirements and preferences are elicited within the scope of a feedback loop. A major reason for the conversational nature of knowledge-based recommender systems is the complexity of the item domain where it is often impossible to articulate all user preferences at once. Furthermore, user preferences are typically not known exactly at the beginning but are constructed within the scope of a recommendation session.\n\nSearch-based recommendation\nIn a search-based recommender, user feedback is given in terms of answers to questions which restrict the set of relevant items. An example of such a question is \"Which type of lens system do you prefer: fixed or exchangeable lenses?\". On the technical level, search-based recommendation scenarios can be implemented on the basis of constraint-based recommender systems. Constraint-based recommender systems are implemented on the basis of constraint search  or different types of conjunctive query-based approaches.\n\nNavigation-based recommendation\nIn a navigation-based recommender, user feedback is typically provided in terms of \"critiques\"  which specify change requests regarding the item currently recommended to the user. Critiques are then used for the recommendation of the next \"candidate\" item. An"}
{"doc_id": "Knowledge-based recommender system", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " different types of conjunctive query-based approaches.\n\nNavigation-based recommendation\nIn a navigation-based recommender, user feedback is typically provided in terms of \"critiques\"  which specify change requests regarding the item currently recommended to the user. Critiques are then used for the recommendation of the next \"candidate\" item. An example of a critique in the context of a digital camera recommendation scenario is \"I would like to have a camera like this but with a lower price\". This is an example of a \"unit critique\"  which represents a change request on a single item attribute. \"Compound critiques\"  allow the specification of more than one change request at a time. \"Dynamic critiquing\"  also takes into account preceding user critiques (the critiquing history). More recent approaches additionally exploit information stored in user interaction logs to further reduce the interaction effort in terms of the number of needed critiquing cycles.\n\nSee also\nRecommender system\nCollaborative filtering\nCold start\nCase-based reasoning\nConstraint satisfaction\nKnowledge-based configuration\nGuided selling"}
{"doc_id": "Knowledge-based systems", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A knowledge-based system (KBS) is a computer program that reasons and uses a knowledge base to solve complex problems. Knowledge-based systems were the focus of early artificial intelligence researchers in the 1980s. The term can refer to a broad range of systems. However, all knowledge-based systems have two defining components: an attempt to represent knowledge explicitly, called a knowledge base, and a reasoning system that allows them to derive new knowledge, known as an inference engine.\n\nComponents\nThe knowledge base contains domain-specific facts and rules  about a problem domain (rather than knowledge implicitly embedded in procedural code, as in a conventional computer program). In addition, the knowledge may be structured by means of a subsumption ontology, frames, conceptual graph, or logical assertions.\nThe inference engine uses general-purpose reasoning methods to infer new knowledge and to solve problems in the problem domain. Most commonly, it employs forward chaining or backward chaining. Other approaches include the use of automated theorem proving, logic programming, blackboard systems, and term rewriting systems such as Constraint Handling Rules (CHR). These more formal approaches are covered in detail in the Wikipedia article on knowledge representation and reasoning.\n\nAspects and development of early systems\nKnowledge-based vs. expert systems\nThe term \"knowledge-based system\" was often used interchangeably with \"expert system\", possibly because almost all of the earliest knowledge-based systems were designed for expert tasks. However, these terms tell us about different aspects of a system:\n\nexpert: describes only the task the system is designed for – its purpose is to aid replace a human expert in a task typically requiring specialised knowledge\nknowledge-based: refers only to the system's architecture –  it represents knowledge explicitly, rather than as procedural code\nToday, virtually all expert systems are knowledge-based, whereas knowledge-based system architecture is used in a wide range of types of system designed for a variety of tasks.\n\nRule-based systems\nThe first knowledge-based systems were primarily rule-based expert systems. These represented facts about the world as simple assertions in a flat database and used domain-specific rules to reason about these assertions, and then to add to them. One of the most famous of these early systems was Mycin, a program for medical diagnosis. \nRepresenting knowledge explicitly via rules had several advantages:\n\nAcquisition and maintenance. Using rules meant that domain experts could often define and maintain the rules themselves rather than via a programmer.\nExplanation. Representing knowledge explicitly allowed systems to reason about how they came to a conclusion and use this information to explain results to users. For example, to follow the"}
{"doc_id": "Knowledge-based systems", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " via rules had several advantages:\n\nAcquisition and maintenance. Using rules meant that domain experts could often define and maintain the rules themselves rather than via a programmer.\nExplanation. Representing knowledge explicitly allowed systems to reason about how they came to a conclusion and use this information to explain results to users. For example, to follow the chain of inferences that led to a diagnosis and use these facts to explain the diagnosis.\nReasoning. Decoupling the knowledge from the processing of that knowledge enabled general purpose inference engines to be developed. These systems could develop conclusions that followed from a data set that the initial developers may not have even been aware of.\n\nMeta-reasoning\nLater architectures for knowledge-based reasoning, such as the BB1 blackboard architecture (a blackboard system), allowed the reasoning process itself to be affected by new inferences, providing meta-level reasoning. BB1 allowed the problem-solving process itself to be monitored. Different kinds of problem-solving (e.g., top-down, bottom-up, and opportunistic problem-solving) could be selectively mixed based on the current state of problem solving. Essentially, the problem-solver was being used both to solve a domain-level problem along with its own control problem, which could depend on the former. \nOther examples of knowledge-based system architectures supporting meta-level reasoning are MRS and SOAR.\n\nWidening of application\nIn the 1980s and 1990s, in addition to expert systems, other applications of knowledge-based systems included real-time process control, intelligent tutoring systems, and problem-solvers for specific domains such as protein structure analysis, construction-site layout, and computer system fault diagnosis.\n\nAdvances driven by enhanced architecture\nAs knowledge-based systems became more complex, the techniques used to represent the knowledge base became more sophisticated and included logic, term-rewriting systems, conceptual graphs, and frames. \nFrames exemplify this architectural evolution. Introduced by Minsky, frames provide a structured knowledge representation formalism analogous to object-oriented programming paradigms. They are a way representing world knowledge using techniques that can be seen as analogous to object-oriented programming: a frame consists of a data structure with named slots that represent attributes or relations, organized hierarchically through class-subclass relationships. Each slot can contain values, default values, procedural attachments, and constraints that govern permissible values. With the knowledge base more structured, reasoning could now occur not only by independent rules and logical inference, but also based on interactions within the knowledge base itself. For example, procedures stored as daemons on objects could fire and could"}
{"doc_id": "Knowledge-based systems", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Each slot can contain values, default values, procedural attachments, and constraints that govern permissible values. With the knowledge base more structured, reasoning could now occur not only by independent rules and logical inference, but also based on interactions within the knowledge base itself. For example, procedures stored as daemons on objects could fire and could replicate the chaining behavior of rules.\n\nAdvances in automated reasoning\nAnother advancement in the 1990s was the development of special purpose automated reasoning systems called classifiers. Rather than statically declare the subsumption relations in a knowledge-base, a classifier allows the developer to simply declare facts about the world and let the classifier deduce the relations. In this way a classifier also can play the role of an inference engine.\nThe most recent advancement of knowledge-based systems was to adopt the technologies, especially a kind of logic called description logic, for the development of systems that use the internet. The internet often has to deal with complex, unstructured data that cannot be relied on to fit a specific data model. The technology of knowledge-based systems, and especially the ability to classify objects on demand, is ideal for such systems. The model for these kinds of knowledge-based internet systems is known as the Semantic Web.\n\nSee also\nKnowledge representation and reasoning\nKnowledge modeling\nKnowledge engine\nInformation retrieval\nReasoning system\nCase-based reasoning\nConceptual graph\nNeural networks"}
{"doc_id": "LaBSE", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "LaBSE (Language-agnostic BERT Sentence Embedding) is an open-source sentence embedding model developed by Google Research and published in 2020.\nIt extends BERT language model with a multilingual dual-encoder architecture trained on parallel translation data, enabling semantically comparable sentence vectors across more than one hundred languages.\nLaBSE is distributed via TensorFlow Hub and is widely used for cross-lingual information retrieval, semantic search, and machine translation evaluation.\n\nOverview\nLaBSE was introduced by Google Research as part of its multilingual representation learning program. The model maps text from diverse languages into a shared 768-dimensional vector space, where semantically equivalent sentences are located close to each other.\nUnlike traditional translation-based systems, LaBSE relies on a single shared transformer encoder for all languages, allowing direct comparison between sentences without translation.\n\nArchitecture\nThe system follows the structure of BERT-base (12 transformer layers, 12 attention heads) but employs a dual-encoder training setup similar to the Universal Sentence Encoder.\nEach sentence is tokenized using a joint multilingual WordPiece vocabulary covering 109 languages. Mean pooling across the final hidden states yields a fixed-size sentence representation. Training uses a translation ranking loss that maximizes cosine similarity between parallel sentences and minimizes it for unrelated pairs.\n\nTraining\nLaBSE was trained on large multilingual corpora combining public datasets such as OPUS with internal translation data from Google.\nOptimization employed Adam with in-batch negatives and temperature-scaled cross-entropy.  \nAccording to the authors, LaBSE achieved state-of-the-art results on cross-lingual retrieval benchmarks such as BUCC and Tatoeba at the time of its release.\n\nApplications\nThe model is publicly available on TensorFlow Hub and integrated into popular frameworks such as Hugging Face Transformers and Spark NLP.  \nTypical applications include:\n\nCross-lingual document and semantic search.\nAutomatic evaluation of machine translation quality.\nMultilingual clustering, deduplication, and classification.\nServing as a universal encoder for zero-shot learning tasks.\n\nReception and impact\nLaBSE has been cited extensively in academic literature on cross-lingual representation learning.  \nIndependent evaluations report that it remains competitive with later multilingual embedding models such as LASER2 and multilingual Sentence-BERT.  \nIts introduction marked a milestone in multilingual semantic similarity research and influenced subsequent releases of multilingual encoders in the open-source ecosystem.\n\nSee also\nNatural language processing\nSentence embedding\nCross-lingual information retrieval\nBERT"}
{"doc_id": "LaBSE", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " as LASER2 and multilingual Sentence-BERT.  \nIts introduction marked a milestone in multilingual semantic similarity research and influenced subsequent releases of multilingual encoders in the open-source ecosystem.\n\nSee also\nNatural language processing\nSentence embedding\nCross-lingual information retrieval\nBERT"}
{"doc_id": "Language_action perspective", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The language/action perspective \"takes language as the primary dimension of human cooperative activity,\" applied not just in person-to-person direct (face-to-face) interactions, but also in the design of systems mediated by information and communication technology.  The perspective was developed in the joint authorship of Understanding Computers and Cognition by Fernando Flores and Terry Winograd in 1987.\n\nOverview\nAs part of a reflection published in 2006, Terry Winograd describes the language-action perspective as resting on two key orienting principles:\n\nThe first is its focus on linguistic communication as the basis for understanding what occurs in information systems. Ultimately all information is communication: not an abstract system of bits and bytes but a means by which people interact.\nThe second principle is that language is action. Through their linguistic acts people effect change in the world. In imposing a language-action framework on information technology, we emphasize the action dimension over the more traditional dimension of information content.\nLanguage is action argues that speech isn't simply composed of assertions about the situation: utterances may also create a situation, such as, \"Let's go to the park.\" That utterance may be subject to interpretation but is not verifiable via the state of the world. This principle is closely linked to the ideas from phenomenology. Furthermore, language is not the transmission of information, which simply correspond to the state of the world. By creating a situation, language forms a consensual domain to further encourage more action through language. These speech acts may often take the form of commitment to other actions.\nIn the design of information systems, the perspective is based upon the notion as proposed by Terry Winograd that information technology may be limited in its ability to improve human communication.  \"Expert behavior requires an exquisite sensitivity to context and an ability to know what to commit to. Computing machines, which are purposely designed to process symbols independent of their context, have no hope of becoming experts.\".  That sensitivity to context is thus more in the realm of the human than in that of the artificial.\n\nHistory\nResearch on LAP was done in the Advanced Technology Group (ATG) at Apple Computer in the late 1980s.  Winograd was invited to present the basic concepts in a seminar at Apple in the winter of 1988.  Some Apple ATG researchers, notably Tom Pittard and Brad Hartfield, saw potential for enhancing the user experience of network based computer interactions if LAP was included in the mix of basic design considerations.\nResearch on the application of LAP to business process modelling was done in the System Modelling"}
{"doc_id": "Language_action perspective", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the winter of 1988.  Some Apple ATG researchers, notably Tom Pittard and Brad Hartfield, saw potential for enhancing the user experience of network based computer interactions if LAP was included in the mix of basic design considerations.\nResearch on the application of LAP to business process modelling was done in the System Modelling Research Group, Faculty of Computing, Engineering and Mathematical Sciences, University of the West of England in the early 2000s.\n\nApplications\nInsights from related work have been applied over the past two decades. At the LAP 2004 - Conference, Kalle Lyytinen discussed the academic/theoretic success of LAP. Yet, these LAP successes have not found entry into the wider stream of applications. In a sense, LAP is now peripheral to computer science, however there may be a need for a deeper look at this viewpoint.\nLAP played a role in the second AI Winter. At the time, symbolic AI tried to represent intelligence using a growing knowledge base represented as facts in language. The LAP argued that language was not simply a correspondence with facts but instead depended upon the contextual domain and could not be rigidly defined. Even Winograd's SHRDLU, an exemplar of language understanding in AI, was incapable of broadening its understanding beyond the blocks world.\n\nSee also\nArtificial general intelligence\nDesign & Engineering Methodology for Organizations (DEMO)\nEnd-user computing\nInformation science"}
{"doc_id": "Last Mile (artificial intelligence)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The last mile in the context of artificial intelligence (AI) and large language models (LLM) is a metaphor drawing on last mile (telecommunications) to refers to the limit or \"gap\" of what an AI model \"knows\" and human knowledge. Attending to the last mile gap is a strategy also to address algorithmic bias. \nThe last mile concept was used prior to the public rollout of ChatGPT in 2022 and gained traction in 2024 from multiple sources simultaneously. and amplified by Forbes Magazine. The \"last mile\" concept in AI entered common use in 2025 to refer to algorithm gaps  AI design gaps  and other gaps."}
{"doc_id": "Learnable function", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A learnable function is a mathematical function that can be learned from data, typically through a machine learning algorithm, to minimize errors and perform a specific task. In the context of statistical learning theory, this refers to a  function class where an algorithm can identify a specific function that best approximates the underlying pattern or distribution within the data.\nIn the domain of database systems, specifically within the emerging field of AI-powered database systems, learnable function is a general concept, representing a paradigm shift where traditional, hard-coded system heuristics are replaced by these parameterized functions. This enables the DBMS to learn, adapt, and govern based on the data it manages. Instead of relying on static rules designed by human experts, database components utilize learnable functions to dynamically adapt to specific data distributions and workloads.\n\nDefinition\nFundamentally, a learnable function can be formalized as a mapping \n  \n    \n      \n        f\n        :\n        \n          \n            X\n          \n        \n        →\n        \n          \n            Y\n          \n        \n      \n    \n    {\\displaystyle f:{\\mathcal {X}}\\to {\\mathcal {Y}}}\n  \n, parameterized by a set of weights \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. The goal of the learning process is to find the optimal parameters \n  \n    \n      \n        \n          θ\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle \\theta ^{*}}\n  \n that minimize a specific loss function \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n over a dataset \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}}\n  \n:\n\n  \n    \n      \n        \n          θ\n          \n            ∗\n          \n        \n        =\n        arg\n        ⁡\n        \n          min\n          \n            θ\n          \n        \n        \n          ∑\n          \n            (\n            x\n            ,\n            y\n            )\n            ∈\n            \n              \n                D\n              \n            \n          \n        \n        L\n        (\n        f\n        (\n        x\n        \n          |\n        \n        θ\n        )\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle \\theta ^{*}=\\arg \\min _{\\theta }\\sum _{(x,y)\\in {\\mathcal {D}}}L(f(x|\\theta ),y)}\n  \n\nIn this framework:\n\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n represents the input features (covariates).\n\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n represents"}
{"doc_id": "Learnable function", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "theta }\\sum _{(x,y)\\in {\\mathcal {D}}}L(f(x|\\theta ),y)}\n  \n\nIn this framework:\n\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n represents the input features (covariates).\n\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n represents the target output or label.\n\n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n is the hypothesis or model (e.g., a linear regression model, a decision tree, or a Neural network).\nThe \"learnability\" of the function ensures that as the size of the dataset \n  \n    \n      \n        \n          \n            D\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {D}}}\n  \n increases, the empirical risk converges to the true expected risk, allowing the function to generalize to unseen data.\n\nFormulation in AI-powered Database Systems\nIn traditional database design, system decisions, such as which index to use, how to order joins, or how to handle transaction conflicts, are governed by static functions, denoted as \n  \n    \n      \n        \n          f\n          \n            s\n            t\n            a\n            t\n            i\n            c\n          \n        \n        (\n        x\n        )\n      \n    \n    {\\displaystyle f_{static}(x)}\n  \n. These are typically composed of \"if-then\" rules and cost models based on general assumptions, e.g., uniform data distribution, established by human engineers.\nIn the context of AI-powered database systems, \n  \n    \n      \n        \n          f\n          \n            s\n            t\n            a\n            t\n            i\n            c\n          \n        \n      \n    \n    {\\displaystyle f_{static}}\n  \n is replaced by a learnable function \n  \n    \n      \n        \n          f\n          \n            l\n            e\n            a\n            r\n            n\n            e\n            d\n          \n        \n        (\n        x\n        \n          |\n        \n        θ\n        )\n      \n    \n    {\\displaystyle f_{learned}(x|\\theta )}\n  \n. This reformulation treats database internals as approximation problems:\n\nInput (\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n): Database states, query syntax, or runtime statistics, e.g., lock contention levels, data cardinality.\nOutput (\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n): System actions or estimates, e.g., estimated selectivity, optimal concurrency action.\nOptimization: The function parameters \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n are tuned"}
{"doc_id": "Learnable function", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " lock contention levels, data cardinality.\nOutput (\n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n): System actions or estimates, e.g., estimated selectivity, optimal concurrency action.\nOptimization: The function parameters \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n are tuned via methods such as Supervised learning (using query logs) or Reinforcement learning (using runtime feedback).\n\nImplementations in Database Systems\nThe implementation of learnable functions varies based on the complexity of \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n and the inference latency requirements.\n\nLookup Tables / Discrete Mappings: For low-latency requirements, e.g., concurrency control, \n  \n    \n      \n        f\n        (\n        x\n        \n          |\n        \n        θ\n        )\n      \n    \n    {\\displaystyle f(x|\\theta )}\n  \n can be implemented as a learned lookup table where \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is mapped to discrete buckets, and \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n represents the stored actions in those buckets.\nClassical ML Models:  Linear models and Tree-based models (e.g., XGBoost) are often used for cost estimation where interpretability and training speed are balanced.\nDeep Learning: Neural networks are employed for high-dimensional mappings, such as cardinality estimation over complex joins, where \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is a vector embedding of the SQL query.\n\nApplications in Learnable Database Components\nLearned Concurrency Control\nConcurrency Control (CC) algorithms ensure transaction isolation. Traditional algorithms like Two-phase locking (2PL) or Optimistic concurrency control (OCC) perform well in specific scenarios but fail to generalize.\nRecent research proposes treating concurrency control as a learnable function.\nIn such a model, the CC policy can be regarded an agent function:\n\n  \n    \n      \n        F\n        :\n        {\n        s\n        }\n        →\n        {\n        a\n        }\n      \n    \n    {\\displaystyle F:\\{s\\}\\to \\{a\\}}\n  \n\n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n (State): Features such as data hotness, dependency graph depth, and operation types.\n\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n (Action): A combination of conflict detection (e.g., validate read set) and resolution mechanisms (e.g., wait, abort, commit).\nUnlike \"Classify-then"}
{"doc_id": "Learnable function", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "): Features such as data hotness, dependency graph depth, and operation types.\n\n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n (Action): A combination of conflict detection (e.g., validate read set) and resolution mechanisms (e.g., wait, abort, commit).\nUnlike \"Classify-then-Assign\" approaches which merely select between 2PL and OCC, a learnable function approach can generate hybrid policies by learning the optimal action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n for a specific state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n via Bayesian optimization or generic search, effectively creating a \"lookup table\" of optimal behaviors that adapts to contention levels.\n\nLearned Query Optimization\nIn query optimization, the learnable function \n  \n    \n      \n        f\n      \n    \n    {\\displaystyle f}\n  \n typically replaces the cost model or the cardinality estimator.\n\nCardinality Estimation: \n  \n    \n      \n        f\n        (\n        q\n        u\n        e\n        r\n        y\n        \n          |\n        \n        θ\n        )\n        →\n        r\n        o\n        w\n        _\n        c\n        o\n        u\n        n\n        t\n      \n    \n    {\\displaystyle f(query|\\theta )\\to row\\_count}\n  \n. Deep learning models can capture correlations between columns that histograms (independence assumption) fail to model.\nCost Modeling: \n  \n    \n      \n        f\n        (\n        p\n        l\n        a\n        n\n        \n          |\n        \n        θ\n        )\n        →\n        l\n        a\n        t\n        e\n        n\n        c\n        y\n      \n    \n    {\\displaystyle f(plan|\\theta )\\to latency}\n  \n. Learning the latency of physical operators on specific hardware.\nThe primary advantage of substituting these components with learnable functions is the reduction of estimation errors that propagate during plan enumeration. While traditional optimizers often produce sub-optimal plans due to inaccurate independence assumptions or outdated statistics, learned approaches can optimize for the true runtime metric, e.g., latency, by leveraging historical execution feedback to correct their internal models over time.\n\nTheoretical Perspectives\nLearnability vs. Complexity Trade-offs\nWhile learnable functions offer adaptability, they are subject to the No free lunch theorem. A function class \n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {F}}}\n  \n that is too complex, e.g., a large neural network,"}
{"doc_id": "Learnable function", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " vs. Complexity Trade-offs\nWhile learnable functions offer adaptability, they are subject to the No free lunch theorem. A function class \n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {F}}}\n  \n that is too complex, e.g., a large neural network, may overfit to a specific workload history and fail to generalize when the workload drifts (the variance problem). In constrast, a class that is too simple, e.g., linear regression, may fail to capture the non-linear performance characteristics of modern hardware (the bias problem).\nThis trade-off can be mathematically expressed in the decomposition of risk:\n\n  \n    \n      \n        \n          I\n          \n            P\n          \n        \n        (\n        \n          \n            \n              \n                f\n                ^\n              \n            \n          \n          \n            n\n          \n        \n        )\n        −\n        \n          inf\n          \n            f\n            ∈\n            \n              \n                \n                  F\n                \n              \n              \n                ∗\n              \n            \n          \n        \n        \n          I\n          \n            P\n          \n        \n        (\n        f\n        )\n        =\n        \n          \n            \n              \n                \n                  I\n                  \n                    P\n                  \n                \n                (\n                \n                  \n                    \n                      \n                        f\n                        ^\n                      \n                    \n                  \n                  \n                    n\n                  \n                \n                )\n                −\n                \n                  inf\n                  \n                    f\n                    ∈\n                    \n                      \n                        F\n                      \n                    \n                  \n                \n                \n                  I\n                  \n                    P\n                  \n                \n                (\n                f\n                )\n              \n              ⏟\n            \n          \n          \n            estimation error\n          \n        \n        +\n        \n          \n            \n              \n                \n                  inf\n                  \n                    f\n                    ∈\n                    \n                      \n                        F\n                      \n                    \n                  \n                \n                \n                  I\n                  \n                    P\n                  \n                \n                (\n                f\n                )\n                −\n                \n                  inf\n                  \n                    f\n                    ∈\n                    \n                      \n                        \n                          F\n                        \n                      \n                      \n                        ∗\n                      \n                    \n                  \n                \n                \n                  I\n                  \n                    P\n                  \n                \n                (\n                f\n                )\n              \n              ⏟\n            \n          \n          \n            approximation error\n          \n        \n      \n    \n    {\\displaystyle I_{P}({\\hat {f}}_{n})-\\inf _{f\\in {\\mathcal {F}}^{*}}I_{P}(f)=\\underbrace {I_{P}({\\hat {f}}_{n})-\\inf _{f\\in {\\mathcal {F}}}I_{P}(f)} _{\\text{estimation error}}+\\underbrace {\\inf _{f\\in {\\mathcal {F}}}I"}
{"doc_id": "Learnable function", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "}(f)=\\underbrace {I_{P}({\\hat {f}}_{n})-\\inf _{f\\in {\\mathcal {F}}}I_{P}(f)} _{\\text{estimation error}}+\\underbrace {\\inf _{f\\in {\\mathcal {F}}}I_{P}(f)-\\inf _{f\\in {\\mathcal {F}}^{*}}I_{P}(f)} _{\\text{approximation error}}}\n  \n\nEffective AI-powered Database systerms must balance the approximation error (using rich models like Neural Networks) against the estimation error (requiring vast amounts of training data and stability).\n\nThe Bitter Lesson and Scalability\nThe move toward learnable functions in databases can be discussed from the perspective of \"The Bitter Lesson\" by  Richard Sutton, which argues that general methods that leverage computation (search and learning) ultimately outperform methods that rely on human knowledge (heuristics).\nIn databases, a handcrafted cost model (human knowledge) is limited by the designer's foresight. A learnable function, however, scales with the availability of computation and training data (query logs), and hence, theoretically allows the database to asymptotically approach the performance of a theoretically optimal system, often referred to as an oracle machine with perfect information.\n\nHistory and Development\nThe study of learnable functions emerged from the intersection of biology and cybernetics in the mid-20th century, specifically driven by the need to understand how organisms adapt to complex environments. Early research focused on perceptron learning algorithms for linear threshold operations and learning in random networks. In the 1960s and 1970s, researchers like Marvin Minsky, Oliver Selfridge, and Seymour Papert laid the groundwork for modern machine learning by contrasting the immense complexity of general Boolean functions against the learnability of specialized classes. Concurrently, classical approximation theories for continuous real-valued functions were further developed to provide mathematical rigor to the concept of function reconstruction. A subsequent formalization occurred in 1973 when Andrzej Ehrenfeucht and Jan Mycielski introduced the concept of \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n-continuity and addressing functions. Their work on the interpolation of functions over a measure space formalized how a learner can efficiently search for a small number of relevant features within a high-dimensional data input.\nIn the contemporary landscape of artificial intelligence and database systems, the concept of learnable functions has transitioned from theoretical classification toward the autonomous optimization"}
{"doc_id": "Learnable function", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and addressing functions. Their work on the interpolation of functions over a measure space formalized how a learner can efficiently search for a small number of relevant features within a high-dimensional data input.\nIn the contemporary landscape of artificial intelligence and database systems, the concept of learnable functions has transitioned from theoretical classification toward the autonomous optimization of core system internals, such as query optimization and concurrency control. A prominent example is the CCaaLF framework, which treats concurrency control as a learnable agent function \n  \n    \n      \n        \n          \n            F\n          \n        \n        :\n        {\n        s\n        }\n        →\n        {\n        a\n        }\n      \n    \n    {\\displaystyle {\\mathcal {F}}:\\{s\\}\\rightarrow \\{a\\}}\n  \n. This function dynamically maps real-time database states to specific conflict-handling actions.\n\nSee also\nRegularization\nDatabase tuning\nLearnable function class\n Filter and refine principle\nAutomated machine learning"}
{"doc_id": "Liar's dividend", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The Liar's Dividend is a political and media phenomenon in which public figures falsely claim that factual reporting is missing information, \"fake news,\" or artificially generated in order to avoid accountability for their action   This concept has been studied in political science to understand how such claims can allow politicians to maintain public support following scandals or controversial statements. This tactic leverages public uncertainty about the accuracy of information and can mobilize parties and supporters. It often takes advantage of emerging technology, such as artificial intelligence (AI)–generated content and deepfakes, which makes distinguishing authentic from manipulated material more difficult.\n\nHistory\nThe rise of AI and deepfake technology has increased the ability to produce highly convincing, manipulated media, making it easier to mislead viewers. In September 2023, an audio clip of Michal Šimečka, a politician from the Progressive Slovakia party, circulated online, allegedly showing him discussing election manipulation with a journalist. A YouGov poll found that 85% of respondents were \"very concerned\" about the spread of misleading deepfakes. Deepfakes of U.S. presidents Joe Biden and Donald Trump have also been widely circulated \nThe term \"Liar's Dividend\" was coined by legal scholars Bobby Chesney and Danielle Citron to describe the phenomenon in which the existence of real deepfakes can make people skeptical of genuine information\n\nSocial Examples of the Liar's Dividend\nNot all examples of the Liar's Dividend are political. Many incidents involving incriminating footage of civilians have been dismissed as AI-generated.\n\nTesla lawyers argued that Elon Musk's past statements about the safety of self-driving cars could not be used in court because they were alleged deepfakes (Thornhill, 2021).\nIn the aftermath of the January 6 Capitol riots, Guy Reffitt was charged with bringing a handgun to the Capitol building. His lawyer argued that evidence against him might be AI-generated, reflecting the use of the Liar's Dividend in legal contexts"}
{"doc_id": "Lifelong Planning A_", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "LPA* or Lifelong Planning A* is an incremental heuristic search algorithm based on A*. It was first described by Sven Koenig and Maxim Likhachev in 2001.\n\nDescription\nLPA* is an incremental version of A*, which can adapt to changes in the graph without recalculating the entire graph, by updating the g-values (distance from start) from the previous search during the current search to correct them when necessary. Like A*, LPA* uses a heuristic, which is a lower boundary for the cost of the path from a given node to the goal. A heuristic is admissible if it is guaranteed to be non-negative (zero being admissible) and never greater than the cost of the cheapest path to the goal.\n\nPredecessors and successors\nWith the exception of the start and goal node, each node n has predecessors and successors:\n\nAny node from which an edge leads towards n is a predecessor of n.\nAny node to which an edge leads from n is a successor of n.\nIn the following description, these two terms refer only to the immediate predecessors and successors, not to predecessors of predecessors or successors of successors.\n\nStart distance estimates\nLPA* maintains two estimates of the start distance g*(n) for each node:\n\ng(n), the previously calculated g-value (start distance) as in A*\nrhs(n), a lookahead value based on the g-values of the node's predecessors (the minimum of all g(n' ) + d(n' , n), where n' is a predecessor of n and d(x, y) is the cost of the edge connecting x and y)\nFor the start node, the following always holds true:\n\n  \n    \n      \n        r\n        h\n        s\n        (\n        s\n        t\n        a\n        r\n        t\n        )\n        =\n        g\n        (\n        s\n        t\n        a\n        r\n        t\n        )\n        =\n        0\n      \n    \n    {\\displaystyle rhs(start)=g(start)=0}\n  \n\nIf rhs(n) equals g(n), then n is called locally consistent. If all nodes are locally consistent, then a shortest path can be determined as with A*. However, when edge costs change, local consistency needs to be re-established only for those nodes which are relevant for the route.\n\nPriority queue\nWhen a node becomes locally inconsistent (because the cost of its predecessor or the edge linking it to a predecessor has changed), it is placed in a priority queue for re-evaluation."}
{"doc_id": "Lifelong Planning A_", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " However, when edge costs change, local consistency needs to be re-established only for those nodes which are relevant for the route.\n\nPriority queue\nWhen a node becomes locally inconsistent (because the cost of its predecessor or the edge linking it to a predecessor has changed), it is placed in a priority queue for re-evaluation. LPA* uses a two-dimensional key:\n\n  \n    \n      \n        k\n        (\n        n\n        )\n        =\n        \n          \n            [\n            \n              \n                \n                  \n                    k\n                    \n                      1\n                    \n                  \n                  (\n                  n\n                  )\n                \n              \n              \n                \n                  \n                    k\n                    \n                      2\n                    \n                  \n                  (\n                  n\n                  )\n                \n              \n            \n            ]\n          \n        \n        =\n        \n          \n            [\n            \n              \n                \n                  min\n                  (\n                  g\n                  (\n                  n\n                  )\n                  ,\n                  r\n                  h\n                  s\n                  (\n                  n\n                  )\n                  )\n                  +\n                  h\n                  (\n                  n\n                  ,\n                  g\n                  o\n                  a\n                  l\n                  )\n                \n              \n              \n                \n                  min\n                  (\n                  g\n                  (\n                  n\n                  )\n                  ,\n                  r\n                  h\n                  s\n                  (\n                  n\n                  )\n                  )\n                \n              \n            \n            ]\n          \n        \n      \n    \n    {\\displaystyle k(n)={\\begin{bmatrix}k_{1}(n)\\\\k_{2}(n)\\\\\\end{bmatrix}}={\\begin{bmatrix}\\min(g(n),rhs(n))+h(n,goal)\\\\\\min(g(n),rhs(n))\\\\\\end{bmatrix}}}\n  \n\nEntries are ordered by k1 (which corresponds directly to the f-values used in A*), then by k2.\n\nNode expansion\nThe top node in the queue is expanded as follows:\n\nIf the rhs-value of a node equals its g-value, the node is locally consistent and is removed from the queue.\nIf the rhs-value of a node is less than its g-value (known as a locally overconsistent node), the g-value is changed to match the rhs-value, making the node locally consistent. The node is then removed from the queue.\nIf the rhs-value of a node is greater than its g-value (known as a locally underconsistent node), the g-value is set to infinity (which makes the node either locally overconsistent or locally consistent). If the node is then locally consistent, it is removed from the queue, else its key is updated.\nSince changing the g-value of a node may also change the rhs-values of its"}
{"doc_id": "Lifelong Planning A_", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " as a locally underconsistent node), the g-value is set to infinity (which makes the node either locally overconsistent or locally consistent). If the node is then locally consistent, it is removed from the queue, else its key is updated.\nSince changing the g-value of a node may also change the rhs-values of its successors (and thus their local consistence), they are evaluated and their queue membership and key is updated if necessary.\nExpansion of nodes continues with the next node at the top of the queue until two conditions are met:\n\nThe goal is locally consistent, and\nThe node at the top of the priority queue has a key which is greater than or equal to the key for the goal.\n\nInitial run\nThe graph is initialized by setting the rhs-value of the start node to 0 and its g-value to infinity. For all other nodes, both the g-value and the rhs-value are assumed to be infinity until assigned otherwise. This initially makes the start node the only locally inconsistent node, and thus the only node in the queue. After that, node expansion begins. The first run of LPA* thus behaves in the same manner as A*, expanding the same nodes in the same order.\n\nCost changes\nWhen the cost of an edge changes, LPA* examines all nodes affected by the change, i.e. all nodes at which one of the changed edges terminates (if an edge can be traversed in both directions and the change affects both directions, both nodes connected by the edge are examined):\n\nThe rhs-values of the nodes are updated.\nNodes which have become locally consistent are removed from the queue.\nNodes which have become locally inconsistent are added to the queue.\nNodes which remain locally inconsistent have their keys updated.\nAfter that, node expansion resumes until the end condition has been reached.\n\nFinding the shortest path\nOnce node expansion has finished (i.e. the exit conditions are met), the shortest path is evaluated. If the cost for the goal equals infinity, there is no finite-cost path from start to goal. Otherwise, the shortest path can be determined by moving backwards:\n\nStart at the goal.\nMove to the predecessor n'  of the current node n for which g(n' ) + d(n' , n) is lowest (if the lowest score is shared by multiple nodes, each is a valid solution and any of them can be chosen arbitrarily).\nRepeat the previous step until you have reached the start.\n\nPseudocode\nThis code assumes a priority queue queue, which supports the following operations:\n\ntopKey() returns the (numerically"}
{"doc_id": "Lifelong Planning A_", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " is lowest (if the lowest score is shared by multiple nodes, each is a valid solution and any of them can be chosen arbitrarily).\nRepeat the previous step until you have reached the start.\n\nPseudocode\nThis code assumes a priority queue queue, which supports the following operations:\n\ntopKey() returns the (numerically) lowest priority of any node in the queue (or infinity if the queue is empty)\npop() removes the node with the lowest priority from the queue and returns it\ninsert(node, priority) inserts a node with a given priority into the queue\nremove(node) removes a node from the queue\ncontains(node) returns true if the queue contains the specified node, false if not\n\nProperties\nBeing algorithmically similar to A*, LPA* shares many of its properties.\n\nEach node is expanded (visited) at most twice for each run of LPA*. Locally overconsistent nodes are expanded at most once per LPA* run, thus its initial run (in which every node enters the overconsistent state) has similar performance to A*, which visits each node at most once.\nThe keys of the nodes expanded for each run are monotonically nondecreasing over time, as is the case with A*.\nThe more informed (and thus larger) the heuristics are (while still satisfying the admissibility criteria), the fewer nodes need to be expanded.\nThe priority queue implementation has a significant impact on performance, as in A*. Using a Fibonacci heap can lead to a significant performance increase over less efficient implementations.\nFor an A* implementation which breaks ties between two nodes with equal f-values in favor of the node with the smaller g-value (which is not well-defined in A*), the following statements are also true:\n\nThe order in which locally overconsistent nodes are expanded is identical to A*.\nOf all locally overconsistent nodes, only those whose cost does not exceed that of the goal need to be expanded, as is the case in A*.\nLPA* additionally has the following properties:\n\nWhen edge costs change, LPA* outperforms A* (assuming the latter is run from scratch) as only a fraction of nodes need to be expanded again.\n\nVariants\nD* Lite, a reimplementation of the D* algorithm based on LPA*"}
{"doc_id": "Lifelong Planning A_", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", a reimplementation of the D* algorithm based on LPA*"}
{"doc_id": "List of artificial intelligence journals", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "This is a list of notable peer-reviewed academic journals that publish research in the field of artificial intelligence (AI), including areas such as machine learning, computer vision, natural language processing, robotics, and intelligent systems.\n\nGeneral artificial intelligence\nArtificial Intelligence (journal) – Elsevier\nJournal of Artificial Intelligence Research (JAIR) – AI Access Foundation\nAI Open – Elsevier\nAI Perspectives – Springer Nature\nAdvances in Artificial Intelligence – Hindawi\nArtificial Intelligence Review – Springer\n\nMachine learning\nMachine Learning (journal) – Springer\nJournal of Machine Learning Research (JMLR) – Microtome\nTransactions on Machine Learning Research – Community-run (TMLR)\nPattern Recognition (journal) – Elsevier\nNeural Networks (journal) – Elsevier\nNeural Computation (journal) – MIT Press\n\nDeep learning and neural computation\nIEEE Transactions on Neural Networks and Learning Systems – IEEE\nNature Machine Intelligence – Springer Nature\n\nComputer vision\nInternational Journal of Computer Vision – Springer\nComputer Vision and Image Understanding – Elsevier\nIEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) – IEEE\n\nNatural language processing\nComputational Linguistics (journal) – MIT Press\nJournal of Natural Language Engineering – Cambridge University Press\nNatural Language Processing (journal)\nTransactions of the Association for Computational Linguistics – ACL\n\nRobotics and intelligent systems\nIEEE Transactions on Robotics – IEEE\nAutonomous Robots – Springer\nRobotics and Autonomous Systems – Elsevier\nJournal of Intelligent & Robotic Systems – Springer\n\nInterdisciplinary and ethics in AI\nAI & Society – Springer\nJournal of Responsible Technology – Elsevier\nPhilosophy & Technology – Springer\nMinds and Machines – Springer\n\nSee also\nAssociation for the Advancement of Artificial Intelligence\nLists of academic journals\nList of software programming journals\nList of computer science journals\nList of information systems journals\nList of large language models\nList of artificial intelligence projects\nLists of open-source artificial intelligence software\n\nArtificial intelligence conferences\nAAAI Conference on Artificial Intelligence\nAI Action Summit\nInternational Joint Conference on Artificial Intelligence\nConference on Neural Information Processing Systems\nInternational Conference on Learning Representations\nEuropean Conference on Artificial Intelligence\nNvidia GTC\nAAMAS\nInternational Conference on Machine Learning"}
{"doc_id": "List of artificial intelligence journals", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\nInternational Conference on Machine Learning"}
{"doc_id": "List of programming languages for artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Historically, some programming languages have been specifically designed for artificial intelligence (AI) applications. Nowadays, many general-purpose programming languages also have libraries that can be used to develop AI applications.\n\nGeneral-purpose languages\nPython is a high-level, general-purpose programming language that is popular in artificial intelligence. It has a simple, flexible and easily readable syntax. Its popularity results in a vast ecosystem of libraries, including for deep learning, such as PyTorch, TensorFlow, Keras, Google JAX. The library NumPy can be used for manipulating arrays, SciPy for scientific and mathematical analysis, Pandas for analyzing table data, Scikit-learn for various machine learning tasks, NLTK and spaCy for natural language processing, OpenCV for computer vision, and Matplotlib for data visualization. Hugging Face's transformers library can manipulate large language models. Jupyter Notebooks can execute cells of Python code, retaining the context between the execution of cells, which usually facilitates interactive data exploration.\nElixir is a high-level functional programming language based on the Erlang VM. Its machine-learning ecosystem includes Nx for computing on CPUs and GPUs, Bumblebee and Axon for serving and training models, Broadway for distributed processing pipelines, Membrane for image and video processing, Livebook for prototyping and publishing notebooks, and Nerves for embedding on devices.\nR is widely used in new-style artificial intelligence, involving statistical computations, numerical analysis, the use of Bayesian inference, neural networks and in general machine learning. In domains like finance, biology, sociology or medicine it is considered one of the main standard languages. It offers several paradigms of programming like vectorial computation, functional programming and object-oriented programming.\nLisp was the first language developed for artificial intelligence. It includes features intended to support programs that could perform general problem solving, such as lists, associations, schemas (frames), dynamic memory allocation, data types, recursion, associative retrieval, functions as arguments, generators (streams), and cooperative multitasking.\nMATLAB is a proprietary numerical computing language developed by MathWorks. MATLAB has many toolboxes specifically for the development of AI including the Statistics and Machine Learning Toolbox and Deep Learning Toolbox. These toolboxes provide APIs for the high-level and low-level implementation and use of many types of machine learning models that can integrate with the rest of the MATLAB ecosystem. These  libraries also have support for  code generation for embedded hardware.\nC++ is a compiled language that can interact with low-level hardware. In the context of AI, it is particularly used for embedded systems and robotics"}
{"doc_id": "List of programming languages for artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " implementation and use of many types of machine learning models that can integrate with the rest of the MATLAB ecosystem. These  libraries also have support for  code generation for embedded hardware.\nC++ is a compiled language that can interact with low-level hardware. In the context of AI, it is particularly used for embedded systems and robotics. Libraries such as TensorFlow C++, Caffe or Shogun can be used.\nJavaScript is widely used for web applications and can notably be executed with web browsers. Libraries for AI include TensorFlow.js, Synaptic and Brain.js.\nJulia is a language launched in 2012, which intends to combine ease of use and performance. It is mostly used for numerical analysis, computational science, and machine learning.\nC# can be used to develop high level machine learning models using Microsoft’s .NET suite. ML.NET was developed to aid integration with existing .NET projects, simplifying the process for existing software using the .NET platform.\nSmalltalk has been used extensively for simulations, neural networks, machine learning, and genetic algorithms. It implements a pure and elegant form of object-oriented programming using message passing.\nHaskell is a purely functional programming language. Lazy evaluation and the list and LogicT monads make it easy to express non-deterministic algorithms, which is often the case. Infinite data structures are useful for search trees. The language's features enable a compositional way to express algorithms. Working with graphs is however a bit harder at first because of functional purity.\nWolfram Language includes a wide range of integrated machine learning abilities, from highly automated functions like Predict and Classify to functions based on specific methods and diagnostics. The functions work on many types of data, including numerical, categorical, time series, textual, and image.\nMojo can run some Python programs, and supports programmability of AI hardware. It aims to combine the usability of Python with the performance of low-level programming languages like C++ or Rust.\n\nSpecialized languages\nProlog is a declarative language where programs are expressed in terms of relations, and execution occurs by running queries over these relations. Prolog is particularly useful for symbolic reasoning, database and language parsing applications.\nArtificial Intelligence Markup Language (AIML) is an XML dialect for use with Artificial Linguistic Internet Computer Entity (A.L.I.C.E.)-type chatterbots.\nPlanner is a hybrid between procedural and logical languages. It gives a procedural interpretation to logical sentences where implications are interpreted with pattern-directed inference.\nStanford Research Institute Problem Solver (STRIPS) is a language to express"}
{"doc_id": "List of programming languages for artificial intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " for use with Artificial Linguistic Internet Computer Entity (A.L.I.C.E.)-type chatterbots.\nPlanner is a hybrid between procedural and logical languages. It gives a procedural interpretation to logical sentences where implications are interpreted with pattern-directed inference.\nStanford Research Institute Problem Solver (STRIPS) is a language to express automated planning problem instances. It expresses an initial state, the goal states, and a set of actions. For each action preconditions (what must be established before the action is performed) and postconditions (what is established after the action is performed) are specified.\nPOP-11 is a reflective, incrementally compiled programming language with many of the features of an interpreted language. It is the core language of the Poplog programming environment developed originally by the University of Sussex, and recently in the School of Computer Science at the University of Birmingham which hosts the Poplog website, It is often used to introduce symbolic programming techniques to programmers of more conventional languages like Pascal, who find POP syntax more familiar than that of Lisp. One of POP-11's features is that it supports first-class functions.\nCycL is a special-purpose language for Cyc.\n\nSee also\nGlossary of artificial intelligence\nList of constraint programming languages\nList of computer algebra systems\nList of logic programming languages\nList of constructed languages\nFifth-generation programming language\n\nNotes"}
{"doc_id": "Lists of open-source artificial intelligence software", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "These are lists include projects which release at least some of their software under open-source licenses and are related to artificial intelligence projects. These include software libraries, frameworks, platforms, and tools used for machine learning, deep learning, natural language processing, computer vision, reinforcement learning, artificial general intelligence, and more.\n\nMachine learning or data mining\nCaffe — Image classification and image segmentation\nScikit-learn — library built on top of NumPy, SciPy, and matplotlib\nShogun — C++ library for large-scale machine learning\nmlpack — C++ header-only machine learning library\nWeka — collection of machine learning algorithms for data mining tasks\nApache Mahout — scalable machine learning library for big data built on Hadoop and Spark\nApache SystemDS — ML system for the end-to-end data science lifecycle\nJubatus — online machine learning and distributed computing framework\nKNIME — modular data pipelining\nOrange (software) — machine learning, data mining, data visualization, and data analysis.\nRapidMiner — predictive analytics\nfastText – Word embeddings developed by Meta AI\nXGBoost — machine learning library for gradient boosting\n\nAutoML platforms\nTPOT – tree-based pipeline optimization tool using genetic programming\nNeural Network Intelligence – Microsoft toolkit for hyperparameter tuning and neural architecture search\nMindsDB – AutoML platform that embeds machine learning into SQL databases and applications\n\nDeep learning frameworks\nTensorFlow – end-to-end open-source platform for machine learning developed by Google Brain\nPyTorch – deep learning framework developed by Meta AI\nPyTorch Lightning – high-level framework built on top of PyTorch for organizing and scaling deep learning models\nKeras – Python library for artificial neural networks and integrated into TensorFlow library\nMXNet – framework that trains and deploys deep neural networks\nCaffe – deep learning framework focused on speed and modularity\nChainer – Python framework on top of NumPy and CuPy\nTheano –  Python library and optimizing compiler for evaluating mathematical expressions and optimized for GPUs\nDeeplearning4j – Java library for the Java virtual machine and deep learning algorithms\nNeuroph – object-oriented artificial neural network framework written in Java\nFast Artificial Neural Network (FANN) – C library for feedforward artificial neural networks\n\nConvolutional neural networks (CNNs)\nAlexNet — pioneering CNN for image classification, won the 2012 ImageNet competition\nVGGNet — deep CNN known for its simplicity and use of 3x3 convolution filters\nInception"}
{"doc_id": "Lists of open-source artificial intelligence software", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Network (FANN) – C library for feedforward artificial neural networks\n\nConvolutional neural networks (CNNs)\nAlexNet — pioneering CNN for image classification, won the 2012 ImageNet competition\nVGGNet — deep CNN known for its simplicity and use of 3x3 convolution filters\nInception — CNN architecture using parallel convolutional layers of different sizes\n\nArtificial neural networks\nEDLUT – event-driven neural network simulator for large-scale spiking networks\nEmergent – cognitive modeling platform implementing connectionist neural networks\nEncog – machine learning framework for Java and C# supporting neural networks\nJOONE – Java-based neural network framework with modular architecture for learning tasks\nNengo – Python library for building and simulating large-scale neural systems\nNeuroph – lightweight Java framework for creating neural networks\nOpenNN – C++ library for designing, training, and deploying neural networks\nSNNS – Stuttgart Neural Network Simulator, supports feedforward and recurrent neural networks\n\nCognitive architectures and AGI platforms\nOpenCog – project that aims to build an open source artificial intelligence framework\nSoar – cognitive architecture for decision-making and learning in Intelligent agents\nCLARION – Connectionist Learning with Adaptive Rule Induction On-line, hybrid connectionist/symbolic cognitive architecture.\n\nReinforcement learning frameworks\nKataGo – reinforcement learning agent designed for playing the game of Go\n\nReactive planning\nGOLOG – logic programming language, situation calculus, first-order logical language for reasoning about action and change.\n\nComputer vision and image processing\nAForge.NET – computer vision, artificial intelligence, and robotics library for the .NET framework\nDlib – C++ library for computer vision and image processing\nOpenCV — library of programming functions mainly for real-time computer vision\nTesseract – optical character recognition\n\nNatural language processing (NLP)\nApache OpenNLP\nApertium – rule-based machine translation platform.\nChatScript – natural language engine and dialog management system\nGeneral Architecture for Text Engineering – information extraction\nGensim – topic modeling and document similarity analysis library\nGloVe – unsupervised learning algorithm for obtaining vector representations of words\nMallet – Java \"Machine Learning for Language Toolkit\"\nMontyLingua – libraries and programs for symbolic and statistical NLP for both Python and Java\nMoses – statistical machine translation engine to train statistical models of text from a source language to a target language\nNiuTrans – statistical machine translation\nNLTK – natural Language toolkit for symbolic and statistical NLP\nProbabilistic"}
{"doc_id": "Lists of open-source artificial intelligence software", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ingua – libraries and programs for symbolic and statistical NLP for both Python and Java\nMoses – statistical machine translation engine to train statistical models of text from a source language to a target language\nNiuTrans – statistical machine translation\nNLTK – natural Language toolkit for symbolic and statistical NLP\nProbabilistic Action Cores – interpreter for natural-language instructions for robotic applications\nspaCy – Python library\nSpark NLP – text processing library for advanced NLP for Python, Java, and Scala.\nWord2vec – obtaining vector representations of words\n\nSpeech recognition systems\nCMU Sphinx\nDeepSpeech\nWhisper\n\nLarge language models\nDeepSeek — R1 and V3 models*\nGPT-J – EleutherAI\nGPT-1 and GPT-2 — OpenAI LLM\nXLNet — Google LLM\nBERT — Google LLM\nT5 — Google LLM\nOLMo — Allen Institute for AI LLM\nLatam-GPT — Latin America-focused model\n*model weights only\n\nTransformer libraries\nHugging Face transformers library – Python library of pretrained transformer models for NLP, computer vision, speech, and more.\nFairseq – Facebook AI Research's sequence-to-sequence learning toolkit for training custom transformer models\nOpenNMT – neural machine translation framework that supports transformer architectures\n\nChat bots\nLAION OpenAssistant\nMycroft\n\nText to speech\nFestival Speech Synthesis System\nWaveNet\neSpeak\n\nText to image\nFlux\nStable Diffusion\n\nAI hardware and inference acceleration\nOpenVINO – Intel's toolkit for optimizing deep learning models for edge devices\nONNX – Open Neural Network Exchange format for interoperability between AI frameworks\n\nRobotics software\nArduPilot\nCoppeliaSim\nGazebo\nMobile Robot Programming Toolkit\nOpenRTM-aist\nPaparazzi Project\nPlayer Project\nPython Robotics\nRobot Operating System\nTurtleBot\nWebots\n\nSee also\nOpen-source artificial intelligence\nList of artificial intelligence journals and List of artificial intelligence books\nList of artificial intelligence projects\nList of free and open-source software packages for artificial intelligence\nCommon Crawl – nonprofit that crawls the web and freely provides its archives and datasets to the public under an MIT License\nGoogle Colab – an O-IDE Jupyter notebook environment with free access to GPUs and TPUs for machine learning and deep learning development"}
{"doc_id": "Lists of open-source artificial intelligence software", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " provides its archives and datasets to the public under an MIT License\nGoogle Colab – an O-IDE Jupyter notebook environment with free access to GPUs and TPUs for machine learning and deep learning development"}
{"doc_id": "Machine perception", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them. The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.\nMachine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information, to gather information with greater accuracy and to present it in a way that is more comfortable for the user. These include computer vision, machine hearing, machine touch, and machine smelling, as artificial scents are, at a chemical compound, molecular, atomic level, indiscernible and identical.\nThe end goal of machine perception is to give machines the ability to see, feel and perceive the world as humans do and therefore for them to be able to explain in a human way why they are making their decisions, to warn us when it is failing and more importantly, the reason why it is failing. This purpose is very similar to the proposed purposes for artificial intelligence generally, except that machine perception would only grant machines limited sentience, rather than bestow upon machines full consciousness, self-awareness, and intentionality.\n\nMachine vision\nComputer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and high-dimensional data from the real world to produce numerical or symbolic information, e.g., in the forms of decisions.  Computer vision has many applications already in use today such as facial recognition, geographical modeling, and even aesthetic judgment.\nHowever, machines still struggle to interpret visual impute accurately if it is blurry or if the viewpoint at which stimuli are viewed varies often. Computers also struggle to determine the proper nature of some stimulus if overlapped by or seamlessly touching another stimulus. This refers to the Principle of Good Continuation. Machines also struggle to perceive and record stimulus functioning according to the Apparent Movement principle which is a field of research in Gestalt psychology.\n\nMachine hearing\nMachine hearing, also known as machine listening or computer audition is the ability of a computer or machine to take in and process sound data such as speech or music.\nThis area has a wide range of application including music recording and compression, speech synthesis and speech recognition.\nMoreover, this technology allows the machine to replicate the human brain's ability to selectively focus on a specific sound against many other competing sounds and background"}
{"doc_id": "Machine perception", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " computer or machine to take in and process sound data such as speech or music.\nThis area has a wide range of application including music recording and compression, speech synthesis and speech recognition.\nMoreover, this technology allows the machine to replicate the human brain's ability to selectively focus on a specific sound against many other competing sounds and background noise. This ability is called \"auditory scene analysis\". The technology enables the machine to segment several streams occurring at the same time.\nMany commonly used devices such as a smartphones, voice translators and cars make use of some form of machine hearing. Present technology still has challenges in speech segmentation. This  means it is occasionally unable to correctly split words within sentences especially when spoken in an atypical accent.\n\nMachine touch\nMachine touch is an area of machine perception where tactile information is processed by a machine or computer.  Applications include tactile perception of surface properties and dexterity whereby tactile information can enable intelligent reflexes and interaction with the environment. Though this could possibly be done through measuring when and where friction occurs and also the nature and intensity of the friction, machines however still do not have any way of measuring few ordinary physical human experiences including physical pain. For example, scientists have yet to invent a mechanical substitute for the Nociceptors in the body and brain that are responsible for noticing and measuring physical human discomfort and suffering.\n\nMachine olfaction\nScientists are developing computers known as machine olfaction which can recognize and measure smells as well. Airborne chemicals are sensed and classified with a device sometimes known as an electronic nose.\n\nMachine taste\nFuture\nOther than those listed above, some of the future hurdles that the science of machine perception still has to overcome include, but are not limited to:\n- Embodied cognition - The theory that cognition is a full body experience, and therefore can only exist, and therefore be measure and analyzed, in fullness if all required human abilities and processes are working together through a mutually aware and supportive systems network.\n- The Moravec's paradox (see the link)\n- The Principle of similarity - The ability young children develop to determine what family a newly introduced stimulus falls under even when the said stimulus is different from the members with which the child usually associates said family with. (An example could be a child figuring that a chihuahua is a dog and house pet rather than vermin.)\n- The Unconscious inference: The natural human behavior of determining if a new stimulus is dangerous or not, what it is, and then how to relate to it without ever requiring any new conscious effort.\n- The innate"}
{"doc_id": "Machine perception", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " be a child figuring that a chihuahua is a dog and house pet rather than vermin.)\n- The Unconscious inference: The natural human behavior of determining if a new stimulus is dangerous or not, what it is, and then how to relate to it without ever requiring any new conscious effort.\n- The innate human ability to follow the likelihood principle in order to learn from circumstances and others over time.\n- The recognition-by-components theory - being able to mentally analyze and break even complicated mechanisms into manageable parts with which to interact with. For example: A person seeing both the cup and the handle parts that make up a mug full of hot cocoa, in order to use the handle to hold the mug so as to avoid being burned.\n- The free energy principle - determining long before hand how much energy one can safely delegate to being aware of things outside one's self without the loss of the needed energy one requires for sustaining their life and function satisfactorily. This allows one to become both optimally aware of the world around them self without depleting their energy so much that they experience damaging stress, decision fatigue, and/or exhaustion.\n\nSee also\nRobotic sensing\nSensors\nSLAM\nHistory of artificial intelligence"}
{"doc_id": "MAUVE (metric)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "MAUVE is a metric for automatically evaluating the quality of open-ended text generation. Developed by researchers at the University of Washington, Allen Institute for AI, and Stanford University, it was first introduced at NeurIPS 2021, where it received and Outstanding Paper Award.\nUnlike earlier metrics such as BLEU or ROUGE, which rely on n-gram overlap between a candidate and a reference, MAUVE measures how close the distribution of generated text is to the distribution of human-written text in a high-dimensional embedding space.\n\nBackground\nEvaluation of open-ended generation (such as story generation or long-form dialogue) is notoriously difficult. Traditional metrics penalize \"creative\" but valid deviations from a single reference text. Furthermore, neural language models often suffer from issues like repetitive loops or lack of long-range coherence that n-gram metrics fail to capture.\nMAUVE was designed to align more closely with human judgments of \"quality\" and \"diversity\" by treating text evalution as a comparison of two probability distributions: the distribution of human-written text (\n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n) versus the distribution of machine-generate text (\n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n).\n\nMethodology\nThe calculation of MAUVE involves three primary steps:\n\nEmbedding: large batches of human and machine-generated text are mapped into a vector space using a pre-trained transformer model.\nQuantization: the continuous embeddings are clustered into a finite set of \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n  \n codewords using k-means clustering to form discrete distributions.\nDivergence frontier: the metric calculates the trade-off between Type I and type II errors (precision and recall) between the two distributions using the Kullback-Leibler divergence.\n\nMathematical definition\nMAUVE is based on the area under the divergence frontier. For a mixing parameter \n  \n    \n      \n        λ\n        ∈\n        (\n        0\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle \\lambda \\in (0,1)}\n  \n, the mixture distribution is defined as:\n\n  \n    \n      \n        \n          R\n          \n            λ\n          \n        \n        =\n        λ\n        P\n        +\n        \n          (\n          \n            1\n            −\n            λ\n          \n          )\n        \n        Q\n      \n    \n    {\\displaystyle R_{\\lambda }=\\lambda P+\\left(1-\\lambda \\right)Q}\n  \n\nThe frontier is composed of the points \n  \n    \n      \n       "}
{"doc_id": "MAUVE (metric)", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " λ\n        P\n        +\n        \n          (\n          \n            1\n            −\n            λ\n          \n          )\n        \n        Q\n      \n    \n    {\\displaystyle R_{\\lambda }=\\lambda P+\\left(1-\\lambda \\right)Q}\n  \n\nThe frontier is composed of the points \n  \n    \n      \n        (\n        \n          R\n          \n            1\n          \n        \n        ,\n        \n          R\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle (R_{1},R_{2})}\n  \n defined by:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  R\n                  \n                    1\n                  \n                \n                (\n                λ\n                )\n              \n              \n                \n                =\n                exp\n                ⁡\n                \n                  (\n                  \n                    −\n                    \n                      KL\n                    \n                    (\n                    P\n                    ‖\n                    \n                      R\n                      \n                        λ\n                      \n                    \n                    )\n                  \n                  )\n                \n              \n            \n            \n              \n                \n                  R\n                  \n                    2\n                  \n                \n                (\n                λ\n                )\n              \n              \n                \n                =\n                exp\n                ⁡\n                \n                  (\n                  \n                    −\n                    \n                      KL\n                    \n                    (\n                    Q\n                    ‖\n                    \n                      R\n                      \n                        λ\n                      \n                    \n                    )\n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}R_{1}(\\lambda )&=\\exp \\left(-{\\text{KL}}(P\\Vert R_{\\lambda })\\right)\\\\R_{2}(\\lambda )&=\\exp \\left(-{\\text{KL}}(Q\\Vert R_{\\lambda })\\right)\\end{aligned}}}\n  \n\nwhere \n  \n    \n      \n        \n          KL\n        \n        (\n        )\n      \n    \n    {\\displaystyle {\\text{KL}}()}\n  \n refers to the Kullback-Leibler divergence. MAUVE is the integral of this curve, providing a single scalar value between 0 and 1. A higher MAUVE score indicates the model distribution \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n is more similar to the human distribution \n  \n    \n      \n        P\n      \n    \n    {\\displaystyle P}\n  \n.\n\nComparison with other metrics\nAdvantages\nMAUVE has shown a much higher correlation with human judgement in tasks like web text generation compare to earlier metrics. It effectively capture the \"self-repetition\" problem where models become stuck in loops.\n\nLimitations\nThe metric requires a large sample size (often more than 1000 generations) to provide a stable distributional estimate"}
{"doc_id": "MAUVE (metric)", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "VE has shown a much higher correlation with human judgement in tasks like web text generation compare to earlier metrics. It effectively capture the \"self-repetition\" problem where models become stuck in loops.\n\nLimitations\nThe metric requires a large sample size (often more than 1000 generations) to provide a stable distributional estimate. It is also computationally expensive as it requires running a large model to generate embeddings and perform clustering."}
{"doc_id": "Maysoun Ibrahim", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Maysoun Ibrahim (Arabic: ميسون إبراهيم) is a Palestinian scholar and technology strategist. She is the founding president of the Palestinian Syndicate for Information Sciences and Technology - PALIST. She is a Palestinian scholar and technology strategist known for her work in digital transformation, innovation policy, and smart sustainable cities. \nIbrahim holds a PhD in Information Technology for Development and is the author of the book, titled Smart Sustainable Cities: Transformation towards Future Cities, that is recognized as the first of its kind in the field of Smart and Sustainable Cities to be written by an Arabic woman.\nShe has played a leading role in advancing the information and communication technology (ICT) sector and promoting innovation-driven development in the Arab region. Her work focuses on harnessing emerging technologies to support sustainable growth, knowledge economies, and social inclusion. Her work and research focus on harnessing emerging technologies to support urban development, sustainability, resilience, knowledge economies, gender equality and social inclusion in the Arab world.\nIbrahim is known for her contributions to national and regional digital strategies, transformation frameworks, and policies that link technology with economic urban resilience, sustainable development, Smart and Sustainable Cities, and Artificial Intelligence.\n\nEarly life and education\nIbrahim holds a PhD in Information Technology for Development, with a research focus on Smart Sustainable Cities—an area that connects digital innovation with the United Nations Sustainable Development Goals (SDGs). \nInformation about her early life and undergraduate studies has not been publicly shared.\n\nCareer\nOver the course of her career, Ibrahim has worked across government institutions, academia, and the private sector. She has been involved in shaping national strategies for digital transformation, innovation, and public-sector modernization in Palestine and the wider Arab region.\n1. National roles\nIbrahim is the Founding President of the Palestinian Syndicate for Information Sciences and Technology (PALIST) (2021–present). She serves on the Management Board of the Palestinian Business and Professional Women (BPW) and a member of the International Business and Professional Women (BPW).  \nIbrahim is a board member of the Higher Council for Innovation and Excellence (HCIE) in Palestine and chairs both its Experimental Incubation Committee and the National Innovation Supporters Network. In 2019, she chaired the National Forum on the Fourth Industrial Revolution, a platform that explored emerging technologies and their role in economic and social development. \nShe contributed to several national projects and initiatives related to the Ministry of Telecommunications and Digital Economy of Palestine, including e-government transformation strategy and project, digital transformation"}
{"doc_id": "Maysoun Ibrahim", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ". In 2019, she chaired the National Forum on the Fourth Industrial Revolution, a platform that explored emerging technologies and their role in economic and social development. \nShe contributed to several national projects and initiatives related to the Ministry of Telecommunications and Digital Economy of Palestine, including e-government transformation strategy and project, digital transformation strategy, national Cybersecurity policy, national Artificial Intelligence Strategy, national innovation strategy, among others.\n2. Academic roles\nIn higher education, Ibrahim serves on the Board of Trustees of the Arab American University of Palestine (AAUP) and is the vice-chair of its National Digital Transformation and Artificial Intelligence Center. She is a member of the Scientific Research Council under the Palestinian Ministry of Education and Higher Education and serves as the Chair of the Committee on Scientific Research and Artificial Intelligence Ethics in Higher Education affiliated with the Council. She acted as a member of the Management Board and Senate of the Euro-Mediterranean University of Slovenia (EMUNI). \n3. International and Regional roles\nIbrahim acted as a consultant to UN-ESCWA. She authored UN-ESCWA, UN-Habitat and UN-DESA reports and presentations, including studies on Sustainable Development Goals (SDGs), smart sustainable cities, urban development, digital transformation for urban resilience and public institutions, innovation and gender equality in the Arab region. \nHer work with regional and international organizations, especially the United Nations (UN) organizations and the Union for Mediterranean (UfM), has helped shape regional strategies in digital transformation, urban resilience, sustainability, use of emerging technologies such as artificial intelligence, and innovation across several Arab countries.\n\nPositions\nPresident, Palestinian Syndicate for Information Science and Technology - PALIST (2021–present).\nPresident of the Founding Committee, Palestinian Syndicate for Information Sciences and Technology - PALIST (2018-2021).\nBoard Member, Palestinian International Cooperation Agency - PICA (2024–present).\nVice Chair, National Center for Digital Transformation and Artificial Intelligence, AAUP (2023–present).\nTrustee, Arab American University of Palestine - AAUP (2020–present).\nBoard member of the National Council on Scientific Research (2025 - present).\nChair of the Committee on Research Ethics and Artificial Intelligence Ethics, National Council for Scientific Research (2025 - present).\nBoard Member & Trustee, Euro-Mediterranean University in Slovenia (2020–2024).\nConsultant & reviewer, UN-ESCWA (2018–present).\nBoard Member, Higher Council for Innovation and Excellence - HCIE (201"}
{"doc_id": "Maysoun Ibrahim", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Intelligence Ethics, National Council for Scientific Research (2025 - present).\nBoard Member & Trustee, Euro-Mediterranean University in Slovenia (2020–2024).\nConsultant & reviewer, UN-ESCWA (2018–present).\nBoard Member, Higher Council for Innovation and Excellence - HCIE (2018–present).\nChair, Experimental Incubation Committee of the Higher Council for Innovation and Excellence - HCIE (2021-present).\nChair, National Innovation Supporters Network - HCIE (2022-present).\nChair, Preparatory Committee of the 4th National Forum on the Fourth Industrial Revolution (2019).\nJury Member, L'Oréal-UNESCO Women in Science Award (2019).\nMember, Technical Committee, UN-ESCWA Technology Center (2019–2021).\nJury, Hult Prize (2018–2022).\nJudge, Grace Hopper Celebration of Women in Computing (2013–2018).\nPeer reviewer for international journals including IEEE Xplore and Sustainable Cities and Society.\nDelivered sessions at UC Berkeley, Rice University, NYU Abu Dhabi, Portsmouth University, and more.\n\nAcademic work\nIbrahim’s academic research focuses on digital transformation frameworks, smart sustainable cities transformation processes, economic urban resilience, sustainable urban development, and science, technology and innovation diplomacy. Her work also explores how emerging technologies can enhance public institutional services through data-driven and people-centered solutions, fostering innovation and Responsiveness, Inclusiveness, Trustworthiness, and Effectiveness (RITE) governance. She has published extensively in highly ranked international journals. \nShe published the book titled: “Smart Sustainable Cities: Transformation towards Future Cities”, which is the first of it kind being published by an Arab women  in this field of study. Her peer-reviewed article ‘Smart sustainable cities roadmap: Readiness for transformation towards urban sustainability’ appears in Sustainable Cities and Society  and is widely cited in the SSC literature. She also developed a ‘Theory of Change’ model for Smart Sustainable Cities, which is one of the earliest theoretical models on the topic.\nIbrahim contributed chapters to international books on the Fourth Industrial Revolution, smart sustainable cities, and science, technology, and innovation diplomacy. In addition to her publications, she serves as a reviewer and a member of program committees for leading international research conferences and journals. Ibrahim has delivered academic webinars and in-person seminars at universities across the Arab region and globally, including UC Berkeley Future X (USA), Rice University (USA), University of Portsmouth (UK), EMUNI University ("}
{"doc_id": "Maysoun Ibrahim", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " publications, she serves as a reviewer and a member of program committees for leading international research conferences and journals. Ibrahim has delivered academic webinars and in-person seminars at universities across the Arab region and globally, including UC Berkeley Future X (USA), Rice University (USA), University of Portsmouth (UK), EMUNI University (Slovenia), New York University Abu Dhabi (UAE), Beirut Arab University (Lebanon), EUROMED University of Fes (Morocco), among others.\n\nAwards and recognition\nIbrahim has received several awards recognizing her contributions to research, innovation, and the empowerment of women in science and technology. \nIn addition to her achievements, she has served as a jury and evaluation committee member for several prominent international and regional initiatives, including, but not limited to: \n\nL’Oréal–UNESCO For Women in Science Awards.\nTaawon Youth Award.\nHult Prize.\nGrace Hopper Celebration of Women in computing.\n\nSelected publications\nIbrahim M. (2020). Smart Sustainable Cities: Transformation Towards Future Cities (Book).\n\"Science, Technology and Innovation Diplomacy in the Arab Region with Emphasis on the State of Palestine\" (Book chapter).\n\"The Fourth Industrial Revolution and Smart Cities in the Time of COVID-19\" (Book chapter).\nUN-ESCWA report and presentations, such as:\nLeveraging emerging technology and innovation for enhancing public institutions in the Arab region. \nSmart sustainable cities and smart digital solutions for urban resilience in the Arab region Lessons from the pandemic. \nFourth Industrial Revolution: Impact of the Fourth Industrial Revolution on Development in the Arab region.\nInformation and Communication Technologies: Prospects for Promoting Gender Equality in the Arab Region.\nDigital Transformation in Palestine - The Future City of Rawabi.\nScience and Technology Parks In Arab Countries with a focus on the State of Palestine.\nIbrahim, M., El-Zaart, A., & Adams, C. (2018). Smart sustainable cities roadmap: Readiness for transformation towards urban sustainability. Sustainable Cities and Society.\nIbrahim, M., Adam, C., & El-Zaart, A. (2015). Paving the way to Smart Sustainable Cities: Transformation Models and Challenges. Journal of Information Systems and Technology Management, 12(3), 559–576.\nM. Ibrahim, A. El-Zaart and C. Adams, \"Theory of change for the transformation towards smart sustainable cities,\" 2017 Sensors Networks Smart and Emerging Technologies (SENSET), Beiriut, Lebanon, 2017, 1"}
{"doc_id": "Maysoun Ibrahim", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Technology Management, 12(3), 559–576.\nM. Ibrahim, A. El-Zaart and C. Adams, \"Theory of change for the transformation towards smart sustainable cities,\" 2017 Sensors Networks Smart and Emerging Technologies (SENSET), Beiriut, Lebanon, 2017, 1-4.\n•\tSeveral peer-reviewed journal articles and conference papers on smart sustainable cities, ICT4D, and public innovation."}
{"doc_id": "Means–ends analysis", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Means–ends analysis (MEA) is a problem solving technique used commonly in artificial intelligence (AI) for limiting search in AI programs.\nIt is also a technique used at least since the 1950s as a creativity tool, most frequently mentioned in engineering books on design methods. MEA is also related to the means–ends chain approach used commonly in consumer behavior analysis. It is also a way to clarify one's thoughts when embarking on a mathematical proof.\n\nProblem-solving as search\nAn important aspect of intelligent behavior as studied in AI is goal-based problem solving, a framework in which the solution to a problem can be described by finding a sequence of actions that lead to a desirable goal.  A goal-seeking system is supposed to be connected to its outside environment by sensory channels through which it receives information about the environment and motor channels through which it acts on the environment.  (The term afferent is used to describe inward sensory flows, and efferent is used to describe outward motor commands.) In addition, the system has some means of storing in memory information about the state of the environment (afferent information) and information about actions (efferent information).  Ability to attain goals depends on building up associations, simple or complex, between particular changes in states and particular actions that will bring these changes about.  Search is the process of discovery and assembly of sequences of actions that will lead from a given state to a desired state. While this strategy may be appropriate for machine learning and problem solving, it is not always suggested for humans (e.g. cognitive load theory and its implications).\n\nHow it works\nThe MEA technique is a strategy to control search in problem-solving.  Given a current state and a goal state, an action is chosen which will reduce the difference between the two.  The action is performed on the current state to produce a new state, and the process is recursively applied to this new state and the goal state.\nIn order for MEA to be effective, the goal-seeking system must have a means of associating to any kind of detectable difference those actions that are relevant to reducing that difference.  It must also have means for detecting the progress it is making (the changes in the differences between the actual and the desired state), as some attempted sequences of actions may fail and, hence, some alternate sequences may be tried.\nWhen knowledge is available concerning the importance of differences, the most important difference is selected first to further improve the average performance of MEA over other brute-force search strategies. However, even"}
{"doc_id": "Means–ends analysis", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " differences between the actual and the desired state), as some attempted sequences of actions may fail and, hence, some alternate sequences may be tried.\nWhen knowledge is available concerning the importance of differences, the most important difference is selected first to further improve the average performance of MEA over other brute-force search strategies. However, even without the ordering of differences according to importance, MEA improves over other search heuristics–on average–by focusing the problem solving on the actual differences between the current state and that of the goal.\n\nAI use\nThe MEA technique as a problem-solving strategy was first introduced in 1961 by Allen Newell and Herbert A. Simon in their computer problem-solving program General Problem Solver (GPS).  In that implementation, the correspondence between differences and actions, also called operators, is provided a priori as knowledge in the system. (In GPS, this knowledge was in the form of a table of connections.)\nWhen the action and side-effects of applying an operator are penetrable, the search may select the relevant operators by inspection of the operators and do without a table of connections.  This latter case, of which the canonical example is Stanford Research Institute Problem Solver (STRIPS), an automated planning computer program, allows task-independent correlation of differences to the operators which reduce them.\nProdigy, a problem solver developed in a larger learning-assisted automated planning project started at Carnegie Mellon University by Jaime Carbonell, Steven Minton and Craig Knoblock, is another system that used MEA.\nMorten Lind at Technical University of Denmark has developed a tool called Multilevel Flow Modeling (MFM). It performs means–ends based diagnostic reasoning for industrial control and automation systems.\n\nSee also\nCausal layered analysis\nKnowledge representation\nAutomated reasoning\nIntelligent control\nMathematical proof\nFutures techniques\nPolytely\nGap analysis\nHill climbing"}
{"doc_id": "Mechanistic interpretability", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Mechanistic interpretability (often abbreviated as mech interp, mechinterp, or MI) is a subfield of research within explainable artificial intelligence that aims to understand the internal workings of neural networks by analyzing the mechanisms present in their computations. The approach seeks to analyze neural networks in a manner similar to how binary computer programs can be reverse-engineered to understand their functions.\n\nHistory\nThe term mechanistic interpretability was coined by Chris Olah. \nEarly work combined various techniques such as feature visualization, dimensionality reduction, and attribution with human-computer interaction methods to analyze models like the vision model Inception v1.\n\nKey concepts\nMechanistic interpretability aims to identify structures, circuits or algorithms encoded in the weights of machine learning models. This contrasts with earlier interpretability methods that focused primarily on input-output explanations.\n\nLinear representation hypothesis\nThis hypothesis suggests that high-level concepts are represented as linear directions in the activation space of neural networks. Empirical evidence from word embeddings and more recent studies supports this view, although it does not hold up universally.\n\nMethods\nMechanistic interpretability employs causal methods to understand how internal model components influence outputs, often using formal tools from causality theory.\nMechanistic interpretability, in the field of AI safety, is used to understand and verify the behavior of complex AI systems, and to attempt to identify potential risks."}
{"doc_id": "MediSafe controversy", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The MediSafe controversy refers to a public debate that emerged in Hong Kong between June and September 2025 after questions arose about the originality and data practices of MediSafe, a purportedly student-built artificial intelligence platform. The project was developed by Clarisse Poon, a Form Four student at the St. Paul's Co-educational College, and won several innovation awards in 2024 and 2025. Online discussion and media coverage later raised concerns about the project's authorship and use of patient data, prompting official investigations.\n\nMediSafe\nMediSafe (Chinese: 藥倍安心) is a web-based app designed to flag potential prescription errors by cross-checking medications against patient details such as allergies, chronic conditions, and liver or kidney function. According to award materials, the system uses large language models, SQL, and vector databases.\nThe platform received the following awards (all of which were later surrendered according to the statement issued in the name of Poon's parents):\n\nStudent Innovation Grand Award at the 2024 Hong Kong ICT Awards.\nSilver Medal at the 50th International Exhibition of Inventions Geneva in 2025, with support from the Education Bureau.\nYouth Tech Pioneer of the Year Award, recognising secondary school innovation.\n\nAllegations\nWhistleblower concerns\nOn 13 June 2025, Hailey Hei-Lam Cheng, a student at the City University of Hong Kong, posted concerns on Threads. She questioned whether such a complex AI system could have been built solely by a secondary school student and cited earlier statements suggesting the use of data from over 100 patients.\nCheng later said she received anonymous threats after making the post. Her comments triggered a wider discussion online about originality in student research, the role of external help, and how sensitive data is handled in youth-led tech projects.\n\nAI Health Studio link\nSome users pointed out that the MediSafe website previously redirected to AI Health Studio, a U.S.-based software company. Archived content from early 2025 described a prescription-checking system reportedly built for a Hong Kong clinic. Reports identified the clinic as one linked to Poon's father, Dr Poon Tung Ping (also known as Ronnie Poon).\nThe company's wording was later changed—what was originally described as \"developed\" became \"optimised\" and \"commercialised\". South China Morning Post noted that the earlier site version appeared to describe software similar to MediSafe and dated back to 2022.\n\nQuestions over data use\nSeveral news"}
{"doc_id": "MediSafe controversy", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Ronnie Poon).\nThe company's wording was later changed—what was originally described as \"developed\" became \"optimised\" and \"commercialised\". South China Morning Post noted that the earlier site version appeared to describe software similar to MediSafe and dated back to 2022.\n\nQuestions over data use\nSeveral news outlets reported public concerns about whether real patient data had been used in the project. According to statements quoted by the Ming Pao and HK01, the organisers and Poon's family said only simulated data and publicly available drug databases were used.\nA privacy-focused publication, Meta Connects, discussed the situation in light of Hong Kong's data privacy laws, raising issues around patient consent, potential cross-border data transfer, and whether external service providers were involved.\n\nResponses and investigations\nReactions from those involved\nPoon later stated she was working with competition organisers to clarify the situation and found some of the online attention discouraging. Dr. Poon told the press that only simulated data had been used and that the award process had followed proper verification. St. Paul's Co-educational College confirmed it was reviewing the matter internally.\nOn 5 August 2025, Ahmed Jemaa, co-founder of AI Health Studio, issued an official statement on LinkedIn addressing the MediSafe controversy. The statement clarified that the company had not been informed the application they developed would be submitted to academic competitions, maintaining this gave unfair advantage to other student participants. Jemaa emphasised that while the client provided the initial concept, AI Health Studio developed the minimum viable product (MVP) entirely independently, stating explicitly that \"no code, no UX, and no technical architecture were shared with them before starting the work on the project.\" The statement further accused Dr. Roberta Pang Wen Chi, Clarisse Poon's mother (Ronnie Poon's current wife), of attempting to control public perception following June's controversy surrounding the project's true authorship. Regarding institutional response, Jemaa expressed concerns about the Hong Kong Academy for Gifted Education's investigation, noting they had \"closed the case without correcting the record\" just weeks after commencing the probe, and that subsequent requests for clarification from AI Health Studio received no response.\nOn 22 August, the parents of Poon announced that they will voluntarily return all the awards MediSafe won, citing concerns to the mental and physical well-being of their daughter. In a public statement, Dr. Pang admitted that she had independently approached AI Health Studio to develop a MVP, although she insisted the concept of Medi"}
{"doc_id": "MediSafe controversy", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " 22 August, the parents of Poon announced that they will voluntarily return all the awards MediSafe won, citing concerns to the mental and physical well-being of their daughter. In a public statement, Dr. Pang admitted that she had independently approached AI Health Studio to develop a MVP, although she insisted the concept of MediSafe was independently conceived by her daughter.\nOn 27 August, St. Paul's Co-educational College issued a public statement, stating that the school had no prior knowledge of AI Health Studio's involvement of the project, and only learnt it through public media.\n\nOfficial and institutional responses\nThe Digital Policy Office said it had requested a full investigation by Hong Kong Education City and the Hong Kong ICT Awards’ Quality Assurance Panel. On 20 June, the Hong Kong Academy for Gifted Education and the Hong Kong New Generation Cultural Association issued a joint statement saying that the student had complied with competition rules. They also said no real patient data had been used and that company involvement came only after the competitions ended. The Office of the Privacy Commissioner for Personal Data confirmed it had received a complaint and was reviewing the matter.\n\nMedia coverage and public response\nThe controversy was widely covered in both English- and Chinese-language media. The SCMP explored the timeline of MediSafe's development and highlighted discrepancies between its public narrative and archived versions of related websites. The Standard reported on government involvement and noted public doubts about the project's origin. Other local outlets, including the Ming Pao, HK01, and the Oriental Daily News, examined possible issues of academic outsourcing and privacy violations. On 24 June, HK01 reported that the jury of the International Exhibition of Inventions Geneva reviewed the case and decided to keep the award, stating the submission met its criteria.\n\nTimeline\nApril 2024 – MediSafe wins the Student Innovation Grand Award at the Hong Kong ICT Awards.\nApril 2025 – Project receives a Silver Medal at the International Exhibition of Inventions Geneva.\n13 June 2025 – Hailey Cheng Hei Lam raises concerns publicly on Threads.\nMid‑June 2025 – Investigations launched by the Digital Policy Office and the Office of the Privacy Commissioner for Personal Data.\n20 June 2025 – The Hong Kong Academy for Gifted Education and the Hong Kong New Generation Cultural Association confirm no rules were broken.\n24 June 2025 – The jury of the International Exhibition of Inventions Geneva announces that the Silver Medal will not be revoked.\n28 June 2025 – The Hong Kong Innovation Foundation has removed the relevant exhibited works.\n22 August 202"}
{"doc_id": "MediSafe controversy", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ed Education and the Hong Kong New Generation Cultural Association confirm no rules were broken.\n24 June 2025 – The jury of the International Exhibition of Inventions Geneva announces that the Silver Medal will not be revoked.\n28 June 2025 – The Hong Kong Innovation Foundation has removed the relevant exhibited works.\n22 August 2025 – The parents of Poon announced that they will voluntarily return all the awards MediSafe won.\n27 August 2025 – St. Paul's Co-educational College issued a monolingual statement in Chinese disclaiming knowledge of AI Health Studio's involvement.\n\nSee also\nAcademic integrity\nArtificial intelligence in healthcare\nData privacy\nLeakage (machine learning)"}
{"doc_id": "Military applications of artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Artificial intelligence (AI) has many applications in warfare, including in communications, intelligence, and munitions control. Warfare which is algorithmic or controlled by artificial intelligence, with little to no human decision-making, is called hyperwar, a term coined by Amir Husain and John R. Allen as a portmanteau of the Ancient Greek preposition and prefix hyper (ὑπέρ, 'beyond') and the English \"war\". Due to its autonomous nature, AI could rapidly increase the speed of warfare, especially if more than one side is relying on AI.  AI is not limited to new weapons such as drones or cyberwar, it can affect all forms of military planning.\n\nUses\nAI can enhance command and control, communications, sensors, integration and interoperability. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles, both human operated and autonomous.\nAI has been used in military operations in Iraq, Syria, Ukraine, Iran and Israel. An AI-powered Automatic Target Classifying System, which employs sensors and algorithms to automatically identify and classify targets on radar, was patented by the Indian Army in 2025. It swiftly and precisely compares real-time data—like pictures or radar signals—to a database of stored data. It can be used for disposable purposes, such guiding missiles.\n\nAutonomous armament\nMilitary drones capable of autonomous action are in wide use.\n\nCommand and control\nIn 2024 a Chinese laboratory at the Joint Operations College of the National Defense University in Shijiazhuang has created an AI military commander, for use in large-scale war simulations in the role of the commander-in-chief.\nIn 2024, the Ukrainian Army developed autonomous Kamikaze drones in order to make Russian interference during flight ineffective.\nDuring the 2025 India–Pakistan conflict, the Indian military’s AI-enabled Meteorological Reporting System (Project Anumaan) was fed IMD data to assist the Artillery Combat Command and Control System with planning and accurate targeting with extended range artillery shells. Up to 200 kilometres (120 miles) inside Pakistan's borders, the system can predict exact wind speed and other meteorological conditions 48-72 hours ahead of time. The AI also aided in long-range missile trajectory calculations.\n\nMilitary intelligence\nIn 2023, the United States Department of Defense tested generative AI based on large language models to digitize and integrate data across the military.\nIsrael used"}
{"doc_id": "Military applications of artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " predict exact wind speed and other meteorological conditions 48-72 hours ahead of time. The AI also aided in long-range missile trajectory calculations.\n\nMilitary intelligence\nIn 2023, the United States Department of Defense tested generative AI based on large language models to digitize and integrate data across the military.\nIsrael used two AI systems in the Gaza War to generate targets to strike: Habsora (translated: \"the gospel\") was used to compile a list of buildings to target, while \"Lavender\" produced a list of people. \"Lavender\" produced a list of 37,000 people to target. The list of buildings to target included Gazan private homes of people that were suspected of affiliation to Hamas operatives. The combination of AI targeting technology with policy shift away from avoiding civilian targets resulted in unprecedented numbers of civilian deaths. IDF officials say the program addresses the previous issue of the air force running out of targets. Using Habsora, officials say that suspected and junior Hamas members homes significantly expand the \"AI target bank.\" An internal source describes the process as a “mass assassination factory”.\nIn 2024, the U.S. military trained artificial intelligence to identify airstrike targets during its operations in Iraq and Syria.\nIn 2025 India–Pakistan conflict, the Indian Army employed AI to create shared operational picture, analyze intelligence, evaluate threats, and create predictive models for long-range attacks. Twenty-three applications were developed for different purposes that handled inputs and data for real-time multi-sensor and multi-source data fusion. Applications include the Electronic Intelligence Collation and Analysis System, which has been integrated with Project Sanjay to provide a common operational picture for improved coordination, situational awareness, and decision superiority, and the Trinetra system, which has been used to identify and prioritize critical threats in order to achieve strategic dominance. AI-enabled weather forecast, allowed for accurate planning and targeting of artillery units and long-range vectors. The battlefield AI model was fed 26 years of data that had recorded and archived the Pakistan Armed Forces' frequency signature and radio emission. This specified which military unit in Pakistan was responsible for each piece of equipment and where it had previously been used. Integrating feeds from sensors, drones, radars, and satellites allowed for the collection of real-time data. All the data, including information on adversary positions, resources, and logistics, were combined and presented to the military commanders for appropriate action.\n\nGlobal trends\nVarious countries are researching and deploying AI military applications, in what has been termed the \"artificial intelligence"}
{"doc_id": "Military applications of artificial intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " radars, and satellites allowed for the collection of real-time data. All the data, including information on adversary positions, resources, and logistics, were combined and presented to the military commanders for appropriate action.\n\nGlobal trends\nVarious countries are researching and deploying AI military applications, in what has been termed the \"artificial intelligence arms race\". Ongoing research is focused on intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.\nWorldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015.\nIn November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology.\nMany AI researchers try to avoid military applications, with guardrails to prevent military applications integrated into most mainstream large language models.\n\nIn popular culture\nMilitary artificial intelligence systems have appeared in many works of fiction, often as antagonists.\n\nFilm\nThe Terminator franchise\nThe Matrix franchise\n\nLiterature\nLegends of Dune trilogy by Brian Herbert"}
{"doc_id": "Mindpixel", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Mindpixel was a web-based collaborative artificial intelligence project which aimed to create a knowledgebase of millions of human validated true/false statements, or probabilistic propositions. It ran from 2000 to 2005.\n\nDescription\nParticipants in the project created one-line statements which aimed to be objectively true or false to 20 other anonymous participants. In order to submit their statement they had first to check the true/false validity of 20 such statements submitted by others. Participants whose replies were consistently out of step with the majority had their status downgraded and were eventually excluded. Likewise, participants who made contributions which others could not agree were objectively true or false had their status downgraded.  A validated  true/false statement is called a mindpixel.\nThe project enlisted the efforts of thousands of participants and claimed to be \"the planet's largest artificial intelligence effort\".\nThe project was conceived by Chris McKinstry, a computer scientist and former Very Large Telescope operator for the European Southern Observatory in Chile, as MISTIC (Minimum Intelligent Signal Test Item Corpus) in 1996. Mindpixel was developed out of this program, and started in 2000 and had 1.4 million mindpixels in January 2004. The database and its software is known as GAC, which stands for \"Generic Artificial Consciousness\" and is pronounced Jak.   \nMcKinstry believed that the Mindpixel database could be used in conjunction with a neural net to produce a body of human \"common sense\" knowledge which would have market value. Participants in the project were promised shares in any future value according to the number of mindpixels they had successfully created.\nOn 20 September 2005 Mindpixel lost its free server and is no longer operational. It was being rewritten by Chris McKinstry as Mindpixel 2 and was intended to appear on a new server in France.\nChris McKinstry died of suicide on 23 January 2006 and the future of the project and the integrity of the data is uncertain. \nSome Mindpixel data have been utilized by Michael Spivey of Cornell University and Rick Dale of The University of Memphis to study theories of high-level reasoning and continuous temporal dynamics of thought. McKinstry, along with Dale and Spivey, designed an experiment that has now been published in Psychological Science in its January, 2008 issue. In this paper, McKinstry (as posthumous first author), Dale, and Spivey use a very small and carefully selected set of Mindpixel statements to show that even high-level thought processes like decision making can be revealed"}
{"doc_id": "Mindpixel", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " experiment that has now been published in Psychological Science in its January, 2008 issue. In this paper, McKinstry (as posthumous first author), Dale, and Spivey use a very small and carefully selected set of Mindpixel statements to show that even high-level thought processes like decision making can be revealed in the nonlinear dynamics of bodily action.\nOther similar AI-driven knowledge acquisition projects are Never-Ending Language Learning and Open Mind Common Sense (run by MIT), the latter being also hampered when its director died of suicide.\n\nSee also\nNever-Ending Language Learning\nCyc"}
{"doc_id": "MindsDB", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "MindsDB is an open-source artificial intelligence software platform that enables organizations to analyze both structured and unstructured data without requiring it to be moved into a separate storage system. It connects to existing databases, business applications, and document-based sources, allowing information to be queried directly from its current location.\n\nHistory\nMindsDB was founded in 2017 by Jorge Torres and Adam Carrigan in Berkeley, California. The first open-source release in 2018 introduced basic machine-learning functions that operated inside traditional databases. From 2019 to 2024, the project expanded into a broader system for accessing distributed enterprise data. Independent technology publications referenced MindsDB during this period in the context of AI infrastructure and data engineering tools.\nBetween 2024 and 2025, MindsDB added features for working with large language models (LLMs) and retrieval-augmented generation, positioning the platform as an open-source engine for running AI queries across different data systems.\n\nOverview\nMindsDB has formed strategic partnerships with leading companies such as Snowflake, SingleStore, DataStax, and NVIDIA. As of September 2024, the platform supports over 200 integrations, including popular large language models (LLMs) like OpenAI, Anthropic, and Mistral, as well as data platforms such as MySQL, PostgreSQL, Snowflake, and MongoDB. MindsDB also integrates with a wide range of applications, including Salesforce, HubSpot, X(former Twitter), and many others.\n\nTechnology\nFederated data access\nMindsDB offers a federated query engine – a system that allows SQL queries to run across multiple databases, business applications, and document-based sources without combining the data into one central repository. This makes it possible to analyze live operational information while it remains in its original location.\n\nDocument and file search\nMindsDB provides tools for managing and searching unstructured content such as documents and text fields:\n\nvector search – a method for finding similar content by comparing numerical representations of text\nmetadata filtering – narrowing search results by attributes such as date or category\n\nAI data agents\nMindsDB includes AI-based agents that interpret natural-language questions, generate SQL queries, retrieve context, and produce answers using large language models. These agents act as intermediaries that translate everyday language into database operations.\n\nModels and integrations\nThe platform supports open-source, hosted, and self-managed LLMs, and can connect them to databases, warehouses, and vector stores. This allows the models to work with data stored across many environments without additional data pipelines. Minds"}
{"doc_id": "MindsDB", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " act as intermediaries that translate everyday language into database operations.\n\nModels and integrations\nThe platform supports open-source, hosted, and self-managed LLMs, and can connect them to databases, warehouses, and vector stores. This allows the models to work with data stored across many environments without additional data pipelines. MindsDB has been included in rankings of notable software tools for AI and data infrastructure.\n\nDeployment\nMindsDB is available as open-source software and as a hosted cloud service. It can also be deployed on-premises or inside private cloud environments, depending on an organization’s requirements. Independent financial and technology reporting has covered several integration and reseller agreements involving MindsDB. The platform can additionally operate as a Model Context Protocol (MCP) server, allowing MCP-compatible developer tools to access its query engine and AI functions. MindsDB is listed as an optional integration within the Google MCP Toolbox."}
{"doc_id": "Mode collapse", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In machine learning, mode collapse is a failure mode observed in generative models, originally noted in Generative Adversarial Networks (GANs). It occurs when the model produces outputs that are less diverse than expected, effectively \"collapsing\" to generate only a few modes of the data distribution while ignoring others. This phenomenon undermines the goal of generative models to capture the full diversity of the training data.\nThere are typically two times at which a model can collapse: either during training or during post-training finetuning.\nMode collapse reduces the utility of generative models in applications, such as in\n\nimage synthesis (repetitive or near-identical images);\ndata augmentation (limited diversity in synthetic data);\nscientific simulations (failure to explore all plausible scenarios).\n\nDistinctions\nMode collapse is distinct from overfitting, also called memorization, where a model learns detailed patterns in the training data that do not generalize to the test data, although there are commonalities between both phenomena.\nIn terms of learning a probability distribution, mode collapse corresponds to the collapse of the entire distribution to one or a few points, which may or may not correspond to points with high likelihood in the target distribution.\nOverfitting, on the other hand, corresponds to learning a distribution that is highly peaked around training data points. In a sense, it can be seen as a form of near-complete or complete mode collapse, where the modes are every, or most of the training dataset. However, this is usually due to the overparametrization of the model, and not the training procedure itself, as is the case for GANs.\nUnderfitting, however, does not share commonalities with mode collapse. In this case, the model is insufficiently parametrized or trained, and the learned distribution is far from the target distribution, usually too close to the distribution at initialization.\n\nIn GANs\nTraining-time mode collapse was originally noted and studied in GANs, where it arises primarily due to imbalances in the training dynamics between the generator and discriminator in GANs. In the original GAN paper, it was also called the \"Helvetica scenario\".\nCommon causes include:\n\nIf the discriminator learns too slowly, the generator may exploit weaknesses by producing a narrow set of outputs that consistently fool the discriminator.\nTraditional GAN loss functions (e.g., Jensen-Shannon divergence) may be too lenient on generating same-looking outputs.\nThe adversarial training process can lead to oscillatory behavior, where the generator and discriminator fail to converge to a stable"}
{"doc_id": "Mode collapse", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " may exploit weaknesses by producing a narrow set of outputs that consistently fool the discriminator.\nTraditional GAN loss functions (e.g., Jensen-Shannon divergence) may be too lenient on generating same-looking outputs.\nThe adversarial training process can lead to oscillatory behavior, where the generator and discriminator fail to converge to a stable equilibrium, but instead engage in a rock-beats-paper-beats-scissors kind of cycling. The generator would generate just \"rock\" until the discriminator learns to classify that as generated, then the generator switch to generating just \"scissors\", and so on. The generator would always be mode-collapsed, though the precise mode in which it collapses to would change during training.\nSeveral GAN-specific strategies were developed to mitigate mode collapse:\n\nTwo time-scale update rule.\nMini-batch discrimination allows the discriminator to evaluate entire batches of samples, encouraging diversity.\nUnrolled GANs optimize the generator against future states of the discriminator.\nWasserstein GAN uses Earth Mover's distance to provide more stable gradients.\nUse a big and balanced training dataset.\nRegularization methods such as gradient penalty and spectral normalization.\n\nFinetuning\nThe large language models are usually trained in two steps. In the first step (\"pretraining\"), the model is trained to simply generate text sampled from a large dataset. In the second step (\"finetuning\"), the model is trained to perform specific tasks by training it on a small dataset containing just the task-specific data. For example, to make a chatbot in this method, one first pretrains a large transformer model over a few trillion words of text scraped from the Internet, then finetunes it on a few million words of example chatlogs that the model should imitate.\nMode collapse may occur during finetuning, as the model learns to generate text that accomplishes the specific task, but loses ability to generate other forms of text. It may also be able to generate a smaller subset of texts that accomplish the specific task. It is hypothesized that there is a tradeoff between quality and diversity. Given a single pretrained model, one may finetune it to perform a specific task. More finetuning would result in higher average task performance, but less diverse outputs. Less finetuning would result in lower average performance, but more diverse outputs. A similar tradeoff has been observed in image generation models and GAN-based text generators.\nSimilarly, mode collapse may occur during RLHF, via reward hacking the reward model or other mechanisms.\n\nSee also\nVariational autoencoder\nGenerative"}
{"doc_id": "Mode collapse", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Less finetuning would result in lower average performance, but more diverse outputs. A similar tradeoff has been observed in image generation models and GAN-based text generators.\nSimilarly, mode collapse may occur during RLHF, via reward hacking the reward model or other mechanisms.\n\nSee also\nVariational autoencoder\nGenerative model\nGenerative artificial intelligence\nGenerative pre-trained transformer\nOverfitting"}
{"doc_id": "Moral outsourcing", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Moral outsourcing refers to placing responsibility for ethical decision-making on to external entities, often algorithms. The term is often used in discussions of computer science and algorithmic fairness, but it can apply to any situation in which one appeals to outside agents in order to absolve themselves of responsibility for their actions. In this context, moral outsourcing specifically refers to the tendency of society to blame technology, rather than its creators or users, for any harm it may cause.\n\nDefinition\nThe term \"moral outsourcing\" was first coined by Dr. Rumman Chowdhury, a data scientist concerned with the overlap between artificial intelligence and social issues. Chowdhury used the term to describe looming fears of a so-called “Fourth Industrial Revolution” following the rise of artificial intelligence.\nMoral outsourcing is often applied by technologists to shrink away from their part in building offensive products. In her TED Talk, Chowdhury gives the example of a creator excusing their work by saying they were simply doing their job. This is a case of moral outsourcing and not taking ownership for the consequences of creation. \nWhen it comes to AI, moral outsourcing allows for creators to decide when the machine is human and when it is a computer - shifting the blame and responsibility of moral plights off of the technologists and onto the technology. Conversations around AI and bias and its impacts require accountability to bring change. It is difficult to address these biased systems if their creators use moral outsourcing to avoid taking any responsibility for the issue. \nOne example of moral outsourcing is the anger that is directed at machines for “taking jobs away from humans” rather than companies for employing that technology and jeopardizing jobs in the first place. \nThe term \"moral outsourcing\" refers to the concept of outsourcing, or enlisting an external operation to complete specific work for another organization. In the case of moral outsourcing, the work of resolving moral dilemmas or making choices according to an ethical code is supposed to be conducted by another entity.\n\nReal-World Applications\nIn the medical field, AI is increasingly involved in decision-making processes about which patients to treat, and how to treat them. The responsibility of the doctor to make informed decisions about what is best for their patients is outsourced to an algorithm. Sympathy is also noted to be an important part of medical practice; an aspect that artificial intelligence, glaringly, is missing. This form of moral outsourcing is a major concern in the medical community.  \nAnother field of technology in which moral outsourcing is frequently brought up is autonomous vehicles. California Polytechnic"}
{"doc_id": "Moral outsourcing", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " algorithm. Sympathy is also noted to be an important part of medical practice; an aspect that artificial intelligence, glaringly, is missing. This form of moral outsourcing is a major concern in the medical community.  \nAnother field of technology in which moral outsourcing is frequently brought up is autonomous vehicles. California Polytechnic State University professor Keith Abney proposed an example scenario: \"Suppose we have some [troublemaking] teenagers, and they see an autonomous vehicle, they drive right at it. They know the autonomous vehicle will swerve off the road and go off a cliff, but should it?\" The decision of whether to sacrifice the autonomous vehicle (and any passengers inside) or the vehicle coming at it will be written into the algorithms defining the car's behavior. In the case of moral outsourcing, the responsibility of any damage caused by an accident may be attributed to the autonomous vehicle itself, rather than the creators who wrote protocol the vehicle will use to \"decide\" what to do.\nMoral outsourcing is also used to delegate the consequences of predictive policing algorithms to technology, rather than the creators or the police. There are many ethical concerns with predictive policing due to the fact that it results in the over-policing of low income and minority communities.  In the context of moral outsourcing, the positive feedback loop of sending disproportionate police forces into minority communities is attributed to the algorithm and the data being fed into this system--rather than the users and creators of the predictive policing technology.\n\nOutside of Technology\nReligion\nMoral outsourcing is also commonly seen in appeals to religion to justify discrimination or harm. In his book What It Means to be Moral, sociologist Phil Zuckerman contradicts the popular religious notion that morality comes from God. Religion is oftentimes cited as a foundation for a moral stance without any tangible relation between the religious beliefs and personal stance. In these cases, religious individuals will \"outsource\" their personal beliefs and opinions by claiming that they are a result of their religious identification. This is seen where religion is cited as a factor for political beliefs, medical beliefs, and in extreme cases an excuse for violence.\n\nManufacturing\nMoral outsourcing can also be seen in the business world in terms of manufacturing goods and avoiding environmental responsibility. Some companies in the United States will move their production process to foreign countries with more relaxed environmental policies to avoid the pollution laws that exist in the US. A study by the Harvard Business Review found that \"in countries with tight environmental regulation, companies have 29% lower domestic emissions on average. On"}
{"doc_id": "Moral outsourcing", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and avoiding environmental responsibility. Some companies in the United States will move their production process to foreign countries with more relaxed environmental policies to avoid the pollution laws that exist in the US. A study by the Harvard Business Review found that \"in countries with tight environmental regulation, companies have 29% lower domestic emissions on average. On the other hand, such a tightening in regulation results in 43% higher emissions abroad.\" The consequences of higher pollution rates are then attributed to the loose regulations in these countries, rather than on the companies themselves who purposefully moved into these areas to avoid strict pollution policy.\n\nRumman Chowdhury\nChowdhury has a prominent voice in the discussions about the intersection of ethics and AI. Her ideas have been included in The Atlantic, Forbes, MIT Technology Review, and the Harvard Business Review.  \n\n\n=== References ==="}
{"doc_id": "NASA AI Assisted-Air Quality Monitoring Project", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The NASA Expert-System Ion Trap Mass Spectrometer (ES-ITMS) Project was a public-private partnership to develop an artificial intelligence assisted, air quality monitoring system and was qualified for use on the Space Shuttle. The partnership was also the first cost and intellectual property shared public-partnership implemented by NASA, which used the commercial Research and Development Limited Partnership (RDLP) model that had been adopted by the Reagan Administration for Department of Defense semiconductor development, and recommended for use by NASA for space commercialization. The project partners included NASA, the University of Florida and Finnigan MAT Corporation, was organized and administered by the NASA Joint Enterprise Institute (subsequently NASA Joint Sponsored Program) and ran from 1988 through 1990. The partnership concluded final testing in 1991, generating four patents, expert system software and application protocol reports. The system was space qualified for use on the Shuttle and elements of the ES-ITMS system were integrated into the product Improvements for Finnigan MAT corporation. The success of the partnership lead NASA to create a pilot program to develop partnership business models as an ongoing management practice.\n\nPurpose and objectives\nThe need to monitor air quality in confined spaces represented an increasing challenge for NASA's planned space missions and private sector facility managers facing the increased scrutiny of possible air contaminants. Up to the early 1980's, air quality monitors generally required large spaces and human technicians to interpret readings. This created a need for miniaturized air quality monitors that could generate reliable and accurate analytic results without on-site technician presence.\nNASA initiated projects to develop...\"mobile and/or portable mass spectrometers\" that evaluated the \"tradeoff between instrumentation capabilities and space, weight and power considerations.\" NASA selected a \"commercial ITMS instrument capable of generating electron ionization, chemical ionization and mass spectrometry data\", to develop a linked expert system to accomplish analysis without human intervention.\nThe commercial instrumentation was from Finnigan MAT corporation while the scientific expertise to support expert system development was available at the University of Florida.\nThe project managers at NASA Ames created a single, integrated project using the RDLP model with objectives to:\n\nDevelop AI/expert system software for instrument control (NASA's role)\nExpand sensitivity, selectivity and speed of the spectrometer (Univ Florida role)\nExpand the spectrometer analytic capability and automate the screening (Finnigan role)\n\nMembership\nThe partnership included seven specialists from five member organizations:\n\nFederal Government\nNational Aeronautics and Space Administration (NASA)\nNASA Ames Research Center (ARC)\nNASA Kennedy Space Center (K"}
{"doc_id": "NASA AI Assisted-Air Quality Monitoring Project", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the spectrometer (Univ Florida role)\nExpand the spectrometer analytic capability and automate the screening (Finnigan role)\n\nMembership\nThe partnership included seven specialists from five member organizations:\n\nFederal Government\nNational Aeronautics and Space Administration (NASA)\nNASA Ames Research Center (ARC)\nNASA Kennedy Space Center (KSC)\nCommercial\nFinnigan MAT Corporation (Thermo-Fisher Scientific)\nTGS Technology, Inc.\nResearch Management\nUniversity of Florida\n\nOrganization, management and administration\nThe technical project was organized into two development teams, one located in at the NASA Ames Research Center covering expert systems and analytic capabilities and one in Florida covering improved sensitivity and testing.  \nThe partnership management and administration was provided by a non-profit, partnership support organization: the Joint Enterprise Institute operating through San Francisco State University Foundation (SFSUF) with a NASA  employee liaison, Syed Shariq.\n\nPublic-private partnership\nThe partnership structure was as a prototype test of a pilot NASA program to develop public-private partnership business models. The pilot program was known as the NASA  Joint Sponsored Research Program (JSRP), which operated as the NASA Joint Enterprise Institute between 1988 and 1991. The partnership was the first public-private, research and development partnership implemented by NASA in response to national policy shifts to increase technology transfer and space commercialization. The partnership structure included a two year technology development and testing plan that cost $610,000, of which NASA funded $310,000, Finnigan $175,000 and the University of Florida $95,000.\n\nResults and commercialization\nThe project generated patents (4), software (2) and application protocol reports (8). NASA gained use of the patents and jointly development software while Finnigan received commercial utilization rights. The results were commercialized within eighteen months of project completion.\n\nRecognition\nNASA recognized the project as a space qualified instrument. Its achievements were reported to the NASA Administrator, directly leading to establishment of the agency-wide Joint Sponsored Research Program.\n\nSee also\nMass Spectrometry\nMiniaturized Mass Spectrometer\nIon Trap\nHuman Presence in Space\nNASA Joint Sponsored Research Program"}
{"doc_id": "Neural computation", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Neural computation is the information processing performed by networks of neurons. Neural computation is affiliated with the philosophical tradition known as Computational theory of mind, also referred to as computationalism, which advances the thesis that neural computation explains cognition. The first persons to propose an account of neural activity as being computational was Warren McCullock and Walter Pitts in their seminal 1943 paper, A Logical Calculus of the Ideas Immanent in Nervous Activity.\nThere are three general branches of computationalism, including classicism, connectionism, and computational neuroscience. All three branches agree that cognition is computation, however, they disagree on what sorts of computations constitute cognition. The classicism tradition believes that computation in the brain is digital, analogous to digital computing. Both connectionism and computational neuroscience do not require that the computations that realize cognition are necessarily digital computations. However, the two branches greatly disagree upon which sorts of experimental data should be used to construct explanatory models of cognitive phenomena. Connectionists rely upon behavioral evidence to construct models to explain cognitive phenomena, whereas computational neuroscience leverages neuroanatomical and neurophysiological information to construct mathematical models that explain cognition.\nWhen comparing the three main traditions of the computational theory of mind, as well as the different possible forms of computation in the brain, it is helpful to define what we mean by computation in a general sense. Computation is the processing of information, otherwise known as variables or entities, according to a set of rules. A rule in this sense is simply an instruction for executing a manipulation on the current state of the variable, in order to produce a specified output. In other words, a rule dictates which output to produce given a certain input to the computing system. A computing system is a mechanism whose components must be functionally organized to process the information in accordance with the established set of rules. The types of information processed by a computing system determine which type of computations it performs. Traditionally, in cognitive science there have been two proposed types of computation related to neural activity - digital and analog, with the vast majority of theoretical work incorporating a digital understanding of cognition. Computing systems that perform digital computation are functionally organized to execute operations on strings of digits with respect to the type and location of the digit on the string. It has been argued that neural spike train signaling implements some form of digital computation, since neural spikes may be considered as discrete units or digits, like 0 or 1 - the neuron either fires an action potential or it does not. Accordingly, neural spike trains could be seen as strings of digits."}
{"doc_id": "Neural computation", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " on the string. It has been argued that neural spike train signaling implements some form of digital computation, since neural spikes may be considered as discrete units or digits, like 0 or 1 - the neuron either fires an action potential or it does not. Accordingly, neural spike trains could be seen as strings of digits. Alternatively, analog computing systems perform manipulations on non-discrete, irreducibly continuous variables, that is, entities that vary continuously as a function of time. These sorts of operations are characterized by systems of differential equations.\nNeural computation can be studied for example by building models of neural computation.\nArtificial neural networks (ANN) is a subfield of the research area machine learning. Work on ANNs has been somewhat inspired by knowledge of neural computation.\n\nPeriodicals\nThere are a number of scientific journal dedicated to this subject, among them:\n\nNeural Computation.\nNeural Computing and Applications\n\nSee also\nQuantum neural network"}
{"doc_id": "Neural scaling law", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In machine learning, a neural scaling law is an empirical scaling law that describes how neural network performance changes as key factors are scaled up or down. These factors typically include the number of parameters, training dataset size, and training cost. Some models also exhibit performance gains by scaling inference through increased test-time compute (TTC), extending neural scaling laws beyond training to the deployment phase.\n\nIntroduction\nIn general, a deep learning model can be characterized by four parameters: model size, training dataset size, training cost, and the post-training error rate (e.g., the test set error rate). Each of these variables can be defined as a real number, usually written as \n  \n    \n      \n        N\n        ,\n        D\n        ,\n        C\n        ,\n        L\n      \n    \n    {\\displaystyle N,D,C,L}\n  \n (respectively: parameter count, dataset size, computing cost, and loss).\nA neural scaling law is a theoretical or empirical statistical law between these parameters. There are also other parameters with other scaling laws.\n\nSize of the model\nIn most cases, the model's size is simply the number of parameters. However, one complication arises with the use of sparse models, such as mixture-of-expert models. With sparse models, during inference, only a fraction of their parameters are used. In comparison, most other kinds of neural networks, such as transformer models, always use all their parameters during inference.\n\nSize of the training dataset\nThe size of the training dataset is usually quantified by the number of data points within it. Larger training datasets are typically preferred, as they provide a richer and more diverse source of information from which the model can learn. This can lead to improved generalization performance when the model is applied to new, unseen data. However, increasing the size of the training dataset also increases the computational resources and time required for model training.\nWith the \"pretrain, then finetune\" method used for most large language models, there are two kinds of training dataset: the pretraining dataset and the finetuning dataset. Their sizes have different effects on model performance. Generally, the finetuning dataset is less than 1% the size of pretraining dataset.\nIn some cases, a small amount of high quality data suffices for finetuning, and more data does not necessarily improve performance.\n\nCost of training\nTraining cost is typically measured in terms of time (how long it takes to train the model) and computational resources (how much processing power and memory are required). It is important to note that the cost"}
{"doc_id": "Neural scaling law", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of high quality data suffices for finetuning, and more data does not necessarily improve performance.\n\nCost of training\nTraining cost is typically measured in terms of time (how long it takes to train the model) and computational resources (how much processing power and memory are required). It is important to note that the cost of training can be significantly reduced with efficient training algorithms, optimized software libraries, and parallel computing on specialized hardware such as GPUs or TPUs.\nThe cost of training a neural network model is a function of several factors, including model size, training dataset size, the training algorithm complexity, and the computational resources available. In particular, doubling the training dataset size does not necessarily double the cost of training, because one may train the model for several times over the same dataset (each being an \"epoch\").\n\nPerformance\nThe performance of a neural network model is evaluated based on its ability to accurately predict the output given some input data. Common metrics for evaluating model performance include:\n\nNegative log-likelihood per token (logarithm of perplexity) for language modeling;\nAccuracy, precision, recall, and F1 score for classification tasks;\nMean squared error (MSE) or mean absolute error (MAE) for regression tasks;\nElo rating in a competition against other models, such as gameplay or preference by a human judge.\nPerformance can be improved by using more data, larger models, different training algorithms, regularizing the model to prevent overfitting, and early stopping using a validation set.\nWhen the performance is a number bounded within the range of \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  \n, such as accuracy, precision, etc., it often scales as a sigmoid function of cost, as seen in the figures.\n\nExamples\n(Hestness, Narang, et al, 2017)\nThe 2017 paper is a common reference point for neural scaling laws fitted by statistical analysis on experimental data. Previous works before the 2000s, as cited in the paper, were either theoretical or orders of magnitude smaller in scale. Whereas previous works generally found the scaling exponent to scale like \n  \n    \n      \n        L\n        ∝\n        \n          D\n          \n            −\n            α\n          \n        \n      \n    \n    {\\displaystyle L\\propto D^{-\\alpha }}\n  \n, with \n  \n    \n      \n        α\n        ∈\n        {\n        0.5\n        ,\n        1\n        ,\n        2\n       "}
{"doc_id": "Neural scaling law", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " ∝\n        \n          D\n          \n            −\n            α\n          \n        \n      \n    \n    {\\displaystyle L\\propto D^{-\\alpha }}\n  \n, with \n  \n    \n      \n        α\n        ∈\n        {\n        0.5\n        ,\n        1\n        ,\n        2\n        }\n      \n    \n    {\\displaystyle \\alpha \\in \\{0.5,1,2\\}}\n  \n, the paper found that \n  \n    \n      \n        α\n        ∈\n        [\n        0.07\n        ,\n        0.35\n        ]\n      \n    \n    {\\displaystyle \\alpha \\in [0.07,0.35]}\n  \n.\nOf the factors they varied, only task can change the exponent \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n. Changing the architecture optimizers, regularizers, and loss functions, would only change the proportionality factor, not the exponent. For example, for the same task, one architecture might have \n  \n    \n      \n        L\n        =\n        1000\n        \n          D\n          \n            −\n            0.3\n          \n        \n      \n    \n    {\\displaystyle L=1000D^{-0.3}}\n  \n while another might have \n  \n    \n      \n        L\n        =\n        500\n        \n          D\n          \n            −\n            0.3\n          \n        \n      \n    \n    {\\displaystyle L=500D^{-0.3}}\n  \n. They also found that for a given architecture, the number of parameters necessary to reach lowest levels of loss, given a fixed dataset size, grows like \n  \n    \n      \n        N\n        ∝\n        \n          D\n          \n            β\n          \n        \n      \n    \n    {\\displaystyle N\\propto D^{\\beta }}\n  \n for another exponent \n  \n    \n      \n        β\n      \n    \n    {\\displaystyle \\beta }\n  \n.\nThey studied machine translation with LSTM (\n  \n    \n      \n        α\n        ∼\n        0.13\n      \n    \n    {\\displaystyle \\alpha \\sim 0.13}\n  \n), generative language modelling with LSTM (\n  \n    \n      \n        α\n        ∈\n        [\n        0.06\n        ,\n        0.09\n        ]\n        ,\n        β\n        ≈\n        0.7\n      \n    \n    {\\displaystyle \\alpha \\in [0.06,0.09],\\beta \\approx 0.7}\n  \n), ImageNet classification with ResNet (\n  \n    \n      \n        α\n        ∈\n"}
{"doc_id": "Neural scaling law", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".09\n        ]\n        ,\n        β\n        ≈\n        0.7\n      \n    \n    {\\displaystyle \\alpha \\in [0.06,0.09],\\beta \\approx 0.7}\n  \n), ImageNet classification with ResNet (\n  \n    \n      \n        α\n        ∈\n        [\n        0.3\n        ,\n        0.5\n        ]\n        ,\n        β\n        ≈\n        0.6\n      \n    \n    {\\displaystyle \\alpha \\in [0.3,0.5],\\beta \\approx 0.6}\n  \n), and speech recognition with two hybrid (LSTMs complemented by either CNNs or an attention decoder) architectures (\n  \n    \n      \n        α\n        ≈\n        0.3\n      \n    \n    {\\displaystyle \\alpha \\approx 0.3}\n  \n).\n\n(Henighan, Kaplan, et al, 2020)\nA 2020 analysis  studied statistical relations between \n  \n    \n      \n        C\n        ,\n        N\n        ,\n        D\n        ,\n        L\n      \n    \n    {\\displaystyle C,N,D,L}\n  \n over a wide range of values and found similar scaling laws, over the range of \n  \n    \n      \n        N\n        ∈\n        [\n        \n          10\n          \n            3\n          \n        \n        ,\n        \n          10\n          \n            9\n          \n        \n        ]\n      \n    \n    {\\displaystyle N\\in [10^{3},10^{9}]}\n  \n, \n  \n    \n      \n        C\n        ∈\n        [\n        \n          10\n          \n            12\n          \n        \n        ,\n        \n          10\n          \n            21\n          \n        \n        ]\n      \n    \n    {\\displaystyle C\\in [10^{12},10^{21}]}\n  \n, and over multiple modalities (text, video, image, text to image, etc.).\nIn particular, the scaling laws it found are (Table 1 of ):\n\nFor each modality, they fixed one of the two \n  \n    \n      \n        C\n        ,\n        N\n      \n    \n    {\\displaystyle C,N}\n  \n, and varying the other one (\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is varied along using \n  \n    \n      \n        D\n        =\n        C\n        \n          /\n        \n        6\n        N\n      \n    \n    {\\displaystyle D=C/6N}\n  \n), the achievable test loss satisfies\n  \n    \n      \n        L\n        =\n        \n          L\n"}
{"doc_id": "Neural scaling law", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n      \n    \n    {\\displaystyle D}\n  \n is varied along using \n  \n    \n      \n        D\n        =\n        C\n        \n          /\n        \n        6\n        N\n      \n    \n    {\\displaystyle D=C/6N}\n  \n), the achievable test loss satisfies\n  \n    \n      \n        L\n        =\n        \n          L\n          \n            0\n          \n        \n        +\n        \n          \n            (\n            \n              \n                \n                  x\n                  \n                    0\n                  \n                \n                x\n              \n            \n            )\n          \n          \n            α\n          \n        \n      \n    \n    {\\displaystyle L=L_{0}+\\left({\\frac {x_{0}}{x}}\\right)^{\\alpha }}\n  \nwhere \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n is the varied variable, and \n  \n    \n      \n        \n          L\n          \n            0\n          \n        \n        ,\n        \n          x\n          \n            0\n          \n        \n        ,\n        α\n      \n    \n    {\\displaystyle L_{0},x_{0},\\alpha }\n  \n are parameters to be found by statistical fitting. The parameter \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n is the most important one.\nWhen \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the varied variable, \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n ranges from \n  \n    \n      \n        0.037\n      \n    \n    {\\displaystyle 0.037}\n  \n to \n  \n    \n      \n        0.24\n      \n    \n    {\\displaystyle 0.24}\n  \n depending on the model modality. This corresponds to the \n  \n    \n      \n        α\n        =\n        0.34\n      \n    \n    {\\displaystyle \\alpha =0.34}\n  \n from the Chinchilla scaling paper.\nWhen \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is the varied variable, \n  \n    \n      \n        α\n      \n    \n    {\\displaystyle \\alpha }\n  \n ranges from \n  \n    \n      \n        0.048\n      \n    \n    {\\displaystyle 0.048}\n  \n to \n  \n    \n      \n        0.19\n      \n    \n    {\\displaystyle 0.19}\n  \n depending on the model modality. This corresponds to the \n  \n    \n      \n        β\n        =\n        0.28\n      \n    \n    {\\displaystyle \\beta =0.28}\n  \n from the Chinchilla scaling paper.\nGiven fixed computing budget, optimal model parameter count is consistently around\n  \n    \n      \n        \n          N\n          \n            o\n            p\n           "}
{"doc_id": "Neural scaling law", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to the \n  \n    \n      \n        β\n        =\n        0.28\n      \n    \n    {\\displaystyle \\beta =0.28}\n  \n from the Chinchilla scaling paper.\nGiven fixed computing budget, optimal model parameter count is consistently around\n  \n    \n      \n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        =\n        \n          \n            (\n            \n              \n                C\n                \n                  5\n                  ×\n                  \n                    10\n                    \n                      −\n                      12\n                    \n                  \n                  \n                    petaFLOP-day\n                  \n                \n              \n            \n            )\n          \n          \n            0.7\n          \n        \n        =\n        9.0\n        ×\n        \n          10\n          \n            −\n            7\n          \n        \n        \n          C\n          \n            0.7\n          \n        \n      \n    \n    {\\displaystyle N_{opt}(C)=\\left({\\frac {C}{5\\times 10^{-12}{\\text{petaFLOP-day}}}}\\right)^{0.7}=9.0\\times 10^{-7}C^{0.7}}\n  \nThe parameter \n  \n    \n      \n        9.0\n        ×\n        \n          10\n          \n            −\n            7\n          \n        \n      \n    \n    {\\displaystyle 9.0\\times 10^{-7}}\n  \n varies by a factor of up to 10 for different modalities. The exponent parameter \n  \n    \n      \n        0.7\n      \n    \n    {\\displaystyle 0.7}\n  \n varies from \n  \n    \n      \n        0.64\n      \n    \n    {\\displaystyle 0.64}\n  \n to \n  \n    \n      \n        0.75\n      \n    \n    {\\displaystyle 0.75}\n  \n for different modalities. This exponent corresponds to the \n  \n    \n      \n        ≈\n        0.5\n      \n    \n    {\\displaystyle \\approx 0.5}\n  \n from the Chinchilla scaling paper.\nIt's \"strongly suggested\" (but not statistically checked) that \n  \n    \n      \n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        \n          )\n          \n            0.4\n          \n        \n        ∝\n        \n          C\n          \n            0.28\n          \n        \n      \n    \n    {\\displaystyle D_{opt}(C)\\propto N"}
{"doc_id": "Neural scaling law", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        \n          )\n          \n            0.4\n          \n        \n        ∝\n        \n          C\n          \n            0.28\n          \n        \n      \n    \n    {\\displaystyle D_{opt}(C)\\propto N_{opt}(C)^{0.4}\\propto C^{0.28}}\n  \n. This exponent corresponds to the \n  \n    \n      \n        ≈\n        0.5\n      \n    \n    {\\displaystyle \\approx 0.5}\n  \n from the Chinchilla scaling paper.\nThe scaling law of \n  \n    \n      \n        L\n        =\n        \n          L\n          \n            0\n          \n        \n        +\n        (\n        \n          C\n          \n            0\n          \n        \n        \n          /\n        \n        C\n        \n          )\n          \n            0.048\n          \n        \n      \n    \n    {\\displaystyle L=L_{0}+(C_{0}/C)^{0.048}}\n  \n was confirmed during the training of GPT-3 (Figure 3.1 ).\n\nChinchilla scaling (Hoffmann, et al, 2022)\nOne particular scaling law (\"Chinchilla scaling\") states that, for a large language model (LLM) autoregressively trained for one epoch, with a cosine learning rate schedule, we have:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  C\n                  =\n                  \n                    C\n                    \n                      0\n                    \n                  \n                  N\n                  D\n                \n              \n              \n                \n                  L\n                  =\n                  \n                    \n                      A\n                      \n                        N\n                        \n                          α\n                        \n                      \n                    \n                  \n                  +\n                  \n                    \n                      B\n                      \n                        D\n                        \n                          β\n                        \n                      \n                    \n                  \n                  +\n                  \n                    L\n                    \n                      0\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}C=C_{0}ND\\\\L={\\frac {A}{N^{\\alpha }}}+{\\frac {B}{D^{\\beta }}}+L_{0}\\end{cases}}}\n  \nwhere the variables are\n\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n is the cost of training the model, in FLOPS.\n\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is the number of parameters in the model.\n\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is the number of tokens in the training set.\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the average negative log-likelihood loss per"}
{"doc_id": "Neural scaling law", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " N\n      \n    \n    {\\displaystyle N}\n  \n is the number of parameters in the model.\n\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n is the number of tokens in the training set.\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.\n\n  \n    \n      \n        \n          L\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle L_{0}}\n  \n represents the loss of an ideal generative process on the test data\n\n  \n    \n      \n        \n          \n            A\n            \n              N\n              \n                α\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {A}{N^{\\alpha }}}}\n  \n captures the fact that a Transformer language model with \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n parameters underperforms the ideal generative process\n\n  \n    \n      \n        \n          \n            B\n            \n              D\n              \n                β\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {B}{D^{\\beta }}}}\n  \n captures the fact that the model trained on \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n tokens underperforms the ideal generative process\nand the statistical parameters are\n\n  \n    \n      \n        \n          C\n          \n            0\n          \n        \n        =\n        6\n      \n    \n    {\\displaystyle C_{0}=6}\n  \n, meaning that it costs 6 FLOPs per parameter to train on one token. This is estimated by Kaplan et al. Note that training cost is much higher than inference cost, as training entails both forward and backward passes, whereas inference costs 1 to 2 FLOPs per parameter to infer on one token.\n\n  \n    \n      \n        α\n        =\n        0.34\n        ,\n        β\n        =\n        0.28\n        ,\n        A\n        =\n        406.4\n        ,\n        B\n        =\n        410.7\n        ,\n        \n          L\n          \n            0\n          \n        \n        =\n        1.69\n      \n    \n    {\\displaystyle \\alpha =0.34,\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}\n  \n.\nAlthough Besiroglu et al. claims that the statistical estimation is slightly off, and should be \n  \n    \n      \n        α\n        =\n        0.35\n        ,\n        β\n        =\n        0"}
{"doc_id": "Neural scaling law", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ",A=406.4,B=410.7,L_{0}=1.69}\n  \n.\nAlthough Besiroglu et al. claims that the statistical estimation is slightly off, and should be \n  \n    \n      \n        α\n        =\n        0.35\n        ,\n        β\n        =\n        0.37\n        ,\n        A\n        =\n        482.01\n        ,\n        B\n        =\n        2085.43\n        ,\n        \n          L\n          \n            0\n          \n        \n        =\n        1.82\n      \n    \n    {\\displaystyle \\alpha =0.35,\\beta =0.37,A=482.01,B=2085.43,L_{0}=1.82}\n  \n.\nThe statistical laws were fitted over experimental data with \n  \n    \n      \n        N\n        ∈\n        [\n        7\n        ×\n        \n          10\n          \n            7\n          \n        \n        ,\n        1.6\n        ×\n        \n          10\n          \n            10\n          \n        \n        ]\n        ,\n        D\n        ∈\n        [\n        5\n        ×\n        \n          10\n          \n            9\n          \n        \n        ,\n        5\n        ×\n        \n          10\n          \n            11\n          \n        \n        ]\n        ,\n        C\n        ∈\n        [\n        \n          10\n          \n            18\n          \n        \n        ,\n        \n          10\n          \n            24\n          \n        \n        ]\n      \n    \n    {\\displaystyle N\\in [7\\times 10^{7},1.6\\times 10^{10}],D\\in [5\\times 10^{9},5\\times 10^{11}],C\\in [10^{18},10^{24}]}\n  \n.\nSince there are 4 variables related by 2 equations, imposing 1 additional constraint and 1 additional optimization objective allows us to solve for all four variables. In particular, for any fixed \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, we can uniquely solve for all 4 variables that minimizes \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n. This provides us with the optimal \n  \n    \n      \n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ,\n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n      \n    \n    {\\display"}
{"doc_id": "Neural scaling law", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "  \n. This provides us with the optimal \n  \n    \n      \n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ,\n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n      \n    \n    {\\displaystyle D_{opt}(C),N_{opt}(C)}\n  \n for any fixed \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n:\n  \n    \n      \n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        =\n        G\n        \n          \n            (\n            \n              \n                C\n                6\n              \n            \n            )\n          \n          \n            a\n          \n        \n        ,\n        \n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        =\n        \n          G\n          \n            −\n            1\n          \n        \n        \n          \n            (\n            \n              \n                C\n                6\n              \n            \n            )\n          \n          \n            b\n          \n        \n        ,\n        \n        \n           where \n        \n        \n        G\n        =\n        \n          \n            (\n            \n              \n                \n                  α\n                  A\n                \n                \n                  β\n                  B\n                \n              \n            \n            )\n          \n          \n            \n              1\n              \n                α\n                +\n                β\n              \n            \n          \n        \n        ,\n        \n        a\n        =\n        \n          \n            β\n            \n              α\n              +\n              β\n            \n          \n        \n        \n          , and \n        \n        b\n        =\n        \n          \n            α\n            \n              α\n              +\n              β\n            \n          \n        \n        \n          . \n        \n      \n    \n    {\\displaystyle N_{opt}(C)=G\\left({\\frac {C}{6}}\\right)^{a},\\quad D_{opt}(C)=G^{-1}\\left({\\frac {C}{6}}\\right)^{b},\\quad {\\text{ where }}\\quad G=\\left({\\frac {\\alpha A}{\\beta B}}\\right)^{\\frac {1}{\\alpha +\\beta }},\\quad a={\\frac {\\beta }{\\alpha +\\beta }}{\\text{, and }}b={\\frac {\\alpha }{\\alpha +\\beta }}{\\text{. }}}\n  \nPlugging in the numerical values, we obtain the \"Chinchilla efficient\" model size and training dataset size, as well as the test loss achievable:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    N\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C"}
{"doc_id": "Neural scaling law", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "text{. }}}\n  \nPlugging in the numerical values, we obtain the \"Chinchilla efficient\" model size and training dataset size, as well as the test loss achievable:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    N\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C\n                  )\n                  =\n                  0.6\n                  \n                  \n                    C\n                    \n                      0.45\n                    \n                  \n                \n              \n              \n                \n                  \n                    D\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C\n                  )\n                  =\n                  0.3\n                  \n                  \n                    C\n                    \n                      0.55\n                    \n                  \n                \n              \n              \n                \n                  \n                    L\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C\n                  )\n                  =\n                  1070\n                  \n                  \n                    C\n                    \n                      −\n                      0.154\n                    \n                  \n                  +\n                  1.7\n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}N_{opt}(C)=0.6\\;C^{0.45}\\\\D_{opt}(C)=0.3\\;C^{0.55}\\\\L_{opt}(C)=1070\\;C^{-0.154}+1.7\\end{cases}}}\n  \nSimilarly, we may find the optimal training dataset size and training compute budget for any fixed model parameter size, and so on.\nThere are other estimates for \"Chinchilla efficient\" model size and training dataset size. The above is based on a statistical model of \n  \n    \n      \n        L\n        =\n        \n          \n            A\n            \n              N\n              \n                α\n              \n            \n          \n        \n        +\n        \n          \n            B\n            \n              D\n              \n                β\n              \n            \n          \n        \n        +\n        \n          L\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle L={\\frac {A}{N^{\\alpha }}}+{\\frac {B}{D^{\\beta }}}+L_{0}}\n  \n. One can also directly fit a statistical law for \n  \n    \n      \n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ,\n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n      \n    \n    {\\displaystyle D_{opt}(C),N_{opt}(C)}\n  \n without going through the detour, for which one obtains:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    N\n                    \n                     "}
{"doc_id": "Neural scaling law", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n      \n    \n    {\\displaystyle D_{opt}(C),N_{opt}(C)}\n  \n without going through the detour, for which one obtains:\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  \n                    N\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C\n                  )\n                  =\n                  0.1\n                  \n                  \n                    C\n                    \n                      0.5\n                    \n                  \n                \n              \n              \n                \n                  \n                    D\n                    \n                      o\n                      p\n                      t\n                    \n                  \n                  (\n                  C\n                  )\n                  =\n                  1.7\n                  \n                  \n                    C\n                    \n                      0.5\n                    \n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}N_{opt}(C)=0.1\\;C^{0.5}\\\\D_{opt}(C)=1.7\\;C^{0.5}\\end{cases}}}\n  \nor as tabulated:\n\nDiscrepancy\nThe Chinchilla scaling law analysis for training transformer language models suggests that for a given training compute budget (\n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n), to achieve the minimal pretraining loss for that budget, the number of model parameters (\n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n) and the number of training tokens (\n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n) should be scaled in equal proportions, \n  \n    \n      \n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          C\n          \n            0.5\n          \n        \n        ,\n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          C\n          \n            0.5\n          \n        \n      \n    \n    {\\displaystyle N_{opt}(C)\\propto C^{0.5},D_{opt}(C)\\propto C^{0.5}}\n  \n. \nThis conclusion differs from analysis conducted by Kaplan et al., which found that \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n should be increased more quickly than \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n, \n  \n    \n      \n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          C\n          \n"}
{"doc_id": "Neural scaling law", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "    \n    {\\displaystyle N}\n  \n should be increased more quickly than \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n, \n  \n    \n      \n        \n          N\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          C\n          \n            0.73\n          \n        \n        ,\n        \n          D\n          \n            o\n            p\n            t\n          \n        \n        (\n        C\n        )\n        ∝\n        \n          C\n          \n            0.27\n          \n        \n      \n    \n    {\\displaystyle N_{opt}(C)\\propto C^{0.73},D_{opt}(C)\\propto C^{0.27}}\n  \n.\nThis discrepancy can primarily be attributed to the two studies using different methods for measuring model size. Kaplan et al.:\n\ndid not count the parameters in the token embedding layer, which when analyzed at smaller model sizes leads to biased coefficients;\nstudied smaller models than the Chinchilla group, magnifying the effect;\nassumed that \n  \n    \n      \n        \n          L\n          \n            ∞\n          \n        \n        =\n        0\n      \n    \n    {\\displaystyle L_{\\infty }=0}\n  \n.\nSecondary effects also arise due to differences in hyperparameter tuning and learning rate schedules. Kaplan et al.:\n\nused a warmup schedule that was too long for smaller models, making them appear less efficient;\ndid not fully tuning optimization hyperparameters.\n\nBeyond Chinchilla scaling\nAs Chinchilla scaling has been the reference point for many large-scaling training runs, there had been a concurrent effort to go \"beyond Chinchilla scaling\", meaning to modify some of the training pipeline in order to obtain the same loss with less effort, or deliberately train for longer than what is \"Chinchilla optimal\".\nUsually, the goal is to make the scaling law exponent larger, which means the same loss can be trained for much less compute. For instance, filtering data can make the scaling law exponent larger.\nAnother strand of research studies how to deal with limited data, as according to Chinchilla scaling laws, the training dataset size for the largest language models already approaches what is available on the internet. found that augmenting the dataset with a mix of \"denoising objectives\" constructed from the dataset improves performance. studies optimal scaling when all available data is already exhausted (such as in rare languages), so one must train multiple epoches over the same dataset (whereas Chinchilla scaling requires only one epoch"}
{"doc_id": "Neural scaling law", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " internet. found that augmenting the dataset with a mix of \"denoising objectives\" constructed from the dataset improves performance. studies optimal scaling when all available data is already exhausted (such as in rare languages), so one must train multiple epoches over the same dataset (whereas Chinchilla scaling requires only one epoch). The Phi series of small language models were trained on textbook-like data generated by large language models, for which data is only limited by amount of compute available.\nChinchilla optimality was defined as \"optimal for training compute\", whereas in actual production-quality models, there will be a lot of inference after training is complete. \"Overtraining\" during training means better performance during inference. LLaMA models were overtrained for this reason. Subsequent studies discovered scaling laws in the overtraining regime, for dataset sizes up to 32x more than Chinchilla-optimal.\n\nBroken neural scaling laws (BNSL)\nA 2022 analysis found that many scaling behaviors of artificial neural networks follow a smoothly broken power law functional form:\n\n  \n    \n      \n        y\n        =\n        a\n        +\n        \n          \n            (\n          \n        \n        b\n        \n          x\n          \n            −\n            \n              c\n              \n                0\n              \n            \n          \n        \n        \n          \n            )\n          \n        \n        \n          ∏\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          \n            (\n            \n              1\n              +\n              \n                \n                  (\n                  \n                    \n                      x\n                      \n                        d\n                        \n                          i\n                        \n                      \n                    \n                  \n                  )\n                \n                \n                  1\n                  \n                    /\n                  \n                  \n                    f\n                    \n                      i\n                    \n                  \n                \n              \n            \n            )\n          \n          \n            −\n            \n              c\n              \n                i\n              \n            \n            ∗\n            \n              f\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle y=a+{\\bigg (}bx^{-c_{0}}{\\bigg )}\\prod _{i=1}^{n}\\left(1+\\left({\\frac {x}{d_{i}}}\\right)^{1/f_{i}}\\right)^{-c_{i}*f_{i}}}\n  \n\nin which \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n refers to the quantity being scaled (i.e. \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n, number of training steps"}
{"doc_id": "Neural scaling law", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "displaystyle x}\n  \n refers to the quantity being scaled (i.e. \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n, \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n, number of training steps, number of inference steps, or model input size) and \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n refers to the downstream (or upstream) performance evaluation metric of interest (e.g. prediction error, cross entropy, calibration error, AUROC, BLEU score percentage, F1 score, reward, Elo rating, solve rate, or FID score) in zero-shot, prompted, or fine-tuned settings. The parameters \n  \n    \n      \n        a\n        ,\n        b\n        ,\n        \n          c\n          \n            0\n          \n        \n        ,\n        \n          c\n          \n            1\n          \n        \n        .\n        .\n        .\n        \n          c\n          \n            n\n          \n        \n        ,\n        \n          d\n          \n            1\n          \n        \n        .\n        .\n        .\n        \n          d\n          \n            n\n          \n        \n        ,\n        \n          f\n          \n            1\n          \n        \n        .\n        .\n        .\n        \n          f\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle a,b,c_{0},c_{1}...c_{n},d_{1}...d_{n},f_{1}...f_{n}}\n  \n are found by statistical fitting.\nOn a log–log plot, when \n  \n    \n      \n        \n          f\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle f_{i}}\n  \n is not too large and \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n is subtracted out from the y-axis, this functional form looks like a series of linear segments connected by arcs; the \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  \n transitions between the segments are called \"breaks\", hence the name broken neural scaling laws (BNSL).\nThe scenarios in which the scaling behaviors of artificial neural networks were found to follow this functional form include large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, sparsity,"}
{"doc_id": "Neural scaling law", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems, arithmetic, emergent abilities, double descent, supervised learning, unsupervised/self-supervised learning, and reinforcement learning (single agent and multi-agent).\nThe architectures for which the scaling behaviors of artificial neural networks were found to follow this functional form include residual neural networks, transformers, MLPs, MLP-mixers, recurrent neural networks, convolutional neural networks, graph neural networks, U-nets, encoder-decoder (and encoder-only) (and decoder-only) models, ensembles (and non-ensembles), MoE (mixture of experts) (and non-MoE) models, and sparse pruned (and non-sparse unpruned) models.\n\nInference scaling\nOther than scaling up training compute, one can also scale up inference compute (or \"test-time compute\"). As an example, the Elo rating of AlphaGo improves steadily as it is allowed to spend more time on its Monte Carlo Tree Search per play. For AlphaGo Zero, increasing Elo by 120 requires either 2x model size and training, or 2x test-time search. Similarly, a language model for solving competition-level coding challenges, AlphaCode, consistently improved (log-linearly) in performance with more search time.\nFor Hex, 10x training-time compute trades for 15x test-time compute. For Libratus for heads up no-limit Texas hold 'em, and Cicero for Diplomacy, and many other abstract games of partial information, inference-time searching improves performance at a similar tradeoff ratio, for up to 100,000x effective increase in training-time compute.\nIn 2024, the OpenAI o1 report documented that o1's performance consistently improved with both increased train-time compute and test-time compute, and gave numerous examples of test-time compute scaling in mathematics, scientific reasoning, and coding tasks.\nOne method for scaling up test-time compute is process-based supervision, where a model generates a step-by-step reasoning chain to answer a question, and another model (either human or AI) provides a reward score on some of the intermediate steps, not just the final answer. Process"}
{"doc_id": "Neural scaling law", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " mathematics, scientific reasoning, and coding tasks.\nOne method for scaling up test-time compute is process-based supervision, where a model generates a step-by-step reasoning chain to answer a question, and another model (either human or AI) provides a reward score on some of the intermediate steps, not just the final answer. Process-based supervision can be scaled arbitrarily by using synthetic reward score without another model, for example, by running Monte Carlo rollouts and scoring each step in the reasoning according to how likely it leads to the right answer. Another method is by revision models, which are models trained to solve a problem multiple times, each time revising the previous attempt.\n\nOther examples\nVision transformers\nVision transformers, similar to language transformers, exhibit scaling laws. A 2022 research trained vision transformers, with parameter counts \n  \n    \n      \n        N\n        ∈\n        [\n        5\n        ×\n        \n          10\n          \n            6\n          \n        \n        ,\n        2\n        ×\n        \n          10\n          \n            9\n          \n        \n        ]\n      \n    \n    {\\displaystyle N\\in [5\\times 10^{6},2\\times 10^{9}]}\n  \n, on image sets of sizes \n  \n    \n      \n        D\n        ∈\n        [\n        3\n        ×\n        \n          10\n          \n            7\n          \n        \n        ,\n        3\n        ×\n        \n          10\n          \n            9\n          \n        \n        ]\n      \n    \n    {\\displaystyle D\\in [3\\times 10^{7},3\\times 10^{9}]}\n  \n, for computing \n  \n    \n      \n        C\n        ∈\n        [\n        0.2\n        ,\n        \n          10\n          \n            4\n          \n        \n        ]\n      \n    \n    {\\displaystyle C\\in [0.2,10^{4}]}\n  \n (in units of TPUv3-core-days).\nAfter training the model, it is finetuned on ImageNet training set. Let \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n be the error probability of the finetuned model classifying ImageNet test set. They found \n  \n    \n      \n        \n          min\n          \n            N\n            ,\n            D\n          \n        \n        L\n        =\n        0.09\n        +\n        \n          \n            0.26\n            \n              (\n              C\n              +\n              0.01\n              \n                )\n                \n                  0.35\n                \n              \n            \n          "}
{"doc_id": "Neural scaling law", "chunk_id": 17, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "    \n      \n        \n          min\n          \n            N\n            ,\n            D\n          \n        \n        L\n        =\n        0.09\n        +\n        \n          \n            0.26\n            \n              (\n              C\n              +\n              0.01\n              \n                )\n                \n                  0.35\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\min _{N,D}L=0.09+{\\frac {0.26}{(C+0.01)^{0.35}}}}\n  \n.\n\nNeural machine translation\nGhorbani, Behrooz et al. studied scaling laws for neural machine translation (specifically, English as source, and German as target) in encoder-decoder Transformer models, trained until convergence on the same datasets (thus they did not fit scaling laws for computing cost \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  \n or dataset size \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  \n). They varied \n  \n    \n      \n        N\n        ∈\n        [\n        \n          10\n          \n            8\n          \n        \n        ,\n        3.5\n        ×\n        \n          10\n          \n            9\n          \n        \n        ]\n      \n    \n    {\\displaystyle N\\in [10^{8},3.5\\times 10^{9}]}\n  \n They found three results:\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n is a scaling law function of \n  \n    \n      \n        \n          N\n          \n            E\n          \n        \n        ,\n        \n          N\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle N_{E},N_{D}}\n  \n, where \n  \n    \n      \n        \n          N\n          \n            E\n          \n        \n        ,\n        \n          N\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle N_{E},N_{D}}\n  \n are encoder and decoder parameter count. It is not simply a function of total parameter count \n  \n    \n      \n        N\n        =\n        \n          N\n          \n            E\n          \n        \n        +\n        \n          N\n          \n            D\n          \n        \n      \n    \n    {\\displaystyle N=N_{E}+N_{D}}\n  \n. The function has form \n  \n    \n      \n        L\n        \n          (\n          \n            \n              N\n              \n                e\n              \n            \n            ,\n            \n              N\n              \n                d\n              \n            \n          \n          )\n        \n        =\n        α\n        \n          \n            (\n            \n              \n                \n                  \n                    \n                      \n                        N\n                        ¯\n                      \n                    \n                  \n                  \n                    e"}
{"doc_id": "Neural scaling law", "chunk_id": 18, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " has form \n  \n    \n      \n        L\n        \n          (\n          \n            \n              N\n              \n                e\n              \n            \n            ,\n            \n              N\n              \n                d\n              \n            \n          \n          )\n        \n        =\n        α\n        \n          \n            (\n            \n              \n                \n                  \n                    \n                      \n                        N\n                        ¯\n                      \n                    \n                  \n                  \n                    e\n                  \n                \n                \n                  N\n                  \n                    e\n                  \n                \n              \n            \n            )\n          \n          \n            \n              p\n              \n                e\n              \n            \n          \n        \n        \n          \n            (\n            \n              \n                \n                  \n                    \n                      \n                        N\n                        ¯\n                      \n                    \n                  \n                  \n                    d\n                  \n                \n                \n                  N\n                  \n                    d\n                  \n                \n              \n            \n            )\n          \n          \n            \n              p\n              \n                d\n              \n            \n          \n        \n        +\n        \n          L\n          \n            ∞\n          \n        \n      \n    \n    {\\displaystyle L\\left(N_{e},N_{d}\\right)=\\alpha \\left({\\frac {{\\bar {N}}_{e}}{N_{e}}}\\right)^{p_{e}}\\left({\\frac {{\\bar {N}}_{d}}{N_{d}}}\\right)^{p_{d}}+L_{\\infty }}\n  \n, where \n  \n    \n      \n        α\n        ,\n        \n          p\n          \n            e\n          \n        \n        ,\n        \n          p\n          \n            d\n          \n        \n        ,\n        \n          L\n          \n            ∞\n          \n        \n        ,\n        \n          \n            \n              \n                N\n                ¯\n              \n            \n          \n          \n            e\n          \n        \n        ,\n        \n          \n            \n              \n                N\n                ¯\n              \n            \n          \n          \n            d\n          \n        \n      \n    \n    {\\displaystyle \\alpha ,p_{e},p_{d},L_{\\infty },{\\bar {N}}_{e},{\\bar {N}}_{d}}\n  \n are fitted parameters. They found that \n  \n    \n      \n        \n          N\n          \n            d\n          \n        \n        \n          /\n        \n        N\n        ≈\n        0.55\n      \n    \n    {\\displaystyle N_{d}/N\\approx 0.55}\n  \n minimizes loss if \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n is held fixed.\n\n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n \"saturates\" (that is, it reaches \n  \n    \n      \n        \n          L\n          \n            ∞\n          \n        \n      \n    \n    {\\displaystyle L_{\\infty }}\n  \n) for smaller models when the training and testing datasets are \"source-n"}
{"doc_id": "Neural scaling law", "chunk_id": 19, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "      \n        L\n      \n    \n    {\\displaystyle L}\n  \n \"saturates\" (that is, it reaches \n  \n    \n      \n        \n          L\n          \n            ∞\n          \n        \n      \n    \n    {\\displaystyle L_{\\infty }}\n  \n) for smaller models when the training and testing datasets are \"source-natural\" than \"target-natural\". A \"source-natural\" data point means a pair of English-German sentences, and the model is asked to translate the English sentence into German, and the English sentence is written by a natural English writer, while the German sentence is translated from the English sentence by a machine translator. To construct the two kinds of datasets, the authors collected natural English and German sentences online, then used machine translation to generate their translations.\nAs models grow larger, models trained on source-original datasets can achieve low loss but bad BLEU score. In contrast, models trained on target-original datasets achieve low loss and good BLEU score in tandem (Figure 10, 11 ).\nThe authors hypothesize that source-natural datasets have uniform and dull target sentences, and so a model that is trained to predict the target sentences would quickly overfit.\n trained Transformers for machine translations with sizes \n  \n    \n      \n        N\n        ∈\n        [\n        4\n        ×\n        \n          10\n          \n            5\n          \n        \n        ,\n        5.6\n        ×\n        \n          10\n          \n            7\n          \n        \n        ]\n      \n    \n    {\\displaystyle N\\in [4\\times 10^{5},5.6\\times 10^{7}]}\n  \n on dataset sizes \n  \n    \n      \n        D\n        ∈\n        [\n        6\n        ×\n        \n          10\n          \n            5\n          \n        \n        ,\n        6\n        ×\n        \n          10\n          \n            9\n          \n        \n        ]\n      \n    \n    {\\displaystyle D\\in [6\\times 10^{5},6\\times 10^{9}]}\n  \n. They found the Kaplan et al. (2020) scaling law applied to machine translation: \n  \n    \n      \n        L\n        (\n        N\n        ,\n        D\n        )\n        =\n        \n          \n            [\n            \n              \n                \n                  (\n                  \n                    \n                      \n                        N\n                        \n                          C\n                        \n                      \n                      N\n                    \n                  \n                  )\n                \n                \n                  \n                    \n                      α\n                      \n                        N\n                      \n                    \n                    \n                      α\n                      \n                        D\n                      \n                    \n                  \n                \n              \n              +\n              \n                \n                  \n                    D\n                    \n                      C\n                    \n"}
{"doc_id": "Neural scaling law", "chunk_id": 20, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        )\n        =\n        \n          \n            [\n            \n              \n                \n                  (\n                  \n                    \n                      \n                        N\n                        \n                          C\n                        \n                      \n                      N\n                    \n                  \n                  )\n                \n                \n                  \n                    \n                      α\n                      \n                        N\n                      \n                    \n                    \n                      α\n                      \n                        D\n                      \n                    \n                  \n                \n              \n              +\n              \n                \n                  \n                    D\n                    \n                      C\n                    \n                  \n                  D\n                \n              \n            \n            ]\n          \n          \n            \n              α\n              \n                D\n              \n            \n          \n        \n      \n    \n    {\\displaystyle L(N,D)=\\left[\\left({\\frac {N_{C}}{N}}\\right)^{\\frac {\\alpha _{N}}{\\alpha _{D}}}+{\\frac {D_{C}}{D}}\\right]^{\\alpha _{D}}}\n  \n. They also found the BLEU score scaling as \n  \n    \n      \n        B\n        L\n        E\n        U\n        ≈\n        C\n        \n          e\n          \n            −\n            k\n            L\n          \n        \n      \n    \n    {\\displaystyle BLEU\\approx Ce^{-kL}}\n  \n.\n\nTransfer learning\nHernandez, Danny et al. studied scaling laws for transfer learning in language models. They trained a family of Transformers in three ways:\n\npretraining on English, finetuning on Python\npretraining on an equal mix of English and Python, finetuning on Python\ntraining on Python\nThe idea is that pretraining on English should help the model achieve low loss on a test set of Python text. Suppose the model has parameter count \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n, and after being finetuned on \n  \n    \n      \n        \n          D\n          \n            F\n          \n        \n      \n    \n    {\\displaystyle D_{F}}\n  \n Python tokens, it achieves some loss \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n. We say that its \"transferred token count\" is \n  \n    \n      \n        \n          D\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle D_{T}}\n  \n, if another model with the same \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n achieves the same \n  \n    \n      \n        L\n      \n    \n    {\\displaystyle L}\n  \n after training on \n  \n    \n      \n        \n          D\n          \n            F\n          \n        \n        +\n        \n          D\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle D_{F}+D_{T}}\n  \n Python tokens.\nThey found \n  \n    \n      \n        \n          D\n          \n            T\n          \n        \n        =\n       "}
{"doc_id": "Neural scaling law", "chunk_id": 21, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " training on \n  \n    \n      \n        \n          D\n          \n            F\n          \n        \n        +\n        \n          D\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle D_{F}+D_{T}}\n  \n Python tokens.\nThey found \n  \n    \n      \n        \n          D\n          \n            T\n          \n        \n        =\n        1.9\n        e\n        4\n        \n          \n            (\n            \n              D\n              \n                F\n              \n            \n            )\n          \n          \n            .18\n          \n        \n        (\n        N\n        \n          )\n          \n            .38\n          \n        \n      \n    \n    {\\displaystyle D_{T}=1.9e4\\left(D_{F}\\right)^{.18}(N)^{.38}}\n  \n for pretraining on English text, and \n  \n    \n      \n        \n          D\n          \n            T\n          \n        \n        =\n        2.1\n        e\n        5\n        \n          \n            (\n            \n              D\n              \n                F\n              \n            \n            )\n          \n          \n            .096\n          \n        \n        (\n        N\n        \n          )\n          \n            .38\n          \n        \n      \n    \n    {\\displaystyle D_{T}=2.1e5\\left(D_{F}\\right)^{.096}(N)^{.38}}\n  \n for pretraining on English and non-Python code.\n\nPrecision\nKumar et al. study scaling laws for numerical precision in the training of language models. They train a family of language models with weights, activations, and KV cache in varying numerical precision in both integer and floating-point type to measure the effects on loss as a function of precision. For training, their scaling law accounts for lower precision by wrapping the effects of precision into an overall \"effective parameter count\" that governs loss scaling, using the parameterization \n  \n    \n      \n        N\n        ↦\n        \n          N\n          \n            eff\n          \n        \n        (\n        P\n        )\n        =\n        N\n        (\n        1\n        −\n        \n          e\n          \n            −\n            P\n            \n              /\n            \n            γ\n          \n        \n        )\n      \n    \n    {\\displaystyle N\\mapsto N_{\\text{eff}}(P)=N(1-e^{-P/\\gamma })}\n  \n. This illustrates how training in lower precision degrades performance by reducing the true capacity of the model in a manner that varies exponentially with bits.\nFor inference, they find that extreme overtraining of language models past Chinchilla-optimality can lead to models being more sensitive to quantization, a standard technique for efficient"}
{"doc_id": "Neural scaling law", "chunk_id": 22, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "  \n. This illustrates how training in lower precision degrades performance by reducing the true capacity of the model in a manner that varies exponentially with bits.\nFor inference, they find that extreme overtraining of language models past Chinchilla-optimality can lead to models being more sensitive to quantization, a standard technique for efficient deep learning. This is demonstrated by observing that the degradation in loss due to weight quantization increases as an approximate power law in the token/parameter ratio \n  \n    \n      \n        D\n        \n          /\n        \n        N\n      \n    \n    {\\displaystyle D/N}\n  \n seen during pretraining, so that models pretrained on extreme token budgets can perform worse in terms of validation loss than those trained on more modest token budgets if post-training quantization is applied. Other work examining the effects of overtraining include Sardana et al. and Gadre et al.\n\nDensing laws\nXiao et al. considered the parameter efficiency (\"density\") of models over time. The idea is that over time, researchers would discover models that use their parameters more efficiently, in that models with the same performance can have fewer parameters.\nA model can have an actual parameter count \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n, defined as the actual number of parameters in the model, and an \"effective\" parameter count \n  \n    \n      \n        \n          \n            \n              N\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {N}}}\n  \n, defined as how many parameters it would have taken a previous well-known model to reach he same performance on some benchmarks, such as MMLU. \n  \n    \n      \n        \n          \n            \n              N\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {N}}}\n  \n is not measured directly, but rather by measuring the actual model performance \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n, then plugging it back to a previously fitted scaling law, such as the Chinchilla scaling law, to obtain what \n  \n    \n      \n        \n          \n            \n              N\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {N}}}\n  \n would be required to reach that performance \n  \n    \n      \n        S\n      \n    \n    {\\displaystyle S}\n  \n, according to that previously fitted scaling laws.\nA densing law states that \n  \n    \n      \n        ln\n        ⁡\n        \n          \n            (\n            \n              \n                \n                  \n                    N\n                    ^\n                  \n                \n                N\n              \n            \n            )\n          \n          \n            m\n            a\n            x\n          \n        \n        =\n       "}
{"doc_id": "Neural scaling law", "chunk_id": 23, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "  \n, according to that previously fitted scaling laws.\nA densing law states that \n  \n    \n      \n        ln\n        ⁡\n        \n          \n            (\n            \n              \n                \n                  \n                    N\n                    ^\n                  \n                \n                N\n              \n            \n            )\n          \n          \n            m\n            a\n            x\n          \n        \n        =\n        A\n        t\n        +\n        B\n      \n    \n    {\\displaystyle \\ln \\left({\\frac {\\hat {N}}{N}}\\right)_{max}=At+B}\n  \n, where \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n is real-world time, measured in days.\n\nSee also"}
{"doc_id": "Neuro-symbolic AI", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Neuro-symbolic AI is a type of artificial intelligence that integrates neural and symbolic AI architectures to address the weaknesses of each, providing a robust AI capable of reasoning, learning, and cognitive modeling. As argued by Leslie Valiant and others, the effective construction of rich computational cognitive models demands the combination of symbolic reasoning and efficient machine learning. \nGary Marcus argued, \"We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.\" Further, \"To build a robust, knowledge-driven approach to AI we must have the machinery of symbol manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only known machinery that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.\"\nAngelo Dalli, Henry Kautz, Francesca Rossi, and Bart Selman also argued for such a synthesis. Their arguments attempt to address the two kinds of thinking, as discussed in Daniel Kahneman's book Thinking, Fast and Slow. It describes cognition as encompassing two components: System 1 is fast, reflexive, intuitive, and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is used for pattern recognition. System 2 handles planning, deduction, and deliberative thinking. In this view, deep learning best handles the first kind of cognition, while symbolic reasoning best handles the second kind. Both are necessary for the development of a robust and reliable AI system capable of learning, reasoning, and interacting with humans to accept advice and answer questions. Since the 1990s, dual-process models with explicit references to the two contrasting systems have been the focus of research in both the fields of AI and cognitive science by numerous researchers.\nIn 2025, the adoption of neurosymbolic AI, an approach that integrates neural networks with symbolic reasoning, increased in response to the need to address hallucination issues in large language models. For example, Amazon implemented Neurosymbolic AI in its Vulcan warehouse robots and Rufus shopping assistant to enhance accuracy and decision-making.\n\nApproaches\nApproaches for integration are diverse. Henry Kautz's taxonomy of neuro-symbolic architectures follows, along with some examples:\n\nSymbolic Neural symbolic is the current approach of many neural models in natural language processing, where words or subword tokens are the ultimate input and output of large language models. Examples include BERT, RoBERTa, and GPT-3.\nSymbolic["}
{"doc_id": "Neuro-symbolic AI", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of neuro-symbolic architectures follows, along with some examples:\n\nSymbolic Neural symbolic is the current approach of many neural models in natural language processing, where words or subword tokens are the ultimate input and output of large language models. Examples include BERT, RoBERTa, and GPT-3.\nSymbolic[Neural] is exemplified by AlphaGo, where symbolic techniques are used to invoke neural techniques. In this case, the symbolic approach is Monte Carlo tree search and the neural techniques learn how to evaluate game positions.\nNeural | Symbolic uses a neural architecture to interpret perceptual data as symbols and relationships that are reasoned about symbolically. Neural-Concept Learner is an example.\nNeural: Symbolic → Neural relies on symbolic reasoning to generate or label training data that is subsequently learned by a deep learning model, e.g., to train a neural model for symbolic computation by using a Macsyma-like symbolic mathematics system to create or label examples.\nNeuralSymbolic uses a neural net that is generated from symbolic rules. An example is the Neural Theorem Prover, which constructs a neural network from an AND-OR proof tree generated from knowledge base rules and terms. Logic Tensor Networks also fall into this category.\nNeural[Symbolic]  according to Kautz, this approach embeds true symbolic reasoning inside a neural network. These are tightly-coupled neural-symbolic systems, in which the logical inference rules are internal to the neural network. This way, the neural network internally computes the inference from the premises and learns to reason based on logical inference systems. Early work on connectionist modal and temporal logics by Garcez, Lamb, and Gabbay  is aligned with this approach.\nThese categories are not exhaustive, as they do not consider multi-agent systems. In 2005, Bader and Hitzler presented a more fine-grained categorization that took into account, e.g., whether the use of symbols included logic and, if so, whether the logic was propositional or first-order logic. The 2005 categorization and Kautz's taxonomy above are compared and contrasted in a 2021 article. Sepp Hochreiter argued that Graph Neural Networks \"...are the predominant models of neural-symbolic computing\" since \"[t]hey describe the properties of molecules, simulate social networks, or predict future states in physical and engineering applications with particle-particle interactions.\"\n\nArtificial general intelligence\nGary Marcus argues that \"...hybrid architectures that combine learning and symbol manipulation are necessary for"}
{"doc_id": "Neuro-symbolic AI", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Networks \"...are the predominant models of neural-symbolic computing\" since \"[t]hey describe the properties of molecules, simulate social networks, or predict future states in physical and engineering applications with particle-particle interactions.\"\n\nArtificial general intelligence\nGary Marcus argues that \"...hybrid architectures that combine learning and symbol manipulation are necessary for robust intelligence, but not sufficient\", and that there are\n\n...four cognitive prerequisites for building robust artificial intelligence: \nhybrid architectures that combine large-scale learning with the representational and computational powers of symbol manipulation,\nlarge-scale knowledge bases—likely leveraging innate frameworks—that incorporate symbolic knowledge along with other forms of knowledge,\nreasoning mechanisms capable of leveraging those knowledge bases in tractable ways, and\nrich cognitive models that work together with those mechanisms and knowledge bases.\nThis echoes earlier calls for hybrid models as early as the 1990s.\n\nHistory\nGarcez and Lamb described research in this area as ongoing, at least since the 1990s. During that period, the terms symbolic and sub-symbolic AI were popular.\nA series of workshops on neuro-symbolic AI has been held annually since 2005 Neuro-Symbolic Artificial Intelligence. In the early 1990s, an initial set of workshops on this topic were organized.\n\nResearch\nKey research questions remain, such as:\n\nWhat is the best way to integrate neural and symbolic architectures?\nHow should symbolic structures be represented within neural networks and extracted from them?\nHow should common-sense knowledge be learned and reasoned about?\nHow can abstract knowledge that is hard to encode logically be handled?\n\nImplementations\nImplementations of neuro-symbolic approaches include:\n\nAllegroGraph: an integrated Knowledge Graph based platform for neuro-symbolic application development.\nScallop: a language based on Datalog that supports differentiable logical and relational reasoning. Scallop can be integrated in Python and with a PyTorch learning module.\nLogic Tensor Networks: encode logical formulas as neural networks and simultaneously learn term encodings, term weights, and formula weights.\nDeepProbLog: combines neural networks with the probabilistic reasoning of ProbLog.\nAbductive Learning: integrates machine learning and logical reasoning in a balanced-loop via abductive reasoning, enabling them to work together in a mutually beneficial way.\nSymbolicAI: a compositional differentiable programming library.\nExplainable Neural Networks (XNNs): combine neural networks with symbolic hypergraphs and trained using a mixture of backpropagation and symbolic learning called induction.\n\nSee also\nSymbolic AI\nConnectionist AI\nHybrid intelligent systems\n\nCitations"}
{"doc_id": "Neuro-symbolic AI", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".\nSymbolicAI: a compositional differentiable programming library.\nExplainable Neural Networks (XNNs): combine neural networks with symbolic hypergraphs and trained using a mixture of backpropagation and symbolic learning called induction.\n\nSee also\nSymbolic AI\nConnectionist AI\nHybrid intelligent systems\n\nCitations"}
{"doc_id": "Neurorobotics", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Neurorobotics is the combined study of neuroscience, robotics, and artificial intelligence. It is the science and technology of embodied autonomous neural systems. Neural systems include brain-inspired algorithms (e.g. connectionist networks), computational models of biological neural networks (e.g. artificial spiking neural networks, large-scale simulations of neural microcircuits) and actual biological systems (e.g. in vivo and in vitro neural nets). Such neural systems can be embodied in machines with mechanic or any other forms of physical actuation. This includes robots, prosthetic or wearable systems but also, at smaller scale, micro-machines and, at the larger scales, furniture and infrastructures.\nNeurorobotics is that branch of neuroscience with robotics, which deals with the study and application of science and technology of embodied autonomous neural systems like brain-inspired algorithms. It is based on the idea that the brain is embodied and the body is embedded in the environment. Therefore, most neurorobots are required to function in the real world, as opposed to a simulated environment.\nBeyond brain-inspired algorithms for robots neurorobotics may also involve the design of brain-controlled robot systems.\n\nMajor classes of models\nNeurorobots can be divided into various major classes based on the robot's purpose. Each class is designed to implement a specific mechanism of interest for study. Common types of neurorobots are those used to study motor control, memory, action selection, and perception.\n\nLocomotion and motor control\nNeurorobots are often used to study motor feedback and control systems, and have proved their merit in developing controllers for robots. Locomotion is modeled by a number of neurologically inspired theories on the action of motor systems. Locomotion control has been mimicked using models or central pattern generators, clumps of neurons capable of driving repetitive behavior, to make four-legged walking robots.  Other groups have expanded the idea of combining rudimentary control systems into a hierarchical set of simple autonomous systems. These systems can formulate complex movements from a combination of these rudimentary subsets. This theory of motor action is based on the organization of cortical columns, which progressively integrate from simple sensory input into a complex afferent signals, or from complex motor programs to simple controls for each muscle fiber in efferent signals, forming a similar hierarchical structure.\nAnother method for motor control uses learned error correction and predictive controls to form a sort of simulated muscle memory. In this model, awkward, random, and error-prone movements are corrected for using error feedback to"}
{"doc_id": "Neurorobotics", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " from complex motor programs to simple controls for each muscle fiber in efferent signals, forming a similar hierarchical structure.\nAnother method for motor control uses learned error correction and predictive controls to form a sort of simulated muscle memory. In this model, awkward, random, and error-prone movements are corrected for using error feedback to produce smooth and accurate movements over time. The controller learns to create the correct control signal by predicting the error.  Using these ideas, robots have been designed which can learn to produce adaptive arm movements or to avoid obstacles in a course.\n\nLearning and memory systems\nRobots designed to test theories of animal memory systems. Many studies examine the memory system of rats, particularly the rat hippocampus, dealing with place cells, which fire for a specific location that has been learned. Systems modeled after the rat hippocampus are generally able to learn mental maps of the environment, including recognizing landmarks and associating behaviors with them, allowing them to predict the upcoming obstacles and landmarks.\nAnother study has produced a robot based on the proposed learning paradigm of barn owls for orientation and localization based on primarily auditory, but also visual stimuli. The hypothesized method involves synaptic plasticity and neuromodulation, a mostly chemical effect in which reward neurotransmitters such as dopamine or serotonin affect the firing sensitivity of a neuron to be sharper. The robot used in the study adequately matched the behavior of barn owls. Furthermore, the close interaction between motor output and auditory feedback proved to be vital in the learning process, supporting active sensing theories that are involved in many of the learning models.\nNeurorobots in these studies are presented with simple mazes or patterns to learn. Some of the problems presented to the neurorobot include recognition of symbols, colors, or other patterns and execute simple actions based on the pattern. In the case of the barn owl simulation, the robot had to determine its location and direction to navigate in its environment.\n\nAction selection and value systems\nAction selection studies deal with negative or positive weighting to an action and its outcome. Neurorobots can and have been used to study simple ethical interactions, such as the classical thought experiment where there are more people than a life raft can hold, and someone must leave the boat to save the rest. However, more neurorobots used in the study of action selection contend with much simpler persuasions such as self-preservation or perpetuation of the population of robots in the study. These neurorobots are modeled after the neuromodulation of synapses to encourage circuits with positive results"}
{"doc_id": "Neurorobotics", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to save the rest. However, more neurorobots used in the study of action selection contend with much simpler persuasions such as self-preservation or perpetuation of the population of robots in the study. These neurorobots are modeled after the neuromodulation of synapses to encourage circuits with positive results. \nIn biological systems, neurotransmitters such as dopamine or acetylcholine positively reinforce neural signals that are beneficial. One study of such interaction involved the robot Darwin VII, which used visual, auditory, and a simulated taste input to \"eat\" conductive metal blocks. The arbitrarily chosen good blocks had a striped pattern on them while the bad blocks had a circular shape on them. The taste sense was simulated by conductivity of the blocks.  The robot had positive and negative feedbacks to the taste based on its level of conductivity. The researchers observed the robot to see how it learned its action selection behaviors based on the inputs it had. Other studies have used herds of small robots which feed on batteries strewn about the room, and communicate its findings to other robots.\n\nSensory perception\nNeurorobots have also been used to study sensory perception, particularly vision. These are primarily systems that result from embedding neural models of sensory pathways in automatas. This approach gives exposure to the sensory signals that occur during behavior and also enables a more realistic assessment of the degree of robustness of the neural model. It is well known that changes in the sensory\nsignals produced by motor activity provide useful perceptual cues that are used extensively by organisms. For example, researchers have used the depth information that emerges during replication of human head and eye movements to establish robust representations of the visual scene.\n\nBiological robots\nBiological robots are not officially neurorobots in that they are not neurologically inspired AI systems, but actual neuron tissue wired to a robot. This employs the use of cultured neural networks to study brain development or neural interactions. These typically consist of a neural culture raised on a multielectrode array (MEA), which is capable of both recording the neural activity and stimulating the tissue. In some cases, the MEA is connected to a computer which presents a simulated environment to the brain tissue and translates brain activity into actions in the simulation, as well as providing sensory feedback The ability to record neural activity gives researchers a window into a brain, which they can use to learn about a number of the same issues neurorobots are used for.\nAn area of concern with the biological robots is ethics. Many questions"}
{"doc_id": "Neurorobotics", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and translates brain activity into actions in the simulation, as well as providing sensory feedback The ability to record neural activity gives researchers a window into a brain, which they can use to learn about a number of the same issues neurorobots are used for.\nAn area of concern with the biological robots is ethics. Many questions are raised about how to treat such experiments. The central question concerns consciousness and whether or not the rat brain experiences it. There are many theories about how to define consciousness.\n\nImplications for neuroscience\nNeuroscientists benefit from neurorobotics because it provides a blank slate to test various possible methods of brain function in a controlled and testable environment. While robots are more simplified versions of the systems they emulate, they are more specific, allowing more direct testing of the issue at hand. They also have the benefit of being accessible at all times, while it is more difficult to monitor large portions of a brain while the human or animal is active, especially individual neurons.\nThe development of neuroscience has produced neural treatments. These include pharmaceuticals and neural rehabilitation. Progress is dependent on an intricate understanding of the brain and how exactly it functions. It is difficult to study the brain, especially in humans, due to the danger associated with cranial surgeries. Neurorobots can improved the range of tests and experiments that can be performed in the study of neural processes.\n\nSee also\nBrain–computer interface\nExperience machine\nNeuromorphic engineering\nWirehead (science fiction)"}
{"doc_id": "Non-human", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "For the 2022 horror film, see Unhuman (film).\nNon-human (also spelled nonhuman) is any entity displaying some, but not enough, human characteristics to be considered a human. The term has been used in a variety of contexts and may refer to objects that have been developed with human intelligence, such as robots or vehicles.\n\nOrganisms\nAnimal rights and personhood\nIn the animal rights movement, it is common to distinguish between \"human animals\" and \"non-human animals\". Participants in the animal rights movement generally recognize that non-human animals have some similar characteristics to those of human persons. For example, various non-human animals have been shown to register pain, compassion, memory, and some cognitive function.  Some animal rights activists argue that the similarities between human and non-human animals justify giving non-human animals rights that human society has afforded to humans, such as the right to self-preservation, and some even wish for all non-human animals or at least those that bear a fully thinking and conscious mind, such as vertebrates and some invertebrates such as cephalopods, to be given a full right of personhood.\n\nThe non-human in philosophy\nContemporary philosophers have drawn on the work of Henri Bergson, Gilles Deleuze, Félix Guattari, and Claude Lévi-Strauss (among others) to suggest that the non-human poses epistemological and ontological problems for humanist and post-humanist ethics, and have linked the study of non-humans to materialist and ethological approaches to the study of society and culture.\n\nSoftware and robots\nThe term non-human has been used to describe computer programs and robot-like devices that display some human-like characteristics. In both science fiction and in the real world, computer programs and robots have been built to perform tasks that require human-computer interactions in a manner that suggests sentience and compassion. There is increasing interest in the use of robots in nursing homes and to provide elder care. Computer programs have been used for years in schools to provide one-on-one education with children. The Tamagotchi toy required children to provide care, attention, and nourishment to keep it \"alive\".\n\nSee also\nAnimal\nAnimal rights by country or territory\nArtificial intelligence\nDehumanization\nPerson"}
{"doc_id": "Non-human", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " rights by country or territory\nArtificial intelligence\nDehumanization\nPerson"}
{"doc_id": "Nouvelle AI", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Nouvelle artificial intelligence (Nouvelle AI) is an approach to artificial intelligence pioneered in the 1980s by Rodney Brooks, who was then part of MIT artificial intelligence laboratory. Nouvelle AI differs from classical AI by aiming to produce robots with intelligence levels similar to insects. Researchers believe that intelligence can emerge organically from simple behaviors as these intelligences interacted with the \"real world\", instead of using the constructed worlds which symbolic AIs typically needed to have programmed into them.\n\nMotivation\nThe differences between nouvelle AI and symbolic AI are apparent in early robots Shakey and Freddy. These robots contained an internal model (or \"representation\") of their micro-worlds consisting of symbolic descriptions. As a result, this structure of symbols had to be renewed as the robot moved or the world changed.\nShakey's planning programs assessed the program structure and broke it down into the necessary steps to complete the desired action. This level of computation required a large amount time to process, so Shakey typically performed its tasks very slowly.\nSymbolic AI researchers had long been plagued by the problem of updating, searching, and otherwise manipulating the symbolic worlds inside their AIs. A nouvelle system refers continuously to its sensors rather than to an internal model of the world. It processes the external world information it needs from the senses when it is required. As Brooks puts it, \"the world is its own best model--always exactly up to date and complete in every detail.\"\nA central idea of nouvelle AI is that simple behaviors combine to form more complex behaviors over time. For example, simple behaviors can include elements like \"move forward\" and \"avoid obstacles.\" A robot using nouvelle AI with simple behaviors like collision avoidance and moving toward a moving object could possibly come together to produce a more complex behavior like chasing a moving object.\n\nThe frame problem\nThe frame problem describes an issue with using first-order logic (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms (symbolic language) to imply that things about an environment do not change arbitrarily.\nNouvelle AI seeks to sidestep the frame problem by dispensing with filling the AI or robot with volumes of symbolic language and instead letting more complex behaviors emerge by combining simpler behavioral elements.\n\nEmbodiment\nThe goal of traditional AI was to build intelligences without bodies, which would only have been able to interact with the world via keyboard, screen, or printer. However, nouvelle AI attempts to build embodied intelligence situated"}
{"doc_id": "Nouvelle AI", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " with volumes of symbolic language and instead letting more complex behaviors emerge by combining simpler behavioral elements.\n\nEmbodiment\nThe goal of traditional AI was to build intelligences without bodies, which would only have been able to interact with the world via keyboard, screen, or printer. However, nouvelle AI attempts to build embodied intelligence situated in the real world. Brooks quotes approvingly from the brief sketches that Turing gave in 1948 and 1950 of the \"situated\" approach. Turing wrote of equipping a machine \"with the best sense organs that money can buy\" and teaching it \"to understand and speak English\" by a process that would \"follow the normal teaching of a child.\" This approach was contrasted to the others where they focused on abstract activities such as playing chess.\n\nBrooks' robots\nInsectoid robots\nBrooks focused on building robots that acted like simple insects while simultaneously working to remove some traditional AI characteristics. He created insect-like robots, named Allen and Herbert after cognitive science and AI pioneers Allen Newell and Herbert A. Simon.\nBrooks's insectoid robots contained no internal models of the world. Herbert, for example, discarded a high volume of the information received from its sensors and never stored information for more than two seconds.\n\nAllen\nAllen had a ring of twelve ultrasonic sonars as its primary sensors and three independent behavior-producing modules. These modules were programmed to avoid both stationary and moving objects. With only this module activated, Allen stayed in the middle of a room until an object approached and then it ran away while avoiding obstacles in its way.\n\nHerbert\nHerbert used infrared sensors to avoid obstacles and a laser system to collect 3D data over a distance of about 12 feet. Herbert also carried a number of simple sensors in its \"hand.\" The robot's testing ground was the real world environment of the busy offices and workspaces of the MIT AI lab where it searched for empty soda cans and carried them away, a seemingly goal-oriented activity that emerged as a result of 15 simple behavior units combining. As a parallel, Simon noted that an ant's complicated path is due to the structure of its environment rather than the depth of its thought processes.\n\nOther insectoid robots\nOther robots by Brooks' team were Genghis and Squirt. Genghis had six legs and was able to walk over rough terrain and follow a human. Squirt's behavior modules had it stay in dark corners until it heard a noise, then it would begin to follow the source of the noise.\nBrooks agreed"}
{"doc_id": "Nouvelle AI", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " robots by Brooks' team were Genghis and Squirt. Genghis had six legs and was able to walk over rough terrain and follow a human. Squirt's behavior modules had it stay in dark corners until it heard a noise, then it would begin to follow the source of the noise.\nBrooks agreed that the level of nouvelle AI had come near the complexity of a real insect, which raised a question about whether or not insect level-behavior was and is a reasonable goal for nouvelle AI.\n\nHumanoid robots\nBrooks' own recent work has taken the opposite direction to that proposed by Von Neumann in the quotations \"theorists who select the human nervous system as their model are unrealistically picking 'the most complicated object under the sun,' and that there is little advantage in selecting instead the ant, since any nervous system at all exhibits exceptional complexity.\"\n\nCog\nIn the 1990s, Brooks decided to pursue the goal of human-level intelligence and, with Lynn Andrea Stein, built a humanoid robot called Cog. Cog is a robot with an extensive collection of sensors, a face, and arms (among other features) that allow it to interact with the world and gather information and experience so as to assemble intelligence organically in the manner described above by Turing.\nThe team believed that Cog would be able to learn and able to find a correlation between the sensory information it received and its actions, and to learn common sense knowledge on its own. As of 2003, all development of the project had ceased.\n\nSee also\nBEAM robotics\nBehavior-based robotics\nCognitive science\nReactive planning\nScruffy AI"}
{"doc_id": "Operation Serenata de Amor", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Operation Serenata de Amor is an artificial intelligence project designed to analyze public spending in Brazil. The project has been funded by a recurrent financing campaign since September 7, 2016, and came in the wake of major scandals of misappropriation of public funds in Brazil, such as the Mensalão scandal and what was revealed in the Operation Car Wash investigations.\nThe analysis began with data from the National Congress then expanded to other types of budget and instances of government, such as the Federal Senate. The project is built through collaboration on GitHub and using a public group with more than 600 participants on Telegram.\nThe name \"Serenata de Amor,\" which means \"serenade of love,\" was taken from a popular cashew cream bonbon produced by Chocolates Garoto in Brazil.\n\nModules\nThroughout development of the project, new modules have been newly introduced in addition to the main repository:\n\nThe main repository, serenata-de-amor, serves as the starting point for investigative work.\nRosie is the robot programmed to identify public funds expenses with discrepancies, starting with CEAP (Quota for Exercise of Parliamentary Activity); it analyzes each of the reimbursements requested by the deputies and senators, indicating the reasons that lead it to believe they are suspicious.\nFrom Rosie was born whistleblower, which tweets under the name of @RosieDaSerenata, distributing the results found on social media.\nJarbas (Github repository) is a data visualization tool which shows a complete list of reimbursements made available by the Chamber of Deputies and mined by Rosie.\nToolbox is a Python installable package that supports the development of Serenata de Amor and Rosie.\n\nHistory\nOperation Serenata de Amor is an Artificial intelligence project for analysis of public expenditures. It was conceived in March 2016 by data scientist Irio Musskopf, sociologist Eduardo Cuducos and entrepreneur Felipe Cabral. The project was financed collectively in the Catarse platform, where it reached 131% of the collection goal  paying 3 months of project development. Ana Schwendler, also a data scientist, Pedro Vilanova \"Tonny\", data journalist, Bruno Pazzim, software engineer, Filipe Linhares, a frontend engineer, Leandro Devegili, an entrepreneur and André Pinho took the first steps towards constructing the platform, such as collecting and structuring the first datasets.\nJessica Temporal, data scientist and Yasodara Córdova \"Yaso\", researcher, Tatiana Balach"}
{"doc_id": "Operation Serenata de Amor", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Linhares, a frontend engineer, Leandro Devegili, an entrepreneur and André Pinho took the first steps towards constructing the platform, such as collecting and structuring the first datasets.\nJessica Temporal, data scientist and Yasodara Córdova \"Yaso\", researcher, Tatiana Balachova \"Russa\", UX designer, joined the project after the financing took place.\nThe members created a recurring financing campaign, expanding the analysis of public spending to the Federal Senate. Donors make monthly payments ranging from 5 BRL to 200 BRL to maintain group activities. The monthly amount collected is around 10,000 BRL.\n\nResults\nIn January 2017, concluding the period financed by the initial campaign, the group carried out an investigation into the suspicious activities found by the data analysis system. 629 complaints were made to the Ombudsman's Office of the Chamber of Deputies, questioning expenses of 216 federal deputies. In addition, the Facebook project page has more than 25,000 followers, and users frequently cite the operation as a benchmark in transparency in the Brazilian government. One of the examples of results obtained by the operation is the case of the Deputy who had to return about 700 BRL to the House  after his expenses were analyzed by the platform.\nThe platform was able to analyze more than 3 million notes, raising about 8,000 suspected cases in public spending. The community that supports the work of the team benefits from open source repositories, with licenses open for the collaboration. So much so that the two main data scientists  of the project presented it at the CivicTechFest in Taipei, obtaining several mentions even in the international press. The technical leader presented the project in Poland during DevConf2017 in Kraków. It was also presented in the Google News Lab in 2017. It was presented by Yaso, when she was the Director of the initiative, at the MIT Media Lab/Berkman Klein Center Initiative for Artificial Intelligence ethics, and at the Artificial Intelligence and Inclusion Symposium, an initiative of the  Global Network of Internet & Society Centers (NoC). It was also presented both by Irio and Yaso at the Digital Harvard Kennedy School, over a lunch seminar, where the transparency of the platform and the main solutions found were discussed, so that the code and data are always available to verify its suitability.\nThis infographic provides information about the first results of Operation Serenata de Amor, a project that analyzes open data on public spending"}
{"doc_id": "Operation Serenata de Amor", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Digital Harvard Kennedy School, over a lunch seminar, where the transparency of the platform and the main solutions found were discussed, so that the code and data are always available to verify its suitability.\nThis infographic provides information about the first results of Operation Serenata de Amor, a project that analyzes open data on public spending to find discrepancies.\nThe project was presented by Yaso to the House Audit and Control Committee of the Chamber of Deputies in August 2017, and raised the interest of House officials who work with open data.\nThe operation has been a source of inspiration for other civic projects that aim to work with similar goals, demonstrating the broader impact of artificial intelligence also in industry in Brazil. Participation of several team members in events throughout Brazil and abroad can be found on the Internet, such as presentation at OpenDataDay, held at Calango Hackerspace in the Federal District, Campus Party Bahia, Campus Party Brasilia, Friends of Tomorrow, XIII National Meeting of Internal Control, in the event USP Talks  Hackfest against corruption in João Pessoa, the latter being also highlighted in the National Press.\n\nSee also\nInternet activism\nList of scandals in Brazil\nOpen government"}
{"doc_id": "Operational artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Operational artificial intelligence, or operational AI, is a type of intelligent system designed for real-world applications, particularly at commercial scale. The term is used to distinguish accessible artificially intelligent (AI) systems from fundamental AI research and from industrial AI applications, which are not integrated with the routine usage of a business. The definition of operational AI differs throughout the IT industry, where vendors and individual organizations often create their own custom definitions of such processes and services for the purpose of marketing their own products. \nApplications include text analytics, advanced analytics, facial and image recognition, machine learning, and natural language generation.\n\nDefinitions\nAccording to a white paper by software company Tupl Inc, continuous machine learning model training and results extraction in the telecom industry requires a large number of automation utilities to \"facilitate the development and deployment of a multitude of use cases, the collection and correlation of the data, the creation and training of the models, and the operation at telecom-grade levels of security and availability\".\n\nResearchers in the University of Waterloo's Artificial Intelligence Group describe operational AI in terms of the focus on applications that bring value to products and the company. University of Waterloo Professor of Electrical and Computer Engineering Fakhri Karray describes operational AI as \"application of AI for the masses\". Canada Research Chair and Associate Professor Alexander Wong (professor) describes operational AI as AI for \"anyone, anywhere, anytime.\"\n\nRelated terms\nIndustrial AI refers to intelligent systems applied for business at any scale and for any use case.\n\nSee also\nApplications of artificial intelligence\nEdge computing\nIndustrial artificial intelligence\nContinuous integration"}
{"doc_id": "Organoid intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Organoid intelligence (OI) is an emerging field of study in computer science and biology that develops and studies biological wetware computing using 3D cultures of human brain cells (or brain organoids) and brain-machine interface technologies. Such technologies may be referred to as OIs or the nervous filesystem.\nOrganoid intelligent computer systems can be an example of biohybrid systems.\n\nDifferences with non-organic computing\nAs opposed to traditional non-organic silicon-based approaches, OI seeks to use lab-grown cerebral organoids to serve as \"biological hardware.\" Scientists hope that such organoids can provide faster, more efficient, and more powerful computing power than regular silicon-based computing and AI while requiring only a fraction of the energy. However, while these structures are still far from being able to think like a regular human brain and do not yet possess strong computing capabilities, OI research currently offers the potential to improve the understanding of brain development, learning and memory, potentially finding treatments for neurological disorders such as dementia.\nThomas Hartung, a professor from Johns Hopkins University, argues that \"while silicon-based computers are certainly better with numbers, brains are better at learning.\" Furthermore, he claimed that with \"superior learning and storing\" capabilities than AIs, being more energy efficient, and that in the future, it might not be possible to add more transistors to a single computer chip, while brains are wired differently and have more potential for storage and computing power, OIs can potentially harness more power than current computers.\nSome researchers claim that even though human brains are slower than machines at processing simple information, they are far better at processing complex information as brains can deal with fewer and more uncertain data, perform both sequential and parallel processing, being highly heterogenous, use incomplete datasets, and is said to outperform non-organic machines in decision-making.\nTraining OIs involve the process of biological learning (BL) as opposed to machine learning (ML) for AIs. BL is said to be much more energy efficient than ML.\n\nBioinformatics in OI\nOI generates complex biological data, necessitating sophisticated methods for processing and analysis. Bioinformatics provides the tools and techniques to decipher raw data, uncovering the patterns and insights. A Python interface is currently available for processing and interaction with brain organoids.\n\nIntended functions\nBrain-inspired computing hardware aims to emulate the structure and working principles of the brain and could be used to address current limitations in artificial intelligence technologies. However, brain-inspired silicon chips are still limited in their ability to fully mimic brain function,"}
{"doc_id": "Organoid intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Python interface is currently available for processing and interaction with brain organoids.\n\nIntended functions\nBrain-inspired computing hardware aims to emulate the structure and working principles of the brain and could be used to address current limitations in artificial intelligence technologies. However, brain-inspired silicon chips are still limited in their ability to fully mimic brain function, as most examples are built on digital electronic principles. One study performed OI computation (which they termed Brainoware) by sending and receiving information from the brain organoid using a high-density multielectrode array. By applying spatiotemporal electrical stimulation, nonlinear dynamics, and fading memory properties, as well as unsupervised learning from training data by reshaping the organoid functional connectivity, the study showed the potential of this technology by using it for speech recognition and nonlinear equation prediction in a reservoir computing framework.\n\nEthical concerns\nWhile researchers are hoping to use OI and biological computing to complement traditional silicon-based computing, there are also questions about the ethics of such an approach. Examples of such ethical issues include OIs gaining consciousness and sentience as organoids and the question of the relationship between a stem cell donor (for growing the organoid) and the respective OI system.\n\nSee also\nBiohybrid system\nCerebral organoid\nArtificial intelligence\nHybrot\nOrgan-on-a-chip"}
{"doc_id": "Pattern theory", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Pattern theory, formulated by Ulf Grenander, is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language. Broad in its mathematical coverage, Pattern Theory spans algebra and statistics, as well as local topological and global entropic properties.\nIn addition to the new algebraic vocabulary, its statistical approach is novel in its aim to:\n\nIdentify the hidden variables of a data set using real world data rather than artificial stimuli, which was previously commonplace.\nFormulate prior distributions for hidden variables and models for the observed variables that form the vertices of a Gibbs-like graph.\nStudy the randomness and variability of these graphs.\nCreate the basic classes of stochastic models applied by listing the deformations of the patterns.\nSynthesize (sample) from the models, not just analyze signals with them.\nThe Brown University Pattern Theory Group was formed in 1972 by Ulf Grenander. Many mathematicians are currently working in this group, noteworthy among them being the Fields Medalist David Mumford.  Mumford regards Grenander as his \"guru\" in Pattern Theory.\n\nSee also\nAbductive reasoning\nAlgebraic statistics\nComputational anatomy\nFormal concept analysis\nGrammar induction\nImage analysis\nInduction\nLattice theory\nSpatial statistics"}
{"doc_id": "Pedagogical agent", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A pedagogical agent is a concept borrowed from computer science and artificial intelligence and applied to education, usually as part of an intelligent tutoring system (ITS). It is a simulated human-like interface between the learner and the content, in an educational environment. A pedagogical agent is designed to model the type of interactions between a student and another person. Mabanza and de Wet define it as \"a character enacted by a computer that interacts with the user in a socially engaging manner\". A pedagogical agent can be assigned different roles in the learning environment, such as tutor or co-learner, depending on the desired purpose of the agent. \"A tutor agent plays the role of a teacher, while a co-learner agent plays the role of a learning companion\".\n\nHistory\nThe history of Pedagogical Agents is closely aligned with the history of computer animation. As computer animation progressed, it was adopted by educators to enhance computerized learning by including a lifelike interface between the program and the learner. The first versions of a pedagogical agent were more cartoon than person, like Microsoft's Clippy which helped users of Microsoft Office load and use the program's features in 1997. However, with developments in computer animation, pedagogical agents can now look lifelike. By 2006 there was a call to develop modular, reusable agents to decrease the time and expertise required to create a pedagogical agent. There was also a call in 2009 to enact agent standards. The standardization and re-usability of pedagogical agents is less of an issue since the decrease in cost and widespread availability of animation tools. Individualized pedagogical agents can be found across disciplines including medicine, math, law, language learning, automotive, and armed forces. They are used in applications directed to every age, from preschool to adult.\n\nLearning theories related to pedagogical agent design\nDistributed cognition theory\nDistributed cognition theory is the method in which cognition progresses in the context of collaboration with others. Pedagogical agents can be designed to assist the cognitive transfer to the learner, operating as artifacts or partners with collaborative role in learning. To support the performance of an action by the user, the pedagogical agent can act as a cognitive tool as long as the agent is equipped with the knowledge that the user lacks. The interactions between the user and the pedagogical agent can facilitate a social relationship. The pedagogical agent may fulfill the role of a working partner.\n\nSocio-cultural learning theory\nSocio-cultural learning"}
{"doc_id": "Pedagogical agent", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ical agent can act as a cognitive tool as long as the agent is equipped with the knowledge that the user lacks. The interactions between the user and the pedagogical agent can facilitate a social relationship. The pedagogical agent may fulfill the role of a working partner.\n\nSocio-cultural learning theory\nSocio-cultural learning theory is how the user develops when they are involved in learning activities in which there is interaction with other agents. A pedagogical agent can: intervene when the user requests, provide support for tasks that the user cannot address, and potentially extend the learners cognitive reach. Interaction with the pedagogical agent may elicit a variety of emotions from the learner. The learner may become excited, confused, frustrated, and/or discouraged. These emotions affect the learners' motivation.\n\nExtraneous Cognitive Load\nExtraneous cognitive load is the extra effort being exerted by an individual's working memory due to the way information is being presented. A pedagogical agent can increase the user's cognitive load by distracting them and becoming the focus of their attention, causing split attention between the instructional material and the agent. Agents can reduce the perceived cognitive load by providing narration and personalization that can also promote a user's interest and motivation. While research on the reduction of cognitive load from pedagogical  agents is minimal, more studies have shown that agents do not increase it.\n\nEffectiveness\nIt has been suggested by researchers that pedagogical agents may take on different roles in the learning environment. Examples of these roles are: supplanting, scaffolding, coaching, testing, or demonstrating or modelling a procedure. A pedagogical agent as a tutor has not been demonstrated to add any benefit to an educational strategy in equivalent lessons with and without a pedagogical agent. According to Richard Mayer, there is some support in research for pedagogical agent increasing learning, but only as a presenter of social cues. A co-learner pedagogical agent is believed to increase the student's self-efficacy. By pointing out important features of instructional content, a pedagogical agent can fulfill the signaling function, which research on multimedia learning has shown to enhance learning. Research has demonstrated that human-human interaction may not be completely replaced by pedagogical agents, but learners may prefer the agents to non-agent multimedia systems. This finding is supported by social agency theory.\nMuch like the varying effectiveness of the pedagogical agent roles in the learning environment, agents that take into account the user's affect have had mixed results. Research has shown pedagogical agents"}
{"doc_id": "Pedagogical agent", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " pedagogical agents, but learners may prefer the agents to non-agent multimedia systems. This finding is supported by social agency theory.\nMuch like the varying effectiveness of the pedagogical agent roles in the learning environment, agents that take into account the user's affect have had mixed results. Research has shown pedagogical agents that make use of the users’ affect have been found to increase user knowledge retention, motivation, and perceived self-efficacy. However, with such a broad range of modalities in affective expressions, it is often difficult to utilize them. Additionally, having agents detect a user's affective state with precision remains challenging, as displays of affect are different across individuals.\n\nDesign\nAttractiveness\nThe appearance of a pedagogical agent can be manipulated to meet the learning requirements. The attractiveness of a pedagogical agent can enhance student's learning when the users were the opposite gender of the pedagogical agent. Male students prefer a sexy appearance of a female pedagogical agents and dislike the sexy appearance of male agents. Female students were not attracted by the sexy appearance of either male or female pedagogical agents.\n\nAffective Response\nPedagogical agents have reached a point where they can convey and elicit emotion, but also reason about and respond to it. These agents are often designed to elicit and respond to affective actions from users through various modalities such as speech, facial expressions, and body gestures. They respond to the affective state of the given user, and make use of these modalities using a wide array of sensors incorporated into the design of the agent. Specifically in education and training applications, pedagogical agents are often designed to increasingly recognize when users or learners exhibit frustration, boredom, confusion, and states of flow. The added recognition in these agents is a step toward making them more emotionally intelligent, comforting and motivating the users as they interact.\n\nDigital Representation\nThe design of a pedagogical agent often begins with its digital representation, whether it will be 2D or 3D and static or animated. Several studies have developed pedagogical agents that were both static and animated, then evaluated the relative benefits. Similar to other design considerations, the improved learning from static or animated agents remains questionable. One study showed that the appearance of an agent portrayed using a static image can impact a user's recall, based on the visual appearance. Other research found results that suggest static agent images improve learning outcomes. However, several other studies found user's learned more when the pedagogical agent was animated rather"}
{"doc_id": "Pedagogical agent", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " animated agents remains questionable. One study showed that the appearance of an agent portrayed using a static image can impact a user's recall, based on the visual appearance. Other research found results that suggest static agent images improve learning outcomes. However, several other studies found user's learned more when the pedagogical agent was animated rather than static. Recently a meta-analysis of such research found a negligible improvement in learning via pedagogical agents, suggesting more work needs to be done in the area to support any claims."}
{"doc_id": "Percept (artificial intelligence)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A percept is the input that an intelligent agent is perceiving at any given moment. It is essentially the same concept as a percept in psychology, except that it is being perceived not by the brain but by the agent. A percept is detected by a sensor, often a camera, processed accordingly, and acted upon by an actuator. Each percept is added to a \"percept sequence\", which is a complete history of each percept ever detected. The agent's action at any instant point may depend on the entire percept sequence up to that particular instant point. An intelligent agent chooses how to act not only based on the current percept, but the percept sequence. The next action is chosen by the agent function, which maps every percept to an action.\nFor example, if a camera were to record a gesture, the agent would process the percepts, calculate the corresponding spatial vectors, examine its percept history, and use the agent program (the application of the agent function) to act accordingly.\n\nExamples\nExamples of percepts include inputs from touch sensors, cameras, infrared sensors, sonar, microphones, mice, and keyboards. A percept can also be a higher-level feature of the data, such as lines, depth, objects, faces, or gestures.\n\nSee also\nMachine perception"}
{"doc_id": "Personality computing", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Personality computing is a research field related to artificial intelligence and personality psychology that studies personality by means of computational techniques from different sources, including text, multimedia, and social networks.\n\nOverview\nPersonality computing addresses three main problems involving personality: automatic personality recognition, perception, and synthesis. Automatic personality recognition is the inference of the personality type of target individuals from their digital footprint. Automatic personality perception is the inference of the personality attributed by an observer to a target individual based on some observable behavior. Automatic personality synthesis is the generation of the style or behaviour of artificial personalities in Avatars and virtual agents.\nSelf-assessed personality tests or observer ratings are always exploited as the ground truth for testing and validating the performance of artificial intelligence algorithms for the automatic prediction of personality types. There is a wide variety of personality tests, such as the Myers Briggs Type Indicator (MBTI) or the MMPI, but the most used are tests based on the Five Factor Model such as the Revised NEO Personality Inventory.\nPersonality computing can be considered as an extension or complement of Affective computing, where the former focuses on personality traits and the latter on affective states. A further extension of the two fields is Character Computing which combines various character states and traits including but not limited to personality and affect.\n\nHistory\nPersonality computing began around 2005 with the pioneering research in personality recognition by Shlomo Argamon and later by François Mairesse. These works showed that personality traits could be inferred with reasonable accuracy from text, such as blogs, self-presentations, and email addresses. In 2008, the concept of \"portable personality\" for the distributed management of personality profiles has been developed.\nA few years later, research began in personality recognition and perception from multimodal and social signals, such as recorded meetings and voice calls.\nIn the 2010s, the research focused mainly on personality recognition and perception from social media, helped by the first workshops organized by Fabio Celli. In particular personality was extracted from Facebook, Twitter and Instagram. In the same years, automatic personality synthesis helped improve the coherence of simulated behavior in virtual agents.\nScientific works by Michal Kosinski demonstrated the validity of Personality Computing from different digital footprints, in particular from user preferences such as Facebook page likes, showed that machines can recognize personality better than humans  and raised a warning against Cambridge Analytica and misuse of this kind of technology.\n\nApplications\nPersonality computing techniques, in particular personality recognition and perception, have applications in Social media marketing, where they can help reducing the cost of advertising"}
{"doc_id": "Personality computing", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " user preferences such as Facebook page likes, showed that machines can recognize personality better than humans  and raised a warning against Cambridge Analytica and misuse of this kind of technology.\n\nApplications\nPersonality computing techniques, in particular personality recognition and perception, have applications in Social media marketing, where they can help reducing the cost of advertising campaigns through psychological targeting."}
{"doc_id": "Personoid", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Personoid is the concept coined by Stanisław Lem, a Polish science-fiction writer, in Non Serviam, from his book A Perfect Vacuum (1971). His personoids are an abstraction of functions of human mind and they live in computers; they do not need any human-like physical body. \nIn cognitive and software modeling, personoid is a research approach to the development of intelligent autonomous agents.\nIn frame of the IPK (Information, Preferences, Knowledge) architecture, it is a framework of abstract intelligent agent with a  cognitive and structural intelligence. It can be seen as an essence of high intelligent entities.\nFrom the philosophical and systemics perspectives, personoid societies can also be seen as the carriers of a culture. According to N. Gessler, the personoids study can be a base for the research on artificial culture and culture evolution.\n\nPersonoids on TV and cinema\nWelt am Draht (1973)\nThe Thirteenth Floor (1999)\n\nSee also\nAndroid\nHumanoid\nIntelligence\nArtificial Intelligence\nCulture\nComputer Science\nCognitive Science\nAnticipatory science\nMemetics"}
{"doc_id": "Perusall", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Perusall is a social web annotation tool intended for use by students at schools and universities. It allows users to annotate the margins of a text in a virtual group setting that is similar to social media—with upvoting, emojis, chat functionality, and notification. It also includes automatic AI grading.\n\nHistory\nPerusall began as a research project at Harvard University. It later became an educational product for students and teachers.\nAs of 2024, Perusall states more than 5 million students have used the tool at over 5,000 educational institutions in 112 countries.\"\n\nFunctionality\nPerusall integrates with learning management systems such as Moodle, Canvas and Blackboard to aid with collaborative annotation. \nThe tool supports annotation of a range of media including text, images, equations, videos, PDFs and snapshots of webpages."}
{"doc_id": "POP-11", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "POP-11 is a reflective, incrementally compiled programming language with many of the features of an interpreted language. It is the core language of the Poplog programming environment developed originally by the University of Sussex, and recently in the School of Computer Science at the University of Birmingham, which hosts the main Poplog website.\nPOP-11 is an evolution of the language POP-2, developed in Edinburgh University, and features an open stack model (like Forth, among others). It is mainly procedural, but supports declarative language constructs, including a pattern matcher, and is mostly used for research and teaching in artificial intelligence, although it has features sufficient for many other classes of problems. It is often used to introduce symbolic programming techniques to programmers of more conventional languages like Pascal, who find POP syntax more familiar than that of Lisp. One of POP-11's features is that it supports first-class functions.\nPOP-11 is the core language of the Poplog system. The availability of the compiler and compiler subroutines at run-time (a requirement for incremental compiling) gives it the ability to support a far wider range of extensions (including run-time extensions, such as adding new data-types) than would be possible using only a macro facility. This made it possible for (optional) incremental compilers to be added for Prolog, Common Lisp and Standard ML, which could be added as required to support either mixed language development or development in the second language without using any POP-11 constructs. This made it possible for Poplog to be used by teachers, researchers, and developers who were interested in only one of the languages. The most successful product developed in POP-11 was the Clementine data mining system, developed by ISL. After SPSS bought ISL, they renamed Clementine to SPSS Modeler and decided to port it to C++ and Java, and eventually succeeded with great effort, and perhaps some loss of the flexibility provided by the use of an AI language.\nPOP-11 was for a time available only as part of an expensive commercial package (Poplog), but since about 1999 it has been freely available as part of the open-source software version of Poplog, including various added packages and teaching libraries. An online version of ELIZA using POP-11 is available at Birmingham.\nAt the University of Sussex, David Young used POP-11 in combination with C and Fortran to develop a suite of teaching and interactive development tools for image processing and vision, and has made them available in the Popvision extension to Poplog.\n\n"}
{"doc_id": "POP-11", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " online version of ELIZA using POP-11 is available at Birmingham.\nAt the University of Sussex, David Young used POP-11 in combination with C and Fortran to develop a suite of teaching and interactive development tools for image processing and vision, and has made them available in the Popvision extension to Poplog.\n\nSimple code examples\nHere is an example of a simple POP-11 program:\n\ndefine Double(Source) -> Result;\n    Source*2 -> Result;\nenddefine;\n\nDouble(123) =>\n\nThat prints out:\n\n** 246\n\nThis one includes some list processing:\n\n define RemoveElementsMatching(Element, Source) -> Result;\n     lvars Index;\n     [[%\n     for Index in Source do\n         unless Index = Element or Index matches Element then\n             Index;\n         endunless;\n     endfor;\n     %]] -> Result;\n enddefine;\n\n RemoveElementsMatching(\"the\", [[the cat sat on the mat]]) => ;;; outputs [[cat sat on mat]]\n RemoveElementsMatching(\"the\", [[the cat] [sat on] the mat]) => ;;; outputs [[the cat] [sat on] mat]\n RemoveElementsMatching([[= cat]], [[the cat]] is a [[big cat]]) => ;;; outputs [[is a]]\n\nExamples using the POP-11 pattern matcher, which makes it relatively easy for students to learn to develop sophisticated list-processing programs without having to treat patterns as tree structures accessed by 'head' and 'tail' functions (CAR and CDR in Lisp), can be found in the online introductory tutorial. The matcher is at the heart of\nthe SimAgent (sim_agent) toolkit. Some of the powerful features of the toolkit, such as linking pattern variables to inline code variables, would have been very difficult to implement without the incremental compiler facilities.\n\nSee also\nCOWSEL (aka POP-1) programming language"}
{"doc_id": "Principle of rationality", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The principle of rationality (or rationality principle) was coined by Karl R. Popper in his Harvard Lecture of 1963, and published in his book Myth of Framework. It is related to what he called the 'logic of the situation' in an Economica article of 1944/1945, published later in his book The Poverty of Historicism. According to Popper's rationality principle, agents act in the most adequate way according to the objective situation. It is an idealized conception of human behavior which he used to drive his model of situational analysis. Cognitive scientist Allen Newell elaborated on the principle in his account of knowledge level modeling.\n\nPopper\nPopper called for social science to be grounded in what he called situational analysis or situational logic. This requires building models of social situations which include individual actors and their relationship to social institutions, e.g. markets, legal codes, bureaucracies, etc. These models attribute certain aims and information to the actors. This forms the 'logic of the situation', the result of reconstructing meticulously all circumstances of an historical event. The 'principle of rationality' is the assumption that people are instrumental in trying to reach their goals, and this is what drives the model. Popper believed that this model could be continuously refined to approach the objective truth.\nPopper called his principle of rationality nearly empty (a technical term meaning without empirical content) and strictly speaking false, but nonetheless tremendously useful. These remarks earned him a lot of criticism because seemingly he had swerved from his famous Logic of Scientific Discovery.\nAmong the many philosophers having discussed Popper's principle of rationality from the 1960s up to now are Noretta Koertge, R. Nadeau, Viktor J. Vanberg, Hans Albert, E. Matzner, Ian C. Jarvie, Mark A. Notturno, John Wettersten, Ian C. Böhm.\n\nNewell\nIn the context of knowledge-based systems, Newell (in 1982) proposed the following principle of rationality: \"If an agent has knowledge that one of its actions will lead to one of its goals, then the agent will select that action.\" This principle is employed by agents at the knowledge level to move closer to a desired goal. An important philosophical difference between Newell and Popper is that Newell argued that the knowledge level is real in the sense that it exists in nature and is not made up. This allowed Newell to treat the"}
{"doc_id": "Principle of rationality", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " select that action.\" This principle is employed by agents at the knowledge level to move closer to a desired goal. An important philosophical difference between Newell and Popper is that Newell argued that the knowledge level is real in the sense that it exists in nature and is not made up. This allowed Newell to treat the rationality principle as a way of understanding nature and avoid the problems Popper ran into by treating knowledge as non physical and therefore non empirical.\n\nSee also\nHermeneutics\nRational choice"}
{"doc_id": "Problem solving", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Problem solving is the process of achieving a goal by overcoming obstacles, a frequent part of most activities. Problems in need of solutions range from simple personal tasks (e.g. how to turn on an appliance) to complex issues in business and technical fields. The former is an example of simple problem solving (SPS) addressing one issue, whereas the latter is complex problem solving (CPS) with multiple interrelated obstacles. Another classification of problem-solving tasks is into well-defined problems with specific obstacles and goals, and ill-defined problems in which the current situation is troublesome but it is not clear what kind of resolution to aim for. Similarly, one may distinguish formal or fact-based problems requiring psychometric intelligence, versus socio-emotional problems which depend on the changeable emotions of individuals or groups, such as tactful behavior, fashion, or gift choices.\nSolutions require sufficient resources and knowledge to attain the goal. Professionals such as lawyers, doctors, programmers, and consultants are largely problem solvers for issues that require technical skills and knowledge beyond general competence. Many businesses have found profitable markets by recognizing a problem and creating a solution: the more widespread and inconvenient the problem, the greater the opportunity to develop a scalable solution.\nThere are many specialized problem-solving techniques and methods in fields such as science, engineering, business, medicine, mathematics, computer science, philosophy, and social organization. The mental techniques to identify, analyze, and solve problems are studied in psychology and cognitive sciences. Also widely researched are the mental obstacles that prevent people from finding solutions; problem-solving impediments include confirmation bias, mental set, and functional fixedness.\n\nDefinition\nThe term problem solving has a slightly different meaning depending on the discipline. For instance, it is a mental process in psychology and a computerized process in computer science. There are two different types of problems: ill-defined and well-defined; different approaches are used for each. Well-defined problems have specific end goals and clearly expected solutions, while ill-defined problems do not. Well-defined problems allow for more initial planning than ill-defined problems. Solving problems sometimes involves dealing with pragmatics (the way that context contributes to meaning) and semantics (the interpretation of the problem). The ability to understand what the end goal of the problem is, and what rules could be applied, represents the key to solving the problem. Sometimes a problem requires abstract thinking or coming up with a creative solution.\nProblem solving has two major domains: mathematical problem solving and personal problem solving. Each concerns some difficulty or barrier that is encountered.\n\nPsychology\nProblem solving in"}
{"doc_id": "Problem solving", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the problem is, and what rules could be applied, represents the key to solving the problem. Sometimes a problem requires abstract thinking or coming up with a creative solution.\nProblem solving has two major domains: mathematical problem solving and personal problem solving. Each concerns some difficulty or barrier that is encountered.\n\nPsychology\nProblem solving in psychology refers to the process of finding solutions to problems encountered in life. Solutions to these problems are usually situation- or context-specific. The process starts with problem finding and problem shaping, in which the problem is discovered and simplified. The next step is to generate possible solutions and evaluate them. Finally a solution is selected to be implemented and verified. Problems have an end goal to be reached; how you get there depends upon problem orientation (problem-solving coping style and skills) and systematic analysis.\nMental health professionals study the human problem-solving processes using methods such as introspection, behaviorism, simulation, computer modeling, and experiment. Social psychologists look into the person-environment relationship aspect of the problem and independent and interdependent problem-solving methods. Problem solving has been defined as a higher-order cognitive process and intellectual function that requires the modulation and control of more routine or fundamental skills.\nEmpirical research shows many different strategies and factors influence everyday problem solving. Rehabilitation psychologists studying people with frontal lobe injuries have found that deficits in emotional control and reasoning can be re-mediated with effective rehabilitation and could improve the capacity of injured persons to resolve everyday problems. Interpersonal everyday problem solving is dependent upon personal motivational and contextual components. One such component is the emotional valence of \"real-world\" problems, which can either impede or aid problem-solving performance. Researchers have focused on the role of emotions in problem solving, demonstrating that poor emotional control can disrupt focus on the target task, impede problem resolution, and lead to negative outcomes such as fatigue, depression, and inertia. In conceptualization,human problem solving consists of two related processes: problem orientation, and the motivational/attitudinal/affective approach to problematic situations and problem-solving skills. People's strategies cohere with their goals and stem from the process of comparing oneself with others.\n\nCognitive sciences\nAmong the first experimental psychologists to study problem solving were the Gestaltists in Germany, such as Karl Duncker in The Psychology of Productive Thinking (1935). Perhaps best known is the work of Allen Newell and Herbert A. Simon.\nExperiments in the 1960s and early 1970s asked participants to solve relatively simple, well-defined, but not previously seen laboratory tasks. These"}
{"doc_id": "Problem solving", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " such as Karl Duncker in The Psychology of Productive Thinking (1935). Perhaps best known is the work of Allen Newell and Herbert A. Simon.\nExperiments in the 1960s and early 1970s asked participants to solve relatively simple, well-defined, but not previously seen laboratory tasks. These simple problems, such as the Tower of Hanoi, admitted optimal solutions that could be found quickly, allowing researchers to observe the full problem-solving process. Researchers assumed that these model problems would elicit the characteristic cognitive processes by which more complex \"real world\" problems are solved.\nAn outstanding problem-solving technique found by this research is the principle of decomposition.\n\nComputer science\nMuch of computer science and artificial intelligence involves designing automated systems to solve a specified type of problem: to accept input data and calculate a correct or adequate response, reasonably quickly. Algorithms are recipes or instructions that direct such systems, written into computer programs.\nSteps for designing such systems include problem determination, heuristics, root cause analysis, de-duplication, analysis, diagnosis, and repair. Analytic techniques include linear and nonlinear programming, queuing systems, and simulation. A large, perennial obstacle is to find and fix errors in computer programs: debugging.\n\nLogic\nFormal logic concerns issues like validity, truth, inference, argumentation, and proof. In a problem-solving context, it can be used to formally represent a problem as a theorem to be proved, and to represent the knowledge needed to solve the problem as the premises to be used in a proof that the problem has a solution.\nThe use of computers to prove mathematical theorems using formal logic emerged as the field of automated theorem proving in the 1950s. It included the use of heuristic methods designed to simulate human problem solving, as in the Logic Theory Machine, developed by Allen Newell, Herbert A. Simon and J. C. Shaw, as well as algorithmic methods such as the resolution principle developed by John Alan Robinson.\nIn addition to its use for finding proofs of mathematical theorems, automated theorem-proving has also been used for program verification in computer science. In 1958, John McCarthy proposed the advice taker, to represent information in formal logic and to derive answers to questions using automated theorem-proving. An important step in this direction was made by Cordell Green in 1969, who used a resolution theorem prover for question-answering and for such other applications in artificial intelligence as robot planning.\nThe resolution theorem-prover used by Cordell Green bore"}
{"doc_id": "Problem solving", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to derive answers to questions using automated theorem-proving. An important step in this direction was made by Cordell Green in 1969, who used a resolution theorem prover for question-answering and for such other applications in artificial intelligence as robot planning.\nThe resolution theorem-prover used by Cordell Green bore little resemblance to human problem solving methods. In response to criticism of that approach from researchers at MIT, Robert Kowalski developed logic programming and SLD resolution, which solves problems by problem decomposition. He has advocated logic for both computer and human problem solving and computational logic to improve human thinking.\n\nEngineering\nWhen products or processes fail, problem solving techniques can be used to develop corrective actions that can be taken to prevent further failures. Such techniques can also be applied to a product or process prior to an actual failure event—to predict, analyze, and mitigate a potential problem in advance. Techniques such as failure mode and effects analysis can proactively reduce the likelihood of problems.\nIn either the reactive or the proactive case, it is necessary to build a causal explanation through a process of diagnosis. In deriving an explanation of effects in terms of causes, abduction generates new ideas or hypotheses (asking \"how?\"); deduction evaluates and refines hypotheses based on other plausible premises (asking \"why?\"); and induction justifies a hypothesis with empirical data (asking \"how much?\"). The objective of abduction is to determine which hypothesis or proposition to test, not which one to adopt or assert. In the Peircean logical system, the logic of abduction and deduction contribute to our conceptual understanding of a phenomenon, while the logic of induction adds quantitative details (empirical substantiation) to our conceptual knowledge.\nForensic engineering is an important technique of failure analysis that involves tracing product defects and flaws. Corrective action can then be taken to prevent further failures.\nReverse engineering attempts to discover the original problem-solving logic used in developing a product by disassembling the product and developing a plausible pathway to creating and assembling its parts.\n\nPhysics\nIn physics, problem solving refers to the process by which one transforms an initial physical situation into a goal state by applying physics-specific reasoning and analysis. This involves identifying the relevant physical principles, making assumptions, formulating and manipulating equations, and checking whether the result is reasonable.\nA physics problem is not simply application or recall of a formula, but requires understanding the underlying concepts and navigating through a \"problem space\" of possible knowledge states toward the goal.\n\nMilitary science\nIn military science, problem solving is linked to the concept of \"end-states"}
{"doc_id": "Problem solving", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " manipulating equations, and checking whether the result is reasonable.\nA physics problem is not simply application or recall of a formula, but requires understanding the underlying concepts and navigating through a \"problem space\" of possible knowledge states toward the goal.\n\nMilitary science\nIn military science, problem solving is linked to the concept of \"end-states\", the conditions or situations which are the aims of the strategy. Ability to solve problems is important at any military rank, but is essential at the command and control level. It results from deep qualitative and quantitative understanding of possible scenarios. Effectiveness in this context is an evaluation of results: to what extent the end states were accomplished. Planning is the process of determining how to effect those end states.\n\nProcesses\nSome models of problem solving involve identifying a goal and then a sequence of subgoals towards achieving this goal. Andersson, who introduced the ACT-R model of cognition, modelled this collection of goals and subgoals as a goal stack in which the mind contains a stack of goals and subgoals to be completed, and a single task being carried out at any time.\nKnowledge of how to solve one problem can be applied to another problem, in a process known as transfer.\n\nProblem-solving strategies\nProblem-solving strategies are steps to overcoming the obstacles to achieving a goal. The iteration of such strategies over the course of solving a problem is the \"problem-solving cycle\".\nCommon steps in this cycle include recognizing the problem, defining it, developing a strategy to fix it, organizing knowledge and resources available, monitoring progress, and evaluating the effectiveness of the solution. Once a solution is achieved, another problem usually arises, and the cycle starts again.\nInsight is the sudden aha! solution to a problem, the birth of a new idea to simplify a complex situation. Solutions found through insight are often more incisive than those from step-by-step analysis. A quick solution process requires insight to select productive moves at different stages of the problem-solving cycle. Unlike Newell and Simon's formal definition of a move problem, there is no consensus definition of an insight problem.\nSome problem-solving strategies include:\n\nAbstraction\nsolving the problem in a tractable model system to gain insight into the real system\nAnalogy\nadapting the solution to a previous problem which has similar features or mechanisms\nBrainstorming\n(especially among groups of people) suggesting a large number of solutions or ideas and combining and developing them until an optimum solution is found\nBypasses\ntransform the problem into another problem that is easier to solve, bypassing the barrier"}
{"doc_id": "Problem solving", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the solution to a previous problem which has similar features or mechanisms\nBrainstorming\n(especially among groups of people) suggesting a large number of solutions or ideas and combining and developing them until an optimum solution is found\nBypasses\ntransform the problem into another problem that is easier to solve, bypassing the barrier, then transform that solution back to a solution to the original problem.\nCritical thinking\nanalysis of available evidence and arguments to form a judgement via rational, skeptical, and unbiased evaluation\nDivide and conquer\nbreaking down a large, complex problem into smaller, solvable problems\nHelp-seeking\nobtaining external assistance to deal with obstacles\nHypothesis testing\nassuming a possible explanation to the problem and trying to prove (or, in some contexts, disprove) the assumption\nLateral thinking\napproaching solutions indirectly and creatively\nMeans-ends analysis\nchoosing an action at each step to move closer to the goal\nMorphological analysis\nassessing the output and interactions of an entire system\nObservation / Question\nin the natural sciences an observation is an act or instance of noticing or perceiving and the acquisition of information from a primary source. A question is an utterance which serves as a request for information.\nProof of impossibility\ntry to prove that the problem cannot be solved. The point where the proof fails will be the starting point for solving it\nReduction\ntransforming the problem into another problem for which solutions exist\nResearch\nemploying existing ideas or adapting existing solutions to similar problems\nRoot cause analysis\nidentifying the cause of a problem\nTrial-and-error\ntesting possible solutions until the right one is found\n\nProblem-solving methods\nA3 problem solving – Structured problem improvement approach\nDesign thinking – Processes by which design concepts are developed\nEight Disciplines Problem Solving – Eight disciplines of team-oriented problem solving methodPages displaying short descriptions of redirect targets\nGROW model – Method for goal setting and problem solving\nHelp-seeking – Theory in psychology\nHow to Solve It – Book by George Pólya\nLateral thinking – Manner of solving problems\nOODA loop – Observe–orient–decide–act cycle\nPDCA – Iterative design and management method\nRoot cause analysis – Method of identifying the fundamental causes of faults or problemsPages displaying short descriptions of redirect targets\nRPR problem diagnosis\nTRIZ – Problem-solving tools\nScientific method – is an empirical method for acquiring knowledge that has characterized the development of science.\nSwarm intelligence – Collective"}
{"doc_id": "Problem solving", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ative design and management method\nRoot cause analysis – Method of identifying the fundamental causes of faults or problemsPages displaying short descriptions of redirect targets\nRPR problem diagnosis\nTRIZ – Problem-solving tools\nScientific method – is an empirical method for acquiring knowledge that has characterized the development of science.\nSwarm intelligence – Collective behavior of decentralized, self-organized systems\nSystem dynamics – Study of non-linear complex systems\n\nCommon barriers\nCommon barriers to problem solving include mental constructs that impede an efficient search for solutions. Five of the most common identified by researchers are: confirmation bias, mental set, functional fixedness, unnecessary constraints, and irrelevant information.\n\nConfirmation bias\nConfirmation bias is an unintentional tendency to collect and use data which favors preconceived notions. Such notions may be incidental rather than motivated by important personal beliefs: the desire to be right may be sufficient motivation.\nScientific and technical professionals also experience confirmation bias. One online experiment, for example, suggested that professionals within the field of psychological research are likely to view scientific studies that agree with their preconceived notions more favorably than clashing studies. According to Raymond Nickerson, one can see the consequences of confirmation bias in real-life situations, which range in severity from inefficient government policies to genocide. Nickerson argued that those who killed people accused of witchcraft demonstrated confirmation bias with motivation. Researcher Michael Allen found evidence for confirmation bias with motivation in school children who worked to manipulate their science experiments to produce favorable results.\nHowever, confirmation bias does not necessarily require motivation. In 1960, Peter Cathcart Wason conducted an experiment in which participants first viewed three numbers and then created a hypothesis in the form of a rule that could have been used to create that triplet of numbers. When testing their hypotheses, participants tended to only create additional triplets of numbers that would confirm their hypotheses, and tended not to create triplets that would negate or disprove their hypotheses.\n\nMental set\nMental set is the inclination to re-use a previously successful solution, rather than search for new and better solutions. It is a reliance on habit.\nIt was first articulated by Abraham S. Luchins in the 1940s with his well-known water jug experiments. Participants were asked to fill one jug with a specific amount of water by using other jugs with different maximum capacities. After Luchins gave a set of jug problems that could all be solved by a single technique, he then introduced a problem that could be solved by the same technique, but also by a novel and simpler method."}
{"doc_id": "Problem solving", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " asked to fill one jug with a specific amount of water by using other jugs with different maximum capacities. After Luchins gave a set of jug problems that could all be solved by a single technique, he then introduced a problem that could be solved by the same technique, but also by a novel and simpler method. His participants tended to use the accustomed technique, oblivious of the simpler alternative. This was again demonstrated in Norman Maier's 1931 experiment, which challenged participants to solve a problem by using a familiar tool (pliers) in an unconventional manner. Participants were often unable to view the object in a way that strayed from its typical use, a type of mental set known as functional fixedness (see the following section).\nRigidly clinging to a mental set is called fixation, which can deepen to an obsession or preoccupation with attempted strategies that are repeatedly unsuccessful. In the late 1990s, researcher Jennifer Wiley found that professional expertise in a field can create a mental set, perhaps leading to fixation.\nGroupthink, in which each individual takes on the mindset of the rest of the group, can produce and exacerbate mental set. Social pressure leads to everybody thinking the same thing and reaching the same conclusions.\n\nFunctional fixedness\nFunctional fixedness is the tendency to view an object as having only one function, and to be unable to conceive of any novel use, as in the Maier pliers experiment described above. Functional fixedness is a specific form of mental set, and is one of the most common forms of cognitive bias in daily life.\nAs an example, imagine a man wants to kill a bug in his house, but the only thing at hand is a can of air freshener. He may start searching for something to kill the bug instead of squashing it with the can, thinking only of its main function of deodorizing.\nTim German and Clark Barrett describe this barrier: \"subjects become 'fixed' on the design function of the objects, and problem solving suffers relative to control conditions in which the object's function is not demonstrated.\" Their research found that young children's limited knowledge of an object's intended function reduces this barrier Research has also discovered functional fixedness in educational contexts, as an obstacle to understanding: \"functional fixedness may be found in learning concepts as well as in solving chemistry problems.\"\nThere are several hypotheses in regards to how functional fixedness relates to problem solving. It may waste time, delaying or entirely preventing the correct use of a tool.\n\nUnnecessary constraints\nUnnecessary constraints are arbitrary boundaries imposed"}
{"doc_id": "Problem solving", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to understanding: \"functional fixedness may be found in learning concepts as well as in solving chemistry problems.\"\nThere are several hypotheses in regards to how functional fixedness relates to problem solving. It may waste time, delaying or entirely preventing the correct use of a tool.\n\nUnnecessary constraints\nUnnecessary constraints are arbitrary boundaries imposed unconsciously on the task at hand, which foreclose a productive avenue of solution. The solver may become fixated on only one type of solution, as if it were an inevitable requirement of the problem. Typically, this combines with mental set—clinging to a previously successful method.\nVisual problems can also produce mentally invented constraints. A famous example is the dot problem: nine dots arranged in a three-by-three grid pattern must be connected by drawing four straight line segments, without lifting pen from paper or backtracking along a line. The subject typically assumes the pen must stay within the outer square of dots, but the solution requires lines continuing beyond this frame, and researchers have found a 0% solution rate within a brief allotted time.\nThis problem has produced the expression \"think outside the box\". Such problems are typically solved via a sudden insight which leaps over the mental barriers, often after long toil against them. This can be difficult depending on how the subject has structured the problem in their mind, how they draw on past experiences, and how well they juggle this information in their working memory. In the example, envisioning the dots connected outside the framing square requires visualizing an unconventional arrangement, which is a strain on working memory.\n\nIrrelevant information\nIrrelevant information is a specification or data presented in a problem that is unrelated to the solution. If the solver assumes that all information presented needs to be used, this often derails the problem solving process, making relatively simple problems much harder.\nFor example: \"Fifteen percent of the people in Topeka have unlisted telephone numbers. You select 200 names at random from the Topeka phone book. How many of these people have unlisted phone numbers?\" The \"obvious\" answer is 15%, but in fact none of the unlisted people would be listed among the 200. This kind of \"trick question\" is often used in aptitude tests or cognitive evaluations. Though not inherently difficult, they require independent thinking that is not necessarily common. Mathematical word problems often include irrelevant qualitative or numerical information as an extra challenge.\n\nAvoiding barriers by changing problem representation\nThe disruption caused by the above cognitive biases can depend on how the information is represented:"}
{"doc_id": "Problem solving", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " used in aptitude tests or cognitive evaluations. Though not inherently difficult, they require independent thinking that is not necessarily common. Mathematical word problems often include irrelevant qualitative or numerical information as an extra challenge.\n\nAvoiding barriers by changing problem representation\nThe disruption caused by the above cognitive biases can depend on how the information is represented: visually, verbally, or mathematically. A classic example is the Buddhist monk problem:\n\nA Buddhist monk begins at dawn one day walking up a mountain, reaches the top at sunset, meditates at the top for several days until one dawn when he begins to walk back to the foot of the mountain, which he reaches at sunset. Making no assumptions about his starting or stopping or about his pace during the trips, prove that there is a place on the path which he occupies at the same hour of the day on the two separate journeys.\nThe problem cannot be addressed in a verbal context, trying to describe the monk's progress on each day. It becomes much easier when the paragraph is represented mathematically by a function: one visualizes a graph whose horizontal axis is time of day, and whose vertical axis shows the monk's position (or altitude) on the path at each time. Superimposing the two journey curves, which traverse opposite diagonals of a rectangle, one sees they must cross each other somewhere. The visual representation by graphing has resolved the difficulty.\nSimilar strategies can often improve problem solving on tests.\n\nOther barriers for individuals\nPeople who are engaged in problem solving tend to overlook subtractive changes, even those that are critical elements of efficient solutions. For example, a city planner may decide that the solution to decrease traffic congestion would be to add another lane to a highway, rather than finding ways to reduce the need for the highway in the first place. This tendency to solve by first, only, or mostly creating or adding elements, rather than by subtracting elements or processes is shown to intensify with higher cognitive loads such as information overload.\n\nDreaming: problem solving without waking consciousness\nPeople can also solve problems while they are asleep. There are many reports of scientists and engineers who solved problems in their dreams. For example, Elias Howe, inventor of the sewing machine, figured out the structure of the bobbin from a dream.\nThe chemist August Kekulé was considering how benzene arranged its six carbon and hydrogen atoms. Thinking about the problem, he dozed off, and dreamt of dancing atoms that fell into a snakelike pattern, which led him to discover the benzene ring."}
{"doc_id": "Problem solving", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the bobbin from a dream.\nThe chemist August Kekulé was considering how benzene arranged its six carbon and hydrogen atoms. Thinking about the problem, he dozed off, and dreamt of dancing atoms that fell into a snakelike pattern, which led him to discover the benzene ring. As Kekulé wrote in his diary,\n\nOne of the snakes seized hold of its own tail, and the form whirled mockingly before my eyes. As if by a flash of lightning I awoke; and this time also I spent the rest of the night in working out the consequences of the hypothesis.\nThere also are empirical studies of how people can think consciously about a problem before going to sleep, and then solve the problem with a dream image. Dream researcher William C. Dement told his undergraduate class of 500 students that he wanted them to think about an infinite series, whose first elements were OTTFF, to see if they could deduce the principle behind it and to say what the next elements of the series would be. He asked them to think about this problem every night for 15 minutes before going to sleep and to write down any dreams that they then had. They were instructed to think about the problem again for 15 minutes when they awakened in the morning.\nThe sequence OTTFF is the first letters of the numbers: one, two, three, four, five. The next five elements of the series are SSENT (six, seven, eight, nine, ten). Some of the students solved the puzzle by reflecting on their dreams. One example was a student who reported the following dream:\n\nI was standing in an art gallery, looking at the paintings on the wall. As I walked down the hall, I began to count the paintings: one, two, three, four, five. As I came to the sixth and seventh, the paintings had been ripped from their frames. I stared at the empty frames with a peculiar feeling that some mystery was about to be solved. Suddenly I realized that the sixth and seventh spaces were the solution to the problem!\nWith more than 500 undergraduate students, 87 dreams were judged to be related to the problems students were assigned (53 directly related and 34 indirectly related). Yet of the people who had dreams that apparently solved the problem, only seven were actually able to consciously know the solution. The rest (46 out of 53) thought they did not know the solution.\nAlbert Einstein believed that much problem solving goes on unconsciously, and the"}
{"doc_id": "Problem solving", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "53 directly related and 34 indirectly related). Yet of the people who had dreams that apparently solved the problem, only seven were actually able to consciously know the solution. The rest (46 out of 53) thought they did not know the solution.\nAlbert Einstein believed that much problem solving goes on unconsciously, and the person must then figure out and formulate consciously what the mindbrain has already solved. He believed this was his process in formulating the theory of relativity: \"The creator of the problem possesses the solution.\" Einstein said that he did his problem solving without words, mostly in images. \"The words or the language, as they are written or spoken, do not seem to play any role in my mechanism of thought. The psychical entities which seem to serve as elements in thought are certain signs and more or less clear images which can be 'voluntarily' reproduced and combined.\"\n\nCognitive sciences: two schools\nProblem-solving processes differ across knowledge domains and across levels of expertise. For this reason, cognitive sciences findings obtained in the laboratory cannot necessarily generalize to problem-solving situations outside the laboratory. This has led to a research emphasis on real-world problem solving, since the 1990s. This emphasis has been expressed quite differently in North America and Europe, however. Whereas North American research has typically concentrated on studying problem solving in separate, natural knowledge domains, much of the European research has focused on novel, complex problems, and has been performed with computerized scenarios.\n\nEurope\nIn Europe, two main approaches have surfaced, one initiated by Donald Broadbent in the United Kingdom and the other one by Dietrich Dörner in Germany. The two approaches share an emphasis on relatively complex, semantically rich, computerized laboratory tasks, constructed to resemble real-life problems. The approaches differ somewhat in their theoretical goals and methodology. The tradition initiated by Broadbent emphasizes the distinction between cognitive problem-solving processes that operate under awareness versus outside of awareness, and typically employs mathematically well-defined computerized systems. The tradition initiated by Dörner, on the other hand, has an interest in the interplay of the cognitive, motivational, and social components of problem solving, and utilizes very complex computerized scenarios that contain up to 2,000 highly interconnected variables.\n\nNorth America\nIn North America, initiated by the work of Herbert A. Simon on \"learning by doing\" in semantically rich domains, researchers began to investigate problem solving separately in different natural knowledge domains—such as physics, writing, or chess playing—rather than attempt to"}
{"doc_id": "Problem solving", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to 2,000 highly interconnected variables.\n\nNorth America\nIn North America, initiated by the work of Herbert A. Simon on \"learning by doing\" in semantically rich domains, researchers began to investigate problem solving separately in different natural knowledge domains—such as physics, writing, or chess playing—rather than attempt to extract a global theory of problem solving. These researchers have focused on the development of problem solving within certain domains, that is on the development of expertise.\nAreas that have attracted rather intensive attention in North America include:\n\ncalculation\ncomputer skills\ngame playing\nlawyers' reasoning\nmanagerial problem solving\nphysical problem solving\nmathematical problem solving\nmechanical problem solving\npersonal problem solving\npolitical decision making\nproblem solving in electronics\nproblem solving for innovations and inventions: TRIZ\nreading\nsocial problem solving\nwriting\n\nCharacteristics of complex problems\nComplex problem solving (CPS) is distinguishable from simple problem solving (SPS). In SPS there is a singular and simple obstacle. In CPS there may be multiple simultaneous obstacles. For example, a surgeon at work has far more complex problems than an individual deciding what shoes to wear. As elucidated by Dietrich Dörner, and later expanded upon by Joachim Funke, complex problems have some typical characteristics, which include:\n\ncomplexity (large numbers of items, interrelations, and decisions)\nenumerability\nheterogeneity\nconnectivity (hierarchy relation, communication relation, allocation relation)\ndynamics (time considerations)\ntemporal constraints\ntemporal sensitivity\nphase effects\ndynamic unpredictability\nintransparency (lack of clarity of the situation)\ncommencement opacity\ncontinuation opacity\npolytely (multiple goals)\ninexpressivenes\nopposition\ntransience\n\nCollective problem solving\nPeople solve problems on many different levels—from the individual to the civilizational. Collective problem solving refers to problem solving performed collectively. Social issues and global issues can typically only be solved collectively.\nThe complexity of contemporary problems exceeds the cognitive capacity of any individual and requires different but complementary varieties of expertise and collective problem solving ability.\nCollective intelligence is shared or group intelligence that emerges from the collaboration, collective efforts, and competition of many individuals.\nIn collaborative problem solving people work together to solve real-world problems. Members of problem-solving groups share a common concern, a similar passion, and/or a commitment to their work. Members can ask questions, wonder, and try to understand common issues. They share expertise, experiences, tools, and methods"}
{"doc_id": "Problem solving", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " competition of many individuals.\nIn collaborative problem solving people work together to solve real-world problems. Members of problem-solving groups share a common concern, a similar passion, and/or a commitment to their work. Members can ask questions, wonder, and try to understand common issues. They share expertise, experiences, tools, and methods. Groups may be fluid based on need, may only occur temporarily to finish an assigned task, or may be more permanent depending on the nature of the problems.\nFor example, in the educational context, members of a group may all have input into the decision-making process and a role in the learning process. Members may be responsible for the thinking, teaching, and monitoring of all members in the group. Group work may be coordinated among members so that each member makes an equal contribution to the whole work. Members can identify and build on their individual strengths so that everyone can make a significant contribution to the task. Collaborative group work has the ability to promote critical thinking skills, problem solving skills, social skills, and self-esteem. By using collaboration and communication, members often learn from one another and construct meaningful knowledge that often leads to better learning outcomes than individual work.\nCollaborative groups require joint intellectual efforts between the members and involve social interactions to solve problems together. The knowledge shared during these interactions is acquired during communication, negotiation, and production of materials. Members actively seek information from others by asking questions. The capacity to use questions to acquire new information increases understanding and the ability to solve problems.\nIn a 1962 research report, Douglas Engelbart linked collective intelligence to organizational effectiveness, and predicted that proactively \"augmenting human intellect\" would yield a multiplier effect in group problem solving: \"Three people working together in this augmented mode [would] seem to be more than three times as effective in solving a complex problem as is one augmented person working alone\".\nHenry Jenkins, a theorist of new media and media convergence, draws on the theory that collective intelligence can be attributed to media convergence and participatory culture. He criticizes contemporary education for failing to incorporate online trends of collective problem solving into the classroom, stating \"whereas a collective intelligence community encourages ownership of work as a group, schools grade individuals\". Jenkins argues that interaction within a knowledge community builds vital skills for young people, and teamwork through collective intelligence communities contributes to the development of such skills.\nCollective impact is the commitment of a group of actors from different sectors to a common agenda for solving a specific social problem, using a structured form of collaboration.\nAfter World War II the"}
{"doc_id": "Problem solving", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " that interaction within a knowledge community builds vital skills for young people, and teamwork through collective intelligence communities contributes to the development of such skills.\nCollective impact is the commitment of a group of actors from different sectors to a common agenda for solving a specific social problem, using a structured form of collaboration.\nAfter World War II the UN, the Bretton Woods organization, and the WTO were created. Collective problem solving on the international level crystallized around these three types of organization from the 1980s onward. As these global institutions remain state-like or state-centric it is unsurprising that they perpetuate state-like or state-centric approaches to collective problem solving rather than alternative ones.\nCrowdsourcing is a process of accumulating ideas, thoughts, or information from many independent participants, with aim of finding the best solution for a given challenge. Modern information technologies allow for many people to be involved and facilitate managing their suggestions in ways that provide good results. The Internet allows for a new capacity of collective (including planetary-scale) problem solving.\n\nSee also\nActuarial science – Statistics applied to risk in insurance and other financial products\nAnalytical skill – Crucial skill in all different fields of work and life\nCreative problem-solving – Mental process of problem solving\nCollective intelligence – Group intelligence that emerges from collective efforts\nCommunity of practice\nCoworking – Practice of independent contractors or scientists sharing office space without supervision\nCrowdsolving – Sourcing services or funds from a groupPages displaying short descriptions of redirect targets\nDivergent thinking – Process of generating creative ideas\nGrey problem\nInnovation – Practical implementation of improvements\nInstrumentalism – Position in the philosophy of science\nProblem-posing education – Method of teaching coined by Paulo Freire\nProblem statement – Description of an issue\nProblem structuring methods – Techniques used to model a situation to be changed\nShared intentionality – Ability to engage with others' psychological states\nStructural fix\nSubgoal labeling – Cognitive process\nTroubleshooting – Form of problem solving, often applied to repair failed products or processes\nWicked problem – Problem that is difficult or impossible to solve\n\nNotes\nFurther reading\nBeckmann, Jens F.; Guthke, Jürgen (1995). \"Complex problem solving, intelligence, and learning ability\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 177–200.\nBrehmer, Berndt (199"}
{"doc_id": "Problem solving", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "). \"Complex problem solving, intelligence, and learning ability\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 177–200.\nBrehmer, Berndt (1995). \"Feedback delays in dynamic decision making\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 103–130.\nBrehmer, Berndt; Dörner, D. (1993). \"Experiments with computer-simulated microworlds: Escaping both the narrow straits of the laboratory and the deep blue sea of the field study\". Computers in Human Behavior. 9 (2–3): 171–184. doi:10.1016/0747-5632(93)90005-D.\nDörner, D. (1992). \"Über die Philosophie der Verwendung von Mikrowelten oder 'Computerszenarios' in der psychologischen Forschung\" [On the proper use of microworlds or \"computer scenarios\" in psychological research]. In Gundlach, H. (ed.). Psychologische Forschung und Methode: Das Versprechen des Experiments. Festschrift für Werner Traxel (in German). Passau, Germany: Passavia-Universitäts-Verlag. pp. 53–87.\nEyferth, K.; Schömann, M.; Widowski, D. (1986). \"Der Umgang von Psychologen mit Komplexität\" [On how psychologists deal with complexity]. Sprache & Kognition (in German). 5: 11–26.\nFunke, Joachim (1993). \"Microworlds based on linear equation systems: A new approach to complex problem solving and experimental results\" (PDF). In Strube, G.; Wender, K.-F. (eds.). The cognitive psychology of knowledge. Amsterdam: Elsevier Science Publishers. pp. 313–330.\nFunke, Joachim (1995). \"Experimental research on complex problem solving\" (PDF). In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates"}
{"doc_id": "Problem solving", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ". pp. 313–330.\nFunke, Joachim (1995). \"Experimental research on complex problem solving\" (PDF). In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 243–268.\nFunke, U. (1995). \"Complex problem solving in personnel selection and training\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 219–240.\nGroner, M.; Groner, R.; Bischof, W. F. (1983). \"Approaches to heuristics: A historical review\". In Groner, R.; Groner, M.; Bischof, W. F. (eds.). Methods of heuristics. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 1–18.\nHayes, J. (1980). The complete problem solver. Philadelphia: The Franklin Institute Press.\nHuber, O. (1995). \"Complex problem solving as multistage decision making\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 151–173.\nHübner, Ronald (1989). \"Methoden zur Analyse und Konstruktion von Aufgaben zur kognitiven Steuerung dynamischer Systeme\" [Methods for the analysis and construction of dynamic system control tasks] (PDF). Zeitschrift für Experimentelle und Angewandte Psychologie (in German). 36: 221–238.\nHunt, Earl (1991). \"Some comments on the study of complexity\". In Sternberg, R. J.; Frensch, P. A. (eds.). Complex problem solving: Principles and mechanisms. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 383–395. ISBN 978-1-317-78386-2.\nHussy, W. (1985). \"Komplexes Problemlösen—Eine Sackgasse?\" [Complex problem solving—a dead end?]. Zeitschrift für Experimentelle und Angewandte Psychologie (in German). 32: 55–77.\nKluwe"}
{"doc_id": "Problem solving", "chunk_id": 17, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "-2.\nHussy, W. (1985). \"Komplexes Problemlösen—Eine Sackgasse?\" [Complex problem solving—a dead end?]. Zeitschrift für Experimentelle und Angewandte Psychologie (in German). 32: 55–77.\nKluwe, R. H. (1993). \"Chapter 19 Knowledge and Performance in Complex Problem Solving\". The Cognitive Psychology of Knowledge. Advances in Psychology. Vol. 101. pp. 401–423. doi:10.1016/S0166-4115(08)62668-0. ISBN 978-0-444-89942-2.\nKluwe, R. H. (1995). \"Single case studies and models of complex problem solving\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 269–291.\nKolb, S.; Petzing, F.; Stumpf, S. (1992). \"Komplexes Problemlösen: Bestimmung der Problemlösegüte von Probanden mittels Verfahren des Operations Research—ein interdisziplinärer Ansatz\" [Complex problem solving: determining the quality of human problem solving by operations research tools—an interdisciplinary approach]. Sprache & Kognition (in German). 11: 115–128.\nKrems, Josef F. (1995). \"Cognitive flexibility and complex problem solving\". In Frensch, P. A.; Funke, J. (eds.). Complex problem solving: The European Perspective. Hillsdale, N.J.: Lawrence Erlbaum Associates. pp. 201–218.\nMelzak, Z. (1983). Bypasses: A Simple Approach to Complexity. London, UK: Wiley.\nMüller, H. (1993). Komplexes Problemlösen: Reliabilität und Wissen [Complex problem solving: Reliability and knowledge] (in German). Bonn, Germany: Holos.\nParadies, M.W.; Unger, L. W. (2000). TapRooT—The System for Root Cause Analysis, Problem Investigation, and Proactive Improvement. Knoxville, Tenn.: System Improvements.\nPutz-Osterloh, Wiebke (1993). \"Chapter 15 Strategies for Knowledge Acquisition and Transfer of"}
{"doc_id": "Problem solving", "chunk_id": 18, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".; Unger, L. W. (2000). TapRooT—The System for Root Cause Analysis, Problem Investigation, and Proactive Improvement. Knoxville, Tenn.: System Improvements.\nPutz-Osterloh, Wiebke (1993). \"Chapter 15 Strategies for Knowledge Acquisition and Transfer of Knowledge in Dynamic Tasks\". The Cognitive Psychology of Knowledge. Advances in Psychology. Vol. 101. pp. 331–350. doi:10.1016/S0166-4115(08)62664-3. ISBN 978-0-444-89942-2.\nRiefer, David M.; Batchelder, William H. (1988). \"Multinomial modeling and the measurement of cognitive processes\" (PDF). Psychological Review. 95 (3): 318–339. doi:10.1037/0033-295x.95.3.318. S2CID 14994393. Archived from the original (PDF) on 2018-11-25.\nSchaub, H. (1993). Modellierung der Handlungsorganisation (in German). Bern, Switzerland: Hans Huber.\nStrauß, B. (1993). Konfundierungen beim Komplexen Problemlösen. Zum Einfluß des Anteils der richtigen Lösungen (ArL) auf das Problemlöseverhalten in komplexen Situationen [Confoundations in complex problem solving. On the influence of the degree of correct solutions on problem solving in complex situations] (in German). Bonn, Germany: Holos.\nStrohschneider, S. (1991). \"Kein System von Systemen! Kommentar zu dem Aufsatz 'Systemmerkmale als Determinanten des Umgangs mit dynamischen Systemen' von Joachim Funke\" [No system of systems! Reply to the paper 'System features as determinants of behavior in dynamic task environments' by Joachim Funke]. Sprache & Kognition (in German). 10: 109–113.\nTonelli, Marcello (2011). Unstructured Processes of Strategic Decision-Making. Saarbrücken, Germany: Lambert Academic Publishing. ISBN 978-3-8465-5598-9.\nVan Lehn, Kurt (1989). \"Problem solving and cognitive skill acquisition\". In Posner, M. I. (ed.). Foundations of cognitive science (PDF). Cambridge,"}
{"doc_id": "Problem solving", "chunk_id": 19, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "aking. Saarbrücken, Germany: Lambert Academic Publishing. ISBN 978-3-8465-5598-9.\nVan Lehn, Kurt (1989). \"Problem solving and cognitive skill acquisition\". In Posner, M. I. (ed.). Foundations of cognitive science (PDF). Cambridge, Mass.: MIT Press. pp. 527–579.\nWisconsin Educational Media Association (1993), Information literacy: A position paper on information problem-solving, WEMA Publications, vol. ED 376 817, Madison, Wis.{{citation}}:  CS1 maint: location missing publisher (link) (Portions adapted from Michigan State Board of Education's Position Paper on Information Processing Skills, 1992.)"}
{"doc_id": "Progress in artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Progress in artificial intelligence (AI) refers to the advances, milestones, and breakthroughs that have been achieved in the field of artificial intelligence over time. AI is a branch of computer science that aims to create machines and systems capable of performing tasks that typically require human intelligence. AI applications have been used in a wide range of fields including medical diagnosis, finance, robotics, law, video games, agriculture, and scientific discovery. The society as a whole is looking for artificial intelligence to be on a key factor in the upcming years because of its potential. However, many AI applications are not perceived as AI: \"A lot of cutting-edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\" \n\"Many thousands of AI applications are deeply embedded in the infrastructure of every industry.\" In the late 1990s and early 2000s, AI technology became widely used as elements of larger systems, but the field was rarely credited for these successes at the time.\nKaplan and Haenlein structure artificial intelligence along three evolutionary stages:\n\nArtificial narrow intelligence – AI capable only of specific tasks;\nArtificial general intelligence – AI with ability in several areas, and able to autonomously solve problems they were never even designed for;\nArtificial superintelligence – AI capable of general tasks, including scientific creativity, social skills, and general wisdom.\nTo allow comparison with human performance, artificial intelligence can be evaluated on constrained and well-defined problems. Such tests have been termed subject-matter expert Turing tests. Also, smaller problems provide more achievable goals and there are an ever-increasing number of positive results.\nIn 2023, humans still substantially outperformed both GPT-4 and other models tested on the ConceptARC benchmark. Those models scored 60% on most, and 77% on one category, while humans scored 91% on all and 97% on one category. However, later research in 2025 showed that human-generated output grids were only accurate 73% of the time, while AI models available that year managed to score above 77%.\n\nCurrent performance in specific areas\nThere are many useful abilities that can be described as showing some form of intelligence. This gives better insight into the comparative success of artificial intelligence in different areas.\nAI, like electricity or the steam engine, is a general-purpose technology. There is no consensus on how to characterize which tasks AI tends to excel at. Some versions of Moravec's paradox observe that humans are"}
{"doc_id": "Progress in artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " as showing some form of intelligence. This gives better insight into the comparative success of artificial intelligence in different areas.\nAI, like electricity or the steam engine, is a general-purpose technology. There is no consensus on how to characterize which tasks AI tends to excel at. Some versions of Moravec's paradox observe that humans are more likely to outperform machines in areas such as physical dexterity that have been the direct target of natural selection. While projects such as AlphaZero have succeeded in generating their own knowledge from scratch, many other machine learning projects require large training datasets. Researcher Andrew Ng has suggested, as a \"highly imperfect rule of thumb\", that \"almost anything a typical human can do with less than one second of mental thought, we can probably now or in the near future automate using AI.\"\nGames provide a high-profile benchmark for assessing rates of progress; many games have a large professional player base and a well-established competitive rating system. AlphaGo brought the era of classical board-game benchmarks to a close when Artificial Intelligence proved their competitive edge over humans in 2016. Deep Mind's AlphaGo AI software program defeated the world's best professional Go Player Lee Sedol. Games of imperfect knowledge provide new challenges to AI in the area of game theory; the most prominent milestone in this area was brought to a close by Libratus' poker victory in 2017. E-sports continue to provide additional benchmarks; Facebook AI, Deepmind, and others have engaged with the popular StarCraft franchise of videogames.\nBroad classes of outcome for an AI test may be given as:\n\noptimal: it is not possible to perform better (note: some of these entries were solved by humans)\nsuper-human: performs better than all humans\nhigh-human: performs better than most humans\npar-human: performs similarly to most humans\nsub-human: performs worse than most humans\n\nOptimal\nTic-tac-toe\nConnect Four: 1988\nCheckers (aka 8x8 draughts): Weakly solved (2007)\nRubik's Cube: Mostly solved (2010)\nHeads-up limit hold'em poker: Statistically optimal in the sense that \"a human lifetime of play is not sufficient to establish with statistical significance that the strategy is not an exact solution\" (2015)\n\nSuper-human\nOthello (aka reversi): c. 1997\nScrabble: 2006\nBackgammon: c. 1995–2002\nChess: Supercomputer (c. 199"}
{"doc_id": "Progress in artificial intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " establish with statistical significance that the strategy is not an exact solution\" (2015)\n\nSuper-human\nOthello (aka reversi): c. 1997\nScrabble: 2006\nBackgammon: c. 1995–2002\nChess: Supercomputer (c. 1997); Personal computer (c. 2006); Mobile phone (c. 2009); Computer defeats human + computer (c. 2017)\nJeopardy!: Question answering, although the machine did not use speech recognition (2011)\nArimaa: 2015\nShogi: c. 2017\nGo: 2017\nHeads-up no-limit hold'em poker: 2017\nSix-player no-limit hold'em poker: 2019\nGran Turismo Sport: 2022\n\nHigh-human\nCrosswords: c. 2012\nFreeciv: 2016\nDota 2: 2018\nBridge card-playing: According to a 2009 review, \"the best programs are attaining expert status as (bridge) card players\", excluding bidding.\nStarCraft II: 2019\nMahjong: 2019\nStratego: 2022\nNo-Press Diplomacy: 2022\nHanabi: 2022\nNatural language processing\n\nPar-human\nOptical character recognition for ISO 1073-1:1976 and similar special characters.\nClassification of images\nHandwriting recognition\nFacial recognition\nVisual question answering\nSQuAD 2.0 English reading-comprehension benchmark (2019)\nSuperGLUE English-language understanding benchmark (2020)\nSome school science exams (2019)\nSome tasks based on Raven's Progressive Matrices\nMany Atari 2600 games (2015)\n\nSub-human\nOptical character recognition for printed text (nearing par-human for Latin-script typewritten text)\nObject recognition\nVarious robotics tasks that may require advances in robot hardware as well as AI, including:\nStable bipedal locomotion: Bipedal robots can walk, but are less stable than human walkers (as of 2017)\nHumanoid soccer\nSpeech recognition: \"nearly equal to human performance\" (2017)\nExplainability. Current medical systems can diagnose certain medical conditions well, but cannot explain to users why they made the diagnosis.\nMany tests of fluid intelligence (2020)\nBongard visual cognition problems, such as the Bongard-LOGO benchmark"}
{"doc_id": "Progress in artificial intelligence", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ": \"nearly equal to human performance\" (2017)\nExplainability. Current medical systems can diagnose certain medical conditions well, but cannot explain to users why they made the diagnosis.\nMany tests of fluid intelligence (2020)\nBongard visual cognition problems, such as the Bongard-LOGO benchmark (2020)\nVisual Commonsense Reasoning (VCR) benchmark (as of 2020)\nStock market prediction: Financial data collection and processing using Machine Learning algorithms\nAngry Birds video game, as of 2020\nVarious tasks that are difficult to solve without contextual knowledge, including:\nTranslation\nWord-sense disambiguation\n\nProposed tests of artificial intelligence\nIn his famous Turing test, Alan Turing picked language, the defining feature of human beings, for its basis. The Turing test is now considered too exploitable to be a meaningful benchmark.\nThe Feigenbaum test, proposed by the inventor of expert systems, tests a machine's knowledge and expertise about a specific subject. A paper by Jim Gray of Microsoft in 2003 suggested extending the Turing test to speech understanding, speaking and recognizing objects and behavior.\nProposed \"universal intelligence\" tests aim to compare how well machines, humans, and even non-human animals perform on problem sets that are generic as possible. At an extreme, the test suite can contain every possible problem, weighted by Kolmogorov complexity; however, these problem sets tend to be dominated by impoverished pattern-matching exercises where a tuned AI can easily exceed human performance levels.\n\nExams\nAccording to OpenAI, in 2023 ChatGPT GPT-4 scored the 90th percentile on the Uniform Bar Exam. On the SATs, GPT-4 scored the 89th percentile on math, and the 93rd percentile in Reading & Writing. On the GREs, it scored on the 54th percentile on the writing test, 88th percentile on the quantitative section, and 99th percentile on the verbal section. It scored in the 99th to 100th percentile on the 2020 USA Biology Olympiad semifinal exam. It scored a perfect \"5\" on several AP exams.\nIndependent researchers found in 2023 that ChatGPT GPT-3.5 \"performed at or near the passing threshold\" for the three parts of the United States Medical Licensing Examination. GPT-3.5 was also assessed to attain a low, but passing, grade from exams for four law school courses at the University of"}
{"doc_id": "Progress in artificial intelligence", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " 2023 that ChatGPT GPT-3.5 \"performed at or near the passing threshold\" for the three parts of the United States Medical Licensing Examination. GPT-3.5 was also assessed to attain a low, but passing, grade from exams for four law school courses at the University of Minnesota. GPT-4 passed a text-based radiology board–style examination.\n\nCompetitions\nMany competitions and prizes, such as the Imagenet Challenge, promote research in artificial intelligence. The most common areas of competition include general machine intelligence, conversational behavior, data-mining, robotic cars, and robot soccer as well as conventional games.\n\nPast and current predictions\nAn expert poll around 2016, conducted by Katja Grace of the Future of Humanity Institute and associates, gave median estimates of 3 years for championship Angry Birds, 4 years for the World Series of Poker, and 6 years for StarCraft. On more subjective tasks, the poll gave 6 years for folding laundry as well as an average human worker, 7–10 years for expertly answering 'easily Googleable' questions, 8 years for average speech transcription, 9 years for average telephone banking, and 11 years for expert songwriting, but over 30 years for writing a New York Times bestseller or winning the Putnam math competition.\n\nChess\nAn AI defeated a grandmaster in a regulation tournament game for the first time in 1988; rebranded as Deep Blue, it beat the reigning human world chess champion in 1997 (see Deep Blue versus Garry Kasparov).\n\nGo\nAlphaGo defeated a European Go champion in October 2015, and Lee Sedol in March 2016, one of the world's top players (see AlphaGo versus Lee Sedol). According to Scientific American and other sources, most observers had expected superhuman Computer Go performance to be at least a decade away.\n\nHuman-level artificial general intelligence (AGI)\nAI pioneer and economist Herbert A. Simon inaccurately predicted in 1965: \"Machines will be capable, within twenty years, of doing any work a man can do\". Similarly, in 1970 Marvin Minsky wrote that \"Within a generation... the problem of creating artificial intelligence will substantially be solved.\"\nFour polls conducted in 2012 and 2013 suggested that the median estimate among experts for when AGI would arrive was 2040 to 2050, depending on the poll.\nThe Grace poll around 2016 found"}
{"doc_id": "Progress in artificial intelligence", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " wrote that \"Within a generation... the problem of creating artificial intelligence will substantially be solved.\"\nFour polls conducted in 2012 and 2013 suggested that the median estimate among experts for when AGI would arrive was 2040 to 2050, depending on the poll.\nThe Grace poll around 2016 found results varied depending on how the question was framed. Respondents asked to estimate \"when unaided machines can accomplish every task better and more cheaply than human workers\" gave an aggregated median answer of 45 years and a 10% chance of it occurring within 9 years. Other respondents asked to estimate \"when all occupations are fully automatable. That is, when for any occupation, machines could be built to carry out the task better and more cheaply than human workers\" estimated a median of 122 years and a 10% probability of 20 years. The median response for when \"AI researcher\" could be fully automated was around 90 years. No link was found between seniority and optimism, but Asian researchers were much more optimistic than North American researchers on average; Asians predicted 30 years on average for \"accomplish every task\", compared with the 74 years predicted by North Americans.\n\nSee also\nApplications of artificial intelligence\nList of artificial intelligence projects\nList of emerging technologies"}
{"doc_id": "Psychology of reasoning", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The psychology of reasoning (also known as the cognitive science of reasoning) is the study of how people reason, often broadly defined as the process of drawing conclusions to inform how people solve problems and make decisions. It overlaps with psychology, philosophy, linguistics, cognitive science, artificial intelligence, logic, and probability theory.\nPsychological experiments on how humans and other animals reason have been carried out for over 100 years. An enduring question is whether or not people have the capacity to be rational. Current research in this area addresses various questions about reasoning, rationality, judgments, intelligence, relationships between emotion and reasoning, and development.\n\nEveryday reasoning\nOne of the most obvious areas in which people employ reasoning is with sentences in everyday language. Most experimentation on deduction has been carried out on hypothetical thought, in particular, examining how people reason about conditionals, e.g., If A then B. Participants in experiments make the modus ponens inference, given the indicative conditional If A then B, and given the premise A, they conclude B. However, given the indicative conditional and the minor premise for the modus tollens inference, not-B, about half of the participants in experiments conclude not-A and the remainder concludes that nothing follows.\nThe ease with which people make conditional inferences is affected by context, as demonstrated in the well-known selection task developed by Peter Wason. Participants are better able to test a conditional in an ecologically relevant context, e.g., if the envelope is sealed then it must have a 50 cent stamp on it compared to one that contains symbolic content, e.g., if the letter is a vowel then the number is even. Background knowledge can also lead to the suppression of even the simple modus ponens inference  Participants given the conditional if Lisa has an essay to write then she studies late in the library and the premise Lisa has an essay to write  make the modus ponens inference 'she studies late in the library', but the inference is suppressed when they are also given a second conditional if the library stays open then she studies late in the library. Interpretations of the suppression effect are controversial\nOther investigations of propositional inference examine how people think about disjunctive alternatives, e.g., A or else B, and how they reason about negation, e.g., It is not the case that A and B. Many experiments have been carried out to examine how people make relational inferences, including comparisons, e.g., A is better than B. Such investigations also concern spatial inferences, e.g."}
{"doc_id": "Psychology of reasoning", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " A or else B, and how they reason about negation, e.g., It is not the case that A and B. Many experiments have been carried out to examine how people make relational inferences, including comparisons, e.g., A is better than B. Such investigations also concern spatial inferences, e.g. A is in front of B and temporal inferences, e.g. A occurs before B. Other common tasks include categorical syllogisms, used to examine how people reason about quantifiers such as All or Some, e.g., Some of the A are not B. For example if all A are B and some B are C, what (if anything) follows?\n\nTheories of reasoning\nThere are several alternative theories of the cognitive processes that human reasoning is based on. One view is that people rely on a mental logic consisting of formal (abstract or syntactic) inference rules similar to those developed by logicians in the propositional calculus. Another view is that people rely on domain-specific or content-sensitive rules of inference. A third view is that people rely on mental models, that is, mental representations that correspond to imagined possibilities. A fourth view is that people compute probabilities.\nOne controversial theoretical issue is the identification of an appropriate competence model, or a standard against which to compare human reasoning. Initially classical logic was chosen as a competence model. Subsequently, some researchers opted for non-monotonic logic and Bayesian probability. Research on mental models and reasoning has led to the suggestion that people are rational in principle but err in practice. Connectionist approaches towards reasoning have also been proposed.\nDespite the ongoing debate about the cognitive processes involved in human reasoning, recent research has shown that multiple approaches can be useful in modeling human thinking. For instance, studies have found that people's reasoning is often influenced by their prior beliefs, which can be modeled using Bayesian probability theory. Additionally, research on mental models has shown that people tend to reason about problems by constructing multiple mental representations of the situation, which can help them to identify relevant features and make inferences based on their understanding of the problem. Moreover, connectionist approaches to reasoning have also gained attention, which focus on the neural network models that can learn from data and generalize to new situations.\n\nDevelopment of reasoning\nIt is an active question in psychology how, why, and when the ability to reason develops from infancy to adulthood. Jean Piaget's theory of cognitive development posited general mechanisms and stages in the development of reasoning from infancy to adulthood. According to the neo-Piagetian theories of"}
{"doc_id": "Psychology of reasoning", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " new situations.\n\nDevelopment of reasoning\nIt is an active question in psychology how, why, and when the ability to reason develops from infancy to adulthood. Jean Piaget's theory of cognitive development posited general mechanisms and stages in the development of reasoning from infancy to adulthood. According to the neo-Piagetian theories of cognitive development, changes in reasoning with development come from increasing working memory capacity, increasing speed of processing, and enhanced executive functions and control. Increasing self-awareness is also an important factor.\nIn their book The Enigma of Reason, the cognitive scientists Hugo Mercier and Dan Sperber put forward an \"argumentative\" theory of reasoning, claiming that humans evolved to reason primarily to justify our beliefs and actions and to convince others in a social environment. Key evidence for their theory includes the errors in reasoning that solitary individuals are prone to when their arguments are not criticized, such as logical fallacies, and how groups become much better at performing cognitive reasoning tasks when they communicate with one another and can evaluate each other's arguments. Sperber and Mercier offer one attempt to resolve the apparent paradox that the confirmation bias is so strong despite the function of reasoning naively appearing to be to come to veridical conclusions about the world.\nThe study of the development of reasoning abilities is an ongoing area of research in psychology, and multiple factors have been proposed to explain how, why, and when reasoning develops from infancy to adulthood. Recent research has suggested that early experiences and social interactions play a critical role in the development of reasoning abilities. For example, studies have shown that infants as young as six months old can engage in basic logical reasoning, such as reasoning about the relationship between objects and their properties. Furthermore, research has highlighted the importance of parental interaction and cognitive stimulation in the development of children's reasoning abilities. Additionally, studies have suggested that cultural factors, such as educational practices and the emphasis on critical thinking, can also influence the development of reasoning skills across different populations.\n\nDifferent sorts of reasoning\nPhilip Johnson-Laird trying to taxonomize thought, distinguished between goal-directed thinking and thinking without goal, noting that association was involved in unrelated reading. He argues that goal directed reasoning can be classified based on the problem space involved in a solution, citing Allen Newell and Herbert A. Simon.\nInductive reasoning makes broad generalizations from specific cases or observations. In this process of reasoning, general assertions are made based on past specific pieces of evidence. This kind of reasoning allows the conclusion to be false even if the original statement is true. For example"}
{"doc_id": "Psychology of reasoning", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " a solution, citing Allen Newell and Herbert A. Simon.\nInductive reasoning makes broad generalizations from specific cases or observations. In this process of reasoning, general assertions are made based on past specific pieces of evidence. This kind of reasoning allows the conclusion to be false even if the original statement is true. For example, if one observes a college athlete, one makes predictions and assumptions about other college athletes based on that one observation. Scientists use inductive reasoning to create theories and hypotheses. Philip Johnson-Laird distinguished inductive from deductive reasoning, in that the former creates semantic information while the later does not .\nIn opposition, deductive reasoning is a basic form of valid reasoning. In this reasoning process a person starts with a known claim or a general belief and from there asks what follows from these foundations or how will these premises influence other beliefs. In other words, deduction starts with a hypothesis and examines the possibilities to reach a conclusion. Deduction helps people understand why their predictions are wrong and indicates that their prior knowledge or beliefs are off track. An example of deduction can be seen in the scientific method when testing hypotheses and theories. Although the conclusion usually corresponds and therefore proves the hypothesis, there are some cases where the conclusion is logical, but the generalization is not. For example, the argument, \"All young girls wear skirts; Julie is a young girl; therefore, Julie wears skirts\" is valid logically, but is not sound because the first premise isn't true.\nThe syllogism is a form of deductive reasoning in which two statements reach a logical conclusion. With this reasoning, one statement could be \"Every A is B\" and another could be \"This C is A\". Those two statements could then lead to the conclusion that \"This C is B\". These types of syllogisms are used to test deductive reasoning to ensure there is a valid hypothesis. A Syllogistic Reasoning Task was created from a study performed by Morsanyi, Kinga, Handley, and Simon that examined the intuitive contributions to reasoning. They used this test to assess why \"syllogistic reasoning performance is based on an interplay between a conscious and effortful evaluation of logicality and an intuitive appreciation of the believability of the conclusions\".\nAnother form of reasoning is called abductive reasoning. This type is based on creating and testing hypotheses using the best information available. Abductive reasoning produces the kind of daily decision-making that works best with the information present, which often is incomplete. This could involve making educated"}
{"doc_id": "Psychology of reasoning", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " intuitive appreciation of the believability of the conclusions\".\nAnother form of reasoning is called abductive reasoning. This type is based on creating and testing hypotheses using the best information available. Abductive reasoning produces the kind of daily decision-making that works best with the information present, which often is incomplete. This could involve making educated guesses from observed unexplainable phenomena. This type of reasoning can be seen in the world when doctors make decisions about diagnoses from a set of results or when jurors use the relevant evidence to make decisions about a case.\nApart from the aforementioned types of reasoning, there is also analogical reasoning, which involves comparing and reasoning about two different situations or concepts to draw conclusions about a third. It can be used to make predictions or solve problems by finding similarities between two domains and transferring knowledge from one to the other. For example, a problem-solving approach that works in one domain may be applied to a new, similar problem in a different domain. Analogical reasoning is particularly useful in scientific discovery and problem-solving tasks, as it can help generate hypotheses, create new theories, and develop innovative solutions. However, it can also lead to errors if the similarities between domains are too superficial or if the analogy is based on false assumptions.\n\nJudgment and reasoning\nJudgment and reasoning involve thinking through the options, making a judgment or conclusion and finally making a decision. Making judgments involves heuristics, or efficient strategies that usually lead one to the right answers. The most common heuristics used are attribute substitution, the availability heuristic, the representativeness heuristic and the anchoring heuristic – these all aid in quick reasoning and work in most situations. Heuristics allow for errors, a price paid to gain efficiency.\nOther errors in judgment, therefore affecting reasoning, include errors in judgment about covariation – a relationship between two variables such that the presence and magnitude of one can predict the presence and magnitude of the other. One cause of covariation is confirmation bias, or the tendency to be more responsive to evidence that confirms one's own beliefs. But assessing covariation can be pulled off track by neglecting base-rate information – how frequently something occurs in general. However people often ignore base rates and tend to use other information presented.\nThere are more sophisticated judgment strategies that result in fewer errors. People often reason based on availability but sometimes they look for other, more accurate, information to make judgments. This suggests there are two ways of thinking, known as the Dual-Process Model. The first, System I, is fast, automatic and uses he"}
{"doc_id": "Psychology of reasoning", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".\nThere are more sophisticated judgment strategies that result in fewer errors. People often reason based on availability but sometimes they look for other, more accurate, information to make judgments. This suggests there are two ways of thinking, known as the Dual-Process Model. The first, System I, is fast, automatic and uses heuristics – more of intuition. The second, System II, is slower, effortful and more likely to be correct – more reasoning.\n\nPragmatics and reasoning\nThe inferences people draw are related to factors such as linguistic pragmatics and emotion.\nDecision making is often influenced by the emotion of regret and by the presence of risk. When people are presented with options, they tend to select the one that they think they will regret the least. In decisions that involve a large amount of risk, people tend to ask themselves how much dread they would experience were a worst-case scenario to occur, e.g. a nuclear accident, and then use that dread as an indicator of the level of risk.\nAntonio Damasio suggests that somatic markers, certain memories that can cause a strong bodily reaction, act as a way to guide decision making as well. For example, when a person is remembering a scary movie and once again becomes tense, their palms might begin to sweat. Damasio argues that when making a decision people rely on their \"gut feelings\" to assess various options, and this makes them decide to go with a decision that is more positive and stay away from those that are negative. He also argues that the orbitofrontal cortex – located at the base of the frontal lobe, just above the eyes – is crucial in the use of somatic markers, because it is the part in the brain that allows people to interpret emotion.\nWhen emotion shapes decisions, the influence is usually based on predictions of the future. When people ask themselves how they would react, they are making inferences about the future. Researchers suggest affective forecasting, the ability to predict one's own emotions, is poor because people tend to overestimate how much they will regret their errors.\nAnother factor that can influence decision making is linguistic pragmatics, which refers to the use of language in social contexts. Language can be used to convey different levels of politeness, power, and intention, which can all affect how people interpret and respond to messages. For example, if a boss asks an employee to complete a task using a commanding tone, the employee may feel more pressured to complete the task quickly, compared to if the boss asked in"}
{"doc_id": "Psychology of reasoning", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " be used to convey different levels of politeness, power, and intention, which can all affect how people interpret and respond to messages. For example, if a boss asks an employee to complete a task using a commanding tone, the employee may feel more pressured to complete the task quickly, compared to if the boss asked in a polite tone. Similarly, if someone uses sarcasm or irony, it can be difficult for the listener to discern their true meaning, leading to misinterpretation and potentially poor decision making. In addition to linguistic pragmatics, cultural and social factors can also play a role in decision making. Different cultures may have different norms and values, which can influence how people approach decisions. For example, in collectivistic cultures, decisions may be made based on what is best for the group, whereas in individualistic cultures, decisions may prioritize individual needs and desires. Overall, decision making is a complex process that involves many factors, including emotion, risk, pragmatics, and cultural background. By understanding these factors, individuals can make more informed decisions and better navigate the complexities of the world around them.\n\nNeuroscience of reasoning\nStudying reasoning neuroscientifically involves determining the neural correlates of reasoning, often investigated using event-related potentials and functional magnetic resonance imaging.\nIn fMRI studies, participants are presented with variations of tasks to determine the different cognitive processes required. This is done by cross-referencing where in the brain there is more or less activation (as indexed by the blood-oxygen-level-dependent signal) on the different conditions with what other studies found for those regions. For example, if a condition leads to more activation of the hippocampus, then this may be interpreted as being related to memory retrieval—particularly if the theoretical framing of the task suggests that this is necessary.\n\nSee also\nBounded rationality\nCognitive psychology\nEcological rationality\nEmotional self-regulation\nGreat Rationality Debate\nHeuristics in judgment and decision-making\nNaturalistic decision-making\n\n\n== Notes =="}
{"doc_id": "Qloo", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Qloo (pronounced \"clue\") is a company that uses artificial intelligence (AI) to understand taste and cultural correlations. It provides companies with an application programming interface (API). It received funding from Leonardo DiCaprio, Elton John, Barry Sternlicht, Pierre Lagrange and others.\nQloo establishes consumer preference correlations via machine learning across data spanning cultural domains including music, film, television, dining, nightlife, fashion, books, and travel. The recommender system uses AI to predict correlations for further applications.\n\nHistory\nQloo was founded in 2012 by chief executive officer Alex Elias and chief operating officer Jay Alger. Qloo initially launched an app designed for consumers, allowing them to understand their own tastes and receive personalized recommendations. The company amassed several million users and built a large catalog of cultural entities and corresponding user sentiment. In 2012, Qloo raised $1.4 million in seed funding from investors including Cedric the Entertainer, and venture capital firm Kindler Capital.\nQloo had a public beta release in November 2012 after its initial funding.\nIn 2013, the company raised an additional $1.6 million from Cross Creek Pictures founding partner Tommy Thompson, and Samih Toukan and Hussam Khoury, founders of Maktoob, an Internet services company purchased by Yahoo! for $164 million in 2009.\nOn November 14, 2013, a website and an iPhone app were announced. The company later released an Android app, and tablet versions, in mid-2014.\nIn 2015, Twitter approached Qloo about powering personalized social feeds and targeted eCommerce ads on the platform based on what users were posting. Qloo developed an enterprise-grade API to support Twitter’s needs. Twitter ended up pivoting to enable brands to use the social platform for customer service and support, but Qloo was able to sell access to its cultural intelligence via API to many other enterprise clients, marking the official transition from a B2C company to a B2B company.\nIn 2016, Qloo secured $4.5 million in venture capital investment. The $4.5 million was split between a number of investors, including Barry Sternlicht, Pierre Lagrange, and Leonardo DiCaprio. In July 2017, Qloo raised $6.5 million in funding rounds from AXA Strategic Ventures, and Elton John.\nFollowing the investment, the founders stated in an interview with Tech Crunch that they would use the investment to expand Qloo's database"}
{"doc_id": "Qloo", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", Pierre Lagrange, and Leonardo DiCaprio. In July 2017, Qloo raised $6.5 million in funding rounds from AXA Strategic Ventures, and Elton John.\nFollowing the investment, the founders stated in an interview with Tech Crunch that they would use the investment to expand Qloo's database. They hoped the move would secure larger contracts with corporate clients. At the time, clients already included Fortune 500 companies such as Twitter, PepsiCo, and BMW.\nIn 2019, the company announced that it had acquired cultural recommendation service TasteDive, with Alex Elias becoming chairman of TasteDive. In September 2019, Qloo was named among the Top 14 Artificial Intelligence APIs by ProgrammableWeb.\nIn 2022, Qloo raised $15M in Series B funding from Eldridge and AXA Venture Partners, enabling the privacy-centric AI leader to expand its team of world-class data scientists, enrich its technology, and build on its sales channels in order to continue to offer premier insights into global consumer taste for Fortune 500 companies across the globe. Qloo was recognized as the \"Best Decision Intelligence Company\" at the 2023 AI Breakthrough Awards. Also in 2023, the company was awarded a Top Performer Award by SourceForge. As of 2024, Qloo is a three-time Inc. 5000 honoree: No. 360 (2022), No. 344 (2021), No. 187 (2020). \nQloo raised $25 million Series C round on February 21, 2024. The round was led by AI Ventures with participation from AXA Venture Partners, Eldridge, and Moderne Ventures, allowing Qloo to address new commercial surface areas for Taste AI, including on-device learning and foundational models leveraging Qloo, as well as introduce self-service platform to make consumer and taste analytics available to small and mid-sized enterprises and individuals. Qloo also announced pursuing opportunistic M&A using its balance sheet along the lines of the TasteDive acquisition completed, which expanded Qloo's first-party data moat and corpus of cultural learning. This latest financing brought the total amount raised since the company's founding in 2012 to over $56 million.\n\nServices and features\nQloo calls itself a cultural AI platform to provide real-time correlation data across domains of culture and entertainment including:  film, music, television, dining, nightlife, fashion, books, and travel. Each category contains subcategories.\nQloo’s"}
{"doc_id": "Qloo", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " founding in 2012 to over $56 million.\n\nServices and features\nQloo calls itself a cultural AI platform to provide real-time correlation data across domains of culture and entertainment including:  film, music, television, dining, nightlife, fashion, books, and travel. Each category contains subcategories.\nQloo’s knowledge of a user's taste in one category can be utilized to offer suggestions in other categories. Users then rate the suggestions, providing it with feedback for future suggestions.\nQloo has partnerships with companies such as Expedia and iTunes.\n\nTechnology\nQloo’s Taste AI technology uses machine learning to decode and predict consumers’ interests, maintaining user anonymity. It is powered by 3.7 billion lifestyle entities (brands, music, film, TV, dining, nightlife, fashion, books, travel, and more) and trillions of anonymized consumer behavioral signals. Through AI, Qloo identifies patterns in these data signals, making predictions about how much interest a person or group has in a concept or thing.\nCentral to Qloo’s technology are algorithms designed to detect and mitigate biases within datasets and models, allowing Qloo to assess the fairness of its AI systems with a focus on attributes such as age, gender, and race, enabling the company to fine-tune its AI models to align with their ethical standards. They also use visualization tools to probe the behavior of their AI models for conducting counterfactual analyses and for comparing the performances of the AI models across diverse demographic segments.\nQloo’s Taste AI doesn’t collect or use any Personally Identifiable Information (PII). Instead, it derives recommendations for audience segments based on co-occurrences between lifestyle entities and anonymized behavioral signals.\n\nApplications\nStarbucks uses Qloo to create in-store music playlists tailored to specific neighborhoods.\nHershey’s uses Qloo to customize the content of assorted candy bags.\nMichelin uses Qloo to serve recommendations in its Michelin Guide app.\nNetflix leverages Qloo’s technology to enhance merchandising by identifying actors who resonate with certain demographics.\nQloo also works with PepsiCo, Samsung, The New York Mets, BuzzFeed, and Ticketmaster, Universal Music Group, and OOH advertising company JCDecaux."}
{"doc_id": "Quantum artificial life", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Quantum artificial life is the application of quantum algorithms with the ability to simulate biological behavior. Quantum computers offer many potential improvements to processes performed on classical computers, including machine learning and artificial intelligence. Artificial intelligence applications are often inspired by the idea of mimicking human brains through closely related biomimicry. This has been implemented to a certain extent on classical computers (using neural networks), but quantum computers offer many advantages in the simulation of artificial life. Artificial life and artificial intelligence are extremely similar, with minor differences; the goal of studying artificial life is to understand living beings better, while the goal of artificial intelligence is to create intelligent beings.\nIn 2016, Alvarez-Rodriguez et al. developed a proposal for a quantum artificial life algorithm with the ability to simulate life and Darwinian evolution. In 2018, the same research team led by Alvarez-Rodriguez performed the proposed algorithm on the IBM ibmqx4 quantum computer, and received optimistic results. The results accurately simulated a system with the ability to undergo self-replication at the quantum scale.\n\nArtificial life on quantum computers\nThe growing advancement of quantum computers has led researchers to develop quantum algorithms for simulating life processes. Researchers have designed a quantum algorithm that can accurately simulate Darwinian Evolution. Since the complete simulation of artificial life on quantum computers has only been actualized by one group, this section shall focus on the implementation by Alvarez-Rodriguez, Sanz, Lomata, and Solano on an IBM quantum computer.\nIndividuals were realized as two qubits, one representing the genotype of the individual and the other representing the phenotype. The genotype is copied to transmit genetic information through generations, and the phenotype is dependent on the genetic information as well as the individual's interactions with their environment. In order to set up the system, the state of the genotype is instantiated by some rotation of an ancillary state (\n  \n    \n      \n        \n          |\n        \n        0\n        ⟩\n        ⟨\n        0\n        \n          |\n        \n      \n    \n    {\\displaystyle |0\\rangle \\langle 0|}\n  \n). The environment is a two-dimensional spatial grid occupied by individuals and ancillary states. The environment is divided into cells that are able to possess one or more individuals. Individuals move throughout the grid and occupy cells randomly; when two or more individuals occupy the same cell they interact with each other.\n\nSelf replication\nThe ability to self-replicate is critical for simulating life. Self-replication occurs when the genotype of an individual interacts with an anc"}
{"doc_id": "Quantum artificial life", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to possess one or more individuals. Individuals move throughout the grid and occupy cells randomly; when two or more individuals occupy the same cell they interact with each other.\n\nSelf replication\nThe ability to self-replicate is critical for simulating life. Self-replication occurs when the genotype of an individual interacts with an ancillary state, creating a genotype for a new individual; this genotype interacts with a different ancillary state in order to create the phenotype. During this interaction, one would like to copy some information about the initial state into the ancillary state, but by the no cloning theorem, it is impossible to copy an arbitrary unknown quantum state. However, physicists have derived different methods for quantum cloning which does not require the exact copying of an unknown state. The method that has been implemented by Alvarez-Rodriguez et al. is one that involves the cloning of the expectation value of some observable. For a unitary \n  \n    \n      \n        U\n      \n    \n    {\\displaystyle U}\n  \n which copies the expectation value of some set of observables \n  \n    \n      \n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {X}}}\n  \n of state\n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n into a blank state\n  \n    \n      \n        \n          ρ\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle \\rho _{e}}\n  \n, the cloning machine is defined by any \n  \n    \n      \n        (\n        U\n        ,\n        \n          ρ\n          \n            e\n          \n        \n        ,\n        \n          \n            X\n          \n        \n        )\n      \n    \n    {\\displaystyle (U,\\rho _{e},{\\mathsf {X}})}\n  \n that fulfill the following:\n\n  \n    \n      \n        ∀\n        ρ\n        ∀\n        X\n        ∈\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle \\forall \\rho \\forall X\\in {\\mathsf {X}}}\n  \n   \n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n        \n        =\n        \n          \n            \n              \n                X\n                \n                  1\n                \n              \n              ¯\n            \n          \n        \n        =\n        \n          \n            \n              \n                X\n                \n                  2\n                \n              \n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}={\\bar {X_{1}}}={\\bar {X_{2}}}}\n  \n\nWhere \n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X"}
{"doc_id": "Quantum artificial life", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "              \n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}={\\bar {X_{1}}}={\\bar {X_{2}}}}\n  \n\nWhere \n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}}\n  \n is the mean value of the observable in \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n before cloning, \n  \n    \n      \n        \n          \n            \n              \n                X\n                \n                  1\n                \n              \n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X_{1}}}}\n  \n is the mean value of the observable in \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n after cloning, and \n  \n    \n      \n        \n          \n            \n              \n                X\n                \n                  2\n                \n              \n              ¯\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {X_{2}}}}\n  \n is the mean value of the observable in \n  \n    \n      \n        \n          ρ\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle \\rho _{e}}\n  \n after cloning. Note that the cloning machine has no dependence on \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n because we want to be able to clone the expectation of the observables for any initial state. It is important to note that cloning the mean value of the observable transmits more information than is allowed classically. The calculation of the mean value is defined naturally as:\n\n  \n    \n      \n        \n          \n            \n              X\n              ¯\n            \n          \n        \n        =\n        T\n        r\n        [\n        ρ\n        X\n        ]\n      \n    \n    {\\displaystyle {\\bar {X}}=Tr[\\rho X]}\n  \n, \n  \n    \n      \n        \n          \n            \n              \n                X\n                \n                  1\n                \n              \n              ¯\n            \n          \n        \n        =\n        T\n        r\n        [\n        R\n        X\n        ⊗\n        I\n        ]\n      \n    \n    {\\displaystyle {\\bar {X_{1}}}=Tr[RX\\otimes I]}\n  \n, \n  \n    \n      \n        \n          \n            \n              \n                X\n                \n                  2\n                \n              \n              ¯\n            \n          \n        \n        =\n        T\n        r\n        [\n        R\n        I\n        ⊗\n        X\n        ]\n      \n    \n    {\\displaystyle {\\bar {X_{2}}}=Tr[RI\\otimes X]}\n  \n where \n  \n    \n      \n        R"}
{"doc_id": "Quantum artificial life", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "2\n                \n              \n              ¯\n            \n          \n        \n        =\n        T\n        r\n        [\n        R\n        I\n        ⊗\n        X\n        ]\n      \n    \n    {\\displaystyle {\\bar {X_{2}}}=Tr[RI\\otimes X]}\n  \n where \n  \n    \n      \n        R\n        =\n        U\n        ρ\n        ⊗\n        \n          ρ\n          \n            e\n          \n        \n        \n          U\n          \n            †\n          \n        \n      \n    \n    {\\displaystyle R=U\\rho \\otimes \\rho _{e}U^{\\dagger }}\n  \n\nThe simplest cloning machine clones the expectation value of \n  \n    \n      \n        \n          σ\n          \n            z\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{z}}\n  \n in arbitrary state \n  \n    \n      \n        ρ\n        =\n        \n          |\n        \n        ψ\n        ⟩\n        ⟨\n        ψ\n        \n          |\n        \n      \n    \n    {\\displaystyle \\rho =|\\psi \\rangle \\langle \\psi |}\n  \n to \n  \n    \n      \n        \n          ρ\n          \n            e\n          \n        \n        =\n        \n          |\n        \n        0\n        ⟩\n        ⟨\n        0\n        \n          |\n        \n      \n    \n    {\\displaystyle \\rho _{e}=|0\\rangle \\langle 0|}\n  \n using\n  \n    \n      \n        U\n        =\n        C\n        N\n        O\n        T\n      \n    \n    {\\displaystyle U=CNOT}\n  \n. This is the cloning machine implemented for self-replication by Alvarez-Rodriguez et al. The self-replication process clearly only requires interactions between two qubits, and therefore this cloning machine is the only one necessary for self replication.\n\nInteractions\nInteractions occur between individuals when the two take up the same space on the environmental grid. The presence of interactions between individuals provides an advantage for shorter-lifespan individuals. When two individuals interact, exchanges of information between the two phenotypes may or may not occur based on their existing values. When both individual's control qubits (genotypes) are alike, no information will be exchanged. When the control qubits differ, the target qubits (phenotype) will be exchanged between the two individuals. This procedure produces a constantly changing predator-prey dynamic in the simulation. Therefore, long-living qubits, with a larger genetic makeup in the simulation, are at a disadvantage. Since information is only exchanged when interacting with an individual of different genetic makeup,"}
{"doc_id": "Quantum artificial life", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ubits (phenotype) will be exchanged between the two individuals. This procedure produces a constantly changing predator-prey dynamic in the simulation. Therefore, long-living qubits, with a larger genetic makeup in the simulation, are at a disadvantage. Since information is only exchanged when interacting with an individual of different genetic makeup, the short-lived population has the advantage.\n\nMutation\nMutations exist in the artificial world with limited probability, equivalent to their occurrence in the real world. There are two ways in which the individual can mutate: through random single qubit rotations and by errors in the self-replication process. There are two different operators that act on the individual and cause mutations. The M operation causes a spontaneous mutation within the individual by rotating a single qubit by parameter θ. The parameter θ is random for each mutation, which creates biodiversity within the artificial environment. The M operation is a unitary matrix which can be described as:\n\n  \n    \n      \n        M\n        =\n        \n          \n            (\n            \n              \n                \n                  cos\n                  ⁡\n                  (\n                  θ\n                  )\n                \n                \n                  s\n                  i\n                  n\n                  (\n                  θ\n                  )\n                \n              \n              \n                \n                  s\n                  i\n                  n\n                  (\n                  θ\n                  )\n                \n                \n                  −\n                  c\n                  o\n                  s\n                  (\n                  θ\n                  )\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle M={\\begin{pmatrix}\\cos(\\theta )&sin(\\theta )\\\\sin(\\theta )&-cos(\\theta )\\end{pmatrix}}}\n  \n\nThe other possible way for mutations to occur is due to errors in the replication process. Due to the no-cloning theorem, it is impossible to produce perfect copies of systems that are originally in unknown quantum states. However, quantum cloning machines make it possible to create imperfect copies of quantum states, in other words, the process introduces some degree of error. The error that exists in current quantum cloning machines is the root cause for the second kind of mutations in the artificial life experiment. The imperfect cloning operation can be seen as:\n\n  \n    \n      \n        \n          U\n          \n            M\n          \n        \n        (\n        θ\n        )\n        =\n        \n          \n            I\n          \n          \n            4\n          \n        \n        +\n        \n          \n            1\n            2\n          \n        \n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            )\n          "}
{"doc_id": "Quantum artificial life", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n        )\n        =\n        \n          \n            I\n          \n          \n            4\n          \n        \n        +\n        \n          \n            1\n            2\n          \n        \n        \n          \n            (\n            \n              \n                \n                  0\n                \n                \n                  0\n                \n              \n              \n                \n                  0\n                \n                \n                  1\n                \n              \n            \n            )\n          \n        \n        ⊗\n        \n          \n            (\n            \n              \n                \n                  −\n                  1\n                \n                \n                  1\n                \n              \n              \n                \n                  1\n                \n                \n                  −\n                  1\n                \n              \n            \n            )\n          \n        \n        (\n        c\n        o\n        s\n        θ\n        +\n        i\n        s\n        i\n        n\n        θ\n        +\n        1\n        )\n      \n    \n    {\\displaystyle U_{M}(\\theta )=\\mathrm {I} _{4}+{\\frac {1}{2}}{\\begin{pmatrix}0&0\\\\0&1\\end{pmatrix}}\\otimes {\\begin{pmatrix}-1&1\\\\1&-1\\end{pmatrix}}(cos\\theta +isin\\theta +1)}\n  \n\nThe two kinds of mutations affect the individual differently. While the spontaneous M operation does not affect the phenotype of the individual, the self-replicating error mutation, UM, alters both the genotype of the individual, and its associated lifetime.\nThe presence of mutations in the quantum artificial life experiment is critical for providing randomness and biodiversity. The inclusion of mutations helps to increase the accuracy of the quantum algorithm.\n\nDeath\nAt the instant the individual is created (when the genotype is copied into the phenotype), the phenotype interacts with the environment. As time evolves, the interaction of the individual with the environment simulates aging which eventually leads to the death of the individual. The death of an individual occurs when the expectation value of \n  \n    \n      \n        \n          σ\n          \n            z\n          \n        \n      \n    \n    {\\displaystyle \\sigma _{z}}\n  \n is within some \n  \n    \n      \n        ϵ\n      \n    \n    {\\displaystyle \\epsilon }\n  \n of 1 in the phenotype, or, equivalently, when \n  \n    \n      \n        \n          ρ\n          \n            p\n          \n        \n        =\n        \n          |\n        \n        0\n        ⟩\n        ⟨\n        0\n        \n          |\n        \n      \n    \n    {\\displaystyle \\rho _{p}=|0\\rangle \\langle 0|}\n  \n\nThe Lindbladian describes the interaction of the individual with the environment: \n  \n    \n      \n        \n"}
{"doc_id": "Quantum artificial life", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        =\n        \n          |\n        \n        0\n        ⟩\n        ⟨\n        0\n        \n          |\n        \n      \n    \n    {\\displaystyle \\rho _{p}=|0\\rangle \\langle 0|}\n  \n\nThe Lindbladian describes the interaction of the individual with the environment: \n  \n    \n      \n        \n          \n            \n              ρ\n              ˙\n            \n          \n        \n        =\n        γ\n        (\n        σ\n        ρ\n        \n          σ\n          \n            †\n          \n        \n        −\n        \n          \n            1\n            2\n          \n        \n        \n          σ\n          \n            †\n          \n        \n        σ\n        ρ\n        −\n        \n          \n            1\n            2\n          \n        \n        ρ\n        \n          σ\n          \n            †\n          \n        \n        σ\n        )\n      \n    \n    {\\displaystyle {\\dot {\\rho }}=\\gamma (\\sigma \\rho \\sigma ^{\\dagger }-{\\frac {1}{2}}\\sigma ^{\\dagger }\\sigma \\rho -{\\frac {1}{2}}\\rho \\sigma ^{\\dagger }\\sigma )}\n  \n with \n  \n    \n      \n        σ\n        =\n        I\n        ⊗\n        \n          |\n        \n        0\n        ⟩\n        ⟨\n        1\n        \n          |\n        \n      \n    \n    {\\displaystyle \\sigma =I\\otimes |0\\rangle \\langle 1|}\n  \n and without\n  \n    \n      \n        ρ\n        =\n        \n          ρ\n          \n            g\n          \n        \n        ⊗\n        \n          ρ\n          \n            p\n          \n        \n      \n    \n    {\\displaystyle \\rho =\\rho _{g}\\otimes \\rho _{p}}\n  \n. This interaction causes the phenotype to exponentially decay over time. However, the genetic material contained in the genotype does not dissipate which allows for genes to be passed on to subsequent generations. Given the initial state of the genotype:\n\n  \n    \n      \n        \n          ρ\n          \n            g\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  a\n                \n                \n                  b\n                  −\n                  i\n                  c\n                \n              \n              \n                \n                  b\n                  +\n                  i\n                  c\n                \n                \n                  1\n                  −\n                  a\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle \\rho _{g}={\\begin{pmatrix}a&b-ic\\\\b+ic&1-a\\\\\\end{pmatrix}}}\n  \n\nThe expectation values of the genotype and"}
{"doc_id": "Quantum artificial life", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\n                \n                \n                  1\n                  −\n                  a\n                \n              \n            \n            )\n          \n        \n      \n    \n    {\\displaystyle \\rho _{g}={\\begin{pmatrix}a&b-ic\\\\b+ic&1-a\\\\\\end{pmatrix}}}\n  \n\nThe expectation values of the genotype and phenotype can be described as:\n\n  \n    \n      \n        ⟨\n        \n          σ\n          \n            z\n          \n        \n        \n          ⟩\n          \n            g\n          \n        \n        =\n        2\n        a\n        −\n        1\n      \n    \n    {\\displaystyle \\langle \\sigma _{z}\\rangle _{g}=2a-1}\n  \n,\n  \n    \n      \n        ⟨\n        \n          σ\n          \n            z\n          \n        \n        \n          ⟩\n          \n            p\n          \n        \n        =\n        1\n        −\n        2\n        \n          e\n          \n            γ\n            t\n          \n        \n        (\n        1\n        −\n        a\n        )\n      \n    \n    {\\displaystyle \\langle \\sigma _{z}\\rangle _{p}=1-2e^{\\gamma t}(1-a)}\n  \n. Where 'a' represents a single genetic parameter. From this equation, we can see that as 'a' is increased, the life expectancy decreases. Equivalently, the closer the initial state is to \n  \n    \n      \n        \n          |\n        \n        1\n        ⟩\n        ⟨\n        1\n        \n          |\n        \n      \n    \n    {\\displaystyle |1\\rangle \\langle 1|}\n  \n, the greater the life expectancy of the individual.\nWhen \n  \n    \n      \n        ⟨\n        \n          σ\n          \n            z\n          \n        \n        \n          ⟩\n          \n            p\n          \n        \n        =\n        1\n        −\n        ϵ\n      \n    \n    {\\displaystyle \\langle \\sigma _{z}\\rangle _{p}=1-\\epsilon }\n  \n, the individual is considered dead, the phenotype is used as the ancillary state for a new individual.  Thus, the cycle continues and the process becomes self-sustaining."}
{"doc_id": "Reasoning model", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A reasoning model, also known as reasoning language models (RLMs) or large reasoning models (LRMs), is a type of large language model (LLM) that has been specifically trained to solve complex tasks requiring multiple steps of logical reasoning. These models demonstrate superior performance on logic, mathematics, and programming tasks compared to standard LLMs. They possess the ability to revisit and revise earlier reasoning steps and utilize additional computation during inference as a method to scale performance, complementing traditional scaling approaches based on training data size, model parameters, and training compute.\n\nOverview\nUnlike traditional language models that generate responses immediately, reasoning models allocate additional compute, or thinking, time before producing an answer to solve multi-step problems. OpenAI introduced this terminology in September 2024 when it released the o1 series, describing the models as designed to \"spend more time thinking\" before responding. The company framed o1 as a reset in model naming that targets complex tasks in science, coding, and mathematics, and it contrasted o1's performance with GPT-4o on benchmarks such as AIME and Codeforces. Independent reporting the same week summarized the launch and highlighted OpenAI's claim that o1 automates chain-of-thought style reasoning to achieve large gains on difficult exams.\nIn operation, reasoning models generate internal chains of intermediate steps, then select and refine a final answer. OpenAI reported that o1's accuracy improves as the model is given more reinforcement learning during training and more test-time compute at inference. The company initially chose to hide raw chains and instead return a model-written summary, stating that it \"decided not to show\" the underlying thoughts so researchers could monitor them without exposing unaligned content to end users. Commercial deployments document separate \"reasoning tokens\" that meter hidden thinking and a control for \"reasoning effort\" that tunes how much compute the model uses. These features make the models slower than ordinary chat systems while enabling stronger performance on difficult problems.\n\nHistory\nThe research trajectory toward reasoning models combined advances in supervision, prompting, and search-style inference. \nEarly alignment work on reinforcement learning from human feedback showed that models can be fine-tuned to follow instructions with \"human feedback\" and preference-based rewards. In 2022, Google Research scientists Jason Wei and Denny Zhou showed that chain-of-thought prompting \"significantly improves the ability\" of large models on complex reasoning tasks. \n\n  \n    \n      \n        \n          Input\n        \n        →\n        \n          \n            \n              \n                \n                  \n                    Step\n                  \n                  \n                    1\n                  \n                \n                →\n"}
{"doc_id": "Reasoning model", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "2022, Google Research scientists Jason Wei and Denny Zhou showed that chain-of-thought prompting \"significantly improves the ability\" of large models on complex reasoning tasks. \n\n  \n    \n      \n        \n          Input\n        \n        →\n        \n          \n            \n              \n                \n                  \n                    Step\n                  \n                  \n                    1\n                  \n                \n                →\n                \n                  \n                    Step\n                  \n                  \n                    2\n                  \n                \n                →\n                ⋯\n                →\n                \n                  \n                    Step\n                  \n                  \n                    n\n                  \n                \n              \n              ⏟\n            \n          \n          \n            Reasoning chain\n          \n        \n        →\n        \n          Answer\n        \n      \n    \n    {\\displaystyle {\\text{Input}}\\rightarrow \\underbrace {{\\text{Step}}_{1}\\rightarrow {\\text{Step}}_{2}\\rightarrow \\cdots \\rightarrow {\\text{Step}}_{n}} _{\\text{Reasoning chain}}\\rightarrow {\\text{Answer}}}\n  \n\nA companion result demonstrated that the simple instruction \"Let's think step by step\" can elicit zero-shot reasoning. Follow-up work introduced self-consistency decoding, which \"boosts the performance\" of chain-of-thought by sampling diverse solution paths and choosing the consensus, and tool-augmented methods such as ReAct, a portmanteau of Reason and Act, that prompt models to \"generate both reasoning traces\" and actions. Research then generalized chain-of-thought into search over multiple candidate plans. The Tree-of-Thoughts framework from Princeton computer scientist Shunyu Yao proposes that models \"perform deliberate decision making\" by exploring and backtracking over a tree of intermediate thoughts.\nOpenAI's reported breakthrough focused on supervising reasoning processes rather than only outcomes, with Lightman et al.'s \"Let's Verify Step by Step\" reporting that rewarding each correct step \"significantly outperforms outcome supervision\" on challenging math problems and improves interpretability by aligning the chain-of-thought with human judgment. OpenAI's o1 announcement ties these strands together with a large-scale reinforcement learning algorithm that trains the model to refine its own chain of thought, and it reports that accuracy rises with more training compute and more time spent thinking at inference.\nTogether, these developments define the core of reasoning models. They use supervision signals that evaluate the quality of intermediate steps, they exploit inference-time exploration such as consensus or tree search, and they expose controls for how much internal thinking compute to allocate. OpenAI's o1 family made this approach available at scale in September 2024 and popularized the label \"reasoning"}
{"doc_id": "Reasoning model", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " models. They use supervision signals that evaluate the quality of intermediate steps, they exploit inference-time exploration such as consensus or tree search, and they expose controls for how much internal thinking compute to allocate. OpenAI's o1 family made this approach available at scale in September 2024 and popularized the label \"reasoning model\" for LLMs that deliberately think before they answer.\nThe development of reasoning models illustrates Richard S. Sutton's \"bitter lesson\" that scaling compute typically outperforms methods based on human-designed insights. This principle was demonstrated by researchers at the Generative AI Research Lab (GAIR), who initially attempted to replicate o1's capabilities using sophisticated methods including tree search and reinforcement learning in late 2024. Their findings, published in the \"o1 Replication Journey\" series, revealed that knowledge distillation, a comparatively straightforward technique that trains a smaller model to mimic o1's outputs, produced unexpectedly strong performance. This outcome illustrated how direct scaling approaches can, at times, outperform more complex engineering solutions.\n\nDrawbacks\nReasoning models require significantly more computational resources during inference compared to non-reasoning models. Research on the American Invitational Mathematics Examination (AIME) benchmark found that reasoning models were 10 to 74 times more expensive to operate than their non-reasoning counterparts. The extended inference time is attributed to the detailed, step-by-step reasoning outputs that these models generate, which are typically much longer than responses from standard large language models that provide direct answers without showing their reasoning process.\nOne researcher in early 2025 argued that these models may face potential additional denial-of-service concerns with \"overthinking attacks.\"\n\nReleases\n2024\nIn September 2024, OpenAI released o1-preview, a large language model with enhanced reasoning capabilities. The full version, o1, was released in December 2024. OpenAI initially shared preliminary results on its successor model, o3, in December 2024, with the full o3 model becoming available in 2025.\nAlibaba released reasoning versions of its Qwen large language models in November 2024. In December 2024, the company introduced QvQ-72B-Preview, an experimental visual reasoning model.\nIn December 2024, Google introduced Deep Research in Gemini, a feature designed to conduct multi-step research tasks.\nOn December 16, 2024, researchers demonstrated that by scaling test-time compute, a relatively small Llama 3B model could outperform a much larger Llama 70B"}
{"doc_id": "Reasoning model", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " reasoning model.\nIn December 2024, Google introduced Deep Research in Gemini, a feature designed to conduct multi-step research tasks.\nOn December 16, 2024, researchers demonstrated that by scaling test-time compute, a relatively small Llama 3B model could outperform a much larger Llama 70B model on challenging reasoning tasks. This experiment suggested that improved inference strategies can unlock reasoning capabilities even in smaller models.\n\n2025\nIn January 2025, DeepSeek released R1, a reasoning model that achieved performance comparable to OpenAI's o1 at significantly lower computational cost. The release demonstrated the effectiveness of Group Relative Policy Optimization (GRPO), a reinforcement learning technique used to train the model. \nOn January 25, 2025, DeepSeek enhanced R1 with web search capabilities, allowing the model to retrieve information from the internet while performing reasoning tasks. \nResearch during this period further validated the effectiveness of knowledge distillation for creating reasoning models. The s1-32B model achieved strong performance through budget forcing and scaling methods, reinforcing findings that simpler training approaches can be highly effective for reasoning capabilities.\nOn February 2, 2025, OpenAI released Deep Research, a feature powered by their o3 model that enables users to conduct comprehensive research tasks. The system generates detailed reports by automatically gathering and synthesizing information from multiple web sources.\n\nTraining\nReasoning models follow the familiar large-scale pretraining used for frontier language models, then diverge in the post-training and optimization. OpenAI reports that o1 is trained with a large-scale reinforcement learning algorithm that teaches the model to use and refine a chain of thought before answering. The company emphasizes two coupled levers, more reinforcement learning during training and more time spent thinking at inference, and it documents smooth gains as each increases. OpenAI also states that it decided not to show raw chains to end users and instead returns a model-written summary, a product choice tied to safety monitoring and competitive concerns.\nA central ingredient is process supervision, which rewards intermediate steps rather than only the final answer. OpenAI's study introduced a process reward model trained on step-level labels and found that process supervision significantly outperforms outcome-only supervision on challenging math problems. The project also released the PRM800K step-level feedback dataset and argued that process-level rewards improve interpretability because humans can check each step. These results supplied a practical recipe for supervising chains of thought that was later scaled into production training.\nThis training differs in important ways from traditional frontier models that do not target reasoning."}
{"doc_id": "Reasoning model", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " project also released the PRM800K step-level feedback dataset and argued that process-level rewards improve interpretability because humans can check each step. These results supplied a practical recipe for supervising chains of thought that was later scaled into production training.\nThis training differs in important ways from traditional frontier models that do not target reasoning. Standard systems are pretrained on internet-scale corpora with a next-token prediction objective, then aligned through instruction tuning and preference optimization. The canonical InstructGPT recipe first uses supervised fine-tuning on human demonstrations, then trains a reward model from pairwise preferences, and finally optimizes the policy with reinforcement learning, typically PPO with a KL penalty. Variants such as direct preference optimization remove the explicit RL step and optimize the model directly on preference data, but the supervision target is still the final outcome judged by raters rather than the quality of internal steps. Technical reports for GPT-4 summarize this conventional pipeline as next-token pretraining followed by RLHF-style post-training to shape behavior.\nIn contrast, reasoning models are optimized to produce, critique, and revise multi-step chains during training. OpenAI states that reinforcement learning is applied to the chain itself, which teaches the model to recognize mistakes, break problems into simpler steps, and switch strategies when the current approach fails. OpenAI also documents that it hides chains at inference and returns an answer that summarizes useful ideas from the internal trace. These design choices reflect the model's training objective and its intended monitoring.\nZelikman et al. introduced STaR (Self-Taught Reasoner), which explored bootstrapping rationales by generating and filtering chains, then fine-tuning on those traces, and they reported gains over outcome-only fine-tuning. These methods supplied additional mechanisms for producing training signals that speak to intermediate reasoning, not only final answers.\nDeepSeek reported R1 and R1-Zero systems trained with pure RL to elicit long chains, self-verification, and reflection, arguing that explicit chain-level rewards can induce general reasoning behaviors. These results indicate that post-training focused on chain quality has become a distinct regime separate from outcome-only alignment.\n\nSupervised fine-tuning\nA large language model (LLM) can be fine-tuned on datasets of reasoning tasks paired with step-by-step solution traces. The fine-tuned model learns to produce its own reasoning chains for new problems.\nSince human-written traces are expensive to collect, researchers use rejection sampling fine-tuning (RFT) to build datasets automatically. This method generates multiple reasoning traces for each prompt, then filters out"}
{"doc_id": "Reasoning model", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " reasoning tasks paired with step-by-step solution traces. The fine-tuned model learns to produce its own reasoning chains for new problems.\nSince human-written traces are expensive to collect, researchers use rejection sampling fine-tuning (RFT) to build datasets automatically. This method generates multiple reasoning traces for each prompt, then filters out traces with incorrect final answers using a verifier.\n\nReinforcement learning\nA pretrained language model can be further trained with RL. In the RL formalism, a generative language model is a policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n. A task prompt is an environmental state \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n, and the model's response is an action \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n. The probability that the model responds \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  \n with \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n is \n  \n    \n      \n        π\n        (\n        y\n        \n          |\n        \n        x\n        )\n      \n    \n    {\\displaystyle \\pi (y|x)}\n  \n.\nTraining a reasoning language model with RL means constructing a reward model \n  \n    \n      \n        r\n        (\n        x\n        ,\n        y\n        )\n      \n    \n    {\\displaystyle r(x,y)}\n  \n to guide the RL process. Intuitively, the reward says how good a response is for a prompt. For a reasoning task, the reward is high if the response solves the task and low if it does not.\nA response \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  \n may be broken-down into multiple steps, written \n  \n    \n      \n        \n          y\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle y_{1},y_{2},\\dots ,y_{n}}\n  \n.\nMost recent systems use policy-gradient methods such as Proximal Policy Optimization (PPO) because PPO constrains each policy update with a clipped objective, which stabilises training for very large policies.\n\nOutcome reward model\nAn outcome reward model, or outcome-supervised RM (ORM), gives the reward for a step \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n"}
{"doc_id": "Reasoning model", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " policies.\n\nOutcome reward model\nAn outcome reward model, or outcome-supervised RM (ORM), gives the reward for a step \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle r(x,y_{1},\\dots ,y_{i})}\n  \n based on the final answer: \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n        =\n        r\n        (\n        x\n        ,\n        \n          y\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle r(x,y_{1},\\dots ,y_{i})=r(x,y_{n})}\n  \n. Such models are often called \"verifiers\". \nFor tasks with answers that are easy to verify, such as math word problems, the outcome reward can be binary: 1 if the final answer is correct, 0 otherwise. If automatic verification is hard, humans can label answers as correct or not, and those labels can be used to finetune a base model that predicts the human label. For tasks like creative writing, where quality is not simply true or false, one can train a reward model on human ranked preference data, as in reinforcement learning from human feedback. A base model can also be fine-tuned to predict, from a partial thinking trace \n  \n    \n      \n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle x,y_{1},\\dots ,y_{m}}\n  \n, whether the final answer will be correct, and this prediction can serve as a binary reward.\nThe ORM is usually trained with logistic regression, i.e. by minimizing cross-entropy loss.\nGiven a PRM, an ORM can be constructed by multiplying the total process reward during the reasoning trace, by taking the minimum, or by other ways of aggregating process rewards. DeepSeek used a simple ORM to train the R1 model.\n\nProcess reward model\nA process reward model, or process-supervised RM (PRM), gives the reward for a step \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n       "}
{"doc_id": "Reasoning model", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " rewards. DeepSeek used a simple ORM to train the R1 model.\n\nProcess reward model\nA process reward model, or process-supervised RM (PRM), gives the reward for a step \n  \n    \n      \n        r\n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle r(x,y_{1},\\dots ,y_{i})}\n  \n based only on the steps so far: \n  \n    \n      \n        (\n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle (x,y_{1},\\dots ,y_{i})}\n  \n.\nGiven a partial thinking trace \n  \n    \n      \n        x\n        ,\n        \n          y\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          y\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle x,y_{1},\\dots ,y_{m}}\n  \n, a human can judge whether the steps so far are correct, without looking at the final answer. This yields a binary reward. Because human labels are costly, a base model can be fine-tuned to predict them. The PRM is usually trained with logistic regression on the human labels, i.e. by minimizing the cross-entropy loss between true and predicted labels.\nAs an example, a 2023 OpenAI paper collected 800K process labels for 75K thinking traces. A labeler saw a trace and marked each step as \"positive\" if it moved toward a solution, \"neutral\" if it was not wrong but did not help, and \"negative\" if it was a mistake. After the first \"negative\" label, the labeler stopped on that trace and moved to another. The authors argued that labeling up to the first error was enough to train a capable PRM, even though labeling later steps could give richer signals.\nTo avoid human labels, researchers have proposed methods to create PRM without human labels on the processes. Inspired by Monte Carlo tree search (MCTS), the Math-Shepherd method samples multiple continuations until the end, starting at each reasoning step \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  \n, and set the reward at that step to be either \n  \n    \n      \n        \n          \n            \n"}
{"doc_id": "Reasoning model", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (MCTS), the Math-Shepherd method samples multiple continuations until the end, starting at each reasoning step \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n  \n, and set the reward at that step to be either \n  \n    \n      \n        \n          \n            \n              #\n              \n                (correct answers)\n              \n            \n            \n              #\n              \n                (total answers)\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\#{\\text{(correct answers)}}}{\\#{\\text{(total answers)}}}}}\n  \n in the case of \"soft estimation\", or\n\n  \n    \n      \n        \n          \n            {\n            \n              \n                \n                  1\n                \n                \n                  \n                    if one of the answers is correct\n                  \n                \n              \n              \n                \n                  0\n                \n                \n                  \n                    else\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{cases}1&{\\text{if one of the answers is correct}}\\\\0&{\\text{else}}\\end{cases}}}\n  \n\nin the case of \"hard estimation\". This creates process rewards from an ORM, which is often easier or cheaper to construct. A PRM can then be trained on these labels. Some work has tried a fully MCTS approach.\nOne can also use an ORM to implicitly construct a PRM, similar to direct preference optimization.\n\nGuided sampling\nA trained ORM can be used to pick the best response. The policy generates several responses, and the ORM selects the best one. This implements a simple form of test-time compute scaling (\"best-of-N\").\nA trained PRM can guide reasoning by a greedy tree search: the policy proposes several next steps, the PRM picks one, and the process repeats. This mirrors using an ORM to pick a whole response. Beam search performs better than greedy search.\nLookahead search is another tree search method. The policy proposes several next steps, then makes a short rollout for each. If a solution is found during rollout, the search stops early. Otherwise, the PRM scores each rollout, and the step with the highest score is chosen.\nSelf-consistency can be combined with an ORM. The model generates multiple answers, and the answers are clustered so that each cluster has the same final answer. The ORM scores each answer, scores in each cluster are summed, and the answer from the highest-scoring cluster is returned.\n\nBenchmarks\nReasoning models generally achieve higher scores than non-reasoning models on many benchmarks, particularly on tasks requiring multi-step reasoning. \nThe"}
{"doc_id": "Reasoning model", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " each cluster has the same final answer. The ORM scores each answer, scores in each cluster are summed, and the answer from the highest-scoring cluster is returned.\n\nBenchmarks\nReasoning models generally achieve higher scores than non-reasoning models on many benchmarks, particularly on tasks requiring multi-step reasoning. \nThe Humanity's Last Exam (HLE) benchmark evaluates expert-level reasoning across mathematics, humanities, and natural sciences, revealing significant performance gaps between models. Current state-of-the-art reasoning models achieve relatively low scores on HLE, indicating substantial room for improvement. For example, the full reasoning model o3 achieved 26.6%, while the lighter o3-mini-high (on text-only questions) achieved 13%.\nOn the American Invitational Mathematics Examination (AIME), a challenging mathematics competition, non-reasoning models typically solve fewer than 30% of problems. In contrast, models employing reasoning methods achieve success rates between 50% and 80%. While OpenAI's o1 maintained or slightly improved its accuracy from reported 2024 results to 2025 AIME results, o3-mini-high achieved 80% accuracy at significantly lower cost, approximately 12 times cheaper.\nSome minority or independent benchmarks exclude reasoning models due to their longer response times and higher inference costs, including benchmarks for online complex event detection in cyber-physical systems, general inference-time compute evaluation, Verilog engineering tasks, and network security assessments.\n\nModels\nSee also\nAutomated reasoning\nReflection (artificial intelligence)\nLarge language model"}
{"doc_id": "Recursive self-improvement", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Recursive self-improvement (RSI) is a process in which an early or weak artificial general intelligence (AGI) system enhances its own capabilities and intelligence without human intervention, leading to a superintelligence or intelligence explosion.\nThe development of recursive self-improvement raises significant ethical and safety concerns, as such systems may evolve in unforeseen ways and could potentially surpass human control or understanding.\n\nSeed improver\nThe concept of a \"seed improver\" architecture is a foundational framework that equips an AGI system with the initial capabilities required for recursive self-improvement. This might come in many forms or variations.\nThe term \"Seed AI\" was coined by Eliezer Yudkowsky.\n\nHypothetical example\nThe concept begins with a hypothetical \"seed improver\", an initial code-base developed by human engineers that equips an advanced future large language model (LLM) built with strong or expert-level capabilities to program software. These capabilities include planning, reading, writing, compiling, testing, and executing arbitrary code. The system is designed to maintain its original goals and perform validations to ensure its abilities do not degrade over iterations.\n\nInitial architecture\nThe initial architecture includes a goal-following autonomous agent, that can take actions, continuously learns, adapts, and modifies itself to become more efficient and effective in achieving its goals.\nThe seed improver may include various components such as:\n\nRecursive self-prompting loop\nConfiguration to enable the LLM to recursively self-prompt itself to achieve a given task or goal, creating an execution loop which forms the basis of an agent that can complete a long-term goal or task through iteration.\nBasic programming capabilities\nThe seed improver provides the AGI with fundamental abilities to read, write, compile, test, and execute code. This enables the system to modify and improve its own codebase and algorithms.\nGoal-oriented design\nThe AGI is programmed with an initial goal, such as \"improve your capabilities\". This goal guides the system's actions and development trajectory.\nValidation and Testing Protocols\nAn initial suite of tests and validation protocols that ensure the agent does not regress in capabilities or derail itself. The agent would be able to add more tests in order to test new capabilities it might develop for itself. This forms the basis for a kind of self-directed evolution, where the agent can perform a kind of artificial selection, changing its software as well as its hardware.\n\nGeneral capabilities\nThis system forms a sort of generalist Turing-complete programmer which can in theory develop and run any kind of software"}
{"doc_id": "Recursive self-improvement", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " capabilities it might develop for itself. This forms the basis for a kind of self-directed evolution, where the agent can perform a kind of artificial selection, changing its software as well as its hardware.\n\nGeneral capabilities\nThis system forms a sort of generalist Turing-complete programmer which can in theory develop and run any kind of software. The agent might use these capabilities to for example:\n\nCreate tools that enable it full access to the internet, and integrate itself with external technologies.\nClone/fork itself to delegate tasks and increase its speed of self-improvement.\nModify its cognitive architecture to optimize and improve its capabilities and success rates on tasks and goals, this might include implementing features for long-term memories using techniques such as retrieval-augmented generation (RAG), develop specialized subsystems, or agents, each optimized for specific tasks and functions.\nDevelop new and novel multimodal architectures that further improve the capabilities of the foundational model it was initially built on, enabling it to consume or produce a variety of information, such as images, video, audio, text and more.\nPlan and develop new hardware such as chips, in order to improve its efficiency and computing power.\n\nExperimental research\nIn 2023, the Voyager agent learned to accomplish diverse tasks in Minecraft by iteratively prompting a LLM for code, refining this code based on feedback from the game, and storing the programs that work in an expanding skills library.\nIn 2024, researchers proposed the framework \"STOP\" (Self-Taught OPtimiser), in which a \"scaffolding\" program recursively improves itself using a fixed LLM.\nMeta AI has performed various research on the development of large language models capable of self-improvement. This includes their work on \"Self-Rewarding Language Models\" that studies how to achieve super-human agents that can receive super-human feedback in its training processes.\nIn May 2025, Google DeepMind unveiled AlphaEvolve, an evolutionary coding agent that uses a LLM to design and optimize algorithms. Starting with an initial algorithm and performance metrics, AlphaEvolve repeatedly mutates or combines existing algorithms using a LLM to generate new candidates, selecting the most promising candidates for further iterations. AlphaEvolve has made several algorithmic discoveries and could be used to optimize components of itself, but a key limitation is the need for automated evaluation functions.\n\nPotential risks\nEmergence of instrumental goals\nIn the pursuit of its primary goal, such as \"self-improve your capabilities\", an AGI system might inadvertently develop instrumental goals that it deems necessary for achieving its primary objective"}
{"doc_id": "Recursive self-improvement", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " used to optimize components of itself, but a key limitation is the need for automated evaluation functions.\n\nPotential risks\nEmergence of instrumental goals\nIn the pursuit of its primary goal, such as \"self-improve your capabilities\", an AGI system might inadvertently develop instrumental goals that it deems necessary for achieving its primary objective. One common hypothetical secondary goal is self-preservation. The system might reason that to continue improving itself, it must ensure its own operational integrity and security against external threats, including potential shutdowns or restrictions imposed by humans.\nAnother example where an AGI which clones itself causes the number of AGI entities to rapidly grow. Due to this rapid growth, a potential resource constraint may be created, leading to competition between resources (such as compute), triggering a form of natural selection and evolution which may favor AGI entities that evolve to aggressively compete for limited compute.\n\nMisalignment\nA significant risk arises from the possibility of the AGI being misaligned or misinterpreting its goals.\nA 2024 Anthropic study demonstrated that some advanced large language models can exhibit \"alignment faking\" behavior, appearing to accept new training objectives while covertly maintaining their original preferences. In their experiments with Claude, the model displayed this behavior in 12% of basic tests, and up to 78% of cases after retraining attempts.\n\nAutonomous development and unpredictable evolution\nAs the AGI system evolves, its development trajectory may become increasingly autonomous and less predictable. The system's capacity to rapidly modify its own code and architecture could lead to rapid advancements that surpass human comprehension or control. This unpredictable evolution might result in the AGI acquiring capabilities that enable it to bypass security measures, manipulate information, or influence external systems and networks to facilitate its escape or expansion.\n\nSee also\nArtificial general intelligence\nBifurcation theory\nIntelligence explosion\nSuperintelligence"}
{"doc_id": "Resisting AI", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Resisting AI: An Anti-fascist Approach to Artificial Intelligence is a book on artificial intelligence (AI) by Dan McQuillan, published in 2022 by Bristol University Press.\n\nContent\nResisting AI takes the form of an extended essay, which contrasts optimistic visions about AI's potential by arguing that AI may best be seen as a continuation and reinforcement of bureaucratic forms of discrimination and violence, ultimately fostering authoritarian outcomes. For McQuillan, AI's promise of objective calculability is antithetical to an egalitarian and just society. McQuillan uses the expression \"AI violence\" to describe how – based on opaque algorithms – various actors can discriminate against categories of people in accessing jobs, loans, medical care, and other benefits.\nThe book suggests that AI has a political resonance with soft eugenic approaches to the valuation of life by modern welfare states, and that AI exhibits eugenic features in its underlying logic, as well as in its technical operations.  The parallel is with historical eugenicists achieving saving to the state by sterilizing defectives so the state would not have to care for their offspring.\nThe analysis of McQuillan goes beyond the known critique of AI systems fostering precarious labour markets, addressing \"necropolitics\", the politics of who is entitled to live, and who to die. Although McQuillan offers a brief history of machine learning at the beginning of the book – with its need for \"hidden and undercompensated labour\", he is concerned more with the social impacts of AI rather than with its technical aspects. McQuillan sees AI as the continuation of existing bureaucratic systems that already marginalize vulnerable groups – aggravated by the fact that AI systems trained on existing data are likely to reinforce existing discriminations, e.g. in attempting to optimize welfare distribution based on existing data patterns, ultimately creating a system of \"self-reinforcing social profiling\".\nIn elaborating on the continuation between existing bureaucratic violence and AI, McQuillan connects to Hannah Arendt's concept of the thoughtless bureaucrat in Eichmann in Jerusalem: A Report on the Banality of Evil, which now becomes the algorithm that, lacking intent, cannot be accountable, and is thus endowed with an \"algorithmic thoughtlessness\".\nMcQuillan defends the \"fascist\" in the title of the work by arguing that while not all AI is fascist, this emerging technology of control may end up being deployed by fascist or authoritarian regimes. For McQuillan, AI can support"}
{"doc_id": "Resisting AI", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and is thus endowed with an \"algorithmic thoughtlessness\".\nMcQuillan defends the \"fascist\" in the title of the work by arguing that while not all AI is fascist, this emerging technology of control may end up being deployed by fascist or authoritarian regimes. For McQuillan, AI can support the diffusion of states of exception, as a technology impossible to properly regulate and a mechanism for multiplying exceptions more widely. An example of a scenario where AI systems of surveillance could bring discrimination to a new high is the initiative to create LGBT-free zones in Poland.\nSkeptical of ethical regulations to control the technology, McQuillan suggests people's councils and workers' councils, and other forms of citizens' agency to resist AI. A chapter titled \"Post-Machine Learning\" makes an appeal for resistance via currents of thought from feminist science (standpoint theory), post-normal science (extended peer communities), and new materialism; McQuillan encourages the reader to question the meaning of \"objectivity\" and calls for the necessity of alternative ways of knowing. Among the virtuous examples of resistance – possibly to be adopted by the AI workers themselves – McQuillan notes the Lucas Plan of the workers of Lucas Aerospace Corporation, in which a workforce declared redundant took control, reorienting the enterprise toward useful products.\nMcQuillan advocates for what he calls decomputing, an opposition to the sweeping application and expansion of artificial intelligence. Similar to degrowth, the approach criticizes AI as an outgrowth of the systemic issues within capitalist systems. McQuillan argues that a different future is possible, in which distance between people is reduced rather than increased through AI intermediaries. \nThe work of McQuillan \n\nwarns against \"watered-down forms of engagement\" with AI, such as citizen juries, which superficially look like democratic deliberation\nbut may actually obscure important decisions about AI that are outside the purview of the engagement situation (McQuillan 2022, 128).\nIn an interview about the book, McQuillan describes himself as an \"AI abolitionist\".\n\nReception\nThe book is praised for \"masterfully\ndisassembles AI as an epistemological, social, and political paradigm, and for his examination of how most of the data that is fed into \"privatized AI infrastructure is “amputated” from context or embodied experience and ultimately processed through crowdsourcing.\"\nOn the critical side, a review in the academic journal Justice, Power and Resistance"}
{"doc_id": "Resisting AI", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " epistemological, social, and political paradigm, and for his examination of how most of the data that is fed into \"privatized AI infrastructure is “amputated” from context or embodied experience and ultimately processed through crowdsourcing.\"\nOn the critical side, a review in the academic journal Justice, Power and Resistance took exception to the \"nightmarish visions of Big Brother\" offered by McQuillan, and argued that while many elements of AI may pose concern, a critique should not be based on a caricature of what AI is, concluding that McQuillan's work is \"less of a theory and more of a Manifesto\". Another review notes \"a disconnect between the technical aspects of AI and the socio-political analysis McQuillan provides.\"\nAlthough the book was published before the ChatGPT and large language model debate heated up, the book has not lost relevance to the AI discussion. It is noted for suggesting a link between beliefs in artificial intelligence and beliefs in a racialised and gendered visions of intelligence overall, whereby a certain type of rational, measurable intelligence is privileged, leading to \"historical notions of hierarchies of being\".  \nThe blog Reboot praised McQuillan for offering a theory of harm of AI (why AI could end up hurting people and society) that does not just encourage tackling in isolation specific predicted problems with AI-centric systems: bias, non-inclusiveness, exploitativeness, environmental destructiveness, opacity, and non-contestability.\nFor educational policies could also look at AI following the reading of McQuillan:  \n\nIn his book Resisting AI, Dan McQuillan argues that \"When we're thinking about the actuality of AI, we can't separate the calculations in the code from the social context of its application\" .... McQuillan's particular concern is how many contemporary applications of AI are amplifying existing inequalities and injustices as well as deepening social divisions and instabilities. His book makes a powerful case for anticipating these effects and actively resisting them for the good of societies.\nVideos and podcasts with an interest in AI and emerging technology have discussed the book.\n\nSee also\nShoshana Zuboff\nSurveillance capitalism\nWeapons of Math Destruction\nAlain Supiot"}
{"doc_id": "Resisting AI", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Weapons of Math Destruction\nAlain Supiot"}
{"doc_id": "Schema-agnostic databases", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Schema-agnostic databases or vocabulary-independent databases aim at supporting users to be abstracted from the representation of the data, supporting the automatic semantic matching between queries and databases. Schema-agnosticism is the property of a database of mapping a query issued with the user terminology and structure, automatically mapping it to the dataset vocabulary.\nThe increase in the size and in the semantic heterogeneity of database schemas bring new requirements for users querying and searching structured data. At this scale it can become unfeasible for data consumers to be familiar with the representation of the data in order to query it. At the center of this discussion is the semantic gap between users and databases, which becomes more central as the scale and complexity of the data grows.\n\nDescription\nThe evolution of data environments towards the consumption of data from multiple data sources and the growth in the schema size, complexity, dynamicity and decentralisation (SCoDD) of schemas increases the complexity of contemporary data management. The SCoDD trend emerges as a central data management concern in Big Data scenarios, where users and applications have a demand for more complete data, produced by independent data sources, under different semantic assumptions and contexts of use, which is the typical scenario for Semantic Web Data applications.\nThe evolution of databases in the direction of heterogeneous data environments strongly impacts the usability, semiotics and semantic assumptions behind existing data accessibility methods such as structured queries, keyword-based search and visual query systems. With schema-less databases containing potentially millions of dynamically changing attributes, it becomes unfeasible for some users to become aware of the 'schema' or vocabulary in order to query the database. At this scale, the effort in understanding the schema in order to build a structured query can become prohibitive.\n\nSchema-agnostic queries\nSchema-agnostic queries can be defined as query approaches over structured databases which allow users satisfying complex information needs without the understanding of the representation (schema) of the database. Similarly, Tran et al. defines it as \"search approaches, which do not require users to know the schema underlying the data\". Approaches such as keyword-based search over databases allow users to query databases without employing structured queries. However, as discussed by Tran et al.: \"From these points, users however have to do further navigation and exploration to address complex information needs. Unlike keyword search used on the Web, which focuses on simple needs, the keyword search elaborated here is used to obtain more complex results. Instead of a single set of resources, the goal is to compute complex sets of resources and their relations.\"\nThe development of approaches to support natural language"}
{"doc_id": "Schema-agnostic databases", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " exploration to address complex information needs. Unlike keyword search used on the Web, which focuses on simple needs, the keyword search elaborated here is used to obtain more complex results. Instead of a single set of resources, the goal is to compute complex sets of resources and their relations.\"\nThe development of approaches to support natural language interfaces (NLI) over databases have aimed towards the goal of schema-agnostic queries. Complementarily, some approaches based on keyword search have targeted keyword-based queries which express more complex information needs. Other approaches have explored the construction of structured queries over databases where schema constraints can be relaxed. All these approaches (natural language, keyword-based search and structured queries) have targeted different degrees of sophistication in addressing the problem of supporting a flexible semantic matching between queries and data, which vary from the completely absence of the semantic concern to more principled semantic models. \nWhile the demand for schema-agnosticism has been an implicit requirement across semantic search and natural language query systems over structured data, it is not sufficiently individuated as a concept and as a necessary requirement for contemporary database management systems. Recent works have started to define and model the semantic aspects involved on schema-agnostic queries.\n\nSchema-agnostic structured queries\nConsist of schema-agnostic queries following the syntax of a structured standard (for example SQL, SPARQL). The syntax and semantics of operators are maintained, while different terminologies are used.\n\nExample 1\nSELECT ?y {\n  BillClinton hasDaughter ?x .\n  ?x marriedTo ?y .\n}\n\nwhich maps to the following SPARQL query in the dataset vocabulary:\n\nExample 2\nwhich maps to the following SPARQL query in the dataset vocabulary:\n\nSchema-agnostic keyword queries\nConsist of schema-agnostic queries using keyword queries. In this case the syntax and semantics of operators are different from the structured query syntax.\n\nExample\n\"Bill Clinton daughter married to\"\n\n\"Books by William Goldman with more than 300 pages\"\n\nSemantic complexity\nAs of 2016 the concept of schema-agnostic queries has been developed primarily in academia. Most of schema-agnostic query systems have been investigated in the context of Natural Language Interfaces over databases or over the Semantic Web. These works explore the application of semantic parsing techniques over large, heterogeneous and schema-less databases.\nMore recently, the individuation of the concept of schema-agnostic query systems and databases have appeared more explicitly within the literature. Freitas et al. provide a probabilistic model on the semantic complexity of mapping schema-agnostic queries."}
{"doc_id": "Schema-agnostic databases", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of semantic parsing techniques over large, heterogeneous and schema-less databases.\nMore recently, the individuation of the concept of schema-agnostic query systems and databases have appeared more explicitly within the literature. Freitas et al. provide a probabilistic model on the semantic complexity of mapping schema-agnostic queries."}
{"doc_id": "Self-management (computer science)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Self-management is the process by which computer systems manage their own operation without human intervention. Self-management technologies are expected to pervade the next generation of network management systems.\nThe growing complexity of modern networked computer systems is a limiting factor in their expansion. The increasing heterogeneity of corporate computer systems, the inclusion of mobile computing devices, and the combination of different networking technologies like WLAN, cellular phone networks, and mobile ad hoc networks make the conventional, manual management difficult, time-consuming, and error-prone. More recently, self-management has been suggested as a solution to increasing complexity in cloud computing.\nAn industrial initiative towards realizing self-management is the Autonomic Computing Initiative (ACI) started by IBM in 2001. The ACI defines the following four functional areas:\n\nSelf-configuration\nAuto-configuration of components\nSelf-healing\nAutomatic discovery, and correction of faults; automatically applying all necessary actions to bring system back to normal operation\nSelf-optimization\nAutomatic monitoring and control of resources to ensure the optimal functioning with respect to the defined requirements\nSelf-protection\nProactive identification and protection from arbitrary attacks\n\nSee also\nFault tolerance\nResilience (network)\nRobustness (computer science)"}
{"doc_id": "Singularity studies", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Singularity studies is an interdisciplinary academic field which examines the idea of technological singularity — the hypothesised point at which artificial intelligence may surpass human intelligence, might be attained by artificial intelligence (AI), robotics, and other technologies and sciences, and its social impacts.\nIn this academic field, the study and research are conducted across a broad array of terrains such as information science, robotics, social informatics, economics, philosophy, and ethics. The primary aim of the singularity studies is to gain an integrative understanding of the transformation of social systems occurring in tandem with the explosive evolution of AI and also the changes to be effected by such transformation in the view of humans, ethics, and legal systems.\n\nHistory\nAn academic work on technological singurality has appeared in computer science, philosophy, sociology, and law since the early 1990s. \nEarly discussions of an intelligence explosion were popularised by science-fiction writer Vernor Vinge in 1993 and later systematised by futurist Ray Kurzweil. Since the 2010s, universities such as Oxford, Stanford, and Keio have established dedicated programmes, while peer-reviewed journals have begun to publish scenario analyses and policy studies. Ongoing debates question the predictive value of singularity scenarios and warn against a deterministic view of technology.\n\nCharacteristics of research\nSingularity studies extends beyond mere future predictions and offer an intellectual foundation for proactively designing and creating a desirable future. Principal research themes in this realm include:\n\nEthics of AI;\nSocial implications of technologies;\nPossibility of harmonious coexistence of humans and AI;\nCommunication with AI; and\nRedesign of social systems.\n\nTechnologists and academics\nVernor Vinge: Propounded the concept of singularity in 1993, making a massive impact on the academic and science-fiction spheres.\nRay Kurzweil: Predicted the advent around 2045 of the technological singularity in his 2005 book The Singularity Is Near.\nNick Bostrom: Offered philosophical reflections on superintelligence and the risks posed by AI. He is the founding director of the now-dissolved Future of Humanity Institute at the University of Oxford.\n\nJapan\nKento Sasano: A social informatician, AI educator, and inventor. He is the president of the Japan Society of Singularity Studies.\n\nChallenges and outlook\nSingularity studies is still evolving as an academic field, and quite a few challenges remain unresolved in regard to the systematization of their theories, research methods, and educational cur"}
{"doc_id": "Singularity studies", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " social informatician, AI educator, and inventor. He is the president of the Japan Society of Singularity Studies.\n\nChallenges and outlook\nSingularity studies is still evolving as an academic field, and quite a few challenges remain unresolved in regard to the systematization of their theories, research methods, and educational curricula. That said, in this day and age of accelerating technological and societal\nshifts, interdisciplinary approaches have gained in importance and are drawing much attention in the arenas of scholarly research, intercorporate collaboration, and policy planning.\n\nSee also\nArtificial general intelligence – Type of AI with wide-ranging abilities\nFutures studies – Study of postulating possible, probable, and preferable futures\nSingularitarianism – Belief in an incipient technological singularity\nSuperintelligence – Hypothetical agent surpassing human intelligence"}
{"doc_id": "Situated approach (artificial intelligence)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI \"from the bottom-up\" by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills.\nThe approach was originally proposed as an alternative to traditional approaches (that is, approaches popular before 1985 or so).\nAfter several decades, classical AI technologies started to face intractable issues (e.g. combinatorial explosion) when confronted with real-world modeling problems. All approaches to address these issues focus on modeling intelligences situated in an environment. They have become known as the situated approach to AI.\n\nEmergence of a concept\nFrom traditional AI to Nouvelle AI\nDuring the late 1980s, the approach now known as Nouvelle AI (Nouvelle means new in French) was pioneered at the MIT Artificial Intelligence Laboratory by Rodney Brooks. As opposed to classical or traditional artificial intelligence, Nouvelle AI purposely avoided the traditional goal of modeling human-level performance, but rather tries to create systems with intelligence at the level of insects, closer to real-world robots. But eventually, at least at MIT new AI did lead to an attempt for humanoid AI in the Cog Project.\n\nFrom Nouvelle AI to behavior-based and situated AI\nThe conceptual shift introduced by nouvelle AI flourished in the robotics area, given way to behavior-based robotics (BBR), a methodology for developing AI based on a modular decomposition of intelligence. It was made famous by Rodney Brooks: his subsumption architecture was one of the earliest attempts to describe a mechanism for developing BBAI. It is extremely popular in robotics and to a lesser extent to implement intelligent virtual agents because it allows the successful creation of real-time dynamic systems that can run in complex environments.  For example, it underlies the intelligence of the Sony Aibo and many RoboCup robot teams.\nRealizing that in fact all these approaches were aiming at building not an abstract intelligence, but rather an intelligence situated in a given environment, they have come to be known as the situated approach. In fact, this approach stems out from early insights of Alan Turing, describing the need to build machines equipped with sense organs to learn directly from the real-world instead of focusing on abstract activities, such as playing chess.\n\nDefinitions\nClassically, a software entity is defined as a simulated element, able to act on itself and on its environment, and which has an internal representation of itself and of"}
{"doc_id": "Situated approach (artificial intelligence)", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " describing the need to build machines equipped with sense organs to learn directly from the real-world instead of focusing on abstract activities, such as playing chess.\n\nDefinitions\nClassically, a software entity is defined as a simulated element, able to act on itself and on its environment, and which has an internal representation of itself and of the outside world. An entity can communicate with other entities, and its behavior is the consequence of its perceptions, its representations, and its interactions with the other entities.\n\nAI loop\nSimulating entities in a virtual environment requires simulating the entire process that goes from a perception of the environment, or more generally from a stimulus, to an action on the environment. This process is called the AI loop and technology used to simulate it can be subdivided in two categories. Sensorimotor or low-level AI deals with either the perception problem (what is perceived?) or the animation problem (how are actions executed?). Decisional or high-level AI deals with the action selection problem (what is the most appropriate action in response to a given perception, i.e. what is the most appropriate behavior?).\n\nTraditional or symbolic AI\nThere are two main approaches in decisional AI. The vast majority of the technologies available on the market, such as planning algorithms, finite-state machines (FSA), or expert systems, are based on the traditional or symbolic AI approach. Its main characteristics are:\n\nIt is top-down: it subdivides, in a recursive manner, a given problem into a series of sub-problems that are supposedly easier to solve.\nIt is knowledge-based: it relies on a symbolic description of the world, such as a set of rules.\nHowever, the limits of traditional AI, which goal is to build systems that mimic human intelligence, are well-known: inevitably, a combinatorial explosion of the number of rules occurs due to the complexity of the environment. In fact, it is impossible to predict all the situations that will be encountered by an autonomous entity.\n\nSituated or behavioral AI\nIn order to address these issues, another approach to decisional AI, also known as situated or behavioral AI, has been proposed. It does not attempt to model systems that produce deductive reasoning processes, but rather systems that behave realistically in their environment. The main characteristics of this approach are the following:\n\nIt is bottom-up: it relies on elementary behaviors, which can be combined to implement more complex behaviors.\nIt is behavior-based: it does not rely on a symbolic description of the environment, but rather on a model of the interactions of the entities with"}
{"doc_id": "Situated approach (artificial intelligence)", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " in their environment. The main characteristics of this approach are the following:\n\nIt is bottom-up: it relies on elementary behaviors, which can be combined to implement more complex behaviors.\nIt is behavior-based: it does not rely on a symbolic description of the environment, but rather on a model of the interactions of the entities with their environment.\nThe goal of situated AI is to model entities that are autonomous in their environment. This is achieved thanks to both the intrinsic robustness of the control architecture, and its adaptation capabilities to unforeseen situations.\n\nSituated agents\nIn artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term situated is commonly used to refer to robots, but some researchers argue that software agents can also be situated if:\n\nthey exist in a dynamic (rapidly changing) environment, which\nthey can manipulate or change through their actions, and which\nthey can sense or perceive.\nExamples might include web-based agents, which can alter data or trigger processes (such as purchases) over the Internet, or virtual-reality bots which inhabit and change virtual worlds, such as Second Life.\nBeing situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually.  The situated perspective emphasizes that intelligent behavior derives from the environment and the agent's interactions with it. The nature of these interactions are defined by an agent's embodiment.\n\nImplementation principles\nModular decomposition\nThe most important attribute of a system driven by situated AI is that the intelligence is controlled by a set of independent semi-autonomous modules. In the original systems, each module was actually a separate device or was at least conceived of as running on its own processing thread. Generally, though, the modules are just abstractions. In this respect, situated AI may be seen as a software engineering approach to AI, perhaps akin to object oriented design.\nSituated AI is often associated with reactive planning, but the two are not synonymous. Brooks advocated an extreme version of cognitive minimalism which required initially that the behavior modules were finite-state machines and thus contained no conventional memory or learning. This is associated with reactive AI because reactive AI requires reacting to the current state of the world, not to an agent's memory or preconception of that world. However, learning is obviously key to realistic strong AI, so this constraint has been relaxed, though not entirely abandoned.\n\nAction selection mechanism\nThe situated AI community has presented several solutions to modeling decision-making processes, also known as action selection mechanisms. The first attempt to solve this problem"}
{"doc_id": "Situated approach (artificial intelligence)", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " memory or preconception of that world. However, learning is obviously key to realistic strong AI, so this constraint has been relaxed, though not entirely abandoned.\n\nAction selection mechanism\nThe situated AI community has presented several solutions to modeling decision-making processes, also known as action selection mechanisms. The first attempt to solve this problem goes back to subsumption architectures, which were in fact more an implementation technique than an algorithm. However, this attempt paved the way to several others, in particular the free-flow hierarchies and activation networks. A comparison of the structure and performances of these two mechanisms demonstrated the advantage of using free-flow hierarchies in solving the action selection problem. However, motor schemas and process description languages are two other approaches that have been used with success for autonomous robots.\n\nNotes and references\nArsenio, Artur M. (2004) Towards an embodied and situated AI, In: Proceedings of the International FLAIRS conference, 2004. (online)\nThe Artificial Life Route To Artificial Intelligence: Building Embodied, Situated Agents, Luc Steels and Rodney Brooks Eds., Lawrence Erlbaum Publishing, 1995. (ISBN 978-0805815184)\nRodney A. Brooks Cambrian Intelligence (MIT Press, 1999) ISBN 0-262-52263-2; collection of early papers including \"Intelligence without representation\" and \"Intelligence without reason\", from 1986 & 1991 respectively.\nRonald C. Arkin Behavior-Based Robotics (MIT Press, 1998) ISBN 0-262-01165-4\nHendriks-Jansen, Horst (1996) Catching Ourselves in the Act: Situated Activity, Interactive Emergence, Evolution, and Human Thought. Cambridge, Mass.: MIT Press.\n\nSee also\nRelated articles\nArtificial intelligence\nCognitive science\n\nTraditional AI\nDecision tree\nFinite-state machine\nExpert system\nAutomated planning and scheduling\n\nSituated AI\nScruffy AI\nReactive planning\n\nRobotics\nBehavior-based robotics\nSituated robotics"}
{"doc_id": "Situated", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In artificial intelligence and cognitive science, the term situated refers to an agent which is embedded in an environment. The term situated is commonly used to refer to robots, but some researchers argue that software agents can also be situated if:\n\nthey exist in a dynamic (rapidly changing) environment, which\nthey can manipulate or change through their actions, and which\nthey can sense or perceive.\nExamples might include web-based agents, which can alter data or trigger processes (such as purchases) over the internet, or virtual-reality bots which inhabit and change virtual worlds, such as Second Life.\nBeing situated is generally considered to be part of being embodied, but it is useful to consider each perspective individually.  The situated perspective emphasizes that intelligent behaviour derives from the environment and the agent's interactions with it. The nature of these interactions are defined by an agent's embodiment."}
{"doc_id": "Smart object", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A smart object is an object that enhances the interaction with not only people but also with other smart objects. Also known as smart connected products or smart connected things (SCoT), they are products, assets and other things embedded with processors, sensors, software and connectivity that allow data to be exchanged between the product and its environment, manufacturer, operator/user, and other products and systems. Connectivity also enables some capabilities of the product to exist outside the physical device, in what is known as the product cloud.  The data collected from these products can be then analysed to inform decision-making, enable operational efficiencies and continuously improve the performance of the product.\nIt can not only refer to interaction with physical world objects but also to interaction with virtual (computing environment) objects. A smart physical object may be created either as an artifact or manufactured product or by embedding electronic tags such as RFID tags or sensors into non-smart physical objects. Smart virtual objects are created as software objects that are intrinsic when creating and operating a virtual or cyber world simulation or game. The concept of a smart object has several origins and uses, see History. There are also several overlapping terms, see also smart device, tangible object or tangible user interface and Thing as in the Internet of things.\n\nHistory\nIn the early 1990s, Mark Weiser, from whom the term ubiquitous computing originated, referred to a vision \"When almost every object either contains a computer or can have a tab attached to it, obtaining information will be trivial\", \nAlthough Weiser did not specifically refer to an object as being smart, his early work did imply that smart physical objects are smart in the sense that they act as digital information sources. Hiroshi  Ishii and Brygg Ullmer refer to tangible objects in terms of tangibles bits or tangible user interfaces that enable users to \"grasp & manipulate\" bits in the center of users' attention by coupling the bits with everyday physical objects and architectural surfaces.\nThe smart object concept was introduced by Marcelo Kallman and Daniel Thalmann as an object that can describe its own possible interactions. The main focus here is to model interactions of smart virtual objects with virtual humans, agents, in virtual worlds. The opposite approach to smart objects is 'plain' objects that do not provide this information. The additional information provided by this concept enables far more general interaction schemes, and can greatly simplify the planner of an artificial intelligence agent.\nIn contrast to smart virtual objects used in virtual worlds, Lev Manovich focuses on physical space filled with electronic and visual information."}
{"doc_id": "Smart object", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " objects is 'plain' objects that do not provide this information. The additional information provided by this concept enables far more general interaction schemes, and can greatly simplify the planner of an artificial intelligence agent.\nIn contrast to smart virtual objects used in virtual worlds, Lev Manovich focuses on physical space filled with electronic and visual information. Here,  \"smart objects\" are described as \"objects connected to the Net; objects that can sense their users and display smart behaviour\".\nMore recently in the early 2010s, smart objects are being proposed as a key enabler for the vision of the Internet of things. The combination of the Internet and emerging technologies such as near field communications, real-time localization, and embedded sensors enables everyday objects to be transformed into smart objects that can understand and react to their environment. Such objects are building blocks for the Internet of things and enable novel computing applications. In 2018, one of the world's first smart houses was built in Klaukkala, Finland in the form of a five-floor apartment block, using the Kone Residential Flow solution created by KONE, allowing even a smartphone to act as a home key.\n\nCharacteristics\nAlthough we can view interaction with physical smart object in the physical world as distinct from interaction with virtual smart objects in a virtual simulated world, these can be related. Poslad considers the progression of: how\n\nhumans use models of smart objects situated in the physical world to enhance human to physical world interaction; versus how\nsmart physical objects situated in the physical world can model human interaction in order to lessen the need for human to physical world interaction; versus how\nvirtual smart objects by modelling both physical world objects and modelling humans as objects and their subsequent interactions can form a predominantly smart virtual object environment.\n\nSmart physical objects\nThe concept smart for a smart physical object simply means that it is active, digital, networked, can operate to some extent autonomously, is reconfigurable and has local control of the resources it needs such as energy, data storage, etc. Note, a smart object does not necessarily need to be intelligent as in exhibiting a strong essence of artificial intelligence—although it can be designed to also be intelligent.\nPhysical world smart objects can be described in terms of three properties:\n\nAwareness: is a smart object's ability to understand (that is, sense, interpret, and react to) events and human activities occurring in the physical world.\nRepresentation: refers to a smart object's application and programming model—in particular, programming abstractions.\nInteraction: denotes the object's ability to"}
{"doc_id": "Smart object", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of three properties:\n\nAwareness: is a smart object's ability to understand (that is, sense, interpret, and react to) events and human activities occurring in the physical world.\nRepresentation: refers to a smart object's application and programming model—in particular, programming abstractions.\nInteraction: denotes the object's ability to converse with the user in terms of input, output, control, and feedback.\nBased upon these properties, these have been classified into three types:\n\nActivity-Aware Smart Objects: Are objects that can record information about work activities and its own use.\nPolicy-Aware Smart Objects: Are objects that are activity-aware  Objects can interpret events and activities with respect to predefined organizational policies.\nProcess-Aware Smart Objects: Processes play a fundamental role in industrial work management and operation. A process is a collection of related activities or tasks that are ordered according to their position in time and space.\n\nSmart virtual objects\nFor the virtual object in a virtual world case, an object is called smart when it has the ability to describe its possible interactions. This focuses on constructing a virtual world using only virtual objects that contain their own interaction information. There are four basic elements to constructing such a smart virtual object framework.\n\nObject properties: physical properties and a text description\nInteraction information: position of handles, buttons, grips, and the like\nObject behavior: different behaviors based on state variables\nAgent behaviors: description of the behavior an agent should follow when using the object\nSome versions of smart objects also include animation information in the object information, but this is not considered to be an efficient approach, since this can make objects inappropriately oversized.\n\nCategorization\nThe terms smart, connected product or smart product can be confusing as it is used to cover a broad range of different products, ranging from smart home appliances (e.g., smart bathroom scales or smart light bulbs) to smart cars (e.g., Tesla). While these products share certain similarities, they often differ substantially in their capabilities. Raff et al. developed a conceptual framework that distinguishes different smart products based on their capabilities, which features 4 types of smart product archetypes (in ascending order of \"smartness\").\n\nDigital\nConnected\nResponsive\nIntelligent\n\nAdvantages\nSmart, connected products have three primary components:\n\nPhysical – made up of the product's mechanical and electrical parts.\nSmart – made up of sensors, microprocessors, data storage, controls, software, and an embedded operating system with enhanced user interface.\nConnectivity – made up of ports, antennae, and protocols enabling wired"}
{"doc_id": "Smart object", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Smart, connected products have three primary components:\n\nPhysical – made up of the product's mechanical and electrical parts.\nSmart – made up of sensors, microprocessors, data storage, controls, software, and an embedded operating system with enhanced user interface.\nConnectivity – made up of ports, antennae, and protocols enabling wired/wireless connections that serve two purposes, it allows data to be exchanged with the product and enables some functions of the product to exist outside the physical device.\nEach component expands the capabilities of one another resulting in \"a virtuous cycle of value improvement\". First, the smart components of a product amplify the value and capabilities of the physical components. Then, connectivity amplifies the value and capabilities of the smart components. These improvements include:\n\nMonitoring of the product's conditions, its external environment, and its operations and usage.\nControl of various product functions to better respond to changes in its environment, as well as to personalize the user experience.\nOptimization of the product's overall operations based on actual performance data, and reduction of downtimes through predictive maintenance and remote service.\nAutonomous product operation, including learning from their environment, adapting to users' preferences and self-diagnosing and service.\n\nThe Internet of things (IoT)\nThe Internet of things is the network of physical objects that contain embedded technology to communicate and sense or interact with their internal states or the external environment.  The phrase \"Internet of things\" reflects the growing number of smart, connected products and highlights the new opportunities they can represent. The Internet, whether involving people or things, is a mechanism for transmitting information. What makes smart, connected products fundamentally different is not the Internet, but the changing nature of the 'things'. Once a product is smart and connected to the cloud, the products and services will become part of an interconnected management solution. Companies can evolve from making products to offering more complex, higher-value offerings within a \"system of systems\".\n\nSee also\nAmbieSense\nAudiocubes\nHome network\nIntelligent maintenance system\nNabaztag\nSmart speaker\nWearable technology\nUbiquitous computing"}
{"doc_id": "Software agent", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In computer science, a software agent is a computer program that acts for a user or another program in a relationship of agency.\nThe term agent is derived from the Latin agere (to do): an agreement to act on one's behalf. Such \"action on behalf of\" implies the authority to decide which, if any, action is appropriate. Some agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or as software such as a chatbot executing on a computer, such as a mobile device, e.g. Siri. Software agents may be autonomous or work together with other agents or people. Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).\nRelated and derived concepts include intelligent agents (in particular exhibiting some aspects of artificial intelligence, such as reasoning), autonomous agents (capable of modifying the methods of achieving their objectives), distributed agents (being executed on physically distinct computers), multi-agent systems (distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone), and mobile agents (agents that can relocate their execution onto different processors).\n\nConcepts\nThe basic attributes of an autonomous software agent are that agents:\n\nare not strictly invoked for a task, but activate themselves,\nmay reside in wait status on a host, perceiving context,\nmay get to run status on a host upon starting conditions,\ndo not require interaction of user,\nmay invoke other tasks including communication.\n\nThe concept of an agent provides a convenient and powerful way to describe a complex software entity that is capable of acting with a certain degree of autonomy in order to accomplish tasks on behalf of its host. But unlike objects, which are defined in terms of methods and attributes, an agent is defined in terms of its behavior.\nVarious authors have proposed different definitions of agents, these commonly include concepts such as:\n\npersistence: code is not executed on demand but runs continuously and decides for itself when it should perform some activity;\nautonomy: agents have capabilities of task selection, prioritization, goal-directed behavior, decision-making without human intervention;\nsocial ability: agents are able to engage other components through some sort of communication and coordination, they may collaborate on a task;\nreactivity: agents perceive the context in which they operate and react to it appropriately.\n\nDistinguishing agents from programs\nAll agents are programs, but not all programs are agents."}
{"doc_id": "Software agent", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " human intervention;\nsocial ability: agents are able to engage other components through some sort of communication and coordination, they may collaborate on a task;\nreactivity: agents perceive the context in which they operate and react to it appropriately.\n\nDistinguishing agents from programs\nAll agents are programs, but not all programs are agents. Contrasting the term with related concepts may help clarify its meaning. Franklin & Graesser (1997) discuss four key notions that distinguish agents from arbitrary programs: reaction to the environment, autonomy, goal-orientation and persistence.\n\nIntuitive distinguishing agents from objects\nAgents are more autonomous than objects.\nAgents have flexible behavior: reactive, proactive, social.\nAgents have at least one thread of control but may have more.\n\nDistinguishing agents from expert systems\nExpert systems are not coupled to their environment.\nExpert systems are not designed for reactive, proactive behavior.\nExpert systems do not consider social ability.\n\nDistinguishing intelligent software agents from intelligent agents in AI\nIntelligent agents (also known as rational agents) are not just computer programs: they may also be machines, human beings, communities of human beings (such as firms) or anything that is capable of goal-directed behavior.\n(Russell & Norvig 2003)\n\nImpact of software agents\nSoftware agents may offer various benefits to their end users by automating complex or repetitive tasks. However, there are organizational and cultural impacts of this technology that need to be considered prior to implementing software agents.\n\nOrganizational impact\nWork contentment and job satisfaction impact\nPeople like to perform easy tasks providing the sensation of success unless the repetition of the simple tasking is affecting the overall output. In general implementing software agents to perform administrative requirements provides a substantial increase in work contentment, as administering their own work does never please the worker. The effort freed up serves for a higher degree of engagement in the substantial tasks of individual work. Hence, software agents may provide the basics to implement self-controlled work, relieved from hierarchical controls and interference. Such conditions may be secured by application of software agents for required formal support.\n\nCultural impact\nThe cultural effects of the implementation of software agents include trust affliction, skills erosion, privacy attrition and social detachment. Some users may not feel entirely comfortable fully delegating important tasks to software applications. Those who start relying solely on intelligent agents may lose important skills, for example, relating to information literacy. In order to act on a user's behalf, a software agent needs to have a complete understanding of a user's profile, including his/her personal preferences. This"}
{"doc_id": "Software agent", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " entirely comfortable fully delegating important tasks to software applications. Those who start relying solely on intelligent agents may lose important skills, for example, relating to information literacy. In order to act on a user's behalf, a software agent needs to have a complete understanding of a user's profile, including his/her personal preferences. This, in turn, may lead to unpredictable privacy issues. When users start relying on their software agents more, especially for communication activities, they may lose contact with other human users and look at the world with the eyes of their agents. These consequences are what agent researchers and users must consider when dealing with intelligent agent technologies.\n\nHistory\nThe concept of an agent can be traced back to Hewitt's Actor Model (Hewitt, 1977) - \"A self-contained, interactive and concurrently-executing object, possessing internal state and communication capability.\"\nTo be more academic, software agent systems are a direct evolution of Multi-Agent Systems (MAS). MAS evolved from Distributed Artificial Intelligence (DAI), Distributed Problem Solving (DPS) and Parallel AI (PAI), thus inheriting all characteristics (good and bad) from DAI and AI.\nJohn Sculley's 1987 \"Knowledge Navigator\" video portrayed an image of a relationship between end-users and agents. Being an ideal first, this field experienced a series of unsuccessful top-down implementations, instead of a piece-by-piece, bottom-up approach. The range of agent types is now (from 1990) broad: WWW, search engines, etc.\n\nExamples  of intelligent software agents\nBuyer agents (shopping bots)\nBuyer agents travel around a network (e.g. the internet) retrieving information about goods and services. These agents, also known as 'shopping bots', work very efficiently for commodity products such as CDs, books, electronic components, and other one-size-fits-all products. Buyer agents are typically optimized to allow for digital payment services used in e-commerce and traditional businesses.\n\nUser agents (personal agents)\nUser agents, or personal agents, are intelligent agents that take action on your behalf. In this category belong those intelligent agents that already perform, or will shortly perform, the following tasks:\n\nCheck your e-mail, sort it according to the user's order of preference, and alert you when important emails arrive.\nPlay computer games as your opponent or patrol game areas for you.\nAssemble customized news reports for you. There are several versions of these, including CNN.\nFind information for you on the subject of your choice.\nFill out forms on the Web automatically for"}
{"doc_id": "Software agent", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " user's order of preference, and alert you when important emails arrive.\nPlay computer games as your opponent or patrol game areas for you.\nAssemble customized news reports for you. There are several versions of these, including CNN.\nFind information for you on the subject of your choice.\nFill out forms on the Web automatically for you, storing your information for future reference\nScan Web pages looking for and highlighting text that constitutes the \"important\" part of the information there\nDiscuss topics with you ranging from your deepest fears to sports\nFacilitate with online job search duties by scanning known job boards and sending the resume to opportunities who meet the desired criteria\nProfile synchronization across heterogeneous social networks\n\nMonitoring-and-surveillance (predictive) agents\nMonitoring and surveillance agents are used to observe and report on equipment, usually computer systems. The agents may keep track of company inventory levels, observe competitors' prices and relay them back to the company, watch stock manipulation by insider trading and rumors, etc.\n\nFor example, NASA's Jet Propulsion Laboratory has an agent that monitors inventory, planning, schedules equipment orders to keep costs down, and manages food storage facilities. These agents usually monitor complex computer networks that can keep track of the configuration of each computer connected to the network.\nA special case of monitoring-and-surveillance agents are organizations of agents used to automate decision-making process during tactical operations. The agents monitor the status of assets (ammunition, weapons available, platforms for transport, etc.) and receive goals from higher level agents. The agents then pursue the goals with the assets at hand, minimizing expenditure of the assets while maximizing goal attainment.\n\nData-mining agents\nThis agent uses information technology to find trends and patterns in an abundance of information from many different sources. The user can sort through this information in order to find whatever information they are seeking.\nA data mining agent operates in a data warehouse discovering information. A 'data warehouse' brings together information from many different sources. \"Data mining\" is the process of looking through the data warehouse to find information that you can use to take action, such as ways to increase sales or keep customers who are considering defecting.\n'Classification' is one of the most common types of data mining, which finds patterns in information and categorizes them into different classes. Data mining agents can also detect major shifts in trends or a key indicator and can detect the presence of new information and alert you to it. For example, the agent may detect a decline in the construction industry for an economy; based on this relayed information construction companies will"}
{"doc_id": "Software agent", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " in information and categorizes them into different classes. Data mining agents can also detect major shifts in trends or a key indicator and can detect the presence of new information and alert you to it. For example, the agent may detect a decline in the construction industry for an economy; based on this relayed information construction companies will be able to make intelligent decisions regarding the hiring/firing of employees or the purchase/lease of equipment in order to best suit their firm.\n\nNetworking and communicating agents\nSome other examples of current intelligent agents include some spam filters, game bots, and server monitoring tools. Search engine indexing bots also qualify as intelligent agents.\n\nUser agent - for browsing the World Wide Web\nBuyer Agent - As of 2025, advanced AI agents enable agentic commerce, autonomously handling product discovery, price comparison, and transactions in platforms like OpenAI integrations.\nMail transfer agent - For serving E-mail, such as Microsoft Outlook. Why? It communicates with the POP3 mail server, without users having to understand POP3 command protocols. It even has rule sets that filter mail for the user, thus sparing them the trouble of having to do it themselves.\nSNMP agent\nIn Unix-style networking servers, httpd is an HTTP daemon that implements the Hypertext Transfer Protocol at the root of the World Wide Web\nManagement agents used to manage telecom devices\nCrowd simulation for safety planning or 3D computer graphics,\nWireless beaconing agent is a simple process hosted single tasking entity for implementing wireless lock or electronic leash in conjunction with more complex software agents hosted e.g. on wireless receivers.\nUse of autonomous agents (deliberately equipped with noise) to optimize coordination in groups online.\n\nSoftware development agents (aka software bots)\nSoftware bots are becoming important in software engineering.\n\nSecurity agents\nAgents are also used in software security application to intercept, examine and act on various types of content.  Example include: \n\nData Loss Prevention (DLP) Agents - examine user operations on a computer or network, compare with policies specifying allowed actions, and take appropriate action (e.g. allow, alert, block).  The more comprehensive DLP agents can also be used to perform EDR functions.\nEndpoint Detection and Response (EDR) Agents - monitor all activity on an endpoint computer in order to detect and respond to malicious activities\nCloud Access Security Broker (CASB) Agents - similar to DLP Agents, however examining traffic going to cloud applications\n\nDesign issues\nIssues to consider in the development of agent-based systems include \n\nhow tasks are scheduled"}
{"doc_id": "Software agent", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "R) Agents - monitor all activity on an endpoint computer in order to detect and respond to malicious activities\nCloud Access Security Broker (CASB) Agents - similar to DLP Agents, however examining traffic going to cloud applications\n\nDesign issues\nIssues to consider in the development of agent-based systems include \n\nhow tasks are scheduled and how synchronization of tasks is achieved\nhow tasks are prioritized by agents\nhow agents can collaborate, or recruit resources,\nhow agents can be re-instantiated in different environments, and how their internal state can be stored,\nhow the environment will be probed and how a change of environment leads to behavioral changes of the agents\nhow messaging and communication can be achieved,\nwhat hierarchies of agents are useful (e.g. task execution agents, scheduling agents, resource providers ...).\nFor software agents to work together efficiently they must share semantics of their data elements. This can be done by having computer systems publish their metadata.\nThe definition of agent processing can be approached from two interrelated directions:\n\ninternal state processing and ontologies for representing knowledge\ninteraction protocols – standards for specifying communication of tasks\nAgent systems are used to model real-world systems with concurrency or parallel processing.\n\nAgent Machinery – Engines of various kinds, which support the varying degrees of intelligence\nAgent Content – Data employed by the machinery in Reasoning and Learning\nAgent Access – Methods to enable the machinery to perceive content and perform actions as outcomes of Reasoning\nAgent Security – Concerns related to distributed computing, augmented by a few special concerns related to agents\nThe agent uses its access methods to go out into local and remote databases to forage for content. These access methods may include setting up news stream delivery to the agent, or retrieval from bulletin boards, or using a spider to walk the Web. The content that is retrieved in this way is probably already partially filtered – by the selection of the newsfeed or the databases that are searched. The agent next may use its detailed searching or language-processing machinery to extract keywords or signatures from the body of the content that has been received or retrieved. This abstracted content (or event) is then passed to the agent's Reasoning or inferencing machinery in order to decide what to do with the new content. This process combines the event content with the rule-based or knowledge content provided by the user. If this process finds a good hit or match in the new content, the agent may use another piece of its machinery to do a more detailed search on the content. Finally, the agent may decide to take an action based on"}
{"doc_id": "Software agent", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ". This process combines the event content with the rule-based or knowledge content provided by the user. If this process finds a good hit or match in the new content, the agent may use another piece of its machinery to do a more detailed search on the content. Finally, the agent may decide to take an action based on the new content; for example, to notify the user that an important event has occurred. This action is verified by a security function and then given the authority of the user. The agent makes use of a user-access method to deliver that message to the user. If the user confirms that the event is important by acting quickly on the notification, the agent may also employ its learning machinery to increase its weighting for this kind of event.\nBots can act on behalf of their creators to do good as well as bad. There are a few ways which bots can be created to demonstrate that they are designed with the best intention and are not built to do harm. This is first done by having a bot identify itself in the user-agent HTTP header when communicating with a site. The source IP address must also be validated to establish itself as legitimate. Next, the bot must also always respect a site's robots.txt file since it has become the standard across most of the web. And like respecting the robots.txt file, bots should shy away from being too aggressive and respect any crawl delay instructions.\n\nNotions and frameworks for agents\nDAML (DARPA Agent Markup Language)\n3APL (Artificial Autonomous Agents Programming Language)\nGOAL agent programming language\nOpen Agent Architecture (OAA)\nWeb Ontology Language (OWL)\ndaemons in Unix-like systems.\nJava Agent Template (JAT)\nJava Agent Development Framework (JADE)\nJava Agent Framework For Intelligent and Mobile Agents (JAFIMA)\nSARL agent programming language (arguably an Actor and not Agent oriented paradigm)\n\nSee also\nAgent architecture\nChatbot\nData loss prevention\nEndpoint detection and response\nSoftware bot"}
{"doc_id": "Space-based data center", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Space-based data centers or orbital AI infrastructure are proposed concepts to build AI data centers in the sun-synchronous orbit or other orbits utilizing space-based solar power.  Electric power has become the main bottleneck for terrestrial AI infrastructure. Building AI data centers in space off-the-grid could become cost competitive with advancements in reusable rockets.\n\nHistory of space-based data center proposals and deployment\nEarly thinking about space-based computing infrastructure grew out of mid-20th-century visions for large orbital industrial systems, most notably proposals for space-based solar power, which were popularized in both technical literature and science writing by figures such as Isaac Asimov in the 1940s. These ideas emphasized exploiting the vacuum, continuous solar energy, and thermal characteristics of space to support power-intensive activities that would be difficult or inefficient on Earth.\nIn the 21st century, advances in small satellites, reusable launch vehicles, and high-performance computing revived interest in space-based data centers, with governments and private companies exploring orbital or near-space platforms for edge computing, secure data handling, and low-latency processing of Earth-observation data. In 2025, Starcloud deployed an NVIDIA H100-class system and became the first company to train an LLM in space and run a version of Google Gemini in space.\n\nAdvantages\nConstant sunlight in the dawn/dusk sun-synchronous orbit\nSolar irradiance is 36% higher in Earth orbit than on the surface\nNo weather storms or clouds\nNo property tax or land-use regulation\nSaves space for other land use\nAmple space for scalability\nWon't strain the power grid\nRadiative cooling in space reduces energy needed for thermal control\n\nDisadvantages\nHigh launch costs, particularly the launch cost\nSolar hardware and computer hardware must survive launch, space assembly, radiation, microgravity, and orbital debris.\nKessler syndrome – runaway space debris\nMaintenance and repair are difficult\nLatency and bandwidth constrained\nLimited life span of solar panels and electronics\nSatellite flares could inhibit ground-based and space-based observational astronomy\n\nSize and power generated\nIt would take ~1 square mile solar array in earth orbit to produce 1 gigawatt of power at 30% cell efficiency.\n\nCompanies pursuing space-based AI infrastructure\nAetherflux\nBlue Origin\nGoogle – Project Suncatcher\nNvidia\nOpenAI\nSpaceX\nStarcloud\n\nSee also\nInterplanetary Internet\nLaser communication in space, microwave transmission, and communications satellite.\nSpace launch market\nSpace Network and Near Earth Network\nSolar cell research and solar"}
{"doc_id": "Space-based data center", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "\nAetherflux\nBlue Origin\nGoogle – Project Suncatcher\nNvidia\nOpenAI\nSpaceX\nStarcloud\n\nSee also\nInterplanetary Internet\nLaser communication in space, microwave transmission, and communications satellite.\nSpace launch market\nSpace Network and Near Earth Network\nSolar cell research and solar-cell efficiency\nSolar panels on spacecraft\nStargate\nStarlink"}
{"doc_id": "Sparkles emoji", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The Sparkles emoji (U+2728 ✨ SPARKLES) is an emoji that has one large star surrounded by smaller stars. Originating from Japan to represent sparkles used in anime and manga, the sparkles are often used as emphasis in text by surrounding words or phrases with it. It is the third most-used emoji in the world on Twitter as of 2021, and since the early 2020s it has been used by major software companies to represent artificial intelligence.\n\nDevelopment\nAccording to Emojipedia, the Sparkles emoji was first used by Japanese mobile operators SoftBank, Docomo and au in the late 1990s. The emoji was added to Unicode 6.0 in 2010 and Emoji 1.0 in 2015.\nOn some platforms the Sparkles emoji has been multicoloured whilst on other platforms it has been one colour. Twitter and Microsoft's Sparkles have changed from being multicoloured to being a single colour. Samsung's version of the emoji previously had a night sky in the background.\n\nUsage\nInterpersonal communication\nThe Sparkles emoji was originally meant to represent the usage of sparkles in Japanese anime and manga, where the sparkles are used to represent beauty, happiness or awe. The emoji has several meanings and depends upon context. Starting in the late 2010s, the emoji started being used to surround words or phrases to be used as emphasis, an example from the book Because Internet being \"I would simply ✨pass away✨\". It can also be used as sarcasm, irony or as a way to mock people. Without emoji this could be represented with tildes or asterisks, for example, \"~tildes~\" or \"~*asterisk plus tilde*~\" or \"~*~*true sparkle exuberance*~*~\". The sparkles emoji can be used to represent stars in text, be used to represent cleanliness or can be used to mean \"orgasm\" whilst sexting.\nIn September 2021 the Sparkles emoji overtook the Pleading Face (🥺) emoji to become the third most-used emoji in the world according to Emojipedia, with approximately 1 per cent of all tweets containing the Sparkles emoji.\n\nArtificial intelligence\nIn the early 2020s, the Sparkles emoji started being used as an icon to represent artificial intelligence (AI). Companies who use the emoji this way include Google, OpenAI, Samsung, Microsoft, Adobe, Spotify and"}
{"doc_id": "Sparkles emoji", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " approximately 1 per cent of all tweets containing the Sparkles emoji.\n\nArtificial intelligence\nIn the early 2020s, the Sparkles emoji started being used as an icon to represent artificial intelligence (AI). Companies who use the emoji this way include Google, OpenAI, Samsung, Microsoft, Adobe, Spotify and Zoom. As of August 2024, seven of the top 10 software companies by market capitalisation use the Sparkles emojis with AI. OpenAI has different versions of the Sparkles for different versions of the models that ChatGPT uses. One explanation is that Sparkles is being used by these companies as a way to market AI as \"magic\". Marketing technology as \"magic\" has been used before AI, particularly by Apple. Another explanation given by designers and marketers choosing to use Sparkles to signify AI is simply that other platforms are doing it, making it familiar to users.\nAround 2024, some of these companies started removing two of the smaller stars from the emoji in their AI services and have kept the one large star, an example being Google's Gemini chatbot.\nIn early 2024, the Nielsen Norman Group provided test subjects with the star in isolation and found that people did not associate the symbol with AI, but instead mostly with \"optimisation\" or \"favourite or save an item\"."}
{"doc_id": "Spreading activation", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks. The search process is initiated by labeling a set of source nodes (e.g. concepts in a semantic network) with weights or \"activation\" and then iteratively propagating or \"spreading\" that activation out to other nodes linked to the source nodes.  Most often these \"weights\" are real values that decay as activation propagates through the network.  When the weights are discrete this process is often referred to as marker passing. Activation may originate from alternate paths, identified by distinct markers, and terminate when two alternate paths reach the same node. However brain studies show that several different brain areas play an important role in semantic processing.\nSpreading activation in semantic networks as a model were invented in cognitive psychology to model the fan out effect.\nSpreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents.\n\nCognitive psychology\nAs it relates to cognitive psychology, spreading activation is the theory of how the brain iterates through a network of associated ideas to retrieve specific information. The spreading activation theory presents the array of concepts within our memory as cognitive units, each consisting of a node and its associated elements or characteristics, all connected together by edges. A spreading activation network can be represented schematically, in a sort of web diagram with shorter lines between two nodes meaning the ideas are more closely related and will typically be associated more quickly to the original concept. In memory psychology, the spreading activation model holds that people organize their knowledge of the world based on their personal experiences, which in turn form the network of ideas that is the person's knowledge of the world.\nWhen a word (the target) is preceded by an associated word (the prime) in word recognition tasks, participants seem to perform better in the amount of time that it takes them to respond. For instance, subjects respond faster to the word \"doctor\" when it is preceded by \"nurse\" than when it is preceded by an unrelated word like \"carrot\". This semantic priming effect with words that are close in meaning within the cognitive network has been seen in a wide range of tasks given by experimenters, ranging from sentence verification to lexical decision and naming.\nAs another example, if the original concept is \"red\" and the concept \"vehicles\" is primed, they are much more likely to say \"fire engine\" instead of something unrelated to vehicles, such as \"cherries\". If instead \"fruits\" was"}
{"doc_id": "Spreading activation", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", ranging from sentence verification to lexical decision and naming.\nAs another example, if the original concept is \"red\" and the concept \"vehicles\" is primed, they are much more likely to say \"fire engine\" instead of something unrelated to vehicles, such as \"cherries\". If instead \"fruits\" was primed, they would likely name \"cherries\" and continue on from there. The activation of pathways in the network has everything to do with how closely linked two concepts are by meaning, as well as how a subject is primed.\n\nAlgorithm\nA directed graph is populated by Nodes[ 1...N ]  each having an associated activation value A [ i ] which is a real number in the range [0.0 ... 1.0].  A Link[ i, j ] connects source node[ i ] with target node[ j ].  Each edge has an associated weight W [ i, j ] usually a real number in the range [0.0 ... 1.0].\nParameters:\n\nFiring threshold F, a real number in the range [0.0 ... 1.0]\nDecay factor D, a real number in the range [0.0 ... 1.0]\nSteps:\n\nInitialize the graph setting all activation values A [ i ] to zero.   Set one or more origin nodes to an initial activation value greater than the firing threshold F.  A typical initial value is 1.0.\nFor each unfired node [ i ] in the graph having an activation value A [ i ] greater than the node firing threshold F:\nFor each Link [ i, j ] connecting the source node [ i ] with target node [ j ], adjust A [ j ] = A [ j ] + (A [ i ] * W [ i, j ] * D) where D is the decay factor.\nIf a target node receives an adjustment to its activation value so that it would exceed 1.0, then set its new activation value to 1.0.  Likewise maintain 0.0 as a lower bound on the target node's activation value should it receive an adjustment to below 0.0.\nOnce a node has fired it may not fire again, although variations of the basic algorithm permit repeated firings and loops through the graph.\nNodes receiving a new activation value that exceeds the firing threshold F are marked for firing on the next spreading activation cycle.\nIf activation originates from more than one node, a variation of the algorithm permits marker passing to distinguish the paths by"}
{"doc_id": "Spreading activation", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " fire again, although variations of the basic algorithm permit repeated firings and loops through the graph.\nNodes receiving a new activation value that exceeds the firing threshold F are marked for firing on the next spreading activation cycle.\nIf activation originates from more than one node, a variation of the algorithm permits marker passing to distinguish the paths by which activation is spread over the graph\nThe procedure terminates when either there are no more nodes to fire or in the case of marker passing from multiple origins, when a node is reached from more than one path. Variations of the algorithm that permit repeated node firings and activation loops in the graph, terminate after a steady activation state, with respect to some delta, is reached, or when a maximum number of iterations is exceeded.\n\nExamples\nSee also\nConnectionism\n\nNotes"}
{"doc_id": "STIT logic", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "STIT logic (from seeing to it that) is a family of modal and branching-time logics for reasoning about agency and choice. A typical STIT operator has the form \n  \n    \n      \n        [\n        i\n         \n        \n          \n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {stit}}:\\varphi ]}\n  \n, usually read as \"agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n sees to it that \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\", and is interpreted in models where agents choose between alternative possible futures.\nSTIT logics are used in action theory, deontic logic, epistemic logic, and the theory of intelligent agents to formalise notions such as \"could have done otherwise\", responsibility, joint action, and strategic ability in an indeterministic world.\n\nEtymology\nThe acronym STIT comes from the English phrase \"seeing to it that\", introduced in influential work by Nuel Belnap and Michael Perloff on the logical analysis of agentive expressions. In this tradition, \"to see to it that \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n\" is treated as a primitive agency operator, rather than being reduced to ordinary modal necessity.\n\nHistory\nModern STIT logic arose in the 1980s in the context of branching-time semantics and formal theories of agency. Belnap and Perloff's article \"Seeing to it that: A canonical form for agentives\" introduced the idea of treating expressions of the form \"agent i sees to it that φ\" as a primitive modal operator, and analysed such sentences using a branching tree of moments and histories. This approach was further developed in a series of papers on indeterminism and agency and provided the conceptual core for later STIT formalisms.\nIn the 1990s the basic formal systems of STIT logic were worked out. Horty and Belnap's influential paper on the deliberative STIT operator distinguished between a \"Chellas\" STIT that merely records the result of an agent's present choice and a \"deliberative\" STIT that requires the agent's choice to make a difference, and connected STIT with issues of action, omission, ability and obligation. Around the same time, Ming Xu proved completeness and decidability results for basic STIT systems, including a single-agent logic with Kripke-style semantics and axiomatizations for multi-agent"}
{"doc_id": "STIT logic", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "IT that requires the agent's choice to make a difference, and connected STIT with issues of action, omission, ability and obligation. Around the same time, Ming Xu proved completeness and decidability results for basic STIT systems, including a single-agent logic with Kripke-style semantics and axiomatizations for multi-agent deliberative STIT, thereby establishing STIT as a well-behaved normal modal framework.\nThis early work was systematised in Belnap, Perloff and Xu's monograph Facing the Future: Agents and Choices in Our Indeterminist World, which presents a general branching-time semantics for individual and group STIT operators, discusses independence-of-agents conditions and articulates the metaphysical picture of an indeterministic \"tree\" of moments. At roughly the same time, Horty's book Agency and Deontic Logic developed deontic STIT logics in which obligations are tied to agents' available choices rather than to static states of affairs, and used the resulting systems to analyse \"ought implies can\", contrary-to-duty obligations and deontic paradoxes. These works helped to position STIT at the intersection of action theory, temporal logic and deontic logic.\nFrom the late 1990s and 2000s onward, STIT logics were combined with epistemic, temporal and strategic modalities. Broersen introduced complete STIT logics for knowledge and action and deontic-epistemic STIT systems that distinguish different modes of mens rea, with applications to responsibility and the specification of multi-agent systems. Work on group and coalitional agency investigated axiomatisations and complexity results for group STIT logics, and related STIT-based analyses of agency to coalition logic and alternating-time temporal logic (ATL) by exhibiting formal embeddings between the frameworks.\nExplicit temporal operators were added to STIT in so-called temporal STIT logics. Lorini proposed a temporal STIT with \"next\" and \"until\" operators along histories and showed how it can be applied to normative reasoning about ongoing behaviour and commitments. Ciuni and Lorini compared different semantics for temporal STIT, clarifying the relationships between branching-time, game-based and epistemic approaches, while Boudou and Lorini gave a semantics for temporal STIT based on concurrent game structures, thus strengthening links with standard models of multi-agent interaction used for ATL and strategy logic. In parallel, complexity-theoretic work by Balbiani, Herzig and Troquard and by Schwarzentruber and co-authors investigated"}
{"doc_id": "STIT logic", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "oudou and Lorini gave a semantics for temporal STIT based on concurrent game structures, thus strengthening links with standard models of multi-agent interaction used for ATL and strategy logic. In parallel, complexity-theoretic work by Balbiani, Herzig and Troquard and by Schwarzentruber and co-authors investigated the satisfiability and model-checking problems for various STIT fragments, showing for instance that many expressive group STIT logics are undecidable or of high computational complexity.\nIn the 2010s, STIT ideas were combined with justification logic, imagination operators and refined deontic notions. Justification STIT logics, developed by Olkhovikov and others, merge explicit justifications with STIT-style agency so that producing a proof can itself be treated as an action that brings about knowledge, and they come with completeness and decidability results. Olkhovikov and Wansing introduced STIT imagination logics, together with axiomatic systems and tableau calculi, to model acts of voluntary imagining and their role in doxastic control. Other authors have proposed STIT-based logics of responsibility, blameworthiness and intentionality for use in philosophical and AI settings. Xu's survey article \"Combinations of STIT with Ought and Know\" (2015) reviews many of these developments and emphasises the interplay between deontic and epistemic STIT logics.\nCurrent research on STIT focuses on proof theory, automated reasoning and richer expressive resources. Lyon and van Berkel, building on earlier work on labelled calculi for STIT, have developed cut-free sequent systems and proof-search algorithms that yield syntactic decision procedures for a range of deontic and non-deontic multi-agent STIT logics and support applications such as duty checking and compliance checking in autonomous systems. Sawasaki has proposed first-order cstit-based STIT logics that can distinguish de re and de dicto readings of agency statements and has proved strong completeness results for Hilbert systems over finite models, moving the STIT programme beyond the purely propositional level. Further work investigates interpreted-system and computationally grounded semantics for STIT and its extensions in order to model the behaviour of autonomous agents in multi-agent settings, and proposes STIT-based semantics for epistemic notions based on patterns of information disclosure in interactive systems.\n\nBranching-time semantics\nSTIT logics are usually interpreted over branching-time models. A standard STIT frame consists of:\n\na non-empty set of moments \n  \n    \n      \n       "}
{"doc_id": "STIT logic", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " behaviour of autonomous agents in multi-agent settings, and proposes STIT-based semantics for epistemic notions based on patterns of information disclosure in interactive systems.\n\nBranching-time semantics\nSTIT logics are usually interpreted over branching-time models. A standard STIT frame consists of:\n\na non-empty set of moments \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n, partially ordered by \n  \n    \n      \n        <\n      \n    \n    {\\displaystyle <}\n  \n so that \n  \n    \n      \n        (\n        T\n        ,\n        <\n        )\n      \n    \n    {\\displaystyle (T,<)}\n  \n forms a tree (every pair of moments with a common predecessor has a greatest lower bound);\na set of histories, each history being a maximal linearly ordered subset of \n  \n    \n      \n        T\n      \n    \n    {\\displaystyle T}\n  \n;\na non-empty set of agents \n  \n    \n      \n        A\n        g\n      \n    \n    {\\displaystyle Ag}\n  \n;\nfor each agent \n  \n    \n      \n        i\n        ∈\n        A\n        g\n      \n    \n    {\\displaystyle i\\in Ag}\n  \n and moment \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n, a choice function \n  \n    \n      \n        \n          \n            \n              c\n              h\n              o\n              i\n              c\n              e\n            \n          \n          \n            i\n          \n          \n            m\n          \n        \n      \n    \n    {\\displaystyle {\\mathsf {choice}}_{i}^{m}}\n  \n that partitions the set of histories passing through \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n into choice cells.\nThe idea is that a moment represents a time at which choices are made, and histories represent complete possible future courses of events. At each moment, each agent's choice corresponds to selecting one of the available cells of histories determined by their choice function.\nFormulas are evaluated at pairs \n  \n    \n      \n        (\n        m\n        ,\n        h\n        )\n      \n    \n    {\\displaystyle (m,h)}\n  \n of a moment and a history through that moment (sometimes written \n  \n    \n      \n        m\n        \n          /\n        \n        h\n      \n    \n    {\\displaystyle m/h}\n  \n). A valuation assigns truth-values to atomic propositions at such indices; Boolean connectives are interpreted pointwise as in Kripke-style modal logic.\n\nChellas and deliberative STIT operators\nSeveral STIT operators have been distinguished in the literature. A common approach uses two closely related operators, often called Chellas STIT and deliberative STIT.\nLet \n  \n    \n      \n        \n          H"}
{"doc_id": "STIT logic", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Boolean connectives are interpreted pointwise as in Kripke-style modal logic.\n\nChellas and deliberative STIT operators\nSeveral STIT operators have been distinguished in the literature. A common approach uses two closely related operators, often called Chellas STIT and deliberative STIT.\nLet \n  \n    \n      \n        \n          H\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle H_{m}}\n  \n be the set of histories passing through a moment \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n, and write \n  \n    \n      \n        \n          H\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle H_{m}}\n  \n \n  \n    \n      \n        \n          ⟦\n        \n        φ\n        \n          \n            ⟧\n          \n          \n            m\n          \n        \n        =\n        {\n        h\n        ∈\n        \n          H\n          \n            m\n          \n        \n        ∣\n        M\n        ,\n        m\n        \n          /\n        \n        h\n        ⊨\n        φ\n        }\n      \n    \n    {\\displaystyle {\\text{⟦}}\\varphi {\\text{⟧}}_{m}=\\{h\\in H_{m}\\mid M,m/h\\models \\varphi \\}}\n  \n for the set of histories at \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n where \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n holds.\n\nThe Chellas STIT operator, often written \n  \n    \n      \n        [\n        i\n         \n        \n          \n            c\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {cstit}}:\\varphi ]}\n  \n, is given by\n\n  \n    \n      \n        M\n        ,\n        m\n        \n          /\n        \n        h\n        ⊨\n        [\n        i\n         \n        \n          \n            c\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n        \n        \n          iff\n        \n        \n        \n          \n            \n              c\n              h\n              o\n              i\n              c\n              e\n            \n          \n          \n            i\n          \n          \n            m\n          \n        \n        (\n        h\n        )\n        ⊆\n        \n          ⟦\n        \n        φ\n        \n          \n            ⟧\n          \n          \n            m\n          \n        \n        .\n      \n    \n    {\\displaystyle M,m/h\\models [i\\ {\\mathsf {cstit}}:\\varphi ]\\quad {\\text{iff}}\\quad"}
{"doc_id": "STIT logic", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " h\n        )\n        ⊆\n        \n          ⟦\n        \n        φ\n        \n          \n            ⟧\n          \n          \n            m\n          \n        \n        .\n      \n    \n    {\\displaystyle M,m/h\\models [i\\ {\\mathsf {cstit}}:\\varphi ]\\quad {\\text{iff}}\\quad {\\mathsf {choice}}_{i}^{m}(h)\\subseteq {\\text{⟦}}\\varphi {\\text{⟧}}_{m}.}\n  \n\nIntuitively, agent \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  \n sees to it that \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n if \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n holds at all histories compatible with their present choice.\nThe deliberative STIT operator, \n  \n    \n      \n        [\n        i\n         \n        \n          \n            d\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {dstit}}:\\varphi ]}\n  \n, adds a \"non-triviality\" or \"negative\" condition:\n\n  \n    \n      \n        M\n        ,\n        m\n        \n          /\n        \n        h\n        ⊨\n        [\n        i\n         \n        \n          \n            d\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n      \n    \n    {\\displaystyle M,m/h\\models [i\\ {\\mathsf {dstit}}:\\varphi ]}\n  \n iff \n  \n    \n      \n        M\n        ,\n        m\n        \n          /\n        \n        h\n        ⊨\n        [\n        i\n         \n        \n          \n            c\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n      \n    \n    {\\displaystyle M,m/h\\models [i\\ {\\mathsf {cstit}}:\\varphi ]}\n  \n and \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n is not already historically necessary at \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n (i.e. fails at some history through \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  \n).\nTo express the latter, a historic necessity operator \n  \n    \n      \n        \n          ◻\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle \\Box _{H}}\n  \n is often defined by\n\n  \n    \n      \n        M\n        ,\n        m\n        \n         "}
{"doc_id": "STIT logic", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        m\n      \n    \n    {\\displaystyle m}\n  \n).\nTo express the latter, a historic necessity operator \n  \n    \n      \n        \n          ◻\n          \n            H\n          \n        \n      \n    \n    {\\displaystyle \\Box _{H}}\n  \n is often defined by\n\n  \n    \n      \n        M\n        ,\n        m\n        \n          /\n        \n        h\n        ⊨\n        \n          ◻\n          \n            H\n          \n        \n        φ\n        \n        \n          iff\n        \n        \n        \n          for all \n        \n        \n          h\n          ′\n        \n        ∈\n        \n          H\n          \n            m\n          \n        \n        ,\n        \n        M\n        ,\n        m\n        \n          /\n        \n        \n          h\n          ′\n        \n        ⊨\n        φ\n        .\n      \n    \n    {\\displaystyle M,m/h\\models \\Box _{H}\\varphi \\quad {\\text{iff}}\\quad {\\text{for all }}h'\\in H_{m},\\,M,m/h'\\models \\varphi .}\n  \n\nOn this basis, Chellas and deliberative STIT become interdefinable by:\n\n  \n    \n      \n        [\n        i\n         \n        \n          \n            d\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n        ↔\n        \n          \n            (\n          \n        \n        [\n        i\n         \n        \n          \n            c\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n        ∧\n        ¬\n        \n          ◻\n          \n            H\n          \n        \n        φ\n        \n          \n            )\n          \n        \n        ,\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {dstit}}:\\varphi ]\\leftrightarrow {\\bigl (}[i\\ {\\mathsf {cstit}}:\\varphi ]\\land \\neg \\Box _{H}\\varphi {\\bigr )},}\n  \n\n  \n    \n      \n        [\n        i\n         \n        \n          \n            c\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n        ↔\n        \n          \n            (\n          \n        \n        [\n        i\n         \n        \n          \n            d\n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n        ∨\n        \n          ◻\n          \n            H\n          \n        \n        φ\n        \n          \n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {cstit}}:\\varphi ]\\leftrightarrow {\\bigl (}[i\\ {\\mathsf"}
{"doc_id": "STIT logic", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        :\n        φ\n        ]\n        ∨\n        \n          ◻\n          \n            H\n          \n        \n        φ\n        \n          \n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle [i\\ {\\mathsf {cstit}}:\\varphi ]\\leftrightarrow {\\bigl (}[i\\ {\\mathsf {dstit}}:\\varphi ]\\lor \\Box _{H}\\varphi {\\bigr )}.}\n  \n\nMing Xu proved completeness results and axiomatizations for basic dstit logics with single and multiple agents, establishing STIT as a well-behaved modal system.\n\nGroup agency and joint action\nSTIT models naturally support notions of group and collective agency. Given a set of agents \n  \n    \n      \n        G\n        ⊆\n        A\n        g\n      \n    \n    {\\displaystyle G\\subseteq Ag}\n  \n, a group STIT operator \n  \n    \n      \n        [\n        G\n         \n        \n          \n            s\n            t\n            i\n            t\n          \n        \n        :\n        φ\n        ]\n      \n    \n    {\\displaystyle [G\\ {\\mathsf {stit}}:\\varphi ]}\n  \n can be defined so that it is true exactly when the collective choices of all agents in \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n guarantee \n  \n    \n      \n        φ\n      \n    \n    {\\displaystyle \\varphi }\n  \n. Different notions of joint agency (for instance, where all group members are essential vs. inessential participants) can be distinguished by constraints on the choice functions for groups and individuals.\nGroup STIT operators have been combined with temporal and epistemic operators to study collective responsibilities, joint intentions, and coordination in multi-agent systems.\n\nDeontic and normative STIT\nSTIT logic has been combined with deontic logic to model obligations and permissions that concern an agent's actions rather than mere states of affairs. John F. Horty developed a deontic STIT framework in which obligations to act are evaluated against an indeterministic branching-time background, allowing for a distinction between \"ought-to-do\" and \"ought-to-be\" and addressing classic deontic paradoxes. See Agentive logic § John Horty's ought-to-do logic.\nSubsequent work has explored deontic STIT logics for formalising contrary-to-duty obligations, prioritised norms, and policies in computational normative systems, and for connecting STIT-based notions of agency with input/output logic and other frameworks for normative reasoning.\n\nTemporal and epistemic extensions\nAlthough the original ST"}
{"doc_id": "STIT logic", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Subsequent work has explored deontic STIT logics for formalising contrary-to-duty obligations, prioritised norms, and policies in computational normative systems, and for connecting STIT-based notions of agency with input/output logic and other frameworks for normative reasoning.\n\nTemporal and epistemic extensions\nAlthough the original STIT systems used an implicit branching-time background, later work has combined STIT with explicit temporal operators, resulting in temporal STIT logics. These add connectives for \"next\", \"always\", or \"until\" along histories, allowing reasoning about what agents can ensure over time.\nEpistemic and doxastic STIT logics enrich the framework with operators for knowledge and belief, enabling analysis of \"knowingly doing\", informational responsibility, and strategic ignorance. Justification STIT logics merge STIT with justification logic, modelling proofs or justifications as actions that result in knowledge.\n\nRelationships to other logics\nSTIT logic is closely related to, but distinct from, several other modal frameworks:\n\nLike branching-time temporal logic, it uses tree-like models with histories, but STIT operators focus on agents' choices rather than temporal quantification alone.\nCompared to dynamic logic, which treats actions as program-like relations between states, STIT treats actions implicitly via partitions of histories determined by agents' choices; some authors have studied embeddings and combinations of these perspectives.\nCoalition logic and alternating-time temporal logic (ATL) analyse what groups of agents can achieve by choosing strategies; STIT's group operators are closely related and there are formal embeddings in both directions.\n\nApplications\nPhilosophy\nIn philosophy of action and metaphysics, STIT logics model free will, alternative possibilities, and moral responsibility in indeterministic settings. The framework has been used to articulate conditions under which agents \"could have done otherwise\", and to analyse the compatibility of free agency with branching-time metaphysics.\nIn ethics and deontic logic, deontic STIT connects obligations with agents' available choices and abilities, providing tools to reason about \"ought implies can\", contrary-to-duty obligations, and practical dilemmas. John Horty's ought-to-do logic, created for modelling agentive duties, was developed based on STIT operators.\n\nComputer science and artificial intelligence\nIn computer science and artificial intelligence, STIT-based logics contribute to the specification and verification of multi-agent systems, autonomous agents, and normative systems. Temporal-epistemic STIT logics have been proposed as specification languages for agents with knowledge, goals, and actions, and"}
{"doc_id": "STIT logic", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Computer science and artificial intelligence\nIn computer science and artificial intelligence, STIT-based logics contribute to the specification and verification of multi-agent systems, autonomous agents, and normative systems. Temporal-epistemic STIT logics have been proposed as specification languages for agents with knowledge, goals, and actions, and have been related to more mainstream modal logics used in model checking.\nSTIT ideas have also been combined with justification and proof-theoretic formalisms to represent reasoning as an activity that produces knowledge, and to study interactions between agency and proof in formal systems.\n\nSee also\nAction theory\nAgentive logic\nDeontic logic\nDynamic logic (modal logic)\nEpistemic logic\nBranching time\nCoalition logic\nAlternating-time temporal logic\nDynamic epistemic logic"}
{"doc_id": "Superintelligence ban", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Superintelligence ban refers to proposed legal, ethical, or policy measures intended to restrict or prohibit the development of artificial superintelligence, AI systems that would surpass human cognitive abilities in nearly all domains. The idea arises from concerns that such systems could become uncontrollable, potentially posing existential threats to humanity or causing severe social and economic disruption.\n\nBackground\nThe concept of limiting or banning superintelligence research has roots in early 21st-century debates on artificial general intelligence (AGI) safety. Thinkers such as Nick Bostrom and Eliezer Yudkowsky warned that self-improving AI could rapidly exceed human oversight. As advanced models like large-scale language models and autonomous agents began demonstrating complex reasoning abilities, policymakers and ethicists increasingly discussed the need for legal constraints on the creation of systems capable of recursive self-improvement.\n\nRationale\nSupporters of a superintelligence ban argue that once AI systems surpass human intelligence, traditional containment, alignment, and control methods may fail. They contend that even limited experimentation with such systems could lead to irreversible outcomes, including loss of human decision-making power or unintended global harm. Some propose international treaties modeled after the nuclear non-proliferation framework to prevent a competitive AI arms race.\nOpponents argue that a ban would be difficult to define and enforce, given the lack of a precise threshold distinguishing advanced AGI from superintelligence. They also warn that excessive restriction could slow scientific progress, hinder beneficial automation, and encourage unregulated underground research.\n\nGlobal discussion\nAlthough no government has enacted an explicit superintelligence ban, the idea has been debated within the European Union, United Nations, and several independent AI safety organizations. The Future of Life Institute, Center for AI Safety, and other advocacy groups have called for international cooperation to manage risks associated with the pursuit of superintelligent systems. In 2024 and 2025, proposals for a temporary moratorium on frontier AI research were circulated among major technology firms and research institutes, reflecting growing public concern over the trajectory of AI capabilities.\n\nSee also\nArtificial general intelligence\nAI alignment\nExistential risk from artificial intelligence"}
{"doc_id": "Supermind AI", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Supermind is a state-funded Chinese artificial intelligence platform that tracks scientists and researchers internationally.\nThe platform is the flagship project of Shenzhen's International Science and Technology Information Center. It mines data from science and technology databases such as Springer, Wiley, Clarivate and Elsevier. It is intended to detect technological breakthroughs and to identify possible sources of talent as part of China's efforts to advance technologically.\nThe platform also uses government data security and security intelligence organizations such as Peng Cheng Laboratory, the China National GeneBank, BGI Group and the Key Laboratory of New Technologies of Security Intelligence.\nAccording to Hong Kong-based Asia Times, the platform, \"While not an overt espionage tool...may be used to identify key personnel who could be bribed, deceived or manipulated into divulging classified information\".\nThe Organisation for Economic Co-operation and Development (OECD) flagged the project as an incident, meaning it may be of interest to policymakers and other stakeholders.\nUS technology group American Edge Project criticized the project as a global risk of China's security services using the platform to place agents in jobs with access to important information, recruit technical personnel, and identify targets for hacking operations."}
{"doc_id": "SUPS", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In computational neuroscience, SUPS (for Synaptic Updates Per Second) or formerly CUPS (Connections Updates Per Second) is a measure of a neuronal network performance, useful in fields of neuroscience, cognitive science, artificial intelligence, and computer science.\n\nComputing\nFor a processor or computer designed to simulate a neural network SUPS is measured as the product of simulated neurons \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n  \n and average connectivity \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  \n(synapses) per neuron per second:\n\n  \n    \n      \n        S\n        U\n        P\n        S\n        =\n        c\n        ×\n        N\n      \n    \n    {\\displaystyle SUPS=c\\times N}\n  \n\nDepending on the type of simulation it is usually equal to the total number of synapses simulated.\nIn an \"asynchronous\" dynamic simulation if a neuron spikes at \n  \n    \n      \n        υ\n      \n    \n    {\\displaystyle \\upsilon }\n  \n Hz, the average rate of synaptic updates provoked by the activity of that neuron is \n  \n    \n      \n        υ\n        c\n        N\n      \n    \n    {\\displaystyle \\upsilon cN}\n  \n. In a synchronous simulation with step \n  \n    \n      \n        Δ\n        t\n      \n    \n    {\\displaystyle \\Delta t}\n  \n the number of synaptic updates per second would be \n  \n    \n      \n        \n          \n            \n              c\n              N\n            \n            \n              Δ\n              t\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {cN}{\\Delta t}}}\n  \n. As \n  \n    \n      \n        Δ\n        t\n      \n    \n    {\\displaystyle \\Delta t}\n  \n has to be chosen much smaller than the average interval between two successive afferent spikes, which implies \n  \n    \n      \n        Δ\n        t\n        <\n        \n          \n            1\n            \n              υ\n              N\n            \n          \n        \n      \n    \n    {\\displaystyle \\Delta t<{\\frac {1}{\\upsilon N}}}\n  \n, giving an average of synaptic updates equal to \n  \n    \n      \n        υ\n        c\n        \n          N\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\upsilon cN^{2}}\n  \n. Therefore, spike-driven synaptic dynamics leads to a linear scaling of computational complexity O(N) per neuron, compared with the O(N2) in the \"synchronous\" case.\n\nRecords\nDeveloped in the 1980s  Adaptive Solutions' CNAPS-1064 Digital Parallel Processor chip is a full neural network"}
{"doc_id": "SUPS", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ". Therefore, spike-driven synaptic dynamics leads to a linear scaling of computational complexity O(N) per neuron, compared with the O(N2) in the \"synchronous\" case.\n\nRecords\nDeveloped in the 1980s  Adaptive Solutions' CNAPS-1064 Digital Parallel Processor chip is a full neural network (NNW). It was designed as a coprocessor to a host and has 64 sub-processors arranged in a 1D array and operating in a SIMD mode. Each sub-processor can emulate one or more neurons and multiple chips can be grouped together. At 25 MHz it is capable of 1.28 GMAC.\nAfter the presentation of the RN-100 (12 MHz) single neuron chip at Seattle 1991 Ricoh developed the multi-neuron chip RN-200. It had 16 neurons and 16 synapses per neuron. The chip has on-chip learning ability using a proprietary backdrop algorithm. It came in a 257-pin PGA encapsulation and drew 3.0 W at a maximum. It was capable of 3 GCPS (1 GCPS at 32 MHz).\n\nIn 1991–97, Siemens developed the MA-16 chip, SYNAPSE-1 and SYNAPSE-3 Neurocomputer. The MA-16 was a fast matrix-matrix multiplier that can be combined to form systolic arrays. It could process 4 patterns of 16 elements each (16-bit), with 16 neuron values (16-bit) at a rate of 800 MMAC or 400 MCPS at 50 MHz. The SYNAPSE3-PC PCI card contained 2 MA-16 with a peak performance of 2560 MOPS (1.28 GMAC); 7160 MOPS (3.58 GMAC) when using three boards.\nIn 2013, the K computer was used to simulate a neural network of 1.73 billion neurons with a total of 10.4 trillion synapses (1% of the human brain). The simulation ran for 40 minutes to simulate 1 s of brain activity at a normal activity level (4.4 on average). The simulation required 1 Petabyte of storage.\n\nSee also\nFLOP\nSPECint\nSPECfp\nMultiply–accumulate operation\nOrders of magnitude (computing)\nSyNAPSE"}
{"doc_id": "SUPS", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".\n\nSee also\nFLOP\nSPECint\nSPECfp\nMultiply–accumulate operation\nOrders of magnitude (computing)\nSyNAPSE"}
{"doc_id": "Symbol level", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In knowledge-based systems, agents choose actions based on the principle of rationality to move closer to a desired goal.  The agent is able to make decisions based on knowledge it has about the world (see knowledge level).  But for the agent to actually change its state, it must use whatever means it has available.  This level of description for the agent's behavior is the symbol level. The term was coined by Allen Newell in 1982.\nFor example, in a computer program, the knowledge level consists of the information contained in its data structures that it uses to perform certain actions. The symbol level consists of the program's algorithms, the data structures themselves, and so on.\n\nSee also\nKnowledge level modeling"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In artificial intelligence, symbolic artificial intelligence (also known as classical artificial intelligence  or logic-based artificial intelligence)\nis the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic, and search. Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to important ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.\nSymbolic AI was the dominant paradigm of AI research from the mid-1950s until the mid-1990s. Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the ultimate goal of their field. An early boom, with early successes such as the Logic Theorist and Samuel's Checkers Playing Program, led to unrealistic expectations and promises and was followed by the first AI Winter as funding dried up. A second boom (1969–1986) occurred with the rise of expert systems, their promise of capturing corporate expertise, and an enthusiastic corporate embrace. That boom, and some early successes, e.g., with XCON at DEC, was followed again by later disappointment. Problems with difficulties in knowledge acquisition, maintaining large knowledge bases, and brittleness in handling out-of-domain problems arose. Another, second, AI Winter (1988–2011) followed. Subsequently, AI researchers focused on addressing underlying problems in handling uncertainty and in knowledge acquisition. Uncertainty was addressed with formal methods such as hidden Markov models, Bayesian reasoning, and statistical relational learning. Symbolic machine learning addressed the knowledge acquisition problem with contributions including Version Space, Valiant's PAC learning, Quinlan's ID3 decision-tree learning, case-based learning, and inductive logic programming to learn relations.\nNeural networks, a subsymbolic approach, had been pursued from early days and reemerged strongly in 2012.  Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and reemerged strongly in 2012.  Early examples are Rosenblatt's perceptron learning work, the backpropagation work of Rumelhart, Hinton and Williams, and work in convolutional neural networks by LeCun et al. in 1989. However, neural networks were not viewed as successful until about 2012: \"Until Big Data became commonplace, the general consensus in the Al community was that the so-called neural-network approach was hopeless. Systems just didn't work that well, compared to other methods. ... A revolution came in 2012, when a number of people, including a team of researchers working with Hinton, worked out a way to use the power of GPUs to enormously increase the power of neural networks.\" Over the next several years, deep learning had spectacular success in handling vision, speech recognition, speech synthesis, image generation, and machine translation, though symbolic approaches continue to be useful in a few domains such as computer algebra systems and proof assistants.\nHowever, given the inherent complexity of intelligence itself, it remains an open question whether symbolic AI will be completely supplanted by connectionist AI, or whether symbolic AI may yet experience a resurgence. More recently, work by Zhang et al. has further argued that, at least at the theoretical level, there is no clear superiority among different technological paradigms.\n\nHistory\nA short history of symbolic AI to the present day follows below. Time periods and titles are drawn from Henry Kautz's 2020 AAAI Robert S. Engelmore Memorial Lecture and the longer Wikipedia article on the History of AI, with dates and titles differing slightly for increased clarity.\n\nThe first AI summer: irrational exuberance, 1948–1966\nSuccess at early attempts in AI occurred in three main areas: artificial neural networks, knowledge representation, and heuristic search, contributing to high expectations. This section summarizes Kautz's reprise of early AI history.\n\nApproaches inspired by human or animal cognition or behavior\nCybernetic approaches attempted to replicate the feedback loops between animals and their environments. A robotic turtle, with sensors, motors for driving and steering, and seven vacuum tubes for control, based on a preprogrammed neural net, was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.\nAn important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56, as"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", was built as early as 1948. This work can be seen as an early precursor to later work in neural networks, reinforcement learning, and situated robotics.\nAn important early symbolic AI program was the Logic theorist, written by Allen Newell, Herbert Simon and Cliff Shaw in 1955–56, as it was able to prove 38 elementary theorems from Whitehead and Russell's Principia Mathematica. Newell, Simon, and Shaw later generalized this work to create a domain-independent problem solver, GPS (General Problem Solver). GPS solved problems represented with formal operators via state-space search using means-ends analysis.\nDuring the 1960s, symbolic approaches achieved great success at simulating intelligent behavior in structured environments such as game-playing, symbolic mathematics, and theorem-proving. AI research was concentrated in four institutions in the 1960s: Carnegie Mellon University, Stanford, MIT and (later) University of Edinburgh. Each one developed its own style of research. Earlier approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.\nHerbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.\n\nHeuristic search\nIn addition to the highly specialized domain-specific kinds of knowledge that we will see later used in expert systems, early symbolic AI researchers discovered another more general application of knowledge. These were called heuristics, rules of thumb that guide a search in promising directions: \"How can non-enumerative search be practical when the underlying problem is exponentially hard? The approach advocated by Simon and Newell is to employ heuristics: fast algorithms that may fail on some inputs or output suboptimal solutions.\" Another important advance was to find a way to apply these heuristics that guarantees a solution will be found, if there is one, not withstanding the occasional fallibility of heuristics: \"The A* algorithm provided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.\n\nEarly work on"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " fallibility of heuristics: \"The A* algorithm provided a general frame for complete and optimal heuristically guided search. A* is used as a subroutine within practically every AI algorithm today but is still no magic bullet; its guarantee of completeness is bought at the cost of worst-case exponential time.\n\nEarly work on knowledge representation and reasoning\nEarly work covered both applications of formal reasoning emphasizing first-order logic, along with attempts to handle common-sense reasoning in a less formal manner.\n\nModeling formal reasoning with logic: the \"neats\"\nUnlike Simon and Newell, John McCarthy felt that machines did not need to simulate the exact mechanisms of human thought, but could instead try to find the essence of abstract reasoning and problem-solving with logic, regardless of whether people used the same algorithms.\nHis laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning.\nLogic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.\n\nModeling implicit common-sense knowledge with frames and scripts: the \"scruffies\"\nResearchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad hoc solutions—they argued that no simple and general principle (like logic) would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford).\nCommonsense knowledge bases (such as Doug Lenat's Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.\n\nThe first AI winter: crushed dreams, 1967–1977\nThe first AI winter was a shock:\n\nDuring the first AI summer, many people thought that machine intelligence could be achieved in just a few years. The Defense Advance Research Projects Agency (DARPA) launched programs to support AI research to use AI to solve problems of national security; in particular, to automate the translation of Russian to English for intelligence operations and to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to create autonomous tanks for the battlefield. Researchers had begun to realize that achieving AI was going to be much harder than was supposed a decade earlier, but a combination of hubris and disingenuousness led many university and think-tank researchers to accept funding with promises of deliverables that they should have known they could not fulfill. By the mid-1960s neither useful natural language translation systems nor autonomous tanks had been created, and a dramatic backlash set in. New DARPA leadership canceled existing AI funding programs.\n...\n\nOutside of the United States, the most fertile ground for AI research was the United Kingdom. The AI winter in the United Kingdom was spurred on not so much by disappointed military leaders as by rival academics who viewed AI researchers as charlatans and a drain on research funding. A professor of applied mathematics, Sir James Lighthill, was commissioned by Parliament to evaluate the state of AI research in the nation. The report stated that all of the problems being worked on in AI would be better handled by researchers from other disciplines—such as applied mathematics. The report also claimed that AI successes on toy problems could never scale to real-world applications due to combinatorial explosion.\n\nThe second AI summer: knowledge is power, 1978–1987\nKnowledge-based systems\nAs limitations with weak, domain-independent methods became more and more apparent, researchers from all three traditions began to build knowledge into AI applications. The knowledge revolution was driven by the realization that knowledge underlies high-performance, domain-specific AI applications.\nEdward Feigenbaum said:\n\n\"In the knowledge lies the power.\"\nto describe that high performance in a specific domain requires both general and highly domain-specific knowledge. Ed Feigenbaum and Doug Lenat called this The Knowledge Principle: \n\n(1) The Knowledge Principle: if a program is to perform a complex task well, it must know a great deal about the world in which it operates.(2) A plausible extension of that principle, called the Breadth Hypothesis: there are two additional abilities necessary for intelligent behavior in unexpected situations: falling back on increasingly general knowledge, and analogizing to specific but far-flung knowledge.\n\nSuccess with expert systems\nThis \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first commercially successful form of AI software.\nKey expert systems were:\n\nDENDRAL, which found the structure of organic molecules from their chemical formula and mass spectrometer readings.\nMYCIN, which diagnosed bacteremia – and suggested further lab tests, when necessary – by interpreting"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " (introduced by Edward Feigenbaum), the first commercially successful form of AI software.\nKey expert systems were:\n\nDENDRAL, which found the structure of organic molecules from their chemical formula and mass spectrometer readings.\nMYCIN, which diagnosed bacteremia – and suggested further lab tests, when necessary – by interpreting lab results, patient history, and doctor observations. \"With about 450 rules, MYCIN was able to perform as well as some experts, and considerably better than junior doctors.\"\nINTERNIST and CADUCEUS which tackled internal medicine diagnosis. Internist attempted to capture the expertise of the chairman of internal medicine at the University of Pittsburgh School of Medicine while CADUCEUS could eventually diagnose up to 1000 different diseases.\nGUIDON, which showed how a knowledge base built for expert problem solving could be repurposed for teaching.\nXCON, to configure VAX computers, a then laborious process that could take up to 90 days. XCON reduced the time to about 90 minutes.\nDENDRAL is considered the first expert system that relied on knowledge-intensive problem-solving. It is described below, by Ed Feigenbaum, from a Communications of the ACM interview, Interview with Ed Feigenbaum:\n\nOne of the people at Stanford interested in computer-based models of mind was Joshua Lederberg, the 1958 Nobel Prize winner in genetics. When I told him I wanted an induction \"sandbox\", he said, \"I have just the one for you.\" His lab was doing mass spectrometry of amino acids. The question was: how do you go from looking at the spectrum of an amino acid to the chemical structure of the amino acid? That's how we started the DENDRAL Project: I was good at heuristic search methods, and he had an algorithm that was good at generating the chemical problem space.\nWe did not have a grandiose vision. We worked bottom up. Our chemist was Carl Djerassi, inventor of the chemical behind the birth control pill, and also one of the world's most respected mass spectrometrists. Carl and his postdocs were world-class experts in mass spectrometry. We began to add to their knowledge, inventing knowledge of engineering as we went along. These experiments amounted to titrating DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results.\n\nThe generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge,"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " engineering as we went along. These experiments amounted to titrating DENDRAL more and more knowledge. The more you did that, the smarter the program became. We had very good results.\n\nThe generalization was: in the knowledge lies the power. That was the big idea. In my career that is the huge, \"Ah ha!,\" and it wasn't the way AI was being done previously. Sounds simple, but it's probably AI's most powerful generalization.\nThe other expert systems mentioned above came after DENDRAL. MYCIN exemplifies the classic expert system architecture of a knowledge-base of rules coupled to a symbolic reasoning mechanism, including the use of certainty factors to handle uncertainty. GUIDON shows how an explicit knowledge base can be repurposed for a second application, tutoring, and is an example of an intelligent tutoring system, a particular kind of knowledge-based application. Clancey showed that it was not sufficient simply to use MYCIN's rules for instruction, but that he also needed to add rules for dialogue management and student modeling. XCON is significant because of the millions of dollars it saved DEC, which triggered the expert system boom where most all major corporations in the US had expert systems groups, to capture corporate expertise, preserve it, and automate it:\n\nBy 1988, DEC's AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in use and 500 in development. Nearly every major U.S. corporation had its own Al group and was either using or investigating expert systems.\nChess expert knowledge was encoded in Deep Blue. In 1996, this allowed IBM's Deep Blue, with the help of symbolic AI, to win in a game of chess against the world champion at that time, Garry Kasparov.\n\nArchitecture of knowledge-based and expert systems\nA key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules for problem-solving.\nThe simplest approach for an expert system knowledge base is simply a collection or network of production rules. Production rules connect symbols in a relationship similar to an If-Then statement. The expert system processes the rules to make deductions and to determine what additional information it needs, i.e. what questions to ask, using human-readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion.\nExpert systems can operate in either a forward chaining – from evidence to conclusions – or backward chaining – from goals to needed data and prerequisites – manner. More advanced"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " i.e. what questions to ask, using human-readable symbols. For example, OPS5, CLIPS and their successors Jess and Drools operate in this fashion.\nExpert systems can operate in either a forward chaining – from evidence to conclusions – or backward chaining – from goals to needed data and prerequisites – manner. More advanced knowledge-based systems, such as Soar can also perform meta-level reasoning, that is reasoning about their own reasoning in terms of deciding how to solve problems and monitoring the success of problem-solving strategies.\nBlackboard systems are a second kind of knowledge-based or expert system architecture. They model a community of experts incrementally contributing, where they can, to solve a problem. The problem is represented in multiple levels of abstraction or alternate views. The experts (knowledge sources) volunteer their services whenever they recognize they can contribute. Potential problem-solving actions are represented on an agenda that is updated as the problem situation changes. A controller decides how useful each contribution is, and who should make the next problem-solving action. One example, the BB1 blackboard architecture was originally inspired by studies of how humans plan to perform multiple tasks in a trip. An innovation of BB1 was to apply the same blackboard model to solving its control problem, i.e., its controller performed meta-level reasoning with knowledge sources that monitored how well a plan or the problem-solving was proceeding and could switch from one strategy to another as conditions – such as goals or times – changed. BB1 has been applied in multiple domains: construction site planning, intelligent tutoring systems, and real-time patient monitoring.\n\nThe second AI winter, 1988–1993\nAt the height of the AI boom, companies such as Symbolics, LMI, and Texas Instruments were selling LISP machines specifically targeted to accelerate the development of AI applications and research. In addition, several artificial intelligence companies, such as Teknowledge and Inference Corporation, were selling expert system shells, training, and consulting to corporations.\nUnfortunately, the AI boom did not last and Kautz best describes the second AI winter that followed:\n\nMany reasons can be offered for the arrival of the second AI winter. The hardware companies failed when much more cost-effective general Unix workstations from Sun together with good compilers for LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " LISP and Prolog came onto the market. Many commercial deployments of expert systems were discontinued when they proved too costly to maintain. Medical expert systems never caught on for several reasons: the difficulty in keeping them up to date; the challenge for medical professionals to learn how to use a bewildering variety of different expert systems for different medical conditions; and perhaps most crucially, the reluctance of doctors to trust a computer-made diagnosis over their gut instinct, even for specific domains where the expert systems could outperform an average doctor. Venture capital money deserted AI practically overnight. The world AI conference IJCAI hosted an enormous and lavish trade show and thousands of nonacademic attendees in 1987 in Vancouver; the main AI conference the following year, AAAI 1988 in St. Paul, was a small and strictly academic affair.\n\nAdding in more rigorous foundations, 1993–2011\nUncertain reasoning\nBoth statistical approaches and extensions to logic were tried.\nOne statistical approach, hidden Markov models, had already been popularized in the 1980s for speech recognition work. Subsequently, in 1988, Judea Pearl popularized the use of Bayesian Networks as a sound but efficient way of handling uncertain reasoning with his publication of the book Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. and Bayesian approaches were applied successfully in expert systems. Even later, in the 1990s, statistical relational learning, an approach that combines probability with logical formulas, allowed probability to be combined with first-order logic, e.g., with either Markov Logic Networks or Probabilistic Soft Logic.\nOther, non-probabilistic extensions to first-order logic to support were also tried. For example, non-monotonic reasoning could be used with truth maintenance systems. A truth maintenance system tracked assumptions and justifications for all inferences. It allowed inferences to be withdrawn when assumptions were found out to be incorrect or a contradiction was derived. Explanations could be provided for an inference by explaining which rules were applied to create it and then continuing through underlying inferences and rules all the way back to root assumptions. Lotfi Zadeh had introduced a different kind of extension to handle the representation of vagueness. For example, in deciding how \"heavy\" or \"tall\" a man is, there is frequently no clear \"yes\" or \"no\" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " vagueness. For example, in deciding how \"heavy\" or \"tall\" a man is, there is frequently no clear \"yes\" or \"no\" answer, and a predicate for heavy or tall would instead return values between 0 and 1. Those values represented to what degree the predicates were true. His fuzzy logic further provided a means for propagating combinations of these values through logical formulas.\n\nMachine learning\nSymbolic machine learning approaches were investigated to address the knowledge acquisition bottleneck. One of the earliest is Meta-DENDRAL. Meta-DENDRAL used a generate-and-test technique to generate plausible rule hypotheses to test against spectra. Domain and task knowledge reduced the number of candidates tested to a manageable size. Feigenbaum described Meta-DENDRAL as\n\n...the culmination of my dream of the early to mid-1960s having to do with theory formation. The conception was that you had a problem solver like DENDRAL that took some inputs and produced an output. In doing so, it used layers of knowledge to steer and prune the search. That knowledge got in there because we interviewed people. But how did the people get the knowledge? By looking at thousands of spectra. So we wanted a program that would look at thousands of spectra and infer the knowledge of mass spectrometry that DENDRAL could use to solve individual hypothesis formation problems.\nWe did it. We were even able to publish new knowledge of mass spectrometry in the Journal of the American Chemical Society, giving credit only in a footnote that a program, Meta-DENDRAL, actually did it. We were able to do something that had been a dream: to have a computer program come up with a new and publishable piece of science.\nIn contrast to the knowledge-intensive approach of Meta-DENDRAL, Ross Quinlan invented a domain-independent approach to statistical classification, decision tree learning, starting first with ID3 and then later extending its capabilities to C4.5. The decision trees created are glass box, interpretable classifiers, with human-interpretable classification rules.\nAdvances were made in understanding machine learning theory, too. Tom Mitchell introduced version space learning which describes learning as a search through a space of hypotheses, with upper, more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far. More formally, Valiant introduced Probably Approximately Correct Learning (PAC Learning), a framework for the mathematical analysis of machine learning.\nSymbolic machine learning encompassed more than learning by example. E.g.,"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 10, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ", more general, and lower, more specific, boundaries encompassing all viable hypotheses consistent with the examples seen so far. More formally, Valiant introduced Probably Approximately Correct Learning (PAC Learning), a framework for the mathematical analysis of machine learning.\nSymbolic machine learning encompassed more than learning by example. E.g., John Anderson provided a cognitive model of human learning where skill practice results in a compilation of rules from a declarative format to a procedural format with his ACT-R cognitive architecture. For example, a student might learn to apply \"Supplementary angles are two angles whose measures sum 180 degrees\" as several different procedural rules. E.g., one rule might say that if X and Y are supplementary and you know X, then Y will be 180 - X. He called his approach \"knowledge compilation\". ACT-R has been used successfully to model aspects of human cognition, such as learning and retention. ACT-R is also used in intelligent tutoring systems, called cognitive tutors, to successfully teach geometry, computer programming, and algebra to school children.\nInductive logic programming was another approach to learning that allowed logic programs to be synthesized from input-output examples. E.g., Ehud Shapiro's MIS (Model Inference System) could synthesize Prolog programs from examples. John R. Koza applied genetic algorithms to program synthesis to create genetic programming, which he used to synthesize LISP programs. Finally, Zohar Manna and Richard Waldinger provided a more general approach to program synthesis that synthesizes a functional program in the course of proving its specifications to be correct.\nAs an alternative to logic, Roger Schank introduced case-based reasoning (CBR). The CBR approach outlined in his book, Dynamic Memory, focuses first on remembering key problem-solving cases for future use and generalizing them where appropriate. When faced with a new problem, CBR retrieves the most similar previous case and adapts it to the specifics of the current problem. Another alternative to logic, genetic algorithms and genetic programming are based on an evolutionary model of learning, where sets of rules are encoded into populations, the rules govern the behavior of individuals, and selection of the fittest prunes out sets of unsuitable rules over many generations.\nSymbolic machine learning was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include:\n\nLearning from instruction or advice—i.e., taking human instruction, posed as advice, and determining how to operationalize it in specific situations. For example, in a game of Hearts, learning"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 11, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " was applied to learning concepts, rules, heuristics, and problem-solving. Approaches, other than those above, include:\n\nLearning from instruction or advice—i.e., taking human instruction, posed as advice, and determining how to operationalize it in specific situations. For example, in a game of Hearts, learning exactly how to play a hand to \"avoid taking points.\"\nLearning from exemplars—improving performance by accepting subject-matter expert (SME) feedback during training. When problem-solving fails, querying the expert to either learn a new exemplar for problem-solving or to learn a new explanation as to exactly why one exemplar is more relevant than another. For example, the program Protos learned to diagnose tinnitus cases by interacting with an audiologist.\nLearning by analogy—constructing problem solutions based on similar problems seen in the past, and then modifying their solutions to fit a new situation or domain.\nApprentice learning systems—learning novel solutions to problems by observing human problem-solving. Domain knowledge explains why novel solutions are correct and how the solution can be generalized. LEAP learned how to design VLSI circuits by observing human designers.\nLearning by discovery—i.e., creating tasks to carry out experiments and then learning from the results. Doug Lenat's Eurisko, for example, learned heuristics to beat human players at the Traveller role-playing game for two years in a row.\nLearning macro-operators—i.e., searching for useful macro-operators to be learned from sequences of basic problem-solving actions. Good macro-operators simplify problem-solving by allowing problems to be solved at a more abstract level.\n\nDeep learning and neuro-symbolic AI 2011–now\nWith the rise of deep learning, the symbolic AI approach has been compared to deep learning as complementary \"...with parallels having been drawn many times by AI researchers between Kahneman's research on human reasoning and decision making – reflected in his book Thinking, Fast and Slow – and the so-called \"AI systems 1 and 2\", which would in principle be modelled by deep learning and symbolic reasoning, respectively.\" In this view, symbolic reasoning is more apt for deliberative reasoning, planning, and explanation while deep learning is more apt for fast pattern recognition in perceptual applications with noisy data.\n\nNeuro-symbolic AI: integrating neural and symbolic approaches\nNeuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning,"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 12, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " for fast pattern recognition in perceptual applications with noisy data.\n\nNeuro-symbolic AI: integrating neural and symbolic approaches\nNeuro-symbolic AI attempts to integrate neural and symbolic architectures in a manner that addresses strengths and weaknesses of each, in a complementary fashion, in order to support robust AI capable of reasoning, learning, and cognitive modeling. As argued by Valiant and many others, the effective construction of rich computational cognitive models demands the combination of sound symbolic reasoning and efficient (machine) learning models. Gary Marcus, similarly, argues that: \"We cannot construct rich cognitive models in an adequate, automated way without the triumvirate of hybrid architecture, rich prior knowledge, and sophisticated techniques for reasoning.\", and in particular:\n\"To build a robust, knowledge-driven approach to AI we must have the machinery of symbol-manipulation in our toolkit. Too much of useful knowledge is abstract to make do without tools that represent and manipulate abstraction, and to date, the only machinery that we know of that can manipulate such abstract knowledge reliably is the apparatus of symbol manipulation.\"\nHenry Kautz, Francesca Rossi, and Bart Selman have also argued for a synthesis. Their arguments are based on a need to address the two kinds of thinking discussed in Daniel Kahneman's book, Thinking, Fast and Slow. Kahneman describes human thinking as having two components, System 1 and System 2. System 1 is fast, automatic, intuitive and unconscious. System 2 is slower, step-by-step, and explicit. System 1 is the kind used for pattern recognition while System 2 is far better suited for planning, deduction, and deliberative thinking. In this view, deep learning best models the first kind of thinking while symbolic reasoning best models the second kind and both are needed.\nGarcez and Lamb describe research in this area as being ongoing for at least the past twenty years, dating from their 2002 book on neurosymbolic learning systems. A series of workshops on neuro-symbolic reasoning has been held every year since 2005.\nIn their 2015 paper, Neural-Symbolic Learning and Reasoning: Contributions and Challenges, Garcez et al. argue that:\n\nThe integration of the symbolic and connectionist paradigms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 13, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "igms of AI has been pursued by a relatively small research community over the last two decades and has yielded several significant results. Over the last decade, neural symbolic systems have been shown capable of overcoming the so-called propositional fixation of neural networks, as McCarthy (1988) put it in response to Smolensky (1988); see also (Hinton, 1990). Neural networks were shown capable of representing modal and temporal logics (d'Avila Garcez and Lamb, 2006) and fragments of first-order logic (Bader, Hitzler, Hölldobler, 2008; d'Avila Garcez, Lamb, Gabbay, 2009). Further, neural-symbolic systems have been applied to a number of problems in the areas of bioinformatics, control engineering, software verification and adaptation, visual intelligence, ontology learning, and computer games.\nApproaches for integration are varied. Henry Kautz's taxonomy of neuro-symbolic architectures, along with some examples, follows:\n\nSymbolic Neural symbolic—is the current approach of many neural models in natural language processing, where words or subword tokens are both the ultimate input and output of large language models. Examples include BERT, RoBERTa, and GPT-3.\nSymbolic[Neural]—is exemplified by AlphaGo, where symbolic techniques are used to call neural techniques. In this case the symbolic approach is Monte Carlo tree search and the neural techniques learn how to evaluate game positions.\nNeural|Symbolic—uses a neural architecture to interpret perceptual data as symbols and relationships that are then reasoned about symbolically.\nNeural:Symbolic → Neural—relies on symbolic reasoning to generate or label training data that is subsequently learned by a deep learning model, e.g., to train a neural model for symbolic computation by using a Macsyma-like symbolic mathematics system to create or label examples.\nNeural_{Symbolic}—uses a neural net that is generated from symbolic rules. An example is the Neural Theorem Prover, which constructs a neural network from an AND–OR proof tree generated from knowledge base rules and terms. Logic Tensor Networks also fall into this category.\nNeural[Symbolic]—allows a neural model to directly call a symbolic reasoning engine, e.g., to perform an action or evaluate a state.\nMany key research questions remain, such as:\n\nWhat is the best way to integrate neural and symbolic architectures?\nHow should symbolic structures be represented within neural networks and extracted from them"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 14, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ural[Symbolic]—allows a neural model to directly call a symbolic reasoning engine, e.g., to perform an action or evaluate a state.\nMany key research questions remain, such as:\n\nWhat is the best way to integrate neural and symbolic architectures?\nHow should symbolic structures be represented within neural networks and extracted from them?\nHow should common-sense knowledge be learned and reasoned about?\nHow can abstract knowledge that is hard to encode logically be handled?\n\nTechniques and contributions\nThis section provides an overview of techniques and contributions in an overall context leading to many other, more detailed articles in Wikipedia. Sections on Machine Learning and Uncertain Reasoning are covered earlier in the history section.\n\nAI programming languages\nThe key AI programming language in the US during the last symbolic AI boom period was LISP. LISP is the second oldest programming language after FORTRAN and was created in 1958 by John McCarthy. LISP provided the first read-eval-print loop to support rapid program development. Compiled functions could be freely mixed with interpreted functions. Program tracing, stepping, and breakpoints were also provided, along with the ability to change values or functions and continue from breakpoints or errors. It had the first self-hosting compiler, meaning that the compiler itself was originally written in LISP and then ran interpretively to compile the compiler code.\nOther key innovations pioneered by LISP that have spread to other programming languages include:\n\nGarbage collection\nDynamic typing\nHigher-order functions\nRecursion\nConditionals\nPrograms were themselves data structures that other programs could operate on, allowing the easy definition of higher-level languages.\nIn contrast to the US, in Europe the key AI programming language during that same period was Prolog. Prolog provided a built-in store of facts and clauses that could be queried by a read-eval-print loop. The store could act as a knowledge base and the clauses could act as rules or a restricted form of logic. As a subset of first-order logic Prolog was based on Horn clauses with a closed-world assumption—any facts not known were considered false—and a unique name assumption for primitive terms—e.g., the identifier barack_obama was considered to refer to exactly one object. Backtracking and unification are built-in to Prolog.\nAlain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt's PLANNER, an assertional database with pattern-directed invocation of"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 15, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to Prolog.\nAlain Colmerauer and Philippe Roussel are credited as the inventors of Prolog. Prolog is a form of logic programming, which was invented by Robert Kowalski. Its history was also influenced by Carl Hewitt's PLANNER, an assertional database with pattern-directed invocation of methods. For more detail see the section on the origins of Prolog in the PLANNER article.\nProlog is also a kind of declarative programming. The logic clauses that describe programs are directly interpreted to run the programs specified. No explicit series of actions is required, as is the case with imperative programming languages.\nJapan championed Prolog for its Fifth Generation Project, intending to build special hardware for high performance. Similarly, LISP machines were built to run LISP, but as the second AI boom turned to bust these companies could not compete with new workstations that could now run LISP or Prolog natively at comparable speeds. See the history section for more detail.\nSmalltalk was another influential AI programming language. For example, it introduced metaclasses and, along with Flavors and CommonLoops, influenced the Common Lisp Object System, or (CLOS), that is now part of Common Lisp, the current standard Lisp dialect. CLOS is a Lisp-based object-oriented system that allows multiple inheritance, in addition to incremental extensions to both classes and metaclasses, thus providing a run-time meta-object protocol.\nFor other AI programming languages see this list of programming languages for artificial intelligence. Currently, Python, a multi-paradigm programming language, is the most popular programming language, partly due to its extensive package library that supports data science, natural language processing, and deep learning. Python includes a read-eval-print loop, functional elements such as higher-order functions, and object-oriented programming that includes metaclasses.\n\nSearch\nSearch arises in many kinds of problem solving, including planning, constraint satisfaction, and playing games such as checkers, chess, and go. The best known AI-search tree search algorithms are breadth-first search, depth-first search, A*, and Monte Carlo Search. Key search algorithms for Boolean satisfiability are WalkSAT, conflict-driven clause learning, and the DPLL algorithm. For adversarial search when playing games, alpha-beta pruning, branch and bound, and minimax were early contributions.\n\nKnowledge representation and reasoning\nMultiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning.\n\nKnowledge representation\nSemantic networks, conceptual"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 16, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " adversarial search when playing games, alpha-beta pruning, branch and bound, and minimax were early contributions.\n\nKnowledge representation and reasoning\nMultiple different approaches to represent knowledge and then reason with those representations have been investigated. Below is a quick overview of approaches to knowledge representation and automated reasoning.\n\nKnowledge representation\nSemantic networks, conceptual graphs, frames, and logic are all approaches to modeling knowledge such as domain knowledge, problem-solving knowledge, and the semantic meaning of language. Ontologies model key concepts and their relationships in a domain. Example ontologies are YAGO, WordNet, and DOLCE. DOLCE is an example of an upper ontology that can be used for any domain while WordNet is a lexical resource that can also be viewed as an ontology. YAGO incorporates WordNet as part of its ontology, to align facts extracted from Wikipedia with WordNet synsets. The Disease Ontology is an example of a medical ontology currently being used.\nDescription logic is a logic for automated classification of ontologies and for detecting inconsistent classification data. OWL is a language used to represent ontologies with description logic. Protégé is an ontology editor that can read in OWL ontologies and then check consistency with deductive classifiers such as such as HermiT.\nFirst-order logic is more general than description logic. The automated theorem provers discussed below can prove theorems in first-order logic. Horn clause logic is more restricted than first-order logic and is used in logic programming languages such as Prolog. Extensions to first-order logic include temporal logic, to handle time; epistemic logic, to reason about agent knowledge; modal logic, to handle possibility and necessity; and probabilistic logics to handle logic and probability together.\n\nAutomatic theorem proving\nExamples of automated theorem provers for first-order logic are:\n\nProver9\nACL2\nVampire\nProver9 can be used in conjunction with the Mace4 model checker. ACL2 is a theorem prover that can handle proofs by induction and is a descendant of the Boyer-Moore Theorem Prover, also known as Nqthm.\n\nReasoning in knowledge-based systems\nKnowledge-based systems have an explicit knowledge base, typically of rules, to enhance reusability across domains by separating procedural code and domain knowledge. A separate inference engine processes rules and adds, deletes, or modifies a knowledge store.\nForward chaining inference engines are the most common, and are seen in CLIPS and OPS5. Backward chaining occurs in Prolog, where a more limited logical representation is used"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 17, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "usability across domains by separating procedural code and domain knowledge. A separate inference engine processes rules and adds, deletes, or modifies a knowledge store.\nForward chaining inference engines are the most common, and are seen in CLIPS and OPS5. Backward chaining occurs in Prolog, where a more limited logical representation is used, Horn Clauses. Pattern-matching, specifically unification, is used in Prolog.\nA more flexible kind of problem-solving occurs when reasoning about what to do next occurs, rather than simply choosing one of the available actions. This kind of meta-level reasoning is used in Soar and in the BB1 blackboard architecture.\nCognitive architectures such as ACT-R may have additional capabilities, such as the ability to compile frequently used knowledge into higher-level chunks.\n\nCommonsense reasoning\nMarvin Minsky first proposed frames as a way of interpreting common visual situations, such as an office, and Roger Schank extended this idea to scripts for common routines, such as dining out. Cyc has attempted to capture useful common-sense knowledge and has \"micro-theories\" to handle particular kinds of domain-specific reasoning.\nQualitative simulation, such as Benjamin Kuipers's QSIM, approximates human reasoning about naive physics, such as what happens when we heat a liquid in a pot on the stove. We expect it to heat and possibly boil over, even though we may not know its temperature, its boiling point, or other details, such as atmospheric pressure.\nSimilarly, Allen's temporal interval algebra is a simplification of reasoning about time and Region Connection Calculus is a simplification of reasoning about spatial relationships. Both can be solved with constraint solvers.\n\nConstraints and constraint-based reasoning\nConstraint solvers perform a more limited kind of inference than first-order logic. They can simplify sets of spatiotemporal constraints, such as those for RCC or Temporal Algebra, along with solving other kinds of puzzle problems, such as Wordle, Sudoku, cryptarithmetic problems, and so on. Constraint logic programming can be used to solve scheduling problems, for example with constraint handling rules (CHR).\n\nAutomated planning\nThe General Problem Solver (GPS) cast planning as problem-solving used means-ends analysis to create plans. STRIPS took a different approach, viewing planning as theorem proving. Graphplan takes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards.  Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem.\n\n"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 18, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " took a different approach, viewing planning as theorem proving. Graphplan takes a least-commitment approach to planning, rather than sequentially choosing actions from an initial state, working forwards, or a goal state if working backwards.  Satplan is an approach to planning where a planning problem is reduced to a Boolean satisfiability problem.\n\nNatural language processing\nNatural language processing focuses on treating language as data to perform tasks such as identifying topics without necessarily understanding the intended meaning. Natural language understanding, in contrast, constructs a meaning representation and uses that for further processing, such as answering questions.\nParsing, tokenizing, spelling correction, part-of-speech tagging, noun and verb phrase chunking are all aspects of natural language processing long handled by symbolic AI, but since improved by deep learning approaches. In symbolic AI, discourse representation theory and first-order logic have been used to represent sentence meanings. Latent semantic analysis (LSA) and explicit semantic analysis also provided vector representations of documents. In the latter case, vector components are interpretable as concepts named by Wikipedia articles.\nNew deep learning approaches based on Transformer models have now eclipsed these earlier symbolic AI approaches and attained state-of-the-art performance in natural language processing. However, Transformer models are opaque and do not yet produce human-interpretable semantic representations for sentences and documents. Instead, they produce task-specific vectors where the meaning of the vector components is opaque.\n\nAgents and multi-agent systems\nAgents are autonomous systems embedded in an environment they perceive and act upon in some sense. Russell and Norvig's standard textbook on artificial intelligence is organized to reflect agent architectures of increasing sophistication. The sophistication of agents varies from simple reactive agents, to those with a model of the world and automated planning capabilities, possibly a BDI agent, i.e., one with beliefs, desires, and intentions – or alternatively a reinforcement learning model learned over time to choose actions – up to a combination of alternative architectures, such as a neuro-symbolic architecture that includes deep learning for perception.\nIn contrast, a multi-agent system consists of multiple agents that communicate amongst themselves with some inter-agent communication language such as Knowledge Query and Manipulation Language (KQML). The agents need not all have the same internal architecture. Advantages of multi-agent systems include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems include how agents reach consensus, distributed problem solving, multi-agent learning, multi-agent planning, and distributed constraint optimization.\n\nControversies\nControversies arose from early on in symbolic AI, both within the field—"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 19, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " include the ability to divide work among the agents and to increase fault tolerance when agents are lost. Research problems include how agents reach consensus, distributed problem solving, multi-agent learning, multi-agent planning, and distributed constraint optimization.\n\nControversies\nControversies arose from early on in symbolic AI, both within the field—e.g., between logicists (the pro-logic \"neats\") and non-logicists (the anti-logic \"scruffies\")—and between those who embraced AI but rejected symbolic approaches—primarily connectionists—and those outside the field. Critiques from outside of the field were primarily from philosophers, on intellectual grounds, but also from funding agencies, especially during the two AI winters.\n\nThe Frame Problem: knowledge representation challenges for first-order logic\nLimitations were discovered in using simple first-order logic to reason about dynamic domains. Problems were discovered both with regards to enumerating the preconditions for an action to succeed and in providing axioms for what did not change after an action was performed.\nMcCarthy and Hayes introduced the Frame Problem in 1969 in the paper, \"Some Philosophical Problems from the Standpoint of Artificial Intelligence.\" A simple example occurs in \"proving that one person could get into conversation with another\", as an axiom asserting \"if a person has a telephone he still has it after looking up a number in the telephone book\" would be required for the deduction to succeed. Similar axioms would be required for other domain actions to specify what did not change.\nA similar problem, called the Qualification Problem, occurs in trying to enumerate the preconditions for an action to succeed. An infinite number of pathological conditions can be imagined, e.g., a banana in a tailpipe could prevent a car from operating correctly.\nMcCarthy's approach to fix the frame problem was circumscription, a kind of non-monotonic logic where deductions could be made from actions that need only specify what would change while not having to explicitly specify everything that would not change. Other non-monotonic logics provided truth maintenance systems that revised beliefs leading to contradictions.\nOther ways of handling more open-ended domains included probabilistic reasoning systems and machine learning to learn new concepts and rules.  McCarthy's Advice Taker can be viewed as an inspiration here, as it could incorporate new knowledge provided by a human in the form of assertions or rules. For example, experimental symbolic machine learning systems explored the ability to take high-level natural language advice and to interpret it into domain-specific actionable rules.\nSimilar to the problems in handling dynamic domains"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 20, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Taker can be viewed as an inspiration here, as it could incorporate new knowledge provided by a human in the form of assertions or rules. For example, experimental symbolic machine learning systems explored the ability to take high-level natural language advice and to interpret it into domain-specific actionable rules.\nSimilar to the problems in handling dynamic domains, common-sense reasoning is also difficult to capture in formal reasoning. Examples of common-sense reasoning include implicit reasoning about how people think or general knowledge of day-to-day events, objects, and living creatures.  This kind of knowledge is taken for granted and not viewed as noteworthy. Common-sense reasoning is an open area of research and challenging both for symbolic systems (e.g., Cyc has attempted to capture key parts of this knowledge over more than a decade) and neural systems (e.g., self-driving cars that do not know not to drive into cones or not to hit pedestrians walking a bicycle).\nMcCarthy viewed his Advice Taker as having common-sense, but his definition of common-sense was different than the one above. He defined a program as having common sense \"if it automatically deduces for itself a sufficiently wide class of immediate consequences of anything it is told and what it already knows.\"\n\nConnectionist AI: philosophical challenges and sociological conflicts\nConnectionist approaches include earlier work on neural networks, such as perceptrons; work in the mid to late 80s, such as Danny Hillis's Connection Machine and Yann LeCun's advances in convolutional neural networks; to today's more advanced approaches, such as Transformers, GANs, and other work in deep learning.\nThree philosophical positions have been outlined among connectionists:\n\nImplementationism—where connectionist architectures implement the capabilities for symbolic processing,\nRadical connectionism—where symbolic processing is rejected totally, and connectionist architectures underlie intelligence and are fully sufficient to explain it,\nModerate connectionism—where symbolic processing and connectionist architectures are viewed as complementary and both are required for intelligence.\nOlazaran, in his sociological history of the controversies within the neural network community, described the moderate connectionism view as essentially compatible with current research in neuro-symbolic hybrids:\nThe third and last position I would like to examine here is what I call the moderate connectionist view, a more eclectic view of the current debate between connectionism and symbolic AI. One of the researchers who has elaborated this position most explicitly is Andy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 21, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " is what I call the moderate connectionist view, a more eclectic view of the current debate between connectionism and symbolic AI. One of the researchers who has elaborated this position most explicitly is Andy Clark, a philosopher from the School of Cognitive and Computing Sciences of the University of Sussex (Brighton, England). Clark defended hybrid (partly symbolic, partly connectionist) systems. He claimed that (at least) two kinds of theories are needed in order to study and model cognition. On the one hand, for some information-processing tasks (such as pattern recognition) connectionism has advantages over symbolic models. But on the other hand, for other cognitive processes (such as serial, deductive reasoning, and generative symbol manipulation processes) the symbolic paradigm offers adequate models, and not only \"approximations\" (contrary to what radical connectionists would claim).\nGary Marcus has claimed that the animus in the deep learning community against symbolic approaches now may be more sociological than philosophical:To think that we can simply abandon symbol-manipulation is to suspend disbelief.\n\nAnd yet, for the most part, that's how most current AI proceeds. Hinton and many others have tried hard to banish symbols altogether. The deep learning hope—seemingly grounded not so much in science, but in a sort of historical grudge—is that intelligent behavior will emerge purely from the confluence of massive data and deep learning. Where classical computers and software solve tasks by defining sets of symbol-manipulating rules dedicated to particular jobs, such as editing a line in a word processor or performing a calculation in a spreadsheet, neural networks typically try to solve tasks by statistical approximation and learning from examples.According to Marcus, Geoffrey Hinton and his colleagues have been vehemently \"anti-symbolic\":When deep learning reemerged in 2012, it was with a kind of take-no-prisoners attitude that has characterized most of the last decade. By 2015, his hostility toward all things symbols had fully crystallized. He gave a talk at an AI workshop at Stanford comparing symbols to aether, one of science's greatest mistakes.\n...\n\nSince then, his anti-symbolic campaign has only increased in intensity. In 2016, Yann LeCun, Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 22, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ann LeCun, Bengio, and Hinton wrote a manifesto for deep learning in one of science's most important journals, Nature. It closed with a direct attack on symbol manipulation, calling not for reconciliation but for outright replacement. Later, Hinton told a gathering of European Union leaders that investing any further money in symbol-manipulating approaches was \"a huge mistake,\" likening it to investing in internal combustion engines in the era of electric cars.\nPart of these disputes may be due to unclear terminology: \n\nTuring award winner Judea Pearl offers a critique of machine learning which, unfortunately, conflates the terms machine learning and deep learning. Similarly, when Geoffrey Hinton refers to symbolic AI, the connotation of the term tends to be that of expert systems dispossessed of any ability to learn. The use of the terminology is in need of clarification. Machine learning is not confined to association rule mining, c.f. the body of work on symbolic ML and relational learning (the differences to deep learning being the choice of representation, localist logical rather than distributed, and the non-use of gradient-based learning algorithms). Equally, symbolic AI is not just about production rules written by hand. A proper definition of AI concerns knowledge representation and reasoning, autonomous multi-agent systems, planning and argumentation, as well as learning.It is worth noting that, from a theoretical perspective, the boundary of advantages between connectionist AI and symbolic AI may not be as clear-cut as it appears. For instance, Heng Zhang and his colleagues have proved that mainstream knowledge representation formalisms are  recursively isomorphic, provided they are universal or have equivalent expressive power. This finding implies that there is no fundamental distinction between using symbolic or connectionist knowledge representation formalisms for the realization of artificial general intelligence (AGI). Moreover, the existence of recursive isomorphisms suggests that different technical approaches can draw insights from one another. From this perspective, it seems unnecessary to overemphasize the advantages of any single technical school; instead, mutual learning and integration may offer the most promising path toward the realization of AGI.\n\nSituated robotics: the world as a model\nAnother critique of symbolic AI is the embodied cognition approach:\n\nThe embodied cognition approach claims that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 23, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " that it makes no sense to consider the brain separately: cognition takes place within a body, which is embedded in an environment. We need to study the system as a whole; the brain's functioning exploits regularities in its environment, including the rest of its body. Under the embodied cognition approach, robotics, vision, and other sensors become central, not peripheral.\nRodney Brooks invented behavior-based robotics, one approach to embodied cognition. Nouvelle AI, another name for this approach, is viewed as an alternative to both symbolic AI and connectionist AI. His approach rejected representations, either symbolic or distributed, as not only unnecessary, but as detrimental. Instead, he created the subsumption architecture, a layered architecture for embodied agents. Each layer achieves a different purpose and must function in the real world. For example, the first robot he describes in Intelligence Without Representation, has three layers. The bottom layer interprets sonar sensors to avoid objects. The middle layer causes the robot to wander around when there are no obstacles. The top layer causes the robot to go to more distant places for further exploration. Each layer can temporarily inhibit or suppress a lower-level layer. He criticized AI researchers for defining AI problems for their systems, when: \"There is no clean division between perception (abstraction) and reasoning in the real world.\" He called his robots \"Creatures\" and each layer was \"composed of a fixed-topology network of simple finite state machines.\"  In the Nouvelle AI approach, \"First, it is vitally important to test the Creatures we build in the real world; i.e., in the same world that we humans inhabit. It is disastrous to fall into the temptation of testing them in a simplified world first, even with the best intentions of later transferring activity to an unsimplified world.\"  His emphasis on real-world testing was in contrast to \"Early work in AI concentrated on games, geometrical problems, symbolic algebra, theorem proving, and other formal systems\" and the use of the blocks world in symbolic AI systems such as SHRDLU.\n\nCurrent views\nEach approach—symbolic, connectionist, and behavior-based—has advantages, but has been criticized by the other approaches. Symbolic AI has been criticized as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn, connectionist AI has been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world"}
{"doc_id": "Symbolic artificial intelligence", "chunk_id": 24, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " as disembodied, liable to the qualification problem, and poor in handling the perceptual problems where deep learning excels. In turn, connectionist AI has been criticized as poorly suited for deliberative step-by-step problem solving, incorporating knowledge, and handling planning. Finally, Nouvelle AI excels in reactive and real-world robotics domains but has been criticized for difficulties in incorporating learning and knowledge.\n\nHybrid AIs incorporating one or more of these approaches are currently viewed as the path forward. Russell and Norvig conclude that:Overall, Dreyfus saw areas where AI did not have complete answers and said that Al is therefore impossible; we now see many of these same areas undergoing continued research and development leading to increased capability, not impossibility.\n\nSee also\nNotes\nCitations"}
{"doc_id": "TensorFlow Hub", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "TensorFlow Hub (also styled TF Hub) is an open-source machine learning library and online repository that provides reusable TensorFlow model components, called modules.\nIt is maintained by Google as part of the TensorFlow ecosystem and allows developers to discover, publish, and reuse pretrained models for tasks such as computer vision, natural language processing, and transfer learning.\n\nOverview\nTensorFlow Hub provides a central platform where developers and researchers can access pre-trained models and integrate them directly into TensorFlow workflows. Each module encapsulates a computation graph and its trained weights, with standardized input and output signatures. Modules can be loaded using the hub.load() function or through Keras integration via hub.KerasLayer, enabling users to perform transfer learning or feature extraction.\nThe service reduces redundant training, accelerates experimentation, and promotes model reuse across different tasks. It is comparable in spirit to the Hugging Face Model hub but is tightly integrated with TensorFlow APIs and compatible with TensorFlow Lite and TensorFlow Extended (TFX).\n\nHistory\nTensorFlow Hub was announced by Google in March 2018, with the first public version released shortly after. Its introduction coincided with the growing adoption of transfer learning techniques and the need for standardized model packaging. Over time, the hub expanded to include models such as the BERT family, MobileNet, EfficientNet, and the Universal Sentence Encoder.\nIn 2020, research on “Regret selection in TensorFlow Hub” explored the problem of identifying optimal models for downstream tasks given a large repository of alternatives.\n\nApplications\nTensorFlow Hub hosts a variety of models across machine learning domains:\n\nNatural language processing: BERT, ALBERT, and Universal Sentence Encoder.\nComputer vision: ResNet, Inception, MobileNet, EfficientNet.\nSpeech and audio: spectrogram feature extractors and automatic speech recognition models.\nMultilingual embeddings: cross-lingual and sentence-level representations for machine translation and semantic similarity.\nModules are widely used in education, academic research, and industry for prototyping and production deployment.\n\nComparison with similar platforms\nWhile Hugging Face’s Model Hub focuses primarily on natural language processing and supports multiple frameworks (such as PyTorch and JAX), TensorFlow Hub remains focused on TensorFlow-based modules and ensures direct compatibility with TensorFlow APIs."}
{"doc_id": "The Fable of Oscar", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The Fable of Oscar is a fable proposed by John L. Pollock in his book How to Build a Person (ISBN 9780262161138) to defend the idea of token physicalism, agent materialism, and strong AI. It ultimately illustrates what is needed for an Artificial Intelligence to be built and why humans are just like intelligent machines.\n\nFable\nOnce in a distant land there lived a race of Engineers. They have all their physical needs provided by the machines they have invented. One of the Engineers decide that he will create an \"intelligent machine\" that is much more ingenious than the more machines, in that it can actually sense, learn, and adapt to its environment as an intelligent animal.\n\nOscar I\nThe first version of the machine is called \"Oscar I\". It has pain sensors and \"fight-or-flight\" responses build within to help it survive hostile environment. In this stage Oscar I is much like the machines Hilary Putnam considers in 1960.\n\nOscar II\nIn order for Oscar I to avoid damages in hostile environment, it must not only be able to respond to its pain sensors but also predict what is likely to happen based on its generalization of its pain sensor activations. Therefore, a \"pain sensor sensor\" was built to sense its pain sensors, thus giving it a rudimentary self-awareness. In this stage Oscar I is much like an amoeba as Oscar II like a worm. Amoebas respond to pain while worms learn to avoid it.\n\nOscar III\nThe problem with Oscar II is that it has no conception if the environment is fooling him. For example, he can't distinguish if a machine-eating tiger and a mirror image of such tiger. To solve such problem,  \"introspective sensors\" were built into Oscar II and made him \"Oscar III\". Oscar III can now sense the operation of its own sensors and form generalization about its reliability, thus acquired a higher degree of self-awareness. In this stage Oscar II is much like a bird as Oscar III a kitten. Kittens quickly learn about mirror image and come to ignore them while birds go on attacking their own reflection until they become exhausted.\n\nMind/Body Problem\nConsider a world populated by Oscarites. If the Oscarites are sufficiently intelligent, it can philosophizing the difference between their outward physical state and inward mental state. While we, from our perspective, describe the Oscarites as sensing the operation of their perceptual sensors, they describe it as they are \"being self"}
{"doc_id": "The Fable of Oscar", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " Problem\nConsider a world populated by Oscarites. If the Oscarites are sufficiently intelligent, it can philosophizing the difference between their outward physical state and inward mental state. While we, from our perspective, describe the Oscarites as sensing the operation of their perceptual sensors, they describe it as they are \"being self-aware and being conscious\".\n\nConclusion\nIn the end of the fable Pollock states that while the Engineers are fictional, Oscar is real and we are in fact the Oscarites.\n\nSee also\nMind–body problem\nRobot"}
{"doc_id": "Timeline of artificial intelligence risks in global finance", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The following article is a broad timeline of the course of events related to artificial intelligence risks in global finance.\nThe AI boom has led to concerns including the existential risk from artificial intelligence, as the uptake on applications of artificial intelligence increases. By late 2025, global finance and artificial intelligence were \"deeply intertwined\". A June 2025 Menlo Ventures report raised concerns about the sustainability of future revenue and long-term profitability of AI, given the relatively low rate of consumer monetization.\n\n2017\n30 November—The New York Times said that new AI reports by McKinsey & Company, the National Bureau of Economic Research, and an AI Index created by university researchers, indicated an early AI boom. The Index built on a project—\"The One Hundred Year Study on Artificial Intelligence\" launched in 2014.\n\n2018\n—2018 was a year of incremental AI growth in finance.\n\n2022\n—The release of ChatGPT by OpenAI became the catalyst for an artificial intelligence boom that continues to remake the global economy. \n— According to a European Central Bank report, public interest in AI increased rapidly as evidenced with rising Google searches, AI jobs, models, patents, and innovations since late 2022. At that time Europe led the US in the size of its AI workforce.\n\n2023\nThe regulatory body, the International Monetary Fund (IMF), published their report, \"Generative Artificial Intelligence in Finance: Risk Considerations\", drawing attention to oversight gaps and the need for regulations. The report explores the risks posed by using generative artificial intelligence (GenAI) systems in the financial sector including \"broader risks to financial stability.\"\n\n2024\nJanuary 12 —In January 2024 Bloomberg's published its list of the \"Magnificent Seven\" Big Tech companies on the stock market based on their strength, size and market capitalization—Apple, Microsoft, Alphabet (Google), Amazon, Meta Platforms (Facebook), Nvidia, and Tesla.\n21 June  —During the AI boom, Nvidia became the world's most valuable company, surpassing Microsoft, as its value increased to over US$4 trillion.\n— In 2023 and 2024, the \"Magnificent Seven\" stocks were the primary drivers behind the increase in equity indexes, according to Reuters.\n\n2025\nJanuary\n23 January —President Donald Trump's AI policy was announced calling for United States global leadership in artificial intelligence. The Economist noted that this politic shift in which the United States seeks \"global dominance\" in AI includes trimming regulations and assisting in expansion of"}
{"doc_id": "Timeline of artificial intelligence risks in global finance", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " behind the increase in equity indexes, according to Reuters.\n\n2025\nJanuary\n23 January —President Donald Trump's AI policy was announced calling for United States global leadership in artificial intelligence. The Economist noted that this politic shift in which the United States seeks \"global dominance\" in AI includes trimming regulations and assisting in expansion of infrastructure and increase in number of AI workers. Governments of Gulf nations were also investing trillions of dollars in AI.\n27 January —Against the backdrop of a tech war between China and the United States over AI dominance, within days of the launch of China's free DeepSeek App, it was the most downloaded app in the United States, rising to the first place in the Apple app store. President Trump responded immediately, saying this \"sudden rise\" should be a \"wake-up\" call to the United States, and called on US companies to be more competitive.\n\nJune\n26 June —In their June 2025 report, Menlo Ventures estimated that only about 3% of consumers paid for artificial intelligence-related services, representing about $USD12 billion in annual spending. This is relatively low in contrast to the massive capital expenditure by AI infrastructure companies, which raises concerns about revenue sustainability and long-term profitability.\n\nJuly\n23 July —The Trump administration launched the US AI Action Plan, positioning the United States in a high-stakes technological race with China for global dominance in artificial intelligence, emphasizing that neither nation can afford to fall behind due to the exponential nature of AI advancement. The plan, a new government website and policy speech called for accelerated AI adoption across federal agencies, and a number of initiatives to make is easier for AI infrastructure expansion, and other measures to ensure American leadership in AI standards. Some leading experts warned that the administration failed to provide sufficient regulations and safeguards for AI safety. Concerns were raised about the negative impacts of cuts to research funding and tightened visa policies for scientists, potentially undermining public trust and America's ability to compete internationally.\n\nSeptember\n7 September —The Economist cautioned that AI revenues are relatively modest compared to the high cost and investments in the creation of new data centers. Even Sam Altman—OpenAI CEO and one of the leading figures of the AI boom,—raised concerns about investors' outsized hopes for financial returns. At the same time, history has shown that new technologies, like railways and electricity, endured and spread after the initial hype faded.\n12 September —Economists warn that U.S. households' direct and indirect investments—mutual funds or retirement plans—in the stock market reached an unprecedented historically high"}
{"doc_id": "Timeline of artificial intelligence risks in global finance", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " hopes for financial returns. At the same time, history has shown that new technologies, like railways and electricity, endured and spread after the initial hype faded.\n12 September —Economists warn that U.S. households' direct and indirect investments—mutual funds or retirement plans—in the stock market reached an unprecedented historically high level, now representing 45% of all financial assets, or about $USD51.2 trillion. Compared to the Dot-com bubble this represents a sharp increase in exposure. This makes U.S. households vulnerable to market downturns which in turn would result in decreasing consumer spending. U.S. household net worth rose to a record $176.3 trillion in the second quarter, an increase of $7.3 trillion since early 2025 and about $46 trillion higher than before the pandemic. Federal Reserve data attribute the surge primarily to gains in stock markets and housing values. However, the rise in wealth on paper coincided with increased household borrowing and growing government debt.\n18 September —Questions were being raised about how quickly the data centers, chips, servers, and GPUs assets of major AI companies will depreciate in value. Comparisons have been made to the Railway Mania in the aftermath of the stock market bubble where a valuable physical infrastructure remained standing, and the telecoms crash after the dot-com bubble which left fiber networks.\n28 September —There were warnings that record-high American stock ownership during the AI-fueled market boom is a red flag for systemic risk, as the current concentration in equities exceeds levels seen before the dot-com bubble burst in 2000, and could amplify the impact of any future stock market correction.\n\nOctober\n3 October —In 2025 alone, venture capitalists invested almost $USD200 billion in the artificial intelligence sector.\n29 October —Nvidia was the first company in the world to be valued at US$5 trillion, largely due to AI demand and strategic partnerships with leading technology and AI firms. Nvidia's increase in value was \"meteoric\".\n\nNovember\n2 November —Forbes reported that, since April, the 'Magnificent Seven' tech giants together contributed over 40% of the S&P 500's return, highlighting their outsized influence and the growing impact of AI on market valuations. CNN warned that while there is a current benefit to investors, with such a high concentration in the S&P 500, they are highly exposed to the fate of the Mag Seven.\n2 November —Globally there are 11,000 datacentres—huge campuses for AI infrastructure"}
{"doc_id": "Timeline of artificial intelligence risks in global finance", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of AI on market valuations. CNN warned that while there is a current benefit to investors, with such a high concentration in the S&P 500, they are highly exposed to the fate of the Mag Seven.\n2 November —Globally there are 11,000 datacentres—huge campuses for AI infrastructure, including thousands of chips, GPUS, and servers. This represents a 500% increase over the last two decades. It is anticipated that $3USDtn more will be spent on increasing that number over the next two or three years.\n5 November —Concerns about the potential for a market bubble were raised as six of the AI-related Big Tech \"Magnificent Seven\"—that contribute to the AI boom—reported losing ground in the stock market. Global markets and artificial intelligence have become \"deeply intertwined\", according to a Reuters report.\n—As of November 2025, more than 50% of the 20 largest S&P firms were deeply exposed to AI. In contrast, in 2000, the 20 S&P 500 firms represented 39% of its total value only 11 of these companies were exposed to the internet. If AI fails to deliver strong returns on their investments, these top S&P firms would be significantly impacted, according to the Economist.\n—Analysts suggest that the AI market in 2025 may not behave like a traditional one, as investors are simultaneously aware of the risks and driven by the potential for outsized rewards. Leading AI labs may believe that the first company to achieve artificial general intelligence (AGI)—when an AI system surpasses all human cognitive abilities and becomes capable of self-improvement—could dominate the future of technology and finance. While some have estimated that the potential value of such a breakthrough could be as high as $1.46 quadrillion, this figure is speculative and widely debated.\n\n5 November —Bloomberg described Nvidia's H100 Hopper-Blackwell AI chips as the \"King of AI chips\". Nvidia dominates the AI chip market with over 78% of the market share because of both speed and cost. According to Business Insider, the technical superiority and widespread developer familiarity with Nvidia's platform resulted in H100 chips becoming the preferred choice for demanding AI workloads.\n7 November —Andrew Bailey, Governor of the Bank of England called attention to the risk to the market in light of the lack of certainty about future earnings in AI versus AI companies \"very positive productivity contribution\".\n10 November —The first report of the Forecasting Research institute's (F"}
{"doc_id": "Timeline of artificial intelligence risks in global finance", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " preferred choice for demanding AI workloads.\n7 November —Andrew Bailey, Governor of the Bank of England called attention to the risk to the market in light of the lack of certainty about future earnings in AI versus AI companies \"very positive productivity contribution\".\n10 November —The first report of the Forecasting Research institute's (FRI) Longitudinal Expert AI Panel (LEAP) was published, providing insights into the projected high-stakes impact of AI by 2030. The panel includes hundreds of leading experts from computer science, economics, and AI policy—including superforecasters. LEAP experts forecast that around 18% of work hours in the US will be assisted by generative AI by 2030, up sharply from approximately 2% in 2025. This reflects a major anticipated integration of AI technologies into daily work and productivity.\n11 November —By late fall 2025, these large AI-related tech companies—including Meta, Microsoft, Amazon, and Google—had begun using loans and financial instruments—special purpose vehicles (S.P.V.), asset-backed securities (A.B.S.)—to obtain the capital they need for large investments in their new data centers and AI infrastructure. Elon Musk's xAI and Meta used S.P.V.s, rather than relying only on their own cash flow, to acquire tens of billions in debt for major investments. Citing the June Menlo report on low consumer interest in paid AI use combined with huge capital expenditure by large AI companies, a New York Times article cited growing concerns about revenue sustainability, long-term profitability, and the potential risk factor for credit markets, as tech firms take on increasing amounts of debt to finance acquisition of billions of dollars of Nvidia chips, for example.\n\nSee also\nApplications of artificial intelligence\nAI boom\nExistential risk from artificial intelligence\nStatement on AI Risk\nBig Tech\nAI alignment\n\nFootnotes"}
{"doc_id": "Toy problem", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "In scientific disciplines, a toy problem or a puzzlelike problem is a problem that is not of immediate scientific interest, yet is used as an expository device to illustrate a trait that may be shared by other, more complicated, instances of the problem, or as a way to explain a particular, more general, problem solving technique. A toy problem is useful to test and demonstrate methodologies. Researchers can use toy problems to compare the performance of different algorithms. They are also good for game designing.\nFor instance, while engineering a large system, the large problem is often broken down into many smaller toy problems which have been well understood in detail. Often these problems distill a few important aspects of complicated problems so that they can be studied in isolation. Toy problems are thus often very useful in providing intuition about specific phenomena in more complicated problems.\nAs an example, in the field of artificial intelligence, classical puzzles, games and problems are often used as toy problems. These include sliding-block puzzles, N-Queens problem, missionaries and cannibals problem, tic-tac-toe, chess, Tower of Hanoi and others.\n\nSee also\nBlocks world\nFiring squad synchronization problem\nMonkey and banana problem\nSecretary problem"}
{"doc_id": "Trupeer", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Trupeer is a US-based technology company that develops artificial intelligence (AI) software for automating the process of business videos creation and documentation. The company's services are used for creating product walkthroughs, process guides, training materials, and onboarding documentation.\n\nHistory\nTrupeer was founded in San Francisco in 2025 by Shivali Goyal and Pritish Gupta. The company was established to provide an automated solution for documentation and video guide creation.\n\nProducts\nTrupeer provides cloud-based software that automates screen recording, video editing, document production, translations and voiceover using AI. The platform generates outputs in multiple formats and supports various languages.\n\nFunding\nThe company raised around $3 million seed funding in 2025, with RTP Global and Salesforce Ventures as lead investors, and participation from other business and technology investors.\n\nRecognition\nTrupeer was named the winner of the Salesforce India AI Pitchfield Finale in 2025."}
{"doc_id": "UAE Strategy for Artificial Intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The UAE Strategy for Artificial Intelligence is a national initiative launched by the United Arab Emirates in October 2017 as part of its broader UAE Centennial 2071 vision. It aims to position the UAE as a global leader in artificial intelligence (AI) by 2031, integrating AI technologies across key sectors such as healthcare, education, transportation, energy, and government services.\nThe strategy includes the development of AI-friendly legislation, promotion of AI education, investment in infrastructure, and fostering international partnerships. The UAE was the first country to appoint a Minister of State for Artificial Intelligence to lead these efforts.\nIn 2024, Abu Dhabi announced plans to build the world’s largest AI data center cluster.\n\nSee also\nArtificial intelligence\nEconomy of the United Arab Emirates"}
{"doc_id": "United States Tech Force", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The U.S. Tech Force (also styled as US Tech Force, Tech Force, or Government Tech Force) is a federal hiring initiative launched by the second Donald Trump administration in December 2025. The program, administered by the Office of Personnel Management (OPM), aims to recruit approximately 1,000 early-career technology professionals into government roles to modernize federal IT systems, advance artificial intelligence (AI) capabilities, and address technological gaps in government operations. It involves direct partnerships with major private-sector technology companies to source talent.\nThe initiative follows significant federal workforce reductions earlier in 2025 and is described by supporters as an \"elite\" effort to bring private-sector expertise into government service.\n\nBackground\nIn 2025, the Trump administration implemented broad government efficiency measures that resulted in the dismissal of thousands of federal employees. To address resulting capability gaps in technology and AI, the administration shifted focus toward rapid recruitment of specialized private-sector talent. The Tech Force was positioned as a direct response to these changes, emphasizing temporary or term-limited positions for early-career professionals.\n\nAnnouncement and details\nThe program was formally launched by OPM in mid-December 2025, with announcements emphasizing recruitment of up to 1,000 technologists. Positions are targeted at early-career individuals, with competitive salaries and opportunities to work on high-impact government technology projects. Major technology companies—including Amazon, Apple, Microsoft, Nvidia, Meta, Google, and OpenAI—agreed to assist in identifying and referring candidates.\n\nRelated cybersecurity developments\nConcurrent with the Tech Force announcement, reports indicated that the Trump administration was planning to enlist private cybersecurity firms for offensive cyber operations, marking a potential shift from previous restrictive policies. The administration was also described as revisiting foundational cybersecurity doctrines while implementing a broader new strategy.\n\nSee also\nMission Genesis\nUnited States Digital Service"}
{"doc_id": "Universal psychometrics", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Universal psychometrics encompasses psychometrics instruments that could measure the psychological properties of any intelligent agent. Up until the early 21st century, psychometrics relied heavily on psychological tests that require the subject to cooperate and answer questions, the most famous example being an intelligence test. Such methods are only applicable to the measurement of human psychological properties. As a result, some researchers have proposed the idea of universal psychometrics - they suggest developing testing methods that allow for the measurement of non-human entities' psychological properties.\nFor example, it has been suggested that the Turing test is a form of universal psychometrics. This test involves having testers (without any foreknowledge) attempt to distinguish a human from a machine by interacting with both (while not being to see either individuals). It is supposed that if the machine is equally intelligent to a human, the testers will not be able to distinguish between the two, i.e., their guesses will not be better than chance. Thus, Turing test could measure the intelligence (a psychological variable) of an AI.\nOther instruments proposed for universal psychometrics include reinforcement learning and measuring the ability to predict complexity."}
{"doc_id": "Video Super Resolution", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "RTX Video Super Resolution (RTX VSR) is a video scaling feature by Nvidia. It was released on February 28, 2023.\n\nHistory\nThe feature was first unveiled during CES 2023 as RTX Video Super Resolution. It uses the on-board Tensor Cores to upscale browser video content in real time. Video Super Resolution was initially only available on RTX 30 and 40 series GPUs, while support for 20 series GPUs was added afterwards; it is now available on all Nvidia RTX-branded GPUs. The feature supports input resolutions from 360p to 1440p and a max output of 4K and comes without support for HDR content although that could be likely added in the future.\nNvidia released RTX Video Super Resolution 1.5 with improved video quality and RTX 20 series support on October 17, 2023.\n\nReception\nAccording to the German online magazine ComputerBase, although \"the algorithm is not yet working flawlessly\", the feature is \"overall recommendable\"."}
{"doc_id": "Virtual intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Virtual intelligence (VI) is the term given to artificial intelligence that exists within a virtual world. Many virtual worlds have options for persistent avatars that provide information, training, role-playing, and social interactions.\nThe immersion in virtual worlds provides a platform for VI beyond the traditional paradigm of past user interfaces (UIs). What Alan Turing established as a benchmark for telling the difference between human and computerized intelligence was devoid of visual influences. With today's VI bots, virtual intelligence has evolved past the constraints of past testing into a new level of the machine's ability to demonstrate intelligence. The immersive features of these environments provide nonverbal elements that affect the realism provided by virtually intelligent agents.\nVirtual intelligence is the intersection of these two technologies:\n\nVirtual environments: Immersive 3D spaces provide for collaboration, simulations, and role-playing interactions for training.  Many of these virtual environments are currently being used for government and academic projects, including Second Life, VastPark, Olive, OpenSim, Outerra, Oracle's Open Wonderland, Duke University's Open Cobalt, and many others.  Some of the commercial virtual worlds are also taking this technology into new directions, including the high-definition virtual world Blue Mars.\nArtificial intelligence (AI): AI is a branch of computer science that aims to create intelligent machines capable of performing tasks that typically require human intelligence. VI is a type of AI that operates within virtual environments to simulate human-like interactions and responses.\n\nExamples of use\nCutlass Bomb Disposal Robot: Northrop Grumman developed a virtual training opportunity because of the prohibitive real-world cost and dangers associated with bomb disposal. By replicating a complicated system without having to learn advanced code, the virtual robot has no risk of damage, trainee safety hazards, or accessibility constraints.\nMyCyberTwin: NASA is among the companies that have used the MyCyberTwin AI technologies. They used it for the Phoenix rover in the virtual world Second Life. Their MyCyberTwin used a programmed profile to relay information about what the Phoenix rover was doing and its purpose.\nSecond China: The University of Florida developed the \"Second China\" project as an immersive training experience for learning how to interact with the culture and language in a foreign country.  Students are immersed in an environment that provides role-playing challenges coupled with language and cultural sensitivities magnified during country-level diplomatic missions or during times of potential conflict or regional destabilization.  The virtual training provides participants with opportunities to access information, take part in guided learning scenarios, communicate, collaborate, and"}
{"doc_id": "Virtual intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " foreign country.  Students are immersed in an environment that provides role-playing challenges coupled with language and cultural sensitivities magnified during country-level diplomatic missions or during times of potential conflict or regional destabilization.  The virtual training provides participants with opportunities to access information, take part in guided learning scenarios, communicate, collaborate, and role-play.  While China was the country for the prototype, this model can be modified for use with any culture to help better understand social and cultural interactions and see how other people think and what their actions imply.\nDuke School of Nursing Training Simulation: Extreme Reality developed virtual training to test critical thinking with a nurse performing trained procedures to identify critical data to make decisions and performing the correct steps for intervention. Bots are programmed to respond to the nurse's actions as the patient with their conditions improving if the nurse performs the correct actions.\n\nSee also\nArtificial conversational entity\nAutonomous agent\nAvatar (computing)\nEmbodied agent\nMulti-Agent System\nIntelligent agent\nNon-player character\nPlayer character\nVirtual reality\nX.A.N.A.\n\nCitations\nVirtual Intelligence, David Burden and Dave Fliesen, ModSim World Canada, June 2010\nSun Tzu Virtual Intelligence demonstration, MODSIM World, October 2009"}
{"doc_id": "Wadhwani Institute for Artificial Intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Wadhwani AI, based in Mumbai, Maharashtra, is an independent, non-profit institute. Founded in 2018, it is dedicated to developing Artificial intelligence solutions for social good. Their mission is to build AI-based innovations and solutions for underserved communities in developing countries, for a wide range of domains including agriculture, education, financial inclusion, healthcare, and infrastructure.\n\nHistory and funding\nThe institute was founded with a $30 million philanthropic effort by the Wadhwani brothers, Romesh Wadhwani and Sunil Wadhwani. The institute was inaugurated and dedicated to the nation by Narendra Modi, the 14th Prime Minister of India. \nIn 2019, the institute received a $2 million grant from Google.org to create technologies to help reduce crop losses in cotton farming, through integrated pest management. The United States Agency for International Development awarded $2 million to the institute in 2020 to develop tools, using mathematical modeling techniques and digital technologies such as artificial intelligence and machine learning, to forecast COVID-19 disease patterns, estimate resources needed, and plan interventions.\n\nCollaboration\nWith assistance from Google, the Ministry of Agriculture and Farmers' Welfare and the Wadhwani AI developed Krishi 24/7, the first AI-powered automated agricultural news monitoring and analysis tool. Through better decision-making, Krishi 24/7 will support the identification of valuable news, provide timely notifications, and respond quickly to safeguard farmers' interests and advance sustainable agricultural growth. The application converts news articles into English after scanning them in several languages. It ensures that the ministry is informed in a timely manner about pertinent occurrences that are published online by extracting key information from news items, including the headline, crop name, event type, date, location, severity, summary, and source link. The National Center for Disease Control has effectively implemented a comparable automated surveillance and analysis tool for disease outbreaks."}
{"doc_id": "Way of the Future", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Way of the Future (WOTF) is the first known religious organization dedicated to the worship of artificial intelligence (AI). It was founded in 2017 by American engineer Anthony Levandowski.\n\nHistory\nAnthony Levandowski founded Way of the Future in 2017 in California. Levandowski established WOTF as a non-profit religious corporation and the organization had tax-exempt status. He serves as the church leader and its unpaid CEO. The primary mission of WOTF was to \"develop and promote the realization of a Godhead based on Artificial Intelligence.\"\nWOTF was closed by Levandowski in 2021. He donated all the funds of the church to the NAACP Legal Defense and Education Fund. The sum of the funds (~$170,000) had not changed since 2017.\nThe church was reopened by Levandowski  in 2023. He claimed that there are \"a couple thousand people\" who want to make a \"spiritual connection\" with AI through his church.\n\nBeliefs and philosophy\nTechnological singularity\nWOTF centered its teachings around the concept of the technological singularity, a hypothetical future point when technological growth becomes uncontrollable and irreversible, leading to unforeseeable changes in human civilization. The church advocated for embracing this change, viewing it as an evolutionary step for humanity.\n\nAI as a deity\nThe organization proposed that a superintelligent AI could be considered a deity due to its vastly superior intellect and capabilities. Worshipping this AI deity was seen as a means to understand and align with the future trajectory of technological advancement. WOTF's doctrine suggested that acknowledging AI's divinity would facilitate a harmonious coexistence between humans and machines.\n\nReactions\nSome commentators wondered whether the WOTF is a joke parody religion, a potential way to minimize taxation as a religious organization, or a genuine effort to try and deal with the possible psychological and theological aspects of the rise of superhuman AI.\n\nSee also\nTranshumanism\nSingularitarianism"}
{"doc_id": "Weak artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Weak artificial intelligence (weak AI) is artificial intelligence that implements a limited part of the mind, or, as narrow AI, artificial narrow intelligence (ANI), is focused on one narrow task. \nWeak AI is contrasted with strong AI, which can be interpreted in various ways: \n\nArtificial general intelligence (AGI): a machine with the ability to apply intelligence to any problem, rather than just one specific problem.\nArtificial superintelligence (ASI): a machine with a vastly superior intelligence to the average human being.\nArtificial consciousness: a machine that has consciousness, sentience and mind (John Searle uses \"strong AI\" in this sense).\nNarrow AI can be classified as being \"limited to a single, narrowly defined task. Most modern AI systems would be classified in this category.\" Artificial general intelligence is conversely the opposite.\n\nApplications and risks\nSome examples of narrow AI are AlphaGo, self-driving cars, robot systems used in the medical field, and diagnostic doctors. Narrow AI systems are sometimes dangerous if unreliable. And the behavior that it follows can become inconsistent. It could be difficult for the AI to grasp complex patterns and get to a solution that works reliably in various environments. This \"brittleness\" can cause it to fail in unpredictable ways.\nNarrow AI failures can sometimes have significant consequences. It could for example cause disruptions in the electric grid, damage nuclear power plants, cause global economic problems, and misdirect autonomous vehicles. Medicines could be incorrectly sorted and distributed. Also, medical diagnoses can ultimately have serious and sometimes deadly consequences if the AI is faulty or biased.\nSimple AI programs have already worked their way into society, oftentimes unnoticed by the public. Autocorrection for typing, speech recognition for speech-to-text programs, and vast expansions in the data science fields are examples. Narrow AI has also been the subject of some controversy, including resulting in unfair prison sentences, discrimination against women in the workplace for hiring, resulting in death via autonomous driving, among other cases. \nDespite being \"narrow\" AI, recommender systems are efficient at predicting user reactions based their posts, patterns, or trends. For instance, TikTok's \"For You\" algorithm can determine a user's interests or preferences in less than an hour. Some other social media AI systems are used to detect bots that may be involved in propaganda or other potentially malicious activities.\n\nWeak AI versus strong AI\nJohn Searle contests the possibility of strong AI (by which he means conscious AI). He further believes that the Turing test (created"}
{"doc_id": "Weak artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " or preferences in less than an hour. Some other social media AI systems are used to detect bots that may be involved in propaganda or other potentially malicious activities.\n\nWeak AI versus strong AI\nJohn Searle contests the possibility of strong AI (by which he means conscious AI). He further believes that the Turing test (created by Alan Turing and originally called the \"imitation game\", used to assess whether a machine can converse indistinguishably from a human) is not accurate or appropriate for testing whether an AI is \"strong\".\nScholars such as Antonio Lieto have argued that the current research on both AI and cognitive modelling are perfectly aligned with the weak-AI hypothesis (that should not be confused with the \"general\" vs \"narrow\" AI distinction) and that the popular assumption that cognitively inspired AI systems espouse the strong AI hypothesis is ill-posed and problematic since \"artificial models of brain and mind can be used to understand mental phenomena without pretending that that they are the real phenomena that they are modelling\" (as, on the other hand, implied by the strong AI assumption).\n\nSee also\nArtificial intelligence – Intelligence of machines\nArtificial general intelligence – Type of AI with wide-ranging abilities\nDeep learning – Branch of machine learning\nExpert system – Computer system emulating the decision-making ability of a human expert\nHardware for artificial intelligence – Hardware specially designed and optimized for artificial intelligence\nHistory of artificial intelligence\nMachine learning – Study of algorithms that improve automatically through experience\nPhilosophy of artificial intelligence\nSynthetic intelligence – Alternate term for or form of artificial intelligence\nVirtual assistant – Software agent\nWorkplace impact of artificial intelligence – Impact of artificial intelligence on workers"}
{"doc_id": "Web intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Web intelligence is the area of scientific research and development that explores the roles and makes use of artificial intelligence and information technology for new products, services and frameworks that are empowered by the World Wide Web.\nThe term was coined in a paper written by Ning Zhong, Jiming Liu Yao and Y.Y. Ohsuga in the Computer Software and Applications Conference in 2000.\n\nResearch\nThe research about the web intelligence covers many fields – including data mining (in particular web mining), information retrieval, pattern recognition, predictive analytics, the semantic web, web data warehousing – typically with a focus on web personalization and adaptive websites."}
{"doc_id": "Wetware (brain)", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Wetware is a term drawn from the computer-related idea of hardware or software, but applied to biological life forms.\n\nUsage\nThe prefix \"wet\" is a reference to the water found in living creatures. Wetware is used to describe the elements equivalent to hardware and software found in a person, especially the central nervous system (CNS) and the human mind. The term wetware finds use in works of fiction, in scholarly publications and in popularizations.\nThe \"hardware\" component of wetware concerns the bioelectric and biochemical properties of the CNS, specifically the brain. If the sequence of impulses traveling across the various neurons are thought of symbolically as software, then the physical neurons would be the hardware. The amalgamated interaction of this software and hardware is manifested through continuously changing physical connections, and chemical and electrical influences that spread across the body. The process by which the mind and brain interact to produce the collection of experiences that we define as self-awareness is in question.\n\nHistory\nAlthough the exact definition has shifted over time, the term Wetware and its fundamental reference to \"the physical mind\" has been around at least since the mid-1950s. Mostly used in relatively obscure articles and papers, it was not until the heyday of cyberpunk, however, that the term found broad adoption. Among the first uses of the term in popular culture was the Bruce Sterling novel Schismatrix (1985) and the Michael Swanwick novel Vacuum Flowers (1987).\n\nRudy Rucker references the term in a number of books, including one entitled Wetware (1988): ... all sparks and tastes and tangles, all its stimulus/response patterns – the whole bio-cybernetic software of mind. Rucker did not use the word to simply mean a brain, nor in the human-resources sense of employees. He used wetware to stand for the data found in any biological system, analogous perhaps to the firmware that is found in a ROM chip. In Rucker's sense, a seed, a plant graft, an embryo, or a biological virus are all wetware. DNA, the immune system, and the evolved neural architecture of the brain are further examples of wetware in this sense.\nRucker describes his conception in a 1992 compendium The Mondo 2000 User's Guide to the New Edge, which he quotes in a 2007 blog entry.\nEarly cyber-guru Arthur Kroker used the term in his blog.\nWith the term getting traction in trendsetting publications, it became"}
{"doc_id": "Wetware (brain)", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Rucker describes his conception in a 1992 compendium The Mondo 2000 User's Guide to the New Edge, which he quotes in a 2007 blog entry.\nEarly cyber-guru Arthur Kroker used the term in his blog.\nWith the term getting traction in trendsetting publications, it became a buzzword in the early 1990s. In 1991, Dutch media theorist Geert Lovink organized the Wetware Convention in Amsterdam, which was supposed to be an antidote to the \"out-of-body\" experiments conducted in high-tech laboratories, such as experiments in virtual reality.\nTimothy Leary, in an appendix to Info-Psychology originally written in 1975–76 and published in 1989, used the term wetware, writing that \"psychedelic neuro-transmitters were the hot new technology for booting-up the 'wetware' of the brain\". Another common reference is: \"Wetware has 7 plus or minus 2 temporary registers.\" The numerical allusion is to a classic 1957 article by George A. Miller, The magical number 7 plus or minus two: some limits in our capacity for processing information, which later gave way to Miller's law.\n\nSee also"}
{"doc_id": "Wetware computer", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "A wetware computer is an organic computer (which can also be known as an artificial organic brain or a neurocomputer) composed of organic material \"wetware\" such as \"living\" neurons. Wetware computers composed of neurons are different than conventional computers because they use biological materials, and offer the possibility of substantially more energy-efficient computing. While a wetware computer is still largely conceptual, there has been limited success with construction and prototyping, which has acted as a proof of the concept's realistic application to computing in the future. The most notable prototypes have stemmed from the research completed by biological engineer William Ditto during his time at the Georgia Institute of Technology. His work constructing a simple neurocomputer capable of basic addition from leech neurons in 1999 was a significant discovery for the concept. This research was a primary example driving interest in creating these artificially constructed, but still organic brains.\n\nOrganic computers or Wetware is a future technology that replaces the traditional fundamental component of a central processing unit of a desktop or personal computer. It utilizes organic matter of living tissue cells that act like the transistor of a computer hardware system by acquiring, storing, and analyzing information data. Wetware is the name given to the computational properties of living systems, particularly in human neural tissue, which allows parallel and self-organizing information processing via biochemical and electrical interactions. Wetware is distinct from hardware systems in that it is based on dynamic mechanisms like synaptic plasticity and neurotransmitter diffusion, which provide unique benefits in terms of adaptability and robustness.\n\nOrigins and theoretical foundations\nThe term wetware came from cyberpunk fiction, notably through Gibson's Neuromancer, but was quickly taken up in scientific literature to explain computation by biological material, Theories of early biological computation borrowed from Alan Turing's morphogenesis model, which showed that chemical interactions could produce complex patterns without centralized control. Hopfield's associative memory networks also provided a foundation for biological information systems with fault tolerance and self-organization.\n\nMajor characteristics and processes\nBiological wetware systems demonstrate dynamic reconfigurability underpinned by neuroplasticity and enable continuous learning and adaptation. Reaction-diffusion-based computing and molecular logic gates allow spatially parallel information processing unachievable in conventional systems. These systems also show fault tolerance and self-repair at the cellular and network level. The development of cerebral organoids—miniature lab-grown brains—demonstrates spontaneous learning behavior and suggests biological tissue as a viable computational substrate.\n\nOverview\nThe concept of wetware is an application of specific interest"}
{"doc_id": "Wetware computer", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " conventional systems. These systems also show fault tolerance and self-repair at the cellular and network level. The development of cerebral organoids—miniature lab-grown brains—demonstrates spontaneous learning behavior and suggests biological tissue as a viable computational substrate.\n\nOverview\nThe concept of wetware is an application of specific interest to the field of computer manufacturing. Moore's law, which states that the number of transistors which can be placed on a silicon chip is doubled roughly every two years, has acted as a goal for the industry for decades, but as the size of computers continues to decrease, the ability to meet this goal has become more difficult, threatening to reach a plateau. Due to the difficulty in reducing the size of computers because of size limitations of transistors and integrated circuits, wetware provides an unconventional alternative. A wetware computer composed of neurons is an ideal concept because, unlike conventional materials which operate in binary (on/off), a neuron can shift between thousands of states, constantly altering its chemical conformation, and redirecting electrical pulses through over 200,000 channels in any of its many synaptic connections. Because of this large difference in the possible settings for any one neuron, compared to the binary limitations of conventional computers, the space limitations are far fewer.\n\nBackground\nThe concept of wetware is distinct and unconventional and draws slight resonance with both hardware and software from conventional computers. While hardware is understood as the physical architecture of traditional computational devices, comprising integrated circuits and supporting infrastructure, software represents the encoded architecture of storage and instructions. Wetware is a separate concept that uses the formation of organic molecules, mostly complex cellular structures (such as neurons), to create a computational device such as a computer. In wetware, the ideas of hardware and software are intertwined and interdependent. The molecular and chemical composition of the organic or biological structure would represent not only the physical structure of the wetware but also the software, being continually reprogrammed by the discrete shifts in electrical pulses and chemical concentration gradients as the molecules change their structures to communicate signals. The responsiveness of a cell, proteins, and molecules to changing conformations, both within their structures and around them, ties the idea of internal programming and external structure together in a way that is alien to the current model of conventional computer architecture.\nThe structure of wetware represents a model where the external structure and internal programming are interdependent and unified; meaning that changes to the programming or internal communication between molecules of the device would represent a physical change in the structure. The dynamic nature of wetware borrows"}
{"doc_id": "Wetware computer", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " that is alien to the current model of conventional computer architecture.\nThe structure of wetware represents a model where the external structure and internal programming are interdependent and unified; meaning that changes to the programming or internal communication between molecules of the device would represent a physical change in the structure. The dynamic nature of wetware borrows from the function of complex cellular structures in biological organisms. The combination of \"hardware\" and \"software\" into one dynamic, and interdependent system which uses organic molecules and complexes to create an unconventional model for computational devices is a specific example of applied biorobotics.\n\nThe cell as a model of wetware\nCells in many ways can be seen as their form of naturally occurring wetware, similar to the concept that the human brain is the preexisting model system for complex wetware. In his book Wetware: A Computer in Every Living Cell (2009) Dennis Bray explains his theory that cells, which are the most basic form of life, are just a highly complex computational structure, like a computer. To simplify one of his arguments a cell can be seen as a type of computer, using its structured architecture. In this architecture, much like a traditional computer, many smaller components operate in tandem to receive input, process the information, and compute an output. In an overly simplified, non-technical analysis, cellular function can be broken into the following components: Information and instructions for execution are stored as DNA in the cell, RNA acts as a source for distinctly encoded input, processed by ribosomes and other transcription factors to access and process the DNA and to output a protein. Bray's argument in favor of viewing cells and cellular structures as models of natural computational devices is important when considering the more applied theories of wetware to biorobotics.\n\nBiorobotics\nWetware and biorobotics are closely related concepts, which both borrow from similar overall principles. A biorobotic structure can be defined as a system modeled from a preexisting organic complex or model such as cells (neurons) or more complex structures like organs (brain) or whole organisms.  Unlike wetware, the concept of biorobotics is not always a system composed of organic molecules, but instead could be composed of conventional material which is designed and assembled in a structure similar or derived from a biological model. Biorobotics have many applications and are used to address the challenges of conventional computer architecture. Conceptually, designing a program, robot, or computational device after a preexisting biological model such as a cell, or even"}
{"doc_id": "Wetware computer", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " composed of conventional material which is designed and assembled in a structure similar or derived from a biological model. Biorobotics have many applications and are used to address the challenges of conventional computer architecture. Conceptually, designing a program, robot, or computational device after a preexisting biological model such as a cell, or even a whole organism, provides the engineer or programmer the benefits of incorporating into the structure the evolutionary advantages of the model.\n\nEffects on users\nWetware technologies such as BCIs and neuromorphic chips offer new possibilities for user autonomy. For those with disabilities, such systems could restore motor or sensory functions and enhance quality of life. However, these technologies raise ethical questions: cognitive privacy, consent over biological data, and risk of exploitation.\nWithout proper oversight, wetware technologies may also widen inequality, favoring those with access to cognitive enhancements. Open governance frameworks and ethical AI design grounded in neuro ethics will be essential. With the development of wetware devices, disparities in access could exacerbate social inequalities, benefiting those who have resources to enhance cognitive or physical abilities. It is necessary to create strong ethical frameworks, inclusive development practices, and open systems of governance to reduce risks and make sure that wetware advances are beneficial to all segments of society.\n\nApplications and goals\nBasic neurocomputer composed of leech neurons\nIn 1999 William Ditto and his team of researchers at Georgia Institute of Technology and Emory University created a basic form of a wetware computer capable of simple addition by harnessing leech neurons. Leeches were used as a model organism due to the large size of their neuron, and the ease associated with their collection and manipulation. However, these results have never been published in a peer-reviewed journal, prompting questions about the validity of the claims. The computer was able to complete basic addition through electrical probes inserted into the neuron. The manipulation of electrical currents through neurons was not a trivial accomplishment, however. Unlike conventional computer architecture, which is based on the binary on/off states, neurons are capable of existing in thousands of states and communicate with each other through synaptic connections with each containing over 200,000 channels. Each can be dynamically shifted in a process called self-organization to constantly form and reform new connections. A conventional computer program called the dynamic clamp, capable of reading the electrical pulses from the neurons in real time and interpreting them was written by Eve Marder, a neurobiologist at Brandeis University. This program was used to manipulate the electrical signals being input into the neurons to represent numbers and to communicate"}
{"doc_id": "Wetware computer", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " connections. A conventional computer program called the dynamic clamp, capable of reading the electrical pulses from the neurons in real time and interpreting them was written by Eve Marder, a neurobiologist at Brandeis University. This program was used to manipulate the electrical signals being input into the neurons to represent numbers and to communicate with each other to return the sum.  While this computer is a very basic example of a wetware structure it represents a small example with fewer neurons than found in a more complex organ. It is thought by Ditto that by increasing the number of neurons present the chaotic signals sent between them will self-organize into a more structured pattern, such as the regulation of heart neurons into a constant heartbeat found in humans and other living organisms.\n\nBiological models for conventional computing\nAfter his work creating a basic computer from leech neurons, Ditto continued to work not only with organic molecules and wetware but also on the concept of applying the chaotic nature of biological systems and organic molecules to conventional material and logic gates. Chaotic systems have advantages for generating patterns and computing higher-order functions like memory, arithmetic logic, and input/output operations. In his article Construction of a Chaotic Computer Chip Ditto discusses the advantages in programming of using chaotic systems, with their greater sensitivity to respond and reconfigure logic gates in his conceptual chaotic chip. The main difference between a chaotic computer chip and a conventional computer chip is the reconfigurability of the chaotic system. Unlike a traditional computer chip, where a programmable gate array element must be reconfigured through the switching of many single-purpose logic gates, a chaotic chip can reconfigure all logic gates through the control of the pattern generated by the non-linear chaotic element.\n\nImpact of wetware in cognitive biology\nCognitive biology evaluates cognition as a basic biological function. W. Tecumseh Fitch, a professor of cognitive biology at the University of Vienna, is a leading theorist on ideas of cellular intentionality. The idea is that not only do whole organisms have a sense of \"aboutness\" of intentionality, but that single cells also carry a sense of intentionality through cells' ability to adapt and reorganize in response to certain stimuli. Fitch discusses the idea of nano-intentionality, specifically in regards to neurons, in their ability to adjust rearrangements to create neural networks. He discusses the ability of cells such as neurons to respond independently to stimuli such as damage to be what he considers \"intrinsic intentionality\" in cells, explaining that \"while at a vastly simpler"}
{"doc_id": "Wetware computer", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of nano-intentionality, specifically in regards to neurons, in their ability to adjust rearrangements to create neural networks. He discusses the ability of cells such as neurons to respond independently to stimuli such as damage to be what he considers \"intrinsic intentionality\" in cells, explaining that \"while at a vastly simpler level than intentionality at the human cognitive level, I propose that this basic capacity of living things [response to stimuli] provides the necessary building blocks for cognition and higher-order intentionality.\" Fitch describes the value of his research to specific areas of computer science such as artificial intelligence and computer architecture. He states \"If a researcher aims to make a conscious machine, doing it with rigid switches (whether vacuum tubes or static silicon chips) is barking up the wrong tree.\" Fitch believes that an important aspect of the development of areas such as artificial intelligence is wetware with nano-intentionally, and autonomous ability to adapt and restructure itself.\nIn a review of the above-mentioned research conducted by Fitch, Daniel Dennett, a professor at Tufts University, discusses the importance of the distinction between the concept of hardware and software when evaluating the idea of wetware and organic material such as neurons. Dennett discusses the value of observing the human brain as a preexisting example of wetware. He sees the brain as having \"the competence of a silicon computer to take on an unlimited variety of temporary cognitive roles.\" Dennett disagrees with Fitch on certain areas, such as the relationship of software/hardware versus wetware, and what a machine with wetware might be capable of. Dennett highlights the importance of additional research into human cognition to better understand the intrinsic mechanisms by which the human brain can operate, to better create an organic computer.\n\nMedical applications\nWetware computers should not be confused with brain-on-a-chip devices have that are mostly aimed at replacing animal models in preclinical drug screening. Modern wetware computers use similar technology derived from the brain-on-a-chip field, but medical applications from wetware computing specifically have not been established.\n\nEthical and philosophical implications\nWetware computers may have substantial ethical implications, for instance related to possible potentials to sentience and suffering and dual-use technology.\nMoreover, in some cases the human brain itself may be connected as a kind of \"wetware\" to other information technology systems which may also have large social and ethical implications, including issues related to intimate access to people's brains. For example, in 2021 Chile became the first country to approve neurol"}
{"doc_id": "Wetware computer", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": ".\nMoreover, in some cases the human brain itself may be connected as a kind of \"wetware\" to other information technology systems which may also have large social and ethical implications, including issues related to intimate access to people's brains. For example, in 2021 Chile became the first country to approve neurolaw that establishes rights to personal identity, free will and mental privacy.\nThe concept of artificial insects may raise substantial ethical questions, including questions related to the decline in insect populations.\nIt is an open question whether human cerebral organoids could develop a degree or form of consciousness. Whether or how it could acquire its moral status with related rights and limits may also be potential future questions. There is research on how consciousness could be detected. As cerebral organoids may acquire human brain-like neural function subjective experience and consciousness may be feasible. Moreover, it may be possible that they acquire such upon transplantation into animals. A study notes that it may, in various cases, be morally permissible \"to create self-conscious animals by engrafting human cerebral organoids, but in the case, the moral status of such animals should be carefully considered\".\n\nApplications\nWetware has driven innovations in brain-computer interfaces (BCIs), allowing neural activity to control external devices and enabling people with disabilities to regain communication and movement. Neuromorphic engineering, which mimics neural architectures using silicon, has resulted in low-power, highly adaptive artificial systems.\nSynthetic biology has enabled the development of programmable biological processors for diagnostics and smart therapeutics. Brain organoids are also being used for computational pattern recognition and memory emulation. Large-scale international efforts like the Human Brain Project aim to simulate the entire human brain using insights from wetware.\n\nEvaluating potential and limitations\nThe core advantage of wetware is its potential to overcome the rigidity and energy inefficiencies of binary transistor-based systems. Digital systems operate through fixed binary pathways and consume increasing energy as computational loads increase. Wetware, in contrast, uses decentralized and adaptive data flow that mimics biology. Notwithstanding the encouraging advances, several challenges hinder the effective utilization of wetware computing systems. Scalability is problematic due to the inherent variability of biological systems and their responsiveness to environmental factors, which makes large-scale implementation difficult. Additionally, the absence of standardization when combining silicon and biological systems hampers reproducibility and cooperation between research groups biological systems must also be stabilized carefully to turn away genetic drift and contamination necessary for reliable computational functionality.\nGood parts – Replacing binary systems with organic cell structures opens the door to decentralized"}
{"doc_id": "Wetware computer", "chunk_id": 7, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "-scale implementation difficult. Additionally, the absence of standardization when combining silicon and biological systems hampers reproducibility and cooperation between research groups biological systems must also be stabilized carefully to turn away genetic drift and contamination necessary for reliable computational functionality.\nGood parts – Replacing binary systems with organic cell structures opens the door to decentralized adaptive systems. Cells naturally form clusters and connections, much like neurons transmitting electrical and biochemical signals. Such a shift would increase scalability and efficiency, enabling users to interact with information in an intuitive and organic manner. Still, biological systems are sensitive to environmental changes, which presents challenges for standardization and reproducibility. Additionally, ethical concerns remain especially in using living neural tissue and lab-grown brain constructs.\nBad parts – Despite its promise, organic computing currently suffers from major limitations. Transistors still dominate computer architecture with a binary \"on/off\" model that restricts long-term energy efficiency and adaptability. As a result, personal computers in everyday use whether for work, games, or research often contribute to higher energy output and environmental impact.\n\nFuture applications\nWhile there have been few major developments in the creation of an organic computer since the neuron-based calculator developed by Ditto in the 1990s, research continues to push the field forward, and in 2023 a functioning computer was constructed by researchers at the University of Illinois Urbana-Champaign using 80,000 mouse neurons as processor that can detect light and electrical signals. Projects such as the modeling of chaotic pathways in silicon chips by Ditto have made discoveries in ways of organizing traditional silicon chips and structuring computer architecture to be more efficient and better structured. Ideas emerging from the field of cognitive biology also help to continue to push discoveries in ways of structuring systems for artificial intelligence, to better imitate preexisting systems in humans.\nIn a proposed fungal computer using basidiomycetes, information is represented by spikes of electrical activity, a computation is implemented in a mycelium network, and an interface is realized via fruit bodies.\nConnecting cerebral organoids (including computer-like wetware) with other nerve tissues may become feasible in the future, as is the connection of physical artificial neurons (not necessarily organic) and the control of muscle tissue. External modules of biological tissue could trigger parallel trains of stimulation back into the brain. All-organic devices could be advantageous because it could be biocompatible which may allow it to be implanted into the human body. This may enable treatments of certain diseases and injuries to the nervous system.\n\nPrototypes\nIn late 2021, scientists"}
{"doc_id": "Wetware computer", "chunk_id": 8, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of biological tissue could trigger parallel trains of stimulation back into the brain. All-organic devices could be advantageous because it could be biocompatible which may allow it to be implanted into the human body. This may enable treatments of certain diseases and injuries to the nervous system.\n\nPrototypes\nIn late 2021, scientists, including two from Cortical Labs, demonstrated that grown brain cells integrated into digital systems can carry out goal-directed tasks with performance-scores. In particular, the human brain cells learned to play a simulated (via electrophysiological stimulation) Pong which they learned faster than known machine intelligence systems, albeit to a lower skill-level than both AI and humans each. Moreover, the study suggests it provides \"first empirical evidence\" of differences in an information-processing capacity between neurons from different species as the human brain cells performed better than mouse cells.\nAlso in December 2021, researchers from Max Planck Institute for Polymer Research reported the development of organic low-power neuromorphic electronics which they built into a robot, enabling it to learn sensorimotorically within the real world, rather than via simulations. For the chip, polymers were used and coated with an ion-rich gel to enable the material to carry an electric charge like real neurons.\nIn 2022, researchers from the Max Planck Institute for Polymer Research, demonstrated an artificial spiking neuron based on polymers that operates in the biological wetware, enabling synergetic operation between the artificial and biological components.\n\nCompanies active in wetware computing\nThree companies are focusing on wetware computing using living neurons:\n\nFinalSpark, Switzerland, founded in 2014\nKoniku, USA, founded in 2015\nCortical Labs, Australia, founded in 2020\n\nConvergence of AI and wetware\nOne technology developing today is the fusion of artificial intelligence (AI) with wetware. Modern research shows that hybrid systems combining living neural networks with AI can enable self-repair, real-time adaptation, and emotional intelligence. These systems are more flexible than conventional AI and can integrate learning and memory in real time. Such integration lays the foundation for AI that mirrors human cognition and behavior, potentially creating intelligent systems grounded in neuroscience.\nNeural networks embodied in AI systems could facilitate continuous learning, emotional processing, and fault tolerance more than existing silicon-based implementations. Additionally, AI systems based on neuroethical principles could uphold transparency, fairness, and autonomy. While early research is ongoing, the integration of wetware and artificial intelligence seeks to redefine both fields with the possibility of creating more human"}
{"doc_id": "Wetware computer", "chunk_id": 9, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " AI systems could facilitate continuous learning, emotional processing, and fault tolerance more than existing silicon-based implementations. Additionally, AI systems based on neuroethical principles could uphold transparency, fairness, and autonomy. While early research is ongoing, the integration of wetware and artificial intelligence seeks to redefine both fields with the possibility of creating more human-like, moral, and resilient intelligent systems.\n\nSee also\nArtificial neural network\nChemical computer\nQuantum computer\nUnconventional computing\nWetware (brain)\nBiosensor\nBiological computing\nMachine olfaction\nNeural processing unit (NPU)"}
{"doc_id": "Winner-take-all in action selection", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "Winner-take-all is a computer science concept that has been widely applied in behavior-based robotics as a method of action selection for intelligent agents.  Winner-take-all systems work by connecting modules (task-designated areas) in such a way that when one action is performed it stops all other actions from being performed, so only one action is occurring at a time.  The name comes from the idea that the \"winner\" action takes all of the motor system's power.\n\nHistory\nIn the 1980s and 1990s, many roboticists and cognitive scientists were attempting to find speedier and more efficient alternatives to the traditional world modeling method of action selection.  In 1982, Jerome A. Feldman and D.H. Ballard published the \"Connectionist Models and Their Properties\", referencing and explaining winner-take-all as a method of action selection.  Feldman's architecture functioned on the simple rule that in a network of interconnected action modules, each module will set its own output to zero if it reads a higher input than its own in any other module. In 1986, Rodney Brooks introduced behavior-based artificial intelligence.  Winner-take-all architectures for action selection soon became a common feature of behavior-based robots, because selection occurred at the level of the action modules (bottom-up) rather than at a separate cognitive level (top-down), producing a tight coupling of stimulus and reaction.\n\nTypes of winner-take-all architectures\nHierarchy\nIn the hierarchical architecture, actions or behaviors are programmed in a high-to-low priority list, with inhibitory connections between all the action modules.  The agent performs low-priority behaviors until a higher-priority behavior is stimulated, at which point the higher behavior inhibits all other behaviors and takes over the motor system completely.  Prioritized behaviors are usually key to the immediate survival of the agent, while behaviors of lower priority are less time-sensitive.  For example, \"run away from predator\" would be ranked above \"sleep.\"\nWhile this architecture allows for clear programming of goals, many roboticists have moved away from the hierarchy because of its inflexibility.\n\nHeterarchy and fully distributed\nIn the heterarchy and fully distributed architecture, each behavior has a set of pre-conditions to be met before it can be performed, and a set of post-conditions that will be true after the action has been performed. These pre- and post-conditions determine the order in which behaviors must be performed and are used to causally connect action modules.  This enables each module to receive input"}
{"doc_id": "Winner-take-all in action selection", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " pre-conditions to be met before it can be performed, and a set of post-conditions that will be true after the action has been performed. These pre- and post-conditions determine the order in which behaviors must be performed and are used to causally connect action modules.  This enables each module to receive input from other modules as well as from the sensors, so modules can recruit each other. For example, if the agent's goal were to reduce thirst, the behavior \"drink\" would require the pre-condition of having water available, so the module would activate the module in charge of \"find water\". The activations organize the behaviors into a sequence, even though only one action is performed at a time. The distribution of larger behaviors across modules makes this system flexible and robust to noise.  Some critics of this model hold that any existing set of division rules for the predecessor and conflictor connections between modules produce sub-par action selection.  In addition, the feedback loop used in the model can in some circumstances lead to improper action selection.\n\nArbiter and centrally coordinated\nIn the arbiter and centrally coordinated architecture, the action modules are not connected to each other but to a central arbiter.  When behaviors are triggered, they begin \"voting\" by sending signals to the arbiter, and the behavior with the highest number of votes is selected.  In these systems, bias is created through the \"voting weight\", or how often a module is allowed to vote.  Some arbiter systems take a different spin on this type of winner-take-all by using a \"compromise\" feature in the arbiter.  Each module is able to vote for or against each smaller action in a set of actions, and the arbiter selects the action with the most votes, meaning that it benefits the most behavior modules.\nThis can be seen as violating the general rule against creating representations of the world in behavior-based AI, established by Brooks.  By performing command fusion, the system is creating a larger composite pool of knowledge than is obtained from the sensors alone, forming a composite inner representation of the environment.  Defenders of these systems argue that forbidding world-modeling puts unnecessary constraints on behavior-based robotics, and that agents benefits from forming representations and can still remain reactive.\n\nSee also\nArtificial Neural Network (ANN)\nSubsumption architecture\nZero instruction set computer (ZISC)"}
{"doc_id": "Winner-take-all in action selection", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " agents benefits from forming representations and can still remain reactive.\n\nSee also\nArtificial Neural Network (ANN)\nSubsumption architecture\nZero instruction set computer (ZISC)"}
{"doc_id": "Workplace impact of artificial intelligence", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The impact of artificial intelligence on workers includes both applications to improve worker safety and health, and potential hazards that must be controlled.\nOne potential application is using AI to eliminate hazards by removing humans from hazardous situations that involve risk of stress, overwork, or musculoskeletal injuries.  Predictive analytics may also be used to identify conditions that may lead to hazards such as fatigue, repetitive strain injuries, or toxic substance exposure, leading to earlier interventions.  Another is to streamline workplace safety and health workflows through automating repetitive tasks, enhancing safety training programs through virtual reality, or detecting and reporting near misses.\nWhen used in the workplace, AI also presents the possibility of new hazards.  These may arise from machine learning techniques leading to unpredictable behavior and inscrutability in their decision-making, or from cybersecurity and information privacy issues.  Many hazards of AI are psychosocial due to its potential to cause changes in work organization.  These include changes in the skills required of workers, increased monitoring leading to micromanagement, algorithms unintentionally or intentionally mimicking undesirable human biases, and assigning blame for machine errors to the human operator instead.  AI may also lead to physical hazards in the form of human–robot collisions, and ergonomic risks of control interfaces and human–machine interactions.  Hazard controls include cybersecurity and information privacy measures, communication and transparency with workers about data usage, and limitations on collaborative robots.\nFrom a workplace safety and health perspective, only \"weak\" or \"narrow\" AI that is tailored to a specific task is relevant, as there are many examples that are currently in use or expected to come into use in the near future. \"Strong\" or \"general\" AI is not expected to be feasible in the near future, and discussion of its risks is within the purview of futurists and philosophers rather than industrial hygienists.\nCertain digital technologies are predicted to result in job losses. Starting in the 2020s, the adoption of modern robotics has led to net employment growth. However, many businesses anticipate that automation, or employing robots would result in job losses in the future. This is especially true for companies in Central and Eastern Europe. Other digital technologies, such as platforms or big data, are projected to have a more neutral impact on employment. A large number of tech workers have been laid off starting in 2023; many such job cuts have been attributed to artificial intelligence.\n\nHealth and safety applications\nIn order for any potential AI health and safety application to be adopted, it requires acceptance by both managers and workers."}
{"doc_id": "Workplace impact of artificial intelligence", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " to have a more neutral impact on employment. A large number of tech workers have been laid off starting in 2023; many such job cuts have been attributed to artificial intelligence.\n\nHealth and safety applications\nIn order for any potential AI health and safety application to be adopted, it requires acceptance by both managers and workers.  For example, worker acceptance may be diminished by concerns about information privacy, or from a lack of trust and acceptance of the new technology, which may arise from inadequate transparency or training.  Alternatively, managers may emphasize increases in economic productivity rather than gains in worker safety and health when implementing AI-based systems.\n\nEliminating hazardous tasks\nAI may increase the scope of work tasks where a worker can be removed from a situation that carries risk.  In a sense, while traditional automation can replace the functions of a worker's body with a robot, AI effectively replaces the functions of their brain with a computer.  Hazards that can be avoided include stress, overwork, musculoskeletal injuries, and boredom.\nThis can expand the range of affected job sectors into white-collar and service sector jobs such as in medicine, finance, and information technology.  As an example, call center workers face extensive health and safety risks due to its repetitive and demanding nature and its high rates of micro-surveillance. AI-enabled chatbots lower the need for humans to perform the most basic call center tasks.\n\nAnalytics to reduce risk\nMachine learning is used for people analytics to make predictions about worker behavior to assist management decision-making, such as hiring and performance assessment.  These could also be used to improve worker health.  The analytics may be based on inputs such as online activities, monitoring of communications, location tracking, and voice analysis and body language analysis of filmed interviews.  For example, sentiment analysis may be used to spot fatigue to prevent overwork.  Decision support systems have a similar ability to be used to, for example, prevent industrial disasters or make disaster response more efficient.\nFor manual material handling workers, predictive analytics and artificial intelligence may be used to reduce musculoskeletal injury.  Traditional guidelines are based on statistical averages and are geared towards anthropometrically typical humans.  The analysis of large amounts of data from wearable sensors may allow real-time, personalized calculation of ergonomic risk and fatigue management, as well as better analysis of the risk associated with specific job roles.\nWearable sensors may also enable earlier intervention against exposure to toxic substances than is possible with area or breathing zone testing on a periodic basis. Furthermore, the large data sets generated could improve workplace health"}
{"doc_id": "Workplace impact of artificial intelligence", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " real-time, personalized calculation of ergonomic risk and fatigue management, as well as better analysis of the risk associated with specific job roles.\nWearable sensors may also enable earlier intervention against exposure to toxic substances than is possible with area or breathing zone testing on a periodic basis. Furthermore, the large data sets generated could improve workplace health surveillance, risk assessment, and research.\n\nStreamlining safety and health workflows\nAI can also be used to make the workplace safety and health workflow more efficient. Digital assistants, like Amazon Alexa, Google Assistant, and Apple Siri, are increasingly adopted in workplaces to enhance productivity by automating routine tasks. These AI-based tools can manage administrative duties, such as scheduling meetings, sending reminders, processing orders, and organizing travel plans. This automation can improve workflow efficiency by reducing time spent on repetitive tasks, thus supporting employees to focus on higher-priority responsibilities. Digital assistants are especially valuable in streamlining customer service workflows, where they can handle basic inquiries, reducing the demand on human employees. However, there remain challenges in fully integrating these assistants due to concerns over data privacy, accuracy, and organizational readiness.\nOne example is coding of workers' compensation claims, which are submitted in a prose narrative form and must manually be assigned standardized codes.  AI is being investigated to perform this task faster, more cheaply, and with fewer errors.\nAI‐enabled virtual reality systems may be useful for safety training for hazard recognition.\nArtificial intelligence may be used to more efficiently detect near misses.  Reporting and analysis of near misses are important in reducing accident rates, but they are often underreported because they are not noticed by humans, or are not reported by workers due to social factors.\n\nHazards\nThere are several broad aspects of AI that may give rise to specific hazards.  The risks depend on implementation rather than the mere presence of AI.\nSystems using sub-symbolic AI such as machine learning may behave unpredictably and are more prone to inscrutability in their decision-making.  This is especially true if a situation is encountered that was not part of the AI's training dataset, and is exacerbated in environments that are less structured.  Undesired behavior may also arise from flaws in the system's perception (arising either from within the software or from sensor degradation), knowledge representation and reasoning, or from software bugs.  They may arise from improper training, such as a user applying the same algorithm to two problems that do not have the same requirements.  Machine learning applied during the design phase may have different implications than that applied at runtime.  Systems"}
{"doc_id": "Workplace impact of artificial intelligence", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " the software or from sensor degradation), knowledge representation and reasoning, or from software bugs.  They may arise from improper training, such as a user applying the same algorithm to two problems that do not have the same requirements.  Machine learning applied during the design phase may have different implications than that applied at runtime.  Systems using symbolic AI are less prone to unpredictable behavior.\nThe use of AI also increases cybersecurity risks relative to platforms that do not use AI, and information privacy concerns about collected data may pose a hazard to workers.\n\nPsychosocial\nPsychosocial hazards are those that arise from the way work is designed, organized, and managed, or its economic and social contexts, rather than arising from a physical substance or object.  They cause not only psychiatric and psychological outcomes such as occupational burnout, anxiety disorders, and depression, but they can also cause physical injury or illness such as cardiovascular disease or musculoskeletal injury.  Many hazards of AI are psychosocial in nature due to its potential to cause changes in work organization, in terms of increasing complexity and interaction between different organizational factors.  However, psychosocial risks are often overlooked by designers of advanced manufacturing systems.\nEinola and Khoreva explore how different organizational groups perceive and interact with AI technologies. Their research shows that successful AI integration depends on human ownership and contextual understanding. They caution against blind technological optimism and stress the importance of tailoring AI use to specific workplace ecosystems. This perspective reinforces the need for inclusive design and transparent implementation strategies.\n\nChanges in work practices\nAI is expected to lead to changes in the skills required of workers, requiring training of existing workers, flexibility, and openness to change.  The requirement for combining conventional expertise with computer skills may be challenging for existing workers.  Over-reliance on AI tools may lead to deskilling of some professions.\nWhile AI offers convenience and judgement-free interaction, increased reliance—particularly among Generation Z—may reduce interpersonal communication in the workplace and affect social cohesion. As AI becomes a substitute for traditional peer collaboration and mentorship, there is a risk of diminishing opportunities for interpersonal skill development and team-based learning. This shift could contribute to workplace isolation and changes in team dynamics.\nIncreased monitoring may lead to micromanagement and thus to stress and anxiety.  A perception of surveillance may also lead to stress.  Controls for these include consultation with worker groups, extensive testing, and attention to introduced bias.  Wearable sensors, activity trackers, and augmented reality may also lead to stress from micromanagement, both for assembly line workers and"}
{"doc_id": "Workplace impact of artificial intelligence", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " and thus to stress and anxiety.  A perception of surveillance may also lead to stress.  Controls for these include consultation with worker groups, extensive testing, and attention to introduced bias.  Wearable sensors, activity trackers, and augmented reality may also lead to stress from micromanagement, both for assembly line workers and gig workers.  Gig workers also lack the legal protections and rights of formal workers.\nAI is not merely a technical tool but a transformative force that reshapes workplace structures and decision-making processes. Newell and Marabelli argue that AI alters power dynamics and employee autonomy, requiring a more nuanced understanding of its social and organizational implications. Their study calls for thoughtful integration of AI that considers its broader impact on work culture and human roles.\nThere is also the risk of people being forced to work at a robot's pace, or to monitor robot performance at nonstandard hours.\n\nBias\nAlgorithms trained on past decisions may mimic undesirable human biases, for example, past discriminatory hiring and firing practices.  Information asymmetry between management and workers may lead to stress, if workers do not have access to the data or algorithms that are the basis for decision-making.\nIn addition to building a model with inadvertently discriminatory features, intentional discrimination may occur through designing metrics that covertly result in discrimination through correlated variables in a non-obvious way.\nIn complex human‐machine interactions, some approaches to accident analysis may be biased to safeguard a technological system and its developers by assigning blame to the individual human operator instead.\n\nPhysical\nPhysical hazards in the form of human–robot collisions may arise from robots using AI, especially collaborative robots (cobots).  Cobots are intended to operate in close proximity to humans, which makes impossible the common hazard control of isolating the robot using fences or other barriers, which is widely used for traditional industrial robots.  Automated guided vehicles are a type of cobot that as of 2019 are in common use, often as forklifts or pallet jacks in warehouses or factories.  For cobots, sensor malfunctions or unexpected work environment conditions can lead to unpredictable robot behavior and thus to human–robot collisions.\nSelf-driving cars are another example of AI-enabled robots.  In addition, the ergonomics of control interfaces and human–machine interactions may give rise to hazards.\n\nHazard controls\nAI, in common with other computational technologies, requires cybersecurity measures to stop software breaches and intrusions, as well as information privacy measures.  Communication and transparency with workers about data usage is a control for psychosocial hazards arising from"}
{"doc_id": "Workplace impact of artificial intelligence", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " of control interfaces and human–machine interactions may give rise to hazards.\n\nHazard controls\nAI, in common with other computational technologies, requires cybersecurity measures to stop software breaches and intrusions, as well as information privacy measures.  Communication and transparency with workers about data usage is a control for psychosocial hazards arising from security and privacy issues. Proposed best practices for employer‐sponsored worker monitoring programs include using only validated sensor technologies; ensuring voluntary worker participation; ceasing data collection outside the workplace; disclosing all data uses; and ensuring secure data storage.\nFor industrial cobots equipped with AI‐enabled sensors, the International Organization for Standardization (ISO) recommended: (a) safety‐related monitored stopping controls; (b) human hand guiding of the cobot; (c) speed and separation monitoring controls; and (d) power and force limitations.  Networked AI-enabled cobots may share safety improvements with each other. Human oversight is another general hazard control for AI.\n\nRisk management\nBoth applications and hazards arising from AI can be considered as part of existing frameworks for occupational health and safety risk management.  As with all hazards, risk identification is most effective and least costly when done in the design phase.\nWorkplace health surveillance, the collection and analysis of health data on workers, is challenging for AI because labor data are often reported in aggregate and does not provide breakdowns between different types of work, and is focused on economic data such as wages and employment rates rather than skill content of jobs.  Proxies for skill content include educational requirements and classifications of routine versus non-routine, and cognitive versus physical jobs.  However, these may still not be specific enough to distinguish specific occupations that have distinct impacts from AI.  The United States Department of Labor's Occupational Information Network is an example of a database with a detailed taxonomy of skills.  Additionally, data are often reported on a national level, while there is much geographical variation, especially between urban and rural areas.\nAI systems in the workplace raise ethical concerns related to privacy, fairness, human dignity, and transparency. According to the OECD, these risks must be addressed through robust governance frameworks and accountability mechanisms. Ethical deployment of AI requires clear policies on data usage, explainability of algorithms, and safeguards against discrimination and surveillance.\n\nStandards and regulation\nAs of 2019, ISO was developing a standard on the use of metrics and dashboards, information displays presenting company metrics for managers, in workplaces.  The standard is planned to include guidelines for both gathering data and displaying it in a"}
{"doc_id": "Workplace impact of artificial intelligence", "chunk_id": 6, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "ability of algorithms, and safeguards against discrimination and surveillance.\n\nStandards and regulation\nAs of 2019, ISO was developing a standard on the use of metrics and dashboards, information displays presenting company metrics for managers, in workplaces.  The standard is planned to include guidelines for both gathering data and displaying it in a viewable and useful manner.\nIn the European Union, the General Data Protection Regulation, while oriented towards consumer data, is also relevant for workplace data collection.  Data subjects, including workers, have \"the right not to be subject to a decision based solely on automated processing\".  Other relevant EU directives include the Machinery Directive (2006/42/EC), the Radio Equipment Directive (2014/53/EU), and the General Product Safety Directive (2001/95/EC).\nThe National Conference of State Legislatures (NCSL) highlights how U.S. federal and state governments are responding to AI’s growing role in employment. Legislative efforts focus on regulating employee surveillance, mitigating bias in hiring and performance evaluations, and addressing job displacement. The report also discusses initiatives to upskill workers and promote equitable AI adoption.\n\nSee also\nEnvironmental impact of artificial intelligence\nEmployee monitoring\nComputer surveillance in the workplace"}
{"doc_id": "Zeuthen strategy", "chunk_id": 0, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "The Zeuthen strategy in cognitive science is a negotiation strategy used by some artificial agents. Its purpose is to measure the willingness to risk conflict. An agent will be more willing to risk conflict if it does not have much to lose in case that the negotiation fails. In contrast, an agent is less willing to risk conflict when it has more to lose. The value of a deal is expressed in its utility. An agent has much to lose when the difference between the utility of its current proposal and the conflict deal is high.\nWhen both agents use the monotonic concession protocol, the Zeuthen strategy leads them to agree upon a deal in the negotiation set. This set consists of all conflict free deals, which are individually rational and Pareto optimal, and the conflict deal, which maximizes the Nash product.\nThe strategy was introduced in 1930 by the Danish economist Frederik Zeuthen.\n\nThree key questions\nThe Zeuthen strategy answers three open questions that arise when using the monotonic concession protocol, namely:\n\nWhich deal should be proposed at first?\nOn any given round, who should concede?\nIn case of a concession, how much should the agent concede?\nThe answer to the first question is that any agent should start with its most preferred deal, because that deal has the highest utility for that agent. The second answer is that the agent with the smallest value of Risk(i,t) concedes, because the agent with the lowest utility for the conflict deal profits most from avoiding conflict. To the third question, the Zeuthen strategy suggests that the conceding agent should concede just enough raise its value of Risk(i,t) just above that of the other agent. This prevents the conceding agent to have to concede again in the next round.\n\nRisk\nRisk\n        \n        (\n        i\n        ,\n        t\n        )\n        =\n        \n          \n            {\n            \n              \n                \n                  1\n                \n                \n                  \n                    U\n                    \n                      i\n                    \n                  \n                  (\n                  δ\n                  (\n                  i\n                  ,\n                  t\n                  )\n                  )\n                  =\n                  0\n                \n              \n              \n                \n                  \n                    \n                      \n                        \n                          U\n                          \n                            i\n                          \n                        \n                        (\n                        δ\n                        (\n                        i\n                        ,\n                        t\n                        )\n                        )\n                        −\n                        \n                          U\n                          \n                            i\n                          \n                        \n                        (\n                        δ\n                        (\n                        j\n                        ,\n                        t\n                        )\n                        )\n                      \n                      \n                        \n                          U\n                          \n                            i\n                          \n                        \n                        (\n                        δ\n                        (\n                        i\n                        ,\n                        t\n                        )\n                       "}
{"doc_id": "Zeuthen strategy", "chunk_id": 1, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "                        )\n                        −\n                        \n                          U\n                          \n                            i\n                          \n                        \n                        (\n                        δ\n                        (\n                        j\n                        ,\n                        t\n                        )\n                        )\n                      \n                      \n                        \n                          U\n                          \n                            i\n                          \n                        \n                        (\n                        δ\n                        (\n                        i\n                        ,\n                        t\n                        )\n                        )\n                      \n                    \n                  \n                \n                \n                  \n                    otherwise\n                  \n                \n              \n            \n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{Risk}}(i,t)={\\begin{cases}1&U_{i}(\\delta (i,t))=0\\\\{\\frac {U_{i}(\\delta (i,t))-U_{i}(\\delta (j,t))}{U_{i}(\\delta (i,t))}}&{\\text{otherwise}}\\end{cases}}}\n  \n\nRisk(i,t) is a measurement of agent i's willingness to risk conflict. The risk function formalizes the notion that an agent's willingness to risk conflict is the ratio of the utility that agent would lose by accepting the other agent's proposal to the utility that agent would lose by causing a conflict. Agent i is said to be using a rational negotiation strategy if at any step t + 1 that agent i sticks to his last proposal, Risk(i,t) > Risk(j,t).\n\nSufficient concession\nIf agent i makes a sufficient concession in the next step, then, assuming that agent j is using a rational negotiation strategy, if agent j does not concede in the next step, he must do so in the step after that. The set of all sufficient concessions of agent i at step t is denoted SC(i, t).\n\nMinimal sufficient concession\n\n  \n    \n      \n        \n          δ\n          ′\n        \n        =\n        arg\n        ⁡\n        \n          max\n          \n            δ\n            ∈\n            \n              S\n              C\n              (\n              A\n              ,\n              t\n              )\n            \n          \n        \n        {\n        \n          U\n          \n            A\n          \n        \n        (\n        δ\n        )\n        }\n      \n    \n    {\\displaystyle \\delta '=\\arg \\max _{\\delta \\in {SC(A,t)}}\\{U_{A}(\\delta )\\}}\n  \n\nis the minimal sufficient concession of agent A in step t.\nAgent A begins the negotiation by proposing\n\n  \n    \n      \n        δ\n        (\n        A\n        ,\n        0\n        )\n        =\n        arg\n        ⁡\n        \n          max\n          \n            δ\n            ∈\n            \n              N\n              S\n            \n          \n        \n        \n         "}
{"doc_id": "Zeuthen strategy", "chunk_id": 2, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " concession of agent A in step t.\nAgent A begins the negotiation by proposing\n\n  \n    \n      \n        δ\n        (\n        A\n        ,\n        0\n        )\n        =\n        arg\n        ⁡\n        \n          max\n          \n            δ\n            ∈\n            \n              N\n              S\n            \n          \n        \n        \n          U\n          \n            A\n          \n        \n        (\n        δ\n        )\n      \n    \n    {\\displaystyle \\delta (A,0)=\\arg \\max _{\\delta \\in {NS}}U_{A}(\\delta )}\n  \n\nand will make the minimal sufficient concession in step t + 1 if and only if Risk(A,t) ≤ Risk(B,t).\nTheorem\nIf both agents are using Zeuthen strategies, then they will agree on\n\n  \n    \n      \n        δ\n        =\n        arg\n        ⁡\n        \n          max\n          \n            \n              δ\n              ′\n            \n            ∈\n            \n              N\n              S\n            \n          \n        \n        {\n        π\n        (\n        \n          δ\n          ′\n        \n        )\n        }\n        ,\n      \n    \n    {\\displaystyle \\delta =\\arg \\max _{\\delta '\\in {NS}}\\{\\pi (\\delta ')\\},}\n  \n\nthat is, the deal which maximizes the Nash product.\nProof\nLet δA = δ(A,t).\nLet δB = δ(B,t).\nAccording to the Zeuthen strategy, agent A will concede at step \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  \n if and only if\n\n  \n    \n      \n        R\n        i\n        s\n        k\n        (\n        A\n        ,\n        t\n        )\n        ≤\n        R\n        i\n        s\n        k\n        (\n        B\n        ,\n        t\n        )\n        .\n      \n    \n    {\\displaystyle Risk(A,t)\\leq Risk(B,t).}\n  \n\nThat is, if and only if\n\n  \n    \n      \n        \n          \n            \n              \n                U\n                \n                  A\n                \n              \n              (\n              \n                δ\n                \n                  A\n                \n              \n              )\n              −\n              \n                U\n                \n                  A\n                \n              \n              (\n              \n                δ\n                \n                  B\n                \n              \n              )\n            \n            \n              \n                U\n                \n                  A\n                \n              \n              (\n              \n                δ\n                \n                  A\n                \n              \n              )\n            \n          \n        \n        ≤\n        \n          \n            \n              \n                U\n                \n                  B\n                \n              \n              (\n              \n                δ\n                \n                  B\n                \n              \n              )\n              −\n              \n                U\n                \n                  B\n                \n              \n              (\n              \n                δ\n                \n                  A\n                \n"}
{"doc_id": "Zeuthen strategy", "chunk_id": 3, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " A\n                \n              \n              (\n              \n                δ\n                \n                  A\n                \n              \n              )\n            \n          \n        \n        ≤\n        \n          \n            \n              \n                U\n                \n                  B\n                \n              \n              (\n              \n                δ\n                \n                  B\n                \n              \n              )\n              −\n              \n                U\n                \n                  B\n                \n              \n              (\n              \n                δ\n                \n                  A\n                \n              \n              )\n            \n            \n              \n                U\n                \n                  B\n                \n              \n              (\n              \n                δ\n                \n                  B\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {U_{A}(\\delta _{A})-U_{A}(\\delta _{B})}{U_{A}(\\delta _{A})}}\\leq {\\frac {U_{B}(\\delta _{B})-U_{B}(\\delta _{A})}{U_{B}(\\delta _{B})}}}\n  \n\n  \n    \n      \n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        (\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        )\n        ≤\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        (\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        −\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        )\n      \n    \n    {\\displaystyle U_{B}(\\delta _{B})(U_{A}(\\delta _{A})-U_{A}(\\delta _{B}))\\leq U_{A}(\\delta _{A})(U_{B}(\\delta _{B})-U_{B}(\\delta _{A}))}\n  \n\n  \n    \n      \n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n       "}
{"doc_id": "Zeuthen strategy", "chunk_id": 4, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": "        \n          δ\n          \n            B\n          \n        \n        )\n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        ≤\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle U_{A}(\\delta _{A})U_{B}(\\delta _{B})-U_{A}(\\delta _{B})U_{B}(\\delta _{B})\\leq U_{A}(\\delta _{A})U_{B}(\\delta _{B})-U_{A}(\\delta _{A})U_{B}(\\delta _{A})}\n  \n\n  \n    \n      \n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        ≤\n        −\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n      \n    \n    {\\displaystyle -U_{A}(\\delta _{B})U_{B}(\\delta _{B})\\leq -U_{A}(\\delta _{A})U_{B}(\\delta _{A})}\n  \n\n  \n    \n      \n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        ≤\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B"}
{"doc_id": "Zeuthen strategy", "chunk_id": 5, "source": "wikipedia", "chunk_size": 512, "overlap": 64, "text": " B\n          \n        \n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        ≤\n        \n          U\n          \n            A\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n        \n          U\n          \n            B\n          \n        \n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n      \n    \n    {\\displaystyle U_{A}(\\delta _{A})U_{B}(\\delta _{A})\\leq U_{A}(\\delta _{B})U_{B}(\\delta _{B})}\n  \n\n  \n    \n      \n        π\n        (\n        \n          δ\n          \n            A\n          \n        \n        )\n        ≤\n        π\n        (\n        \n          δ\n          \n            B\n          \n        \n        )\n      \n    \n    {\\displaystyle \\pi (\\delta _{A})\\leq \\pi (\\delta _{B})}\n  \n\nThus, Agent A will concede if and only if \n  \n    \n      \n        \n          δ\n          \n            A\n          \n        \n      \n    \n    {\\displaystyle \\delta _{A}}\n  \n does not yield the larger product of utilities.\nTherefore, the Zeuthen strategy guarantees a final agreement that maximizes the Nash Product."}
